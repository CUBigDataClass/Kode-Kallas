[
    {
        "sha": "9dc9f4eac31a465a630f88b566f622da04e6f958",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjlkYzlmNGVhYzMxYTQ2NWE2MzBmODhiNTY2ZjYyMmRhMDRlNmY5NTg=",
        "commit": {
            "author": {
                "name": "Shreyas Gopalakrishna",
                "email": "11889130+shreyas-gopalakrishna@users.noreply.github.com",
                "date": "2020-04-18T19:27:05Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-18T19:27:05Z"
            },
            "message": "Delete setup.py",
            "tree": {
                "sha": "169394be88ef27d0e19f942555f1e7484c0584fd",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/169394be88ef27d0e19f942555f1e7484c0584fd"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/9dc9f4eac31a465a630f88b566f622da04e6f958",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJem1SJCRBK7hj4Ov3rIwAAdHIIACh0gzy4a2uXI+XFscF5Az+K\nddN+HopxLxAisA+LPkcgE+EG7UaNz0nK2V2+yAiWgtDyWO6cJlLEUakX/ROGN2J8\nIBfMl5XGC4R67lgkhlGD35ey0YnHk5KB9/QpduytBhj9Kr8zQUy0yuJ3cub+8XqT\nstkKSmDTuQq2mpDiOhn5nK7DAr35ZJPKrfHosQAC2Vu+lX/R8ZSVXq2BdI7UZnSr\nNQ25EuHH85dBS6UwBr2qlKOuCcGjJJC18JxHt2j1FhwscMJkAg6OJ83wdjgzt6Gu\nCDQLBrllcfkR+LxvUgQkXpvgtmeY7f42x5jeoyCTO5WXvFiNR7ndAeQVRUizhlc=\n=kOAi\n-----END PGP SIGNATURE-----\n",
                "payload": "tree 169394be88ef27d0e19f942555f1e7484c0584fd\nparent 01cf44a28076ed9b24ed063f0f218399c350abc8\nauthor Shreyas Gopalakrishna <11889130+shreyas-gopalakrishna@users.noreply.github.com> 1587238025 -0600\ncommitter GitHub <noreply@github.com> 1587238025 -0600\n\nDelete setup.py"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/9dc9f4eac31a465a630f88b566f622da04e6f958",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/9dc9f4eac31a465a630f88b566f622da04e6f958",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/9dc9f4eac31a465a630f88b566f622da04e6f958/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "01cf44a28076ed9b24ed063f0f218399c350abc8",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/01cf44a28076ed9b24ed063f0f218399c350abc8",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/01cf44a28076ed9b24ed063f0f218399c350abc8"
            }
        ],
        "stats": {
            "total": 66,
            "additions": 0,
            "deletions": 66
        },
        "files": [
            {
                "sha": "6a40f5332707f1a1197f898fd97b172438acab9d",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/setup.py",
                "status": "removed",
                "additions": 0,
                "deletions": 66,
                "changes": 66,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/01cf44a28076ed9b24ed063f0f218399c350abc8/github-analytics/libs/elasticsearch7-7.6.0/setup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/01cf44a28076ed9b24ed063f0f218399c350abc8/github-analytics/libs/elasticsearch7-7.6.0/setup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/setup.py?ref=01cf44a28076ed9b24ed063f0f218399c350abc8",
                "patch": "@@ -1,66 +0,0 @@\n-# -*- coding: utf-8 -*-\n-from os.path import join, dirname\n-from setuptools import setup, find_packages\n-import sys\n-\n-VERSION = (7, 6, 0)\n-__version__ = VERSION\n-__versionstr__ = \"7.6.0\"\n-\n-with open(join(dirname(__file__), \"README\")) as f:\n-    long_description = f.read().strip()\n-\n-install_requires = [\"urllib3>=1.21.1\"]\n-tests_require = [\n-    \"requests>=2.0.0, <3.0.0\",\n-    \"nose\",\n-    \"coverage\",\n-    \"mock\",\n-    \"pyyaml\",\n-    \"nosexcover\",\n-]\n-\n-docs_require = [\"sphinx<1.7\", \"sphinx_rtd_theme\"]\n-generate_require = [\"black\", \"jinja2\"]\n-\n-setup(\n-    name=\"elasticsearch7\",\n-    description=\"Python client for Elasticsearch\",\n-    license=\"Apache-2.0\",\n-    url=\"https://github.com/elastic/elasticsearch-py\",\n-    download_url=\"https://github.com/shdkpr2008/elasticsearch-py/archive/7.6.0.tar.gz\",\n-    long_description=long_description,\n-    long_description_content_type=\"text/x-rst\",\n-    version=__versionstr__,\n-    author=\"Honza Kr\u00e1l, Nick Lang\",\n-    author_email=\"honza.kral@gmail.com, nick@nicklang.com\",\n-    maintainer=\"Seth Michael Larson\",\n-    maintainer_email=\"seth.larson@elastic.co\",\n-    packages=find_packages(where=\".\", exclude=(\"test_elasticsearch*\",)),\n-    classifiers=[\n-        \"Development Status :: 5 - Production/Stable\",\n-        \"License :: OSI Approved :: Apache Software License\",\n-        \"Intended Audience :: Developers\",\n-        \"Operating System :: OS Independent\",\n-        \"Programming Language :: Python\",\n-        \"Programming Language :: Python :: 2\",\n-        \"Programming Language :: Python :: 2.7\",\n-        \"Programming Language :: Python :: 3\",\n-        \"Programming Language :: Python :: 3.4\",\n-        \"Programming Language :: Python :: 3.5\",\n-        \"Programming Language :: Python :: 3.6\",\n-        \"Programming Language :: Python :: 3.7\",\n-        \"Programming Language :: Python :: 3.8\",\n-        \"Programming Language :: Python :: Implementation :: CPython\",\n-        \"Programming Language :: Python :: Implementation :: PyPy\",\n-    ],\n-    python_requires=\">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, <4\",\n-    install_requires=install_requires,\n-    test_suite=\"test_elasticsearch7.run_tests.run_all\",\n-    tests_require=tests_require,\n-    extras_require={\n-        \"develop\": tests_require + docs_require + generate_require,\n-        \"docs\": docs_require,\n-        \"requests\": [\"requests>=2.4.0, <3.0.0\"],\n-    },\n-)"
            }
        ]
    },
    {
        "sha": "01cf44a28076ed9b24ed063f0f218399c350abc8",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjAxY2Y0NGEyODA3NmVkOWIyNGVkMDYzZjBmMjE4Mzk5YzM1MGFiYzg=",
        "commit": {
            "author": {
                "name": "Shreyas Gopalakrishna",
                "email": "11889130+shreyas-gopalakrishna@users.noreply.github.com",
                "date": "2020-04-18T19:26:53Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-18T19:26:53Z"
            },
            "message": "Delete setup.py",
            "tree": {
                "sha": "e6548c8e0dc00962b059c19daa72033bc1f90ffc",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/e6548c8e0dc00962b059c19daa72033bc1f90ffc"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/01cf44a28076ed9b24ed063f0f218399c350abc8",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJem1R9CRBK7hj4Ov3rIwAAdHIIAEMGwk2aHGcgXmR26HC17gMF\nwUCkc23orHRbR83B5iShMoWUKA35puKP3cIibAKUG7drBFvNSr8Ndmldd7EnO4li\ncXt/S+n8JTQoCUtDhdLIKgoW2mBQdpVT2Ok/3ZSCkf95geldyoI2c1qlaeRgtAE2\nhkaDVpCl+Y3oDZYrD+m9oyc7pb5EbpxxAl111w3ifqhWkzXHJF/LfpGw8YFKpeIg\nVPHKWg0hFFH3Gok9pDDk0uwaiwqhBFiTiFPbQyXPVnrFvEhY8QzE4La2Ti0QRGKF\ntzyNrc8+p+d7zXbf3s/qwk62cY4urShq8r3vhMSzbKGcPK4DYZW3K6nX/8n0tXk=\n=y4iA\n-----END PGP SIGNATURE-----\n",
                "payload": "tree e6548c8e0dc00962b059c19daa72033bc1f90ffc\nparent c01947a1e729d91b68f9323166e3e7ab88d30486\nauthor Shreyas Gopalakrishna <11889130+shreyas-gopalakrishna@users.noreply.github.com> 1587238013 -0600\ncommitter GitHub <noreply@github.com> 1587238013 -0600\n\nDelete setup.py"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/01cf44a28076ed9b24ed063f0f218399c350abc8",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/01cf44a28076ed9b24ed063f0f218399c350abc8",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/01cf44a28076ed9b24ed063f0f218399c350abc8/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "c01947a1e729d91b68f9323166e3e7ab88d30486",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c01947a1e729d91b68f9323166e3e7ab88d30486",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c01947a1e729d91b68f9323166e3e7ab88d30486"
            }
        ],
        "stats": {
            "total": 39,
            "additions": 0,
            "deletions": 39
        },
        "files": [
            {
                "sha": "9538d34fc73f63d9f53c4a34c231a99ca41579d4",
                "filename": "github-analytics/libs/Flask-Cassandra-0.14/setup.py",
                "status": "removed",
                "additions": 0,
                "deletions": 39,
                "changes": 39,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c01947a1e729d91b68f9323166e3e7ab88d30486/github-analytics/libs/Flask-Cassandra-0.14/setup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c01947a1e729d91b68f9323166e3e7ab88d30486/github-analytics/libs/Flask-Cassandra-0.14/setup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/Flask-Cassandra-0.14/setup.py?ref=c01947a1e729d91b68f9323166e3e7ab88d30486",
                "patch": "@@ -1,39 +0,0 @@\n-\"\"\"\n-Flask-Cassandra\n--------------\n-\n-Flask-Cassandra provides an application-level connection\n-to an Apache Cassandra database. This connection can be\n-used to interact with a Cassandra cluster.\n-\n-\"\"\"\n-from setuptools import setup\n-\n-\n-setup(\n-    name='Flask-Cassandra',\n-    version='0.14',\n-    url='http://terbiumlabs.com/flask-cassandra/',\n-    license='BSD',\n-    author='Michael Moore',\n-    author_email='michael@terbiumlabs.com',\n-    description='Provides a connection to a Cassandra cluster in a Flask app',\n-    long_description=__doc__,\n-    py_modules=['flask_cassandra'],\n-    zip_safe=False,\n-    include_package_data=True,\n-    platforms='any',\n-    install_requires=[\n-        'Flask',\n-        'cassandra-driver'\n-    ],\n-    classifiers=[\n-        'Environment :: Web Environment',\n-        'Intended Audience :: Developers',\n-        'License :: OSI Approved :: BSD License',\n-        'Operating System :: OS Independent',\n-        'Programming Language :: Python',\n-        'Topic :: Internet :: WWW/HTTP :: Dynamic Content',\n-        'Topic :: Software Development :: Libraries :: Python Modules'\n-    ]\n-)"
            }
        ]
    },
    {
        "sha": "c01947a1e729d91b68f9323166e3e7ab88d30486",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmMwMTk0N2ExZTcyOWQ5MWI2OGY5MzIzMTY2ZTNlN2FiODhkMzA0ODY=",
        "commit": {
            "author": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-18T19:22:29Z"
            },
            "committer": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-18T19:22:29Z"
            },
            "message": "Updated missing thread count information",
            "tree": {
                "sha": "5269d30c5c90b9eb089ba25be325a9d38ffccee8",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/5269d30c5c90b9eb089ba25be325a9d38ffccee8"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/c01947a1e729d91b68f9323166e3e7ab88d30486",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c01947a1e729d91b68f9323166e3e7ab88d30486",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c01947a1e729d91b68f9323166e3e7ab88d30486",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c01947a1e729d91b68f9323166e3e7ab88d30486/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd"
            }
        ],
        "stats": {
            "total": 2999,
            "additions": 6,
            "deletions": 2993
        },
        "files": [
            {
                "sha": "913f90e60848efef45a502ff9e1bf34c11ee9152",
                "filename": "github-analytics/CassandraHelper/config.py",
                "status": "modified",
                "additions": 6,
                "deletions": 1,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c01947a1e729d91b68f9323166e3e7ab88d30486/github-analytics/CassandraHelper/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c01947a1e729d91b68f9323166e3e7ab88d30486/github-analytics/CassandraHelper/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/config.py?ref=c01947a1e729d91b68f9323166e3e7ab88d30486",
                "patch": "@@ -8,4 +8,9 @@\n \n # Github Config\n GITHUB_USER = \"shreyas-gopalakrishna\"\n-GITHUB_TOKEN = \"f02831928492a6bfe9f1bca49693f69d1d0e44b3\"\n\\ No newline at end of file\n+GITHUB_TOKEN = \"f02831928492a6bfe9f1bca49693f69d1d0e44b3\"\n+\n+\n+# Others\n+\n+THREAD_COUNT = 10\n\\ No newline at end of file"
            },
            {
                "sha": "f53fa24e39ce5b4abc2f3182231f90efbfec62a2",
                "filename": "github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoup.py",
                "status": "removed",
                "additions": 0,
                "deletions": 2032,
                "changes": 2032,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoup.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -1,2032 +0,0 @@\n-\"\"\"Beautiful Soup\n-Elixir and Tonic\n-\"The Screen-Scraper's Friend\"\n-http://www.crummy.com/software/BeautifulSoup/\n-\n-Beautiful Soup parses a (possibly invalid) XML or HTML document into a\n-tree representation. It provides methods and Pythonic idioms that make\n-it easy to navigate, search, and modify the tree.\n-\n-A well-formed XML/HTML document yields a well-formed data\n-structure. An ill-formed XML/HTML document yields a correspondingly\n-ill-formed data structure. If your document is only locally\n-well-formed, you can use this library to find and process the\n-well-formed part of it.\n-\n-Beautiful Soup works with Python 2.2 and up. It has no external\n-dependencies, but you'll have more success at converting data to UTF-8\n-if you also install these three packages:\n-\n-* chardet, for auto-detecting character encodings\n-  http://chardet.feedparser.org/\n-* cjkcodecs and iconv_codec, which add more encodings to the ones supported\n-  by stock Python.\n-  http://cjkpython.i18n.org/\n-\n-Beautiful Soup defines classes for two main parsing strategies:\n-\n- * BeautifulStoneSoup, for parsing XML, SGML, or your domain-specific\n-   language that kind of looks like XML.\n-\n- * BeautifulSoup, for parsing run-of-the-mill HTML code, be it valid\n-   or invalid. This class has web browser-like heuristics for\n-   obtaining a sensible parse tree in the face of common HTML errors.\n-\n-Beautiful Soup also defines a class (UnicodeDammit) for autodetecting\n-the encoding of an HTML or XML document, and converting it to\n-Unicode. Much of this code is taken from Mark Pilgrim's Universal Feed Parser.\n-\n-For more than you ever wanted to know about Beautiful Soup, see the\n-documentation:\n-http://www.crummy.com/software/BeautifulSoup/documentation.html\n-\n-Here, have some legalese:\n-\n-Copyright (c) 2004-2019, Leonard Richardson\n-\n-All rights reserved.\n-\n-Redistribution and use in source and binary forms, with or without\n-modification, are permitted provided that the following conditions are\n-met:\n-\n-  * Redistributions of source code must retain the above copyright\n-    notice, this list of conditions and the following disclaimer.\n-\n-  * Redistributions in binary form must reproduce the above\n-    copyright notice, this list of conditions and the following\n-    disclaimer in the documentation and/or other materials provided\n-    with the distribution.\n-\n-  * Neither the name of the the Beautiful Soup Consortium and All\n-    Night Kosher Bakery nor the names of its contributors may be\n-    used to endorse or promote products derived from this software\n-    without specific prior written permission.\n-\n-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n-\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n-LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n-A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n-CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n-EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n-PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n-PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n-LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n-NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n-SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE, DAMMIT.\n-\n-\"\"\"\n-from __future__ import generators\n-\n-__author__ = \"Leonard Richardson (leonardr@segfault.org)\"\n-__version__ = \"3.2.2\"\n-__copyright__ = \"Copyright (c) 2004-2019 Leonard Richardson\"\n-__license__ = \"New-style BSD\"\n-\n-from sgmllib import SGMLParser, SGMLParseError\n-import codecs\n-import markupbase\n-import types\n-import re\n-import sgmllib\n-try:\n-  from htmlentitydefs import name2codepoint\n-except ImportError:\n-  name2codepoint = {}\n-try:\n-    set\n-except NameError:\n-    from sets import Set as set\n-\n-# The very first thing we do is give a useful error if someone is\n-# running this code under Python 3.\n-\"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"<>\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n-\n-# Everyone else gets a deprecation warning.\n-import warnings\n-warnings.warn(\"\"\"You are using a very old release of Beautiful Soup, last updated in 2011. If you installed the 'beautifulsoup' package through pip, you should know the 'beautifulsoup' package name is about to be reclaimed by a more recent version of Beautiful Soup which is incompatible with this version.\n-\n-This will happen at some point after January 1, 2021.\n-\n-If you just started this project, this is easy to fix. Install the 'beautifulsoup4' package instead of 'beautifulsoup' and start using Beautiful Soup 4.\n-\n-If this is an existing project that depends on Beautiful Soup 3, the project maintainer (potentially you) needs to start the process of migrating to Beautiful Soup 4. This should be a relatively easy part of the Python 3 migration.\n-\"\"\")\n-\n-#These hacks make Beautiful Soup able to parse XML with namespaces\n-sgmllib.tagfind = re.compile('[a-zA-Z][-_.:a-zA-Z0-9]*')\n-markupbase._declname_match = re.compile(r'[a-zA-Z][-_.:a-zA-Z0-9]*\\s*').match\n-\n-DEFAULT_OUTPUT_ENCODING = \"utf-8\"\n-\n-def _match_css_class(str):\n-    \"\"\"Build a RE to match the given CSS class.\"\"\"\n-    return re.compile(r\"(^|.*\\s)%s($|\\s)\" % str)\n-\n-# First, the classes that represent markup elements.\n-\n-class PageElement(object):\n-    \"\"\"Contains the navigational information for some part of the page\n-    (either a tag or a piece of text)\"\"\"\n-\n-    def _invert(h):\n-        \"Cheap function to invert a hash.\"\n-        i = {}\n-        for k,v in h.items():\n-            i[v] = k\n-        return i\n-\n-    XML_ENTITIES_TO_SPECIAL_CHARS = { \"apos\" : \"'\",\n-                                      \"quot\" : '\"',\n-                                      \"amp\" : \"&\",\n-                                      \"lt\" : \"<\",\n-                                      \"gt\" : \">\" }\n-\n-    XML_SPECIAL_CHARS_TO_ENTITIES = _invert(XML_ENTITIES_TO_SPECIAL_CHARS)\n-\n-    def setup(self, parent=None, previous=None):\n-        \"\"\"Sets up the initial relations between this element and\n-        other elements.\"\"\"\n-        self.parent = parent\n-        self.previous = previous\n-        self.next = None\n-        self.previousSibling = None\n-        self.nextSibling = None\n-        if self.parent and self.parent.contents:\n-            self.previousSibling = self.parent.contents[-1]\n-            self.previousSibling.nextSibling = self\n-\n-    def replaceWith(self, replaceWith):\n-        oldParent = self.parent\n-        myIndex = self.parent.index(self)\n-        if hasattr(replaceWith, \"parent\")\\\n-                  and replaceWith.parent is self.parent:\n-            # We're replacing this element with one of its siblings.\n-            index = replaceWith.parent.index(replaceWith)\n-            if index and index < myIndex:\n-                # Furthermore, it comes before this element. That\n-                # means that when we extract it, the index of this\n-                # element will change.\n-                myIndex = myIndex - 1\n-        self.extract()\n-        oldParent.insert(myIndex, replaceWith)\n-\n-    def replaceWithChildren(self):\n-        myParent = self.parent\n-        myIndex = self.parent.index(self)\n-        self.extract()\n-        reversedChildren = list(self.contents)\n-        reversedChildren.reverse()\n-        for child in reversedChildren:\n-            myParent.insert(myIndex, child)\n-\n-    def extract(self):\n-        \"\"\"Destructively rips this element out of the tree.\"\"\"\n-        if self.parent:\n-            try:\n-                del self.parent.contents[self.parent.index(self)]\n-            except ValueError:\n-                pass\n-\n-        #Find the two elements that would be next to each other if\n-        #this element (and any children) hadn't been parsed. Connect\n-        #the two.\n-        lastChild = self._lastRecursiveChild()\n-        nextElement = lastChild.next\n-\n-        if self.previous:\n-            self.previous.next = nextElement\n-        if nextElement:\n-            nextElement.previous = self.previous\n-        self.previous = None\n-        lastChild.next = None\n-\n-        self.parent = None\n-        if self.previousSibling:\n-            self.previousSibling.nextSibling = self.nextSibling\n-        if self.nextSibling:\n-            self.nextSibling.previousSibling = self.previousSibling\n-        self.previousSibling = self.nextSibling = None\n-        return self\n-\n-    def _lastRecursiveChild(self):\n-        \"Finds the last element beneath this object to be parsed.\"\n-        lastChild = self\n-        while hasattr(lastChild, 'contents') and lastChild.contents:\n-            lastChild = lastChild.contents[-1]\n-        return lastChild\n-\n-    def insert(self, position, newChild):\n-        if isinstance(newChild, basestring) \\\n-            and not isinstance(newChild, NavigableString):\n-            newChild = NavigableString(newChild)\n-\n-        position =  min(position, len(self.contents))\n-        if hasattr(newChild, 'parent') and newChild.parent is not None:\n-            # We're 'inserting' an element that's already one\n-            # of this object's children.\n-            if newChild.parent is self:\n-                index = self.index(newChild)\n-                if index > position:\n-                    # Furthermore we're moving it further down the\n-                    # list of this object's children. That means that\n-                    # when we extract this element, our target index\n-                    # will jump down one.\n-                    position = position - 1\n-            newChild.extract()\n-\n-        newChild.parent = self\n-        previousChild = None\n-        if position == 0:\n-            newChild.previousSibling = None\n-            newChild.previous = self\n-        else:\n-            previousChild = self.contents[position-1]\n-            newChild.previousSibling = previousChild\n-            newChild.previousSibling.nextSibling = newChild\n-            newChild.previous = previousChild._lastRecursiveChild()\n-        if newChild.previous:\n-            newChild.previous.next = newChild\n-\n-        newChildsLastElement = newChild._lastRecursiveChild()\n-\n-        if position >= len(self.contents):\n-            newChild.nextSibling = None\n-\n-            parent = self\n-            parentsNextSibling = None\n-            while not parentsNextSibling:\n-                parentsNextSibling = parent.nextSibling\n-                parent = parent.parent\n-                if not parent: # This is the last element in the document.\n-                    break\n-            if parentsNextSibling:\n-                newChildsLastElement.next = parentsNextSibling\n-            else:\n-                newChildsLastElement.next = None\n-        else:\n-            nextChild = self.contents[position]\n-            newChild.nextSibling = nextChild\n-            if newChild.nextSibling:\n-                newChild.nextSibling.previousSibling = newChild\n-            newChildsLastElement.next = nextChild\n-\n-        if newChildsLastElement.next:\n-            newChildsLastElement.next.previous = newChildsLastElement\n-        self.contents.insert(position, newChild)\n-\n-    def append(self, tag):\n-        \"\"\"Appends the given tag to the contents of this tag.\"\"\"\n-        self.insert(len(self.contents), tag)\n-\n-    def findNext(self, name=None, attrs={}, text=None, **kwargs):\n-        \"\"\"Returns the first item that matches the given criteria and\n-        appears after this Tag in the document.\"\"\"\n-        return self._findOne(self.findAllNext, name, attrs, text, **kwargs)\n-\n-    def findAllNext(self, name=None, attrs={}, text=None, limit=None,\n-                    **kwargs):\n-        \"\"\"Returns all items that match the given criteria and appear\n-        after this Tag in the document.\"\"\"\n-        return self._findAll(name, attrs, text, limit, self.nextGenerator,\n-                             **kwargs)\n-\n-    def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n-        \"\"\"Returns the closest sibling to this Tag that matches the\n-        given criteria and appears after this Tag in the document.\"\"\"\n-        return self._findOne(self.findNextSiblings, name, attrs, text,\n-                             **kwargs)\n-\n-    def findNextSiblings(self, name=None, attrs={}, text=None, limit=None,\n-                         **kwargs):\n-        \"\"\"Returns the siblings of this Tag that match the given\n-        criteria and appear after this Tag in the document.\"\"\"\n-        return self._findAll(name, attrs, text, limit,\n-                             self.nextSiblingGenerator, **kwargs)\n-    fetchNextSiblings = findNextSiblings # Compatibility with pre-3.x\n-\n-    def findPrevious(self, name=None, attrs={}, text=None, **kwargs):\n-        \"\"\"Returns the first item that matches the given criteria and\n-        appears before this Tag in the document.\"\"\"\n-        return self._findOne(self.findAllPrevious, name, attrs, text, **kwargs)\n-\n-    def findAllPrevious(self, name=None, attrs={}, text=None, limit=None,\n-                        **kwargs):\n-        \"\"\"Returns all items that match the given criteria and appear\n-        before this Tag in the document.\"\"\"\n-        return self._findAll(name, attrs, text, limit, self.previousGenerator,\n-                           **kwargs)\n-    fetchPrevious = findAllPrevious # Compatibility with pre-3.x\n-\n-    def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):\n-        \"\"\"Returns the closest sibling to this Tag that matches the\n-        given criteria and appears before this Tag in the document.\"\"\"\n-        return self._findOne(self.findPreviousSiblings, name, attrs, text,\n-                             **kwargs)\n-\n-    def findPreviousSiblings(self, name=None, attrs={}, text=None,\n-                             limit=None, **kwargs):\n-        \"\"\"Returns the siblings of this Tag that match the given\n-        criteria and appear before this Tag in the document.\"\"\"\n-        return self._findAll(name, attrs, text, limit,\n-                             self.previousSiblingGenerator, **kwargs)\n-    fetchPreviousSiblings = findPreviousSiblings # Compatibility with pre-3.x\n-\n-    def findParent(self, name=None, attrs={}, **kwargs):\n-        \"\"\"Returns the closest parent of this Tag that matches the given\n-        criteria.\"\"\"\n-        # NOTE: We can't use _findOne because findParents takes a different\n-        # set of arguments.\n-        r = None\n-        l = self.findParents(name, attrs, 1)\n-        if l:\n-            r = l[0]\n-        return r\n-\n-    def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n-        \"\"\"Returns the parents of this Tag that match the given\n-        criteria.\"\"\"\n-\n-        return self._findAll(name, attrs, None, limit, self.parentGenerator,\n-                             **kwargs)\n-    fetchParents = findParents # Compatibility with pre-3.x\n-\n-    #These methods do the real heavy lifting.\n-\n-    def _findOne(self, method, name, attrs, text, **kwargs):\n-        r = None\n-        l = method(name, attrs, text, 1, **kwargs)\n-        if l:\n-            r = l[0]\n-        return r\n-\n-    def _findAll(self, name, attrs, text, limit, generator, **kwargs):\n-        \"Iterates over a generator looking for things that match.\"\n-\n-        if isinstance(name, SoupStrainer):\n-            strainer = name\n-        # (Possibly) special case some findAll*(...) searches\n-        elif text is None and not limit and not attrs and not kwargs:\n-            # findAll*(True)\n-            if name is True:\n-                return [element for element in generator()\n-                        if isinstance(element, Tag)]\n-            # findAll*('tag-name')\n-            elif isinstance(name, basestring):\n-                return [element for element in generator()\n-                        if isinstance(element, Tag) and\n-                        element.name == name]\n-            else:\n-                strainer = SoupStrainer(name, attrs, text, **kwargs)\n-        # Build a SoupStrainer\n-        else:\n-            strainer = SoupStrainer(name, attrs, text, **kwargs)\n-        results = ResultSet(strainer)\n-        g = generator()\n-        while True:\n-            try:\n-                i = g.next()\n-            except StopIteration:\n-                break\n-            if i:\n-                found = strainer.search(i)\n-                if found:\n-                    results.append(found)\n-                    if limit and len(results) >= limit:\n-                        break\n-        return results\n-\n-    #These Generators can be used to navigate starting from both\n-    #NavigableStrings and Tags.\n-    def nextGenerator(self):\n-        i = self\n-        while i is not None:\n-            i = i.next\n-            yield i\n-\n-    def nextSiblingGenerator(self):\n-        i = self\n-        while i is not None:\n-            i = i.nextSibling\n-            yield i\n-\n-    def previousGenerator(self):\n-        i = self\n-        while i is not None:\n-            i = i.previous\n-            yield i\n-\n-    def previousSiblingGenerator(self):\n-        i = self\n-        while i is not None:\n-            i = i.previousSibling\n-            yield i\n-\n-    def parentGenerator(self):\n-        i = self\n-        while i is not None:\n-            i = i.parent\n-            yield i\n-\n-    # Utility methods\n-    def substituteEncoding(self, str, encoding=None):\n-        encoding = encoding or \"utf-8\"\n-        return str.replace(\"%SOUP-ENCODING%\", encoding)\n-\n-    def toEncoding(self, s, encoding=None):\n-        \"\"\"Encodes an object to a string in some encoding, or to Unicode.\n-        .\"\"\"\n-        if isinstance(s, unicode):\n-            if encoding:\n-                s = s.encode(encoding)\n-        elif isinstance(s, str):\n-            if encoding:\n-                s = s.encode(encoding)\n-            else:\n-                s = unicode(s)\n-        else:\n-            if encoding:\n-                s  = self.toEncoding(str(s), encoding)\n-            else:\n-                s = unicode(s)\n-        return s\n-\n-    BARE_AMPERSAND_OR_BRACKET = re.compile(\"([<>]|\"\n-                                           + \"&(?!#\\d+;|#x[0-9a-fA-F]+;|\\w+;)\"\n-                                           + \")\")\n-\n-    def _sub_entity(self, x):\n-        \"\"\"Used with a regular expression to substitute the\n-        appropriate XML entity for an XML special character.\"\"\"\n-        return \"&\" + self.XML_SPECIAL_CHARS_TO_ENTITIES[x.group(0)[0]] + \";\"\n-\n-\n-class NavigableString(unicode, PageElement):\n-\n-    def __new__(cls, value):\n-        \"\"\"Create a new NavigableString.\n-\n-        When unpickling a NavigableString, this method is called with\n-        the string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be\n-        passed in to the superclass's __new__ or the superclass won't know\n-        how to handle non-ASCII characters.\n-        \"\"\"\n-        if isinstance(value, unicode):\n-            return unicode.__new__(cls, value)\n-        return unicode.__new__(cls, value, DEFAULT_OUTPUT_ENCODING)\n-\n-    def __getnewargs__(self):\n-        return (NavigableString.__str__(self),)\n-\n-    def __getattr__(self, attr):\n-        \"\"\"text.string gives you text. This is for backwards\n-        compatibility for Navigable*String, but for CData* it lets you\n-        get the string without the CData wrapper.\"\"\"\n-        if attr == 'string':\n-            return self\n-        else:\n-            raise AttributeError, \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, attr)\n-\n-    def __unicode__(self):\n-        return str(self).decode(DEFAULT_OUTPUT_ENCODING)\n-\n-    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        # Substitute outgoing XML entities.\n-        data = self.BARE_AMPERSAND_OR_BRACKET.sub(self._sub_entity, self)\n-        if encoding:\n-            return data.encode(encoding)\n-        else:\n-            return data\n-\n-class CData(NavigableString):\n-\n-    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        return \"<![CDATA[%s]]>\" % NavigableString.__str__(self, encoding)\n-\n-class ProcessingInstruction(NavigableString):\n-    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        output = self\n-        if \"%SOUP-ENCODING%\" in output:\n-            output = self.substituteEncoding(output, encoding)\n-        return \"<?%s?>\" % self.toEncoding(output, encoding)\n-\n-class Comment(NavigableString):\n-    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        return \"<!--%s-->\" % NavigableString.__str__(self, encoding)\n-\n-class Declaration(NavigableString):\n-    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        return \"<!%s>\" % NavigableString.__str__(self, encoding)\n-\n-class Tag(PageElement):\n-\n-    \"\"\"Represents a found HTML tag with its attributes and contents.\"\"\"\n-\n-    def _convertEntities(self, match):\n-        \"\"\"Used in a call to re.sub to replace HTML, XML, and numeric\n-        entities with the appropriate Unicode characters. If HTML\n-        entities are being converted, any unrecognized entities are\n-        escaped.\"\"\"\n-        x = match.group(1)\n-        if self.convertHTMLEntities and x in name2codepoint:\n-            return unichr(name2codepoint[x])\n-        elif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:\n-            if self.convertXMLEntities:\n-                return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]\n-            else:\n-                return u'&%s;' % x\n-        elif len(x) > 0 and x[0] == '#':\n-            # Handle numeric entities\n-            if len(x) > 1 and x[1] == 'x':\n-                return unichr(int(x[2:], 16))\n-            else:\n-                return unichr(int(x[1:]))\n-\n-        elif self.escapeUnrecognizedEntities:\n-            return u'&amp;%s;' % x\n-        else:\n-            return u'&%s;' % x\n-\n-    def __init__(self, parser, name, attrs=None, parent=None,\n-                 previous=None):\n-        \"Basic constructor.\"\n-\n-        # We don't actually store the parser object: that lets extracted\n-        # chunks be garbage-collected\n-        self.parserClass = parser.__class__\n-        self.isSelfClosing = parser.isSelfClosingTag(name)\n-        self.name = name\n-        if attrs is None:\n-            attrs = []\n-        elif isinstance(attrs, dict):\n-            attrs = attrs.items()\n-        self.attrs = attrs\n-        self.contents = []\n-        self.setup(parent, previous)\n-        self.hidden = False\n-        self.containsSubstitutions = False\n-        self.convertHTMLEntities = parser.convertHTMLEntities\n-        self.convertXMLEntities = parser.convertXMLEntities\n-        self.escapeUnrecognizedEntities = parser.escapeUnrecognizedEntities\n-\n-        # Convert any HTML, XML, or numeric entities in the attribute values.\n-        convert = lambda(k, val): (k,\n-                                   re.sub(\"&(#\\d+|#x[0-9a-fA-F]+|\\w+);\",\n-                                          self._convertEntities,\n-                                          val))\n-        self.attrs = map(convert, self.attrs)\n-\n-    def getString(self):\n-        if (len(self.contents) == 1\n-            and isinstance(self.contents[0], NavigableString)):\n-            return self.contents[0]\n-\n-    def setString(self, string):\n-        \"\"\"Replace the contents of the tag with a string\"\"\"\n-        self.clear()\n-        self.append(string)\n-\n-    string = property(getString, setString)\n-\n-    def getText(self, separator=u\"\"):\n-        if not len(self.contents):\n-            return u\"\"\n-        stopNode = self._lastRecursiveChild().next\n-        strings = []\n-        current = self.contents[0]\n-        while current is not stopNode:\n-            if isinstance(current, NavigableString):\n-                strings.append(current.strip())\n-            current = current.next\n-        return separator.join(strings)\n-\n-    text = property(getText)\n-\n-    def get(self, key, default=None):\n-        \"\"\"Returns the value of the 'key' attribute for the tag, or\n-        the value given for 'default' if it doesn't have that\n-        attribute.\"\"\"\n-        return self._getAttrMap().get(key, default)\n-\n-    def clear(self):\n-        \"\"\"Extract all children.\"\"\"\n-        for child in self.contents[:]:\n-            child.extract()\n-\n-    def index(self, element):\n-        for i, child in enumerate(self.contents):\n-            if child is element:\n-                return i\n-        raise ValueError(\"Tag.index: element not in tag\")\n-\n-    def has_key(self, key):\n-        return self._getAttrMap().has_key(key)\n-\n-    def __getitem__(self, key):\n-        \"\"\"tag[key] returns the value of the 'key' attribute for the tag,\n-        and throws an exception if it's not there.\"\"\"\n-        return self._getAttrMap()[key]\n-\n-    def __iter__(self):\n-        \"Iterating over a tag iterates over its contents.\"\n-        return iter(self.contents)\n-\n-    def __len__(self):\n-        \"The length of a tag is the length of its list of contents.\"\n-        return len(self.contents)\n-\n-    def __contains__(self, x):\n-        return x in self.contents\n-\n-    def __nonzero__(self):\n-        \"A tag is non-None even if it has no contents.\"\n-        return True\n-\n-    def __setitem__(self, key, value):\n-        \"\"\"Setting tag[key] sets the value of the 'key' attribute for the\n-        tag.\"\"\"\n-        self._getAttrMap()\n-        self.attrMap[key] = value\n-        found = False\n-        for i in range(0, len(self.attrs)):\n-            if self.attrs[i][0] == key:\n-                self.attrs[i] = (key, value)\n-                found = True\n-        if not found:\n-            self.attrs.append((key, value))\n-        self._getAttrMap()[key] = value\n-\n-    def __delitem__(self, key):\n-        \"Deleting tag[key] deletes all 'key' attributes for the tag.\"\n-        for item in self.attrs:\n-            if item[0] == key:\n-                self.attrs.remove(item)\n-                #We don't break because bad HTML can define the same\n-                #attribute multiple times.\n-            self._getAttrMap()\n-            if self.attrMap.has_key(key):\n-                del self.attrMap[key]\n-\n-    def __call__(self, *args, **kwargs):\n-        \"\"\"Calling a tag like a function is the same as calling its\n-        findAll() method. Eg. tag('a') returns a list of all the A tags\n-        found within this tag.\"\"\"\n-        return apply(self.findAll, args, kwargs)\n-\n-    def __getattr__(self, tag):\n-        #print \"Getattr %s.%s\" % (self.__class__, tag)\n-        if len(tag) > 3 and tag.rfind('Tag') == len(tag)-3:\n-            return self.find(tag[:-3])\n-        elif tag.find('__') != 0:\n-            return self.find(tag)\n-        raise AttributeError, \"'%s' object has no attribute '%s'\" % (self.__class__, tag)\n-\n-    def __eq__(self, other):\n-        \"\"\"Returns true iff this tag has the same name, the same attributes,\n-        and the same contents (recursively) as the given tag.\n-\n-        NOTE: right now this will return false if two tags have the\n-        same attributes in a different order. Should this be fixed?\"\"\"\n-        if other is self:\n-            return True\n-        if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n-            return False\n-        for i in range(0, len(self.contents)):\n-            if self.contents[i] != other.contents[i]:\n-                return False\n-        return True\n-\n-    def __ne__(self, other):\n-        \"\"\"Returns true iff this tag is not identical to the other tag,\n-        as defined in __eq__.\"\"\"\n-        return not self == other\n-\n-    def __repr__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        \"\"\"Renders this tag as a string.\"\"\"\n-        return self.__str__(encoding)\n-\n-    def __unicode__(self):\n-        return self.__str__(None)\n-\n-    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING,\n-                prettyPrint=False, indentLevel=0):\n-        \"\"\"Returns a string or Unicode representation of this tag and\n-        its contents. To get Unicode, pass None for encoding.\n-\n-        NOTE: since Python's HTML parser consumes whitespace, this\n-        method is not certain to reproduce the whitespace present in\n-        the original string.\"\"\"\n-\n-        encodedName = self.toEncoding(self.name, encoding)\n-\n-        attrs = []\n-        if self.attrs:\n-            for key, val in self.attrs:\n-                fmt = '%s=\"%s\"'\n-                if isinstance(val, basestring):\n-                    if self.containsSubstitutions and '%SOUP-ENCODING%' in val:\n-                        val = self.substituteEncoding(val, encoding)\n-\n-                    # The attribute value either:\n-                    #\n-                    # * Contains no embedded double quotes or single quotes.\n-                    #   No problem: we enclose it in double quotes.\n-                    # * Contains embedded single quotes. No problem:\n-                    #   double quotes work here too.\n-                    # * Contains embedded double quotes. No problem:\n-                    #   we enclose it in single quotes.\n-                    # * Embeds both single _and_ double quotes. This\n-                    #   can't happen naturally, but it can happen if\n-                    #   you modify an attribute value after parsing\n-                    #   the document. Now we have a bit of a\n-                    #   problem. We solve it by enclosing the\n-                    #   attribute in single quotes, and escaping any\n-                    #   embedded single quotes to XML entities.\n-                    if '\"' in val:\n-                        fmt = \"%s='%s'\"\n-                        if \"'\" in val:\n-                            # TODO: replace with apos when\n-                            # appropriate.\n-                            val = val.replace(\"'\", \"&squot;\")\n-\n-                    # Now we're okay w/r/t quotes. But the attribute\n-                    # value might also contain angle brackets, or\n-                    # ampersands that aren't part of entities. We need\n-                    # to escape those to XML entities too.\n-                    val = self.BARE_AMPERSAND_OR_BRACKET.sub(self._sub_entity, val)\n-\n-                attrs.append(fmt % (self.toEncoding(key, encoding),\n-                                    self.toEncoding(val, encoding)))\n-        close = ''\n-        closeTag = ''\n-        if self.isSelfClosing:\n-            close = ' /'\n-        else:\n-            closeTag = '</%s>' % encodedName\n-\n-        indentTag, indentContents = 0, 0\n-        if prettyPrint:\n-            indentTag = indentLevel\n-            space = (' ' * (indentTag-1))\n-            indentContents = indentTag + 1\n-        contents = self.renderContents(encoding, prettyPrint, indentContents)\n-        if self.hidden:\n-            s = contents\n-        else:\n-            s = []\n-            attributeString = ''\n-            if attrs:\n-                attributeString = ' ' + ' '.join(attrs)\n-            if prettyPrint:\n-                s.append(space)\n-            s.append('<%s%s%s>' % (encodedName, attributeString, close))\n-            if prettyPrint:\n-                s.append(\"\\n\")\n-            s.append(contents)\n-            if prettyPrint and contents and contents[-1] != \"\\n\":\n-                s.append(\"\\n\")\n-            if prettyPrint and closeTag:\n-                s.append(space)\n-            s.append(closeTag)\n-            if prettyPrint and closeTag and self.nextSibling:\n-                s.append(\"\\n\")\n-            s = ''.join(s)\n-        return s\n-\n-    def decompose(self):\n-        \"\"\"Recursively destroys the contents of this tree.\"\"\"\n-        self.extract()\n-        if len(self.contents) == 0:\n-            return\n-        current = self.contents[0]\n-        while current is not None:\n-            next = current.next\n-            if isinstance(current, Tag):\n-                del current.contents[:]\n-            current.parent = None\n-            current.previous = None\n-            current.previousSibling = None\n-            current.next = None\n-            current.nextSibling = None\n-            current = next\n-\n-    def prettify(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        return self.__str__(encoding, True)\n-\n-    def renderContents(self, encoding=DEFAULT_OUTPUT_ENCODING,\n-                       prettyPrint=False, indentLevel=0):\n-        \"\"\"Renders the contents of this tag as a string in the given\n-        encoding. If encoding is None, returns a Unicode string..\"\"\"\n-        s=[]\n-        for c in self:\n-            text = None\n-            if isinstance(c, NavigableString):\n-                text = c.__str__(encoding)\n-            elif isinstance(c, Tag):\n-                s.append(c.__str__(encoding, prettyPrint, indentLevel))\n-            if text and prettyPrint:\n-                text = text.strip()\n-            if text:\n-                if prettyPrint:\n-                    s.append(\" \" * (indentLevel-1))\n-                s.append(text)\n-                if prettyPrint:\n-                    s.append(\"\\n\")\n-        return ''.join(s)\n-\n-    #Soup methods\n-\n-    def find(self, name=None, attrs={}, recursive=True, text=None,\n-             **kwargs):\n-        \"\"\"Return only the first child of this Tag matching the given\n-        criteria.\"\"\"\n-        r = None\n-        l = self.findAll(name, attrs, recursive, text, 1, **kwargs)\n-        if l:\n-            r = l[0]\n-        return r\n-    findChild = find\n-\n-    def findAll(self, name=None, attrs={}, recursive=True, text=None,\n-                limit=None, **kwargs):\n-        \"\"\"Extracts a list of Tag objects that match the given\n-        criteria.  You can specify the name of the Tag and any\n-        attributes you want the Tag to have.\n-\n-        The value of a key-value pair in the 'attrs' map can be a\n-        string, a list of strings, a regular expression object, or a\n-        callable that takes a string and returns whether or not the\n-        string matches for some custom definition of 'matches'. The\n-        same is true of the tag name.\"\"\"\n-        generator = self.recursiveChildGenerator\n-        if not recursive:\n-            generator = self.childGenerator\n-        return self._findAll(name, attrs, text, limit, generator, **kwargs)\n-    findChildren = findAll\n-\n-    # Pre-3.x compatibility methods\n-    first = find\n-    fetch = findAll\n-\n-    def fetchText(self, text=None, recursive=True, limit=None):\n-        return self.findAll(text=text, recursive=recursive, limit=limit)\n-\n-    def firstText(self, text=None, recursive=True):\n-        return self.find(text=text, recursive=recursive)\n-\n-    #Private methods\n-\n-    def _getAttrMap(self):\n-        \"\"\"Initializes a map representation of this tag's attributes,\n-        if not already initialized.\"\"\"\n-        if not getattr(self, 'attrMap'):\n-            self.attrMap = {}\n-            for (key, value) in self.attrs:\n-                self.attrMap[key] = value\n-        return self.attrMap\n-\n-    #Generator methods\n-    def childGenerator(self):\n-        # Just use the iterator from the contents\n-        return iter(self.contents)\n-\n-    def recursiveChildGenerator(self):\n-        if not len(self.contents):\n-            raise StopIteration\n-        stopNode = self._lastRecursiveChild().next\n-        current = self.contents[0]\n-        while current is not stopNode:\n-            yield current\n-            current = current.next\n-\n-\n-# Next, a couple classes to represent queries and their results.\n-class SoupStrainer:\n-    \"\"\"Encapsulates a number of ways of matching a markup element (tag or\n-    text).\"\"\"\n-\n-    def __init__(self, name=None, attrs={}, text=None, **kwargs):\n-        self.name = name\n-        if isinstance(attrs, basestring):\n-            kwargs['class'] = _match_css_class(attrs)\n-            attrs = None\n-        if kwargs:\n-            if attrs:\n-                attrs = attrs.copy()\n-                attrs.update(kwargs)\n-            else:\n-                attrs = kwargs\n-        self.attrs = attrs\n-        self.text = text\n-\n-    def __str__(self):\n-        if self.text:\n-            return self.text\n-        else:\n-            return \"%s|%s\" % (self.name, self.attrs)\n-\n-    def searchTag(self, markupName=None, markupAttrs={}):\n-        found = None\n-        markup = None\n-        if isinstance(markupName, Tag):\n-            markup = markupName\n-            markupAttrs = markup\n-        callFunctionWithTagData = callable(self.name) \\\n-                                and not isinstance(markupName, Tag)\n-\n-        if (not self.name) \\\n-               or callFunctionWithTagData \\\n-               or (markup and self._matches(markup, self.name)) \\\n-               or (not markup and self._matches(markupName, self.name)):\n-            if callFunctionWithTagData:\n-                match = self.name(markupName, markupAttrs)\n-            else:\n-                match = True\n-                markupAttrMap = None\n-                for attr, matchAgainst in self.attrs.items():\n-                    if not markupAttrMap:\n-                         if hasattr(markupAttrs, 'get'):\n-                            markupAttrMap = markupAttrs\n-                         else:\n-                            markupAttrMap = {}\n-                            for k,v in markupAttrs:\n-                                markupAttrMap[k] = v\n-                    attrValue = markupAttrMap.get(attr)\n-                    if not self._matches(attrValue, matchAgainst):\n-                        match = False\n-                        break\n-            if match:\n-                if markup:\n-                    found = markup\n-                else:\n-                    found = markupName\n-        return found\n-\n-    def search(self, markup):\n-        #print 'looking for %s in %s' % (self, markup)\n-        found = None\n-        # If given a list of items, scan it for a text element that\n-        # matches.\n-        if hasattr(markup, \"__iter__\") \\\n-                and not isinstance(markup, Tag):\n-            for element in markup:\n-                if isinstance(element, NavigableString) \\\n-                       and self.search(element):\n-                    found = element\n-                    break\n-        # If it's a Tag, make sure its name or attributes match.\n-        # Don't bother with Tags if we're searching for text.\n-        elif isinstance(markup, Tag):\n-            if not self.text:\n-                found = self.searchTag(markup)\n-        # If it's text, make sure the text matches.\n-        elif isinstance(markup, NavigableString) or \\\n-                 isinstance(markup, basestring):\n-            if self._matches(markup, self.text):\n-                found = markup\n-        else:\n-            raise Exception, \"I don't know how to match against a %s\" \\\n-                  % markup.__class__\n-        return found\n-\n-    def _matches(self, markup, matchAgainst):\n-        #print \"Matching %s against %s\" % (markup, matchAgainst)\n-        result = False\n-        if matchAgainst is True:\n-            result = markup is not None\n-        elif callable(matchAgainst):\n-            result = matchAgainst(markup)\n-        else:\n-            #Custom match methods take the tag as an argument, but all\n-            #other ways of matching match the tag name as a string.\n-            if isinstance(markup, Tag):\n-                markup = markup.name\n-            if markup and not isinstance(markup, basestring):\n-                markup = unicode(markup)\n-            #Now we know that chunk is either a string, or None.\n-            if hasattr(matchAgainst, 'match'):\n-                # It's a regexp object.\n-                result = markup and matchAgainst.search(markup)\n-            elif hasattr(matchAgainst, '__iter__'): # list-like\n-                result = markup in matchAgainst\n-            elif hasattr(matchAgainst, 'items'):\n-                result = markup.has_key(matchAgainst)\n-            elif matchAgainst and isinstance(markup, basestring):\n-                if isinstance(markup, unicode):\n-                    matchAgainst = unicode(matchAgainst)\n-                else:\n-                    matchAgainst = str(matchAgainst)\n-\n-            if not result:\n-                result = matchAgainst == markup\n-        return result\n-\n-class ResultSet(list):\n-    \"\"\"A ResultSet is just a list that keeps track of the SoupStrainer\n-    that created it.\"\"\"\n-    def __init__(self, source):\n-        list.__init__([])\n-        self.source = source\n-\n-# Now, some helper functions.\n-\n-def buildTagMap(default, *args):\n-    \"\"\"Turns a list of maps, lists, or scalars into a single map.\n-    Used to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\n-    NESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n-    built = {}\n-    for portion in args:\n-        if hasattr(portion, 'items'):\n-            #It's a map. Merge it.\n-            for k,v in portion.items():\n-                built[k] = v\n-        elif hasattr(portion, '__iter__'): # is a list\n-            #It's a list. Map each item to the default.\n-            for k in portion:\n-                built[k] = default\n-        else:\n-            #It's a scalar. Map it to the default.\n-            built[portion] = default\n-    return built\n-\n-# Now, the parser classes.\n-\n-class BeautifulStoneSoup(Tag, SGMLParser):\n-\n-    \"\"\"This class contains the basic parser and search code. It defines\n-    a parser that knows nothing about tag behavior except for the\n-    following:\n-\n-      You can't close a tag without closing all the tags it encloses.\n-      That is, \"<foo><bar></foo>\" actually means\n-      \"<foo><bar></bar></foo>\".\n-\n-    [Another possible explanation is \"<foo><bar /></foo>\", but since\n-    this class defines no SELF_CLOSING_TAGS, it will never use that\n-    explanation.]\n-\n-    This class is useful for parsing XML or made-up markup languages,\n-    or when BeautifulSoup makes an assumption counter to what you were\n-    expecting.\"\"\"\n-\n-    SELF_CLOSING_TAGS = {}\n-    NESTABLE_TAGS = {}\n-    RESET_NESTING_TAGS = {}\n-    QUOTE_TAGS = {}\n-    PRESERVE_WHITESPACE_TAGS = []\n-\n-    MARKUP_MASSAGE = [(re.compile('(<[^<>]*)/>'),\n-                       lambda x: x.group(1) + ' />'),\n-                      (re.compile('<!\\s+([^<>]*)>'),\n-                       lambda x: '<!' + x.group(1) + '>')\n-                      ]\n-\n-    ROOT_TAG_NAME = u'[document]'\n-\n-    HTML_ENTITIES = \"html\"\n-    XML_ENTITIES = \"xml\"\n-    XHTML_ENTITIES = \"xhtml\"\n-    # TODO: This only exists for backwards-compatibility\n-    ALL_ENTITIES = XHTML_ENTITIES\n-\n-    # Used when determining whether a text node is all whitespace and\n-    # can be replaced with a single space. A text node that contains\n-    # fancy Unicode spaces (usually non-breaking) should be left\n-    # alone.\n-    STRIP_ASCII_SPACES = { 9: None, 10: None, 12: None, 13: None, 32: None, }\n-\n-    def __init__(self, markup=\"\", parseOnlyThese=None, fromEncoding=None,\n-                 markupMassage=True, smartQuotesTo=XML_ENTITIES,\n-                 convertEntities=None, selfClosingTags=None, isHTML=False):\n-        \"\"\"The Soup object is initialized as the 'root tag', and the\n-        provided markup (which can be a string or a file-like object)\n-        is fed into the underlying parser.\n-\n-        sgmllib will process most bad HTML, and the BeautifulSoup\n-        class has some tricks for dealing with some HTML that kills\n-        sgmllib, but Beautiful Soup can nonetheless choke or lose data\n-        if your data uses self-closing tags or declarations\n-        incorrectly.\n-\n-        By default, Beautiful Soup uses regexes to sanitize input,\n-        avoiding the vast majority of these problems. If the problems\n-        don't apply to you, pass in False for markupMassage, and\n-        you'll get better performance.\n-\n-        The default parser massage techniques fix the two most common\n-        instances of invalid HTML that choke sgmllib:\n-\n-         <br/> (No space between name of closing tag and tag close)\n-         <! --Comment--> (Extraneous whitespace in declaration)\n-\n-        You can pass in a custom list of (RE object, replace method)\n-        tuples to get Beautiful Soup to scrub your input the way you\n-        want.\"\"\"\n-\n-        self.parseOnlyThese = parseOnlyThese\n-        self.fromEncoding = fromEncoding\n-        self.smartQuotesTo = smartQuotesTo\n-        self.convertEntities = convertEntities\n-        # Set the rules for how we'll deal with the entities we\n-        # encounter\n-        if self.convertEntities:\n-            # It doesn't make sense to convert encoded characters to\n-            # entities even while you're converting entities to Unicode.\n-            # Just convert it all to Unicode.\n-            self.smartQuotesTo = None\n-            if convertEntities == self.HTML_ENTITIES:\n-                self.convertXMLEntities = False\n-                self.convertHTMLEntities = True\n-                self.escapeUnrecognizedEntities = True\n-            elif convertEntities == self.XHTML_ENTITIES:\n-                self.convertXMLEntities = True\n-                self.convertHTMLEntities = True\n-                self.escapeUnrecognizedEntities = False\n-            elif convertEntities == self.XML_ENTITIES:\n-                self.convertXMLEntities = True\n-                self.convertHTMLEntities = False\n-                self.escapeUnrecognizedEntities = False\n-        else:\n-            self.convertXMLEntities = False\n-            self.convertHTMLEntities = False\n-            self.escapeUnrecognizedEntities = False\n-\n-        self.instanceSelfClosingTags = buildTagMap(None, selfClosingTags)\n-        SGMLParser.__init__(self)\n-\n-        if hasattr(markup, 'read'):        # It's a file-type object.\n-            markup = markup.read()\n-        self.markup = markup\n-        self.markupMassage = markupMassage\n-        try:\n-            self._feed(isHTML=isHTML)\n-        except StopParsing:\n-            pass\n-        self.markup = None                 # The markup can now be GCed\n-\n-    def convert_charref(self, name):\n-        \"\"\"This method fixes a bug in Python's SGMLParser.\"\"\"\n-        try:\n-            n = int(name)\n-        except ValueError:\n-            return\n-        if not 0 <= n <= 127 : # ASCII ends at 127, not 255\n-            return\n-        return self.convert_codepoint(n)\n-\n-    def _feed(self, inDocumentEncoding=None, isHTML=False):\n-        # Convert the document to Unicode.\n-        markup = self.markup\n-        if isinstance(markup, unicode):\n-            if not hasattr(self, 'originalEncoding'):\n-                self.originalEncoding = None\n-        else:\n-            dammit = UnicodeDammit\\\n-                     (markup, [self.fromEncoding, inDocumentEncoding],\n-                      smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)\n-            markup = dammit.unicode\n-            self.originalEncoding = dammit.originalEncoding\n-            self.declaredHTMLEncoding = dammit.declaredHTMLEncoding\n-        if markup:\n-            if self.markupMassage:\n-                if not hasattr(self.markupMassage, \"__iter__\"):\n-                    self.markupMassage = self.MARKUP_MASSAGE\n-                for fix, m in self.markupMassage:\n-                    markup = fix.sub(m, markup)\n-                # TODO: We get rid of markupMassage so that the\n-                # soup object can be deepcopied later on. Some\n-                # Python installations can't copy regexes. If anyone\n-                # was relying on the existence of markupMassage, this\n-                # might cause problems.\n-                del(self.markupMassage)\n-        self.reset()\n-\n-        SGMLParser.feed(self, markup)\n-        # Close out any unfinished strings and close all the open tags.\n-        self.endData()\n-        while self.currentTag.name != self.ROOT_TAG_NAME:\n-            self.popTag()\n-\n-    def __getattr__(self, methodName):\n-        \"\"\"This method routes method call requests to either the SGMLParser\n-        superclass or the Tag superclass, depending on the method name.\"\"\"\n-        #print \"__getattr__ called on %s.%s\" % (self.__class__, methodName)\n-\n-        if methodName.startswith('start_') or methodName.startswith('end_') \\\n-               or methodName.startswith('do_'):\n-            return SGMLParser.__getattr__(self, methodName)\n-        elif not methodName.startswith('__'):\n-            return Tag.__getattr__(self, methodName)\n-        else:\n-            raise AttributeError\n-\n-    def isSelfClosingTag(self, name):\n-        \"\"\"Returns true iff the given string is the name of a\n-        self-closing tag according to this parser.\"\"\"\n-        return self.SELF_CLOSING_TAGS.has_key(name) \\\n-               or self.instanceSelfClosingTags.has_key(name)\n-\n-    def reset(self):\n-        Tag.__init__(self, self, self.ROOT_TAG_NAME)\n-        self.hidden = 1\n-        SGMLParser.reset(self)\n-        self.currentData = []\n-        self.currentTag = None\n-        self.tagStack = []\n-        self.quoteStack = []\n-        self.pushTag(self)\n-\n-    def popTag(self):\n-        tag = self.tagStack.pop()\n-\n-        #print \"Pop\", tag.name\n-        if self.tagStack:\n-            self.currentTag = self.tagStack[-1]\n-        return self.currentTag\n-\n-    def pushTag(self, tag):\n-        #print \"Push\", tag.name\n-        if self.currentTag:\n-            self.currentTag.contents.append(tag)\n-        self.tagStack.append(tag)\n-        self.currentTag = self.tagStack[-1]\n-\n-    def endData(self, containerClass=NavigableString):\n-        if self.currentData:\n-            currentData = u''.join(self.currentData)\n-            if (currentData.translate(self.STRIP_ASCII_SPACES) == '' and\n-                not set([tag.name for tag in self.tagStack]).intersection(\n-                    self.PRESERVE_WHITESPACE_TAGS)):\n-                if '\\n' in currentData:\n-                    currentData = '\\n'\n-                else:\n-                    currentData = ' '\n-            self.currentData = []\n-            if self.parseOnlyThese and len(self.tagStack) <= 1 and \\\n-                   (not self.parseOnlyThese.text or \\\n-                    not self.parseOnlyThese.search(currentData)):\n-                return\n-            o = containerClass(currentData)\n-            o.setup(self.currentTag, self.previous)\n-            if self.previous:\n-                self.previous.next = o\n-            self.previous = o\n-            self.currentTag.contents.append(o)\n-\n-\n-    def _popToTag(self, name, inclusivePop=True):\n-        \"\"\"Pops the tag stack up to and including the most recent\n-        instance of the given tag. If inclusivePop is false, pops the tag\n-        stack up to but *not* including the most recent instqance of\n-        the given tag.\"\"\"\n-        #print \"Popping to %s\" % name\n-        if name == self.ROOT_TAG_NAME:\n-            return\n-\n-        numPops = 0\n-        mostRecentTag = None\n-        for i in range(len(self.tagStack)-1, 0, -1):\n-            if name == self.tagStack[i].name:\n-                numPops = len(self.tagStack)-i\n-                break\n-        if not inclusivePop:\n-            numPops = numPops - 1\n-\n-        for i in range(0, numPops):\n-            mostRecentTag = self.popTag()\n-        return mostRecentTag\n-\n-    def _smartPop(self, name):\n-\n-        \"\"\"We need to pop up to the previous tag of this type, unless\n-        one of this tag's nesting reset triggers comes between this\n-        tag and the previous tag of this type, OR unless this tag is a\n-        generic nesting trigger and another generic nesting trigger\n-        comes between this tag and the previous tag of this type.\n-\n-        Examples:\n-         <p>Foo<b>Bar *<p>* should pop to 'p', not 'b'.\n-         <p>Foo<table>Bar *<p>* should pop to 'table', not 'p'.\n-         <p>Foo<table><tr>Bar *<p>* should pop to 'tr', not 'p'.\n-\n-         <li><ul><li> *<li>* should pop to 'ul', not the first 'li'.\n-         <tr><table><tr> *<tr>* should pop to 'table', not the first 'tr'\n-         <td><tr><td> *<td>* should pop to 'tr', not the first 'td'\n-        \"\"\"\n-\n-        nestingResetTriggers = self.NESTABLE_TAGS.get(name)\n-        isNestable = nestingResetTriggers != None\n-        isResetNesting = self.RESET_NESTING_TAGS.has_key(name)\n-        popTo = None\n-        inclusive = True\n-        for i in range(len(self.tagStack)-1, 0, -1):\n-            p = self.tagStack[i]\n-            if (not p or p.name == name) and not isNestable:\n-                #Non-nestable tags get popped to the top or to their\n-                #last occurance.\n-                popTo = name\n-                break\n-            if (nestingResetTriggers is not None\n-                and p.name in nestingResetTriggers) \\\n-                or (nestingResetTriggers is None and isResetNesting\n-                    and self.RESET_NESTING_TAGS.has_key(p.name)):\n-\n-                #If we encounter one of the nesting reset triggers\n-                #peculiar to this tag, or we encounter another tag\n-                #that causes nesting to reset, pop up to but not\n-                #including that tag.\n-                popTo = p.name\n-                inclusive = False\n-                break\n-            p = p.parent\n-        if popTo:\n-            self._popToTag(popTo, inclusive)\n-\n-    def unknown_starttag(self, name, attrs, selfClosing=0):\n-        #print \"Start tag %s: %s\" % (name, attrs)\n-        if self.quoteStack:\n-            #This is not a real tag.\n-            #print \"<%s> is not real!\" % name\n-            attrs = ''.join([' %s=\"%s\"' % (x, y) for x, y in attrs])\n-            self.handle_data('<%s%s>' % (name, attrs))\n-            return\n-        self.endData()\n-\n-        if not self.isSelfClosingTag(name) and not selfClosing:\n-            self._smartPop(name)\n-\n-        if self.parseOnlyThese and len(self.tagStack) <= 1 \\\n-               and (self.parseOnlyThese.text or not self.parseOnlyThese.searchTag(name, attrs)):\n-            return\n-\n-        tag = Tag(self, name, attrs, self.currentTag, self.previous)\n-        if self.previous:\n-            self.previous.next = tag\n-        self.previous = tag\n-        self.pushTag(tag)\n-        if selfClosing or self.isSelfClosingTag(name):\n-            self.popTag()\n-        if name in self.QUOTE_TAGS:\n-            #print \"Beginning quote (%s)\" % name\n-            self.quoteStack.append(name)\n-            self.literal = 1\n-        return tag\n-\n-    def unknown_endtag(self, name):\n-        #print \"End tag %s\" % name\n-        if self.quoteStack and self.quoteStack[-1] != name:\n-            #This is not a real end tag.\n-            #print \"</%s> is not real!\" % name\n-            self.handle_data('</%s>' % name)\n-            return\n-        self.endData()\n-        self._popToTag(name)\n-        if self.quoteStack and self.quoteStack[-1] == name:\n-            self.quoteStack.pop()\n-            self.literal = (len(self.quoteStack) > 0)\n-\n-    def handle_data(self, data):\n-        self.currentData.append(data)\n-\n-    def _toStringSubclass(self, text, subclass):\n-        \"\"\"Adds a certain piece of text to the tree as a NavigableString\n-        subclass.\"\"\"\n-        self.endData()\n-        self.handle_data(text)\n-        self.endData(subclass)\n-\n-    def handle_pi(self, text):\n-        \"\"\"Handle a processing instruction as a ProcessingInstruction\n-        object, possibly one with a %SOUP-ENCODING% slot into which an\n-        encoding will be plugged later.\"\"\"\n-        if text[:3] == \"xml\":\n-            text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\n-        self._toStringSubclass(text, ProcessingInstruction)\n-\n-    def handle_comment(self, text):\n-        \"Handle comments as Comment objects.\"\n-        self._toStringSubclass(text, Comment)\n-\n-    def handle_charref(self, ref):\n-        \"Handle character references as data.\"\n-        if self.convertEntities:\n-            data = unichr(int(ref))\n-        else:\n-            data = '&#%s;' % ref\n-        self.handle_data(data)\n-\n-    def handle_entityref(self, ref):\n-        \"\"\"Handle entity references as data, possibly converting known\n-        HTML and/or XML entity references to the corresponding Unicode\n-        characters.\"\"\"\n-        data = None\n-        if self.convertHTMLEntities:\n-            try:\n-                data = unichr(name2codepoint[ref])\n-            except KeyError:\n-                pass\n-\n-        if not data and self.convertXMLEntities:\n-                data = self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n-\n-        if not data and self.convertHTMLEntities and \\\n-            not self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n-                # TODO: We've got a problem here. We're told this is\n-                # an entity reference, but it's not an XML entity\n-                # reference or an HTML entity reference. Nonetheless,\n-                # the logical thing to do is to pass it through as an\n-                # unrecognized entity reference.\n-                #\n-                # Except: when the input is \"&carol;\" this function\n-                # will be called with input \"carol\". When the input is\n-                # \"AT&T\", this function will be called with input\n-                # \"T\". We have no way of knowing whether a semicolon\n-                # was present originally, so we don't know whether\n-                # this is an unknown entity or just a misplaced\n-                # ampersand.\n-                #\n-                # The more common case is a misplaced ampersand, so I\n-                # escape the ampersand and omit the trailing semicolon.\n-                data = \"&amp;%s\" % ref\n-        if not data:\n-            # This case is different from the one above, because we\n-            # haven't already gone through a supposedly comprehensive\n-            # mapping of entities to Unicode characters. We might not\n-            # have gone through any mapping at all. So the chances are\n-            # very high that this is a real entity, and not a\n-            # misplaced ampersand.\n-            data = \"&%s;\" % ref\n-        self.handle_data(data)\n-\n-    def handle_decl(self, data):\n-        \"Handle DOCTYPEs and the like as Declaration objects.\"\n-        self._toStringSubclass(data, Declaration)\n-\n-    def parse_declaration(self, i):\n-        \"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\n-        declaration as a CData object.\"\"\"\n-        j = None\n-        if self.rawdata[i:i+9] == '<![CDATA[':\n-             k = self.rawdata.find(']]>', i)\n-             if k == -1:\n-                 k = len(self.rawdata)\n-             data = self.rawdata[i+9:k]\n-             j = k+3\n-             self._toStringSubclass(data, CData)\n-        else:\n-            try:\n-                j = SGMLParser.parse_declaration(self, i)\n-            except SGMLParseError:\n-                toHandle = self.rawdata[i:]\n-                self.handle_data(toHandle)\n-                j = i + len(toHandle)\n-        return j\n-\n-class BeautifulSoup(BeautifulStoneSoup):\n-\n-    \"\"\"This parser knows the following facts about HTML:\n-\n-    * Some tags have no closing tag and should be interpreted as being\n-      closed as soon as they are encountered.\n-\n-    * The text inside some tags (ie. 'script') may contain tags which\n-      are not really part of the document and which should be parsed\n-      as text, not tags. If you want to parse the text as tags, you can\n-      always fetch it and parse it explicitly.\n-\n-    * Tag nesting rules:\n-\n-      Most tags can't be nested at all. For instance, the occurance of\n-      a <p> tag should implicitly close the previous <p> tag.\n-\n-       <p>Para1<p>Para2\n-        should be transformed into:\n-       <p>Para1</p><p>Para2\n-\n-      Some tags can be nested arbitrarily. For instance, the occurance\n-      of a <blockquote> tag should _not_ implicitly close the previous\n-      <blockquote> tag.\n-\n-       Alice said: <blockquote>Bob said: <blockquote>Blah\n-        should NOT be transformed into:\n-       Alice said: <blockquote>Bob said: </blockquote><blockquote>Blah\n-\n-      Some tags can be nested, but the nesting is reset by the\n-      interposition of other tags. For instance, a <tr> tag should\n-      implicitly close the previous <tr> tag within the same <table>,\n-      but not close a <tr> tag in another table.\n-\n-       <table><tr>Blah<tr>Blah\n-        should be transformed into:\n-       <table><tr>Blah</tr><tr>Blah\n-        but,\n-       <tr>Blah<table><tr>Blah\n-        should NOT be transformed into\n-       <tr>Blah<table></tr><tr>Blah\n-\n-    Differing assumptions about tag nesting rules are a major source\n-    of problems with the BeautifulSoup class. If BeautifulSoup is not\n-    treating as nestable a tag your page author treats as nestable,\n-    try ICantBelieveItsBeautifulSoup, MinimalSoup, or\n-    BeautifulStoneSoup before writing your own subclass.\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        if not kwargs.has_key('smartQuotesTo'):\n-            kwargs['smartQuotesTo'] = self.HTML_ENTITIES\n-        kwargs['isHTML'] = True\n-        BeautifulStoneSoup.__init__(self, *args, **kwargs)\n-\n-    SELF_CLOSING_TAGS = buildTagMap(None,\n-                                    ('br' , 'hr', 'input', 'img', 'meta',\n-                                    'spacer', 'link', 'frame', 'base', 'col'))\n-\n-    PRESERVE_WHITESPACE_TAGS = set(['pre', 'textarea'])\n-\n-    QUOTE_TAGS = {'script' : None, 'textarea' : None}\n-\n-    #According to the HTML standard, each of these inline tags can\n-    #contain another tag of the same type. Furthermore, it's common\n-    #to actually use these tags this way.\n-    NESTABLE_INLINE_TAGS = ('span', 'font', 'q', 'object', 'bdo', 'sub', 'sup',\n-                            'center')\n-\n-    #According to the HTML standard, these block tags can contain\n-    #another tag of the same type. Furthermore, it's common\n-    #to actually use these tags this way.\n-    NESTABLE_BLOCK_TAGS = ('blockquote', 'div', 'fieldset', 'ins', 'del')\n-\n-    #Lists can contain other lists, but there are restrictions.\n-    NESTABLE_LIST_TAGS = { 'ol' : [],\n-                           'ul' : [],\n-                           'li' : ['ul', 'ol'],\n-                           'dl' : [],\n-                           'dd' : ['dl'],\n-                           'dt' : ['dl'] }\n-\n-    #Tables can contain other tables, but there are restrictions.\n-    NESTABLE_TABLE_TAGS = {'table' : [],\n-                           'tr' : ['table', 'tbody', 'tfoot', 'thead'],\n-                           'td' : ['tr'],\n-                           'th' : ['tr'],\n-                           'thead' : ['table'],\n-                           'tbody' : ['table'],\n-                           'tfoot' : ['table'],\n-                           }\n-\n-    NON_NESTABLE_BLOCK_TAGS = ('address', 'form', 'p', 'pre')\n-\n-    #If one of these tags is encountered, all tags up to the next tag of\n-    #this type are popped.\n-    RESET_NESTING_TAGS = buildTagMap(None, NESTABLE_BLOCK_TAGS, 'noscript',\n-                                     NON_NESTABLE_BLOCK_TAGS,\n-                                     NESTABLE_LIST_TAGS,\n-                                     NESTABLE_TABLE_TAGS)\n-\n-    NESTABLE_TAGS = buildTagMap([], NESTABLE_INLINE_TAGS, NESTABLE_BLOCK_TAGS,\n-                                NESTABLE_LIST_TAGS, NESTABLE_TABLE_TAGS)\n-\n-    # Used to detect the charset in a META tag; see start_meta\n-    CHARSET_RE = re.compile(\"((^|;)\\s*charset=)([^;]*)\", re.M)\n-\n-    def start_meta(self, attrs):\n-        \"\"\"Beautiful Soup can detect a charset included in a META tag,\n-        try to convert the document to that charset, and re-parse the\n-        document from the beginning.\"\"\"\n-        httpEquiv = None\n-        contentType = None\n-        contentTypeIndex = None\n-        tagNeedsEncodingSubstitution = False\n-\n-        for i in range(0, len(attrs)):\n-            key, value = attrs[i]\n-            key = key.lower()\n-            if key == 'http-equiv':\n-                httpEquiv = value\n-            elif key == 'content':\n-                contentType = value\n-                contentTypeIndex = i\n-\n-        if httpEquiv and contentType: # It's an interesting meta tag.\n-            match = self.CHARSET_RE.search(contentType)\n-            if match:\n-                if (self.declaredHTMLEncoding is not None or\n-                    self.originalEncoding == self.fromEncoding):\n-                    # An HTML encoding was sniffed while converting\n-                    # the document to Unicode, or an HTML encoding was\n-                    # sniffed during a previous pass through the\n-                    # document, or an encoding was specified\n-                    # explicitly and it worked. Rewrite the meta tag.\n-                    def rewrite(match):\n-                        return match.group(1) + \"%SOUP-ENCODING%\"\n-                    newAttr = self.CHARSET_RE.sub(rewrite, contentType)\n-                    attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\n-                                               newAttr)\n-                    tagNeedsEncodingSubstitution = True\n-                else:\n-                    # This is our first pass through the document.\n-                    # Go through it again with the encoding information.\n-                    newCharset = match.group(3)\n-                    if newCharset and newCharset != self.originalEncoding:\n-                        self.declaredHTMLEncoding = newCharset\n-                        self._feed(self.declaredHTMLEncoding)\n-                        raise StopParsing\n-                    pass\n-        tag = self.unknown_starttag(\"meta\", attrs)\n-        if tag and tagNeedsEncodingSubstitution:\n-            tag.containsSubstitutions = True\n-\n-class StopParsing(Exception):\n-    pass\n-\n-class ICantBelieveItsBeautifulSoup(BeautifulSoup):\n-\n-    \"\"\"The BeautifulSoup class is oriented towards skipping over\n-    common HTML errors like unclosed tags. However, sometimes it makes\n-    errors of its own. For instance, consider this fragment:\n-\n-     <b>Foo<b>Bar</b></b>\n-\n-    This is perfectly valid (if bizarre) HTML. However, the\n-    BeautifulSoup class will implicitly close the first b tag when it\n-    encounters the second 'b'. It will think the author wrote\n-    \"<b>Foo<b>Bar\", and didn't close the first 'b' tag, because\n-    there's no real-world reason to bold something that's already\n-    bold. When it encounters '</b></b>' it will close two more 'b'\n-    tags, for a grand total of three tags closed instead of two. This\n-    can throw off the rest of your document structure. The same is\n-    true of a number of other tags, listed below.\n-\n-    It's much more common for someone to forget to close a 'b' tag\n-    than to actually use nested 'b' tags, and the BeautifulSoup class\n-    handles the common case. This class handles the not-co-common\n-    case: where you can't believe someone wrote what they did, but\n-    it's valid HTML and BeautifulSoup screwed up by assuming it\n-    wouldn't be.\"\"\"\n-\n-    I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS = \\\n-     ('em', 'big', 'i', 'small', 'tt', 'abbr', 'acronym', 'strong',\n-      'cite', 'code', 'dfn', 'kbd', 'samp', 'strong', 'var', 'b',\n-      'big')\n-\n-    I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS = ('noscript',)\n-\n-    NESTABLE_TAGS = buildTagMap([], BeautifulSoup.NESTABLE_TAGS,\n-                                I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS,\n-                                I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS)\n-\n-class MinimalSoup(BeautifulSoup):\n-    \"\"\"The MinimalSoup class is for parsing HTML that contains\n-    pathologically bad markup. It makes no assumptions about tag\n-    nesting, but it does know which tags are self-closing, that\n-    <script> tags contain Javascript and should not be parsed, that\n-    META tags may contain encoding information, and so on.\n-\n-    This also makes it better for subclassing than BeautifulStoneSoup\n-    or BeautifulSoup.\"\"\"\n-\n-    RESET_NESTING_TAGS = buildTagMap('noscript')\n-    NESTABLE_TAGS = {}\n-\n-class BeautifulSOAP(BeautifulStoneSoup):\n-    \"\"\"This class will push a tag with only a single string child into\n-    the tag's parent as an attribute. The attribute's name is the tag\n-    name, and the value is the string child. An example should give\n-    the flavor of the change:\n-\n-    <foo><bar>baz</bar></foo>\n-     =>\n-    <foo bar=\"baz\"><bar>baz</bar></foo>\n-\n-    You can then access fooTag['bar'] instead of fooTag.barTag.string.\n-\n-    This is, of course, useful for scraping structures that tend to\n-    use subelements instead of attributes, such as SOAP messages. Note\n-    that it modifies its input, so don't print the modified version\n-    out.\n-\n-    I'm not sure how many people really want to use this class; let me\n-    know if you do. Mainly I like the name.\"\"\"\n-\n-    def popTag(self):\n-        if len(self.tagStack) > 1:\n-            tag = self.tagStack[-1]\n-            parent = self.tagStack[-2]\n-            parent._getAttrMap()\n-            if (isinstance(tag, Tag) and len(tag.contents) == 1 and\n-                isinstance(tag.contents[0], NavigableString) and\n-                not parent.attrMap.has_key(tag.name)):\n-                parent[tag.name] = tag.contents[0]\n-        BeautifulStoneSoup.popTag(self)\n-\n-#Enterprise class names! It has come to our attention that some people\n-#think the names of the Beautiful Soup parser classes are too silly\n-#and \"unprofessional\" for use in enterprise screen-scraping. We feel\n-#your pain! For such-minded folk, the Beautiful Soup Consortium And\n-#All-Night Kosher Bakery recommends renaming this file to\n-#\"RobustParser.py\" (or, in cases of extreme enterprisiness,\n-#\"RobustParserBeanInterface.class\") and using the following\n-#enterprise-friendly class aliases:\n-class RobustXMLParser(BeautifulStoneSoup):\n-    pass\n-class RobustHTMLParser(BeautifulSoup):\n-    pass\n-class RobustWackAssHTMLParser(ICantBelieveItsBeautifulSoup):\n-    pass\n-class RobustInsanelyWackAssHTMLParser(MinimalSoup):\n-    pass\n-class SimplifyingSOAPParser(BeautifulSOAP):\n-    pass\n-\n-######################################################\n-#\n-# Bonus library: Unicode, Dammit\n-#\n-# This class forces XML data into a standard format (usually to UTF-8\n-# or Unicode).  It is heavily based on code from Mark Pilgrim's\n-# Universal Feed Parser. It does not rewrite the XML or HTML to\n-# reflect a new encoding: that happens in BeautifulStoneSoup.handle_pi\n-# (XML) and BeautifulSoup.start_meta (HTML).\n-\n-# Autodetects character encodings.\n-# Download from http://chardet.feedparser.org/\n-try:\n-    import chardet\n-#    import chardet.constants\n-#    chardet.constants._debug = 1\n-except ImportError:\n-    chardet = None\n-\n-# cjkcodecs and iconv_codec make Python know about more character encodings.\n-# Both are available from http://cjkpython.i18n.org/\n-# They're built in if you use Python 2.4.\n-try:\n-    import cjkcodecs.aliases\n-except ImportError:\n-    pass\n-try:\n-    import iconv_codec\n-except ImportError:\n-    pass\n-\n-class UnicodeDammit:\n-    \"\"\"A class for detecting the encoding of a *ML document and\n-    converting it to a Unicode string. If the source encoding is\n-    windows-1252, can replace MS smart quotes with their HTML or XML\n-    equivalents.\"\"\"\n-\n-    # This dictionary maps commonly seen values for \"charset\" in HTML\n-    # meta tags to the corresponding Python codec names. It only covers\n-    # values that aren't in Python's aliases and can't be determined\n-    # by the heuristics in find_codec.\n-    CHARSET_ALIASES = { \"macintosh\" : \"mac-roman\",\n-                        \"x-sjis\" : \"shift-jis\" }\n-\n-    def __init__(self, markup, overrideEncodings=[],\n-                 smartQuotesTo='xml', isHTML=False):\n-        self.declaredHTMLEncoding = None\n-        self.markup, documentEncoding, sniffedEncoding = \\\n-                     self._detectEncoding(markup, isHTML)\n-        self.smartQuotesTo = smartQuotesTo\n-        self.triedEncodings = []\n-        if markup == '' or isinstance(markup, unicode):\n-            self.originalEncoding = None\n-            self.unicode = unicode(markup)\n-            return\n-\n-        u = None\n-        for proposedEncoding in overrideEncodings:\n-            u = self._convertFrom(proposedEncoding)\n-            if u: break\n-        if not u:\n-            for proposedEncoding in (documentEncoding, sniffedEncoding):\n-                u = self._convertFrom(proposedEncoding)\n-                if u: break\n-\n-        # If no luck and we have auto-detection library, try that:\n-        if not u and chardet and not isinstance(self.markup, unicode):\n-            u = self._convertFrom(chardet.detect(self.markup)['encoding'])\n-\n-        # As a last resort, try utf-8 and windows-1252:\n-        if not u:\n-            for proposed_encoding in (\"utf-8\", \"windows-1252\"):\n-                u = self._convertFrom(proposed_encoding)\n-                if u: break\n-\n-        self.unicode = u\n-        if not u: self.originalEncoding = None\n-\n-    def _subMSChar(self, orig):\n-        \"\"\"Changes a MS smart quote character to an XML or HTML\n-        entity.\"\"\"\n-        sub = self.MS_CHARS.get(orig)\n-        if isinstance(sub, tuple):\n-            if self.smartQuotesTo == 'xml':\n-                sub = '&#x%s;' % sub[1]\n-            else:\n-                sub = '&%s;' % sub[0]\n-        return sub\n-\n-    def _convertFrom(self, proposed):\n-        proposed = self.find_codec(proposed)\n-        if not proposed or proposed in self.triedEncodings:\n-            return None\n-        self.triedEncodings.append(proposed)\n-        markup = self.markup\n-\n-        # Convert smart quotes to HTML if coming from an encoding\n-        # that might have them.\n-        if self.smartQuotesTo and proposed.lower() in(\"windows-1252\",\n-                                                      \"iso-8859-1\",\n-                                                      \"iso-8859-2\"):\n-            markup = re.compile(\"([\\x80-\\x9f])\").sub \\\n-                     (lambda(x): self._subMSChar(x.group(1)),\n-                      markup)\n-\n-        try:\n-            # print \"Trying to convert document to %s\" % proposed\n-            u = self._toUnicode(markup, proposed)\n-            self.markup = u\n-            self.originalEncoding = proposed\n-        except Exception, e:\n-            # print \"That didn't work!\"\n-            # print e\n-            return None\n-        #print \"Correct encoding: %s\" % proposed\n-        return self.markup\n-\n-    def _toUnicode(self, data, encoding):\n-        '''Given a string and its encoding, decodes the string into Unicode.\n-        %encoding is a string recognized by encodings.aliases'''\n-\n-        # strip Byte Order Mark (if present)\n-        if (len(data) >= 4) and (data[:2] == '\\xfe\\xff') \\\n-               and (data[2:4] != '\\x00\\x00'):\n-            encoding = 'utf-16be'\n-            data = data[2:]\n-        elif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') \\\n-                 and (data[2:4] != '\\x00\\x00'):\n-            encoding = 'utf-16le'\n-            data = data[2:]\n-        elif data[:3] == '\\xef\\xbb\\xbf':\n-            encoding = 'utf-8'\n-            data = data[3:]\n-        elif data[:4] == '\\x00\\x00\\xfe\\xff':\n-            encoding = 'utf-32be'\n-            data = data[4:]\n-        elif data[:4] == '\\xff\\xfe\\x00\\x00':\n-            encoding = 'utf-32le'\n-            data = data[4:]\n-        newdata = unicode(data, encoding)\n-        return newdata\n-\n-    def _detectEncoding(self, xml_data, isHTML=False):\n-        \"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n-        xml_encoding = sniffed_xml_encoding = None\n-        try:\n-            if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n-                # EBCDIC\n-                xml_data = self._ebcdic_to_ascii(xml_data)\n-            elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n-                # UTF-16BE\n-                sniffed_xml_encoding = 'utf-16be'\n-                xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n-            elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n-                     and (xml_data[2:4] != '\\x00\\x00'):\n-                # UTF-16BE with BOM\n-                sniffed_xml_encoding = 'utf-16be'\n-                xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n-            elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n-                # UTF-16LE\n-                sniffed_xml_encoding = 'utf-16le'\n-                xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n-            elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n-                     (xml_data[2:4] != '\\x00\\x00'):\n-                # UTF-16LE with BOM\n-                sniffed_xml_encoding = 'utf-16le'\n-                xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n-            elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n-                # UTF-32BE\n-                sniffed_xml_encoding = 'utf-32be'\n-                xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n-            elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n-                # UTF-32LE\n-                sniffed_xml_encoding = 'utf-32le'\n-                xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n-            elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n-                # UTF-32BE with BOM\n-                sniffed_xml_encoding = 'utf-32be'\n-                xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n-            elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n-                # UTF-32LE with BOM\n-                sniffed_xml_encoding = 'utf-32le'\n-                xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n-            elif xml_data[:3] == '\\xef\\xbb\\xbf':\n-                # UTF-8 with BOM\n-                sniffed_xml_encoding = 'utf-8'\n-                xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n-            else:\n-                sniffed_xml_encoding = 'ascii'\n-                pass\n-        except:\n-            xml_encoding_match = None\n-        xml_encoding_match = re.compile(\n-            '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\n-        if not xml_encoding_match and isHTML:\n-            regexp = re.compile('<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]', re.I)\n-            xml_encoding_match = regexp.search(xml_data)\n-        if xml_encoding_match is not None:\n-            xml_encoding = xml_encoding_match.groups()[0].lower()\n-            if isHTML:\n-                self.declaredHTMLEncoding = xml_encoding\n-            if sniffed_xml_encoding and \\\n-               (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n-                                 'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n-                                 'utf-16', 'utf-32', 'utf_16', 'utf_32',\n-                                 'utf16', 'u16')):\n-                xml_encoding = sniffed_xml_encoding\n-        return xml_data, xml_encoding, sniffed_xml_encoding\n-\n-\n-    def find_codec(self, charset):\n-        return self._codec(self.CHARSET_ALIASES.get(charset, charset)) \\\n-               or (charset and self._codec(charset.replace(\"-\", \"\"))) \\\n-               or (charset and self._codec(charset.replace(\"-\", \"_\"))) \\\n-               or charset\n-\n-    def _codec(self, charset):\n-        if not charset: return charset\n-        codec = None\n-        try:\n-            codecs.lookup(charset)\n-            codec = charset\n-        except (LookupError, ValueError):\n-            pass\n-        return codec\n-\n-    EBCDIC_TO_ASCII_MAP = None\n-    def _ebcdic_to_ascii(self, s):\n-        c = self.__class__\n-        if not c.EBCDIC_TO_ASCII_MAP:\n-            emap = (0,1,2,3,156,9,134,127,151,141,142,11,12,13,14,15,\n-                    16,17,18,19,157,133,8,135,24,25,146,143,28,29,30,31,\n-                    128,129,130,131,132,10,23,27,136,137,138,139,140,5,6,7,\n-                    144,145,22,147,148,149,150,4,152,153,154,155,20,21,158,26,\n-                    32,160,161,162,163,164,165,166,167,168,91,46,60,40,43,33,\n-                    38,169,170,171,172,173,174,175,176,177,93,36,42,41,59,94,\n-                    45,47,178,179,180,181,182,183,184,185,124,44,37,95,62,63,\n-                    186,187,188,189,190,191,192,193,194,96,58,35,64,39,61,34,\n-                    195,97,98,99,100,101,102,103,104,105,196,197,198,199,200,\n-                    201,202,106,107,108,109,110,111,112,113,114,203,204,205,\n-                    206,207,208,209,126,115,116,117,118,119,120,121,122,210,\n-                    211,212,213,214,215,216,217,218,219,220,221,222,223,224,\n-                    225,226,227,228,229,230,231,123,65,66,67,68,69,70,71,72,\n-                    73,232,233,234,235,236,237,125,74,75,76,77,78,79,80,81,\n-                    82,238,239,240,241,242,243,92,159,83,84,85,86,87,88,89,\n-                    90,244,245,246,247,248,249,48,49,50,51,52,53,54,55,56,57,\n-                    250,251,252,253,254,255)\n-            import string\n-            c.EBCDIC_TO_ASCII_MAP = string.maketrans( \\\n-            ''.join(map(chr, range(256))), ''.join(map(chr, emap)))\n-        return s.translate(c.EBCDIC_TO_ASCII_MAP)\n-\n-    MS_CHARS = { '\\x80' : ('euro', '20AC'),\n-                 '\\x81' : ' ',\n-                 '\\x82' : ('sbquo', '201A'),\n-                 '\\x83' : ('fnof', '192'),\n-                 '\\x84' : ('bdquo', '201E'),\n-                 '\\x85' : ('hellip', '2026'),\n-                 '\\x86' : ('dagger', '2020'),\n-                 '\\x87' : ('Dagger', '2021'),\n-                 '\\x88' : ('circ', '2C6'),\n-                 '\\x89' : ('permil', '2030'),\n-                 '\\x8A' : ('Scaron', '160'),\n-                 '\\x8B' : ('lsaquo', '2039'),\n-                 '\\x8C' : ('OElig', '152'),\n-                 '\\x8D' : '?',\n-                 '\\x8E' : ('#x17D', '17D'),\n-                 '\\x8F' : '?',\n-                 '\\x90' : '?',\n-                 '\\x91' : ('lsquo', '2018'),\n-                 '\\x92' : ('rsquo', '2019'),\n-                 '\\x93' : ('ldquo', '201C'),\n-                 '\\x94' : ('rdquo', '201D'),\n-                 '\\x95' : ('bull', '2022'),\n-                 '\\x96' : ('ndash', '2013'),\n-                 '\\x97' : ('mdash', '2014'),\n-                 '\\x98' : ('tilde', '2DC'),\n-                 '\\x99' : ('trade', '2122'),\n-                 '\\x9a' : ('scaron', '161'),\n-                 '\\x9b' : ('rsaquo', '203A'),\n-                 '\\x9c' : ('oelig', '153'),\n-                 '\\x9d' : '?',\n-                 '\\x9e' : ('#x17E', '17E'),\n-                 '\\x9f' : ('Yuml', ''),}\n-\n-#######################################################################\n-\n-\n-#By default, act as an HTML pretty-printer.\n-if __name__ == '__main__':\n-    import sys\n-    soup = BeautifulSoup(sys.stdin)\n-    print soup.prettify()"
            },
            {
                "sha": "d3b35368514cbdb649b314329ee1b1973ca1e750",
                "filename": "github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoupTests.py",
                "status": "removed",
                "additions": 0,
                "deletions": 906,
                "changes": 906,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoupTests.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoupTests.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoupTests.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -1,906 +0,0 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"Unit tests for Beautiful Soup.\n-\n-These tests make sure the Beautiful Soup works as it should. If you\n-find a bug in Beautiful Soup, the best way to express it is as a test\n-case like this that fails.\"\"\"\n-\n-# The very first thing we do is give a useful error if someone is\n-# running this code under Python 3.\n-\"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"<>\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n-\n-import unittest\n-from BeautifulSoup import *\n-\n-class SoupTest(unittest.TestCase):\n-\n-    def assertSoupEquals(self, toParse, rep=None, c=BeautifulSoup):\n-        \"\"\"Parse the given text and make sure its string rep is the other\n-        given text.\"\"\"\n-        if rep == None:\n-            rep = toParse\n-        self.assertEqual(str(c(toParse)), rep)\n-\n-\n-class FollowThatTag(SoupTest):\n-\n-    \"Tests the various ways of fetching tags from a soup.\"\n-\n-    def setUp(self):\n-        ml = \"\"\"\n-        <a id=\"x\">1</a>\n-        <A id=\"a\">2</a>\n-        <b id=\"b\">3</a>\n-        <b href=\"foo\" id=\"x\">4</a>\n-        <ac width=100>4</ac>\"\"\"\n-        self.soup = BeautifulStoneSoup(ml)\n-\n-    def testFindAllByName(self):\n-        matching = self.soup('a')\n-        self.assertEqual(len(matching), 2)\n-        self.assertEqual(matching[0].name, 'a')\n-        self.assertEqual(matching, self.soup.findAll('a'))\n-        self.assertEqual(matching, self.soup.findAll(SoupStrainer('a')))\n-\n-    def testFindAllByAttribute(self):\n-        matching = self.soup.findAll(id='x')\n-        self.assertEqual(len(matching), 2)\n-        self.assertEqual(matching[0].name, 'a')\n-        self.assertEqual(matching[1].name, 'b')\n-\n-        matching2 = self.soup.findAll(attrs={'id' : 'x'})\n-        self.assertEqual(matching, matching2)\n-\n-        strainer = SoupStrainer(attrs={'id' : 'x'})\n-        self.assertEqual(matching, self.soup.findAll(strainer))\n-\n-        self.assertEqual(len(self.soup.findAll(id=None)), 1)\n-\n-        self.assertEqual(len(self.soup.findAll(width=100)), 1)\n-        self.assertEqual(len(self.soup.findAll(junk=None)), 5)\n-        self.assertEqual(len(self.soup.findAll(junk=[1, None])), 5)\n-\n-        self.assertEqual(len(self.soup.findAll(junk=re.compile('.*'))), 0)\n-        self.assertEqual(len(self.soup.findAll(junk=True)), 0)\n-\n-        self.assertEqual(len(self.soup.findAll(junk=True)), 0)\n-        self.assertEqual(len(self.soup.findAll(href=True)), 1)\n-\n-    def testFindallByClass(self):\n-        soup = BeautifulSoup('<b class=\"foo\">Foo</b><a class=\"1 23 4\">Bar</a>')\n-        self.assertEqual(soup.find(attrs='foo').string, \"Foo\")\n-        self.assertEqual(soup.find('a', '1').string, \"Bar\")\n-        self.assertEqual(soup.find('a', '23').string, \"Bar\")\n-        self.assertEqual(soup.find('a', '4').string, \"Bar\")\n-\n-        self.assertEqual(soup.find('a', '2'), None)\n-\n-    def testFindAllByList(self):\n-        matching = self.soup(['a', 'ac'])\n-        self.assertEqual(len(matching), 3)\n-\n-    def testFindAllByHash(self):\n-        matching = self.soup({'a' : True, 'b' : True})\n-        self.assertEqual(len(matching), 4)\n-\n-    def testFindAllText(self):\n-        soup = BeautifulSoup(\"<html>\\xbb</html>\")\n-        self.assertEqual(soup.findAll(text=re.compile('.*')),\n-                         [u'\\xbb'])\n-\n-    def testFindAllByRE(self):\n-        import re\n-        r = re.compile('a.*')\n-        self.assertEqual(len(self.soup(r)), 3)\n-\n-    def testFindAllByMethod(self):\n-        def matchTagWhereIDMatchesName(tag):\n-            return tag.name == tag.get('id')\n-\n-        matching = self.soup.findAll(matchTagWhereIDMatchesName)\n-        self.assertEqual(len(matching), 2)\n-        self.assertEqual(matching[0].name, 'a')\n-\n-    def testFindByIndex(self):\n-        \"\"\"For when you have the tag and you want to know where it is.\"\"\"\n-        tag = self.soup.find('a', id=\"a\")\n-        self.assertEqual(self.soup.index(tag), 3)\n-\n-        # It works for NavigableStrings as well.\n-        s = tag.string\n-        self.assertEqual(tag.index(s), 0)\n-\n-        # If the tag isn't present, a ValueError is raised.\n-        soup2 = BeautifulSoup(\"<b></b>\")\n-        tag2 = soup2.find('b')\n-        self.assertRaises(ValueError, self.soup.index, tag2)\n-\n-    def testConflictingFindArguments(self):\n-        \"\"\"The 'text' argument takes precedence.\"\"\"\n-        soup = BeautifulSoup('Foo<b>Bar</b>Baz')\n-        self.assertEqual(soup.find('b', text='Baz'), 'Baz')\n-        self.assertEqual(soup.findAll('b', text='Baz'), ['Baz'])\n-\n-        self.assertEqual(soup.find(True, text='Baz'), 'Baz')\n-        self.assertEqual(soup.findAll(True, text='Baz'), ['Baz'])\n-\n-    def testParents(self):\n-        soup = BeautifulSoup('<ul id=\"foo\"></ul><ul id=\"foo\"><ul><ul id=\"foo\" a=\"b\"><b>Blah')\n-        b = soup.b\n-        self.assertEquals(len(b.findParents('ul', {'id' : 'foo'})), 2)\n-        self.assertEquals(b.findParent('ul')['a'], 'b')\n-\n-    PROXIMITY_TEST = BeautifulSoup('<b id=\"1\"><b id=\"2\"><b id=\"3\"><b id=\"4\">')\n-\n-    def testNext(self):\n-        soup = self.PROXIMITY_TEST\n-        b = soup.find('b', {'id' : 2})\n-        self.assertEquals(b.findNext('b')['id'], '3')\n-        self.assertEquals(b.findNext('b')['id'], '3')\n-        self.assertEquals(len(b.findAllNext('b')), 2)\n-        self.assertEquals(len(b.findAllNext('b', {'id' : 4})), 1)\n-\n-    def testPrevious(self):\n-        soup = self.PROXIMITY_TEST\n-        b = soup.find('b', {'id' : 3})\n-        self.assertEquals(b.findPrevious('b')['id'], '2')\n-        self.assertEquals(b.findPrevious('b')['id'], '2')\n-        self.assertEquals(len(b.findAllPrevious('b')), 2)\n-        self.assertEquals(len(b.findAllPrevious('b', {'id' : 2})), 1)\n-\n-\n-    SIBLING_TEST = BeautifulSoup('<blockquote id=\"1\"><blockquote id=\"1.1\"></blockquote></blockquote><blockquote id=\"2\"><blockquote id=\"2.1\"></blockquote></blockquote><blockquote id=\"3\"><blockquote id=\"3.1\"></blockquote></blockquote><blockquote id=\"4\">')\n-\n-    def testNextSibling(self):\n-        soup = self.SIBLING_TEST\n-        tag = 'blockquote'\n-        b = soup.find(tag, {'id' : 2})\n-        self.assertEquals(b.findNext(tag)['id'], '2.1')\n-        self.assertEquals(b.findNextSibling(tag)['id'], '3')\n-        self.assertEquals(b.findNextSibling(tag)['id'], '3')\n-        self.assertEquals(len(b.findNextSiblings(tag)), 2)\n-        self.assertEquals(len(b.findNextSiblings(tag, {'id' : 4})), 1)\n-\n-    def testPreviousSibling(self):\n-        soup = self.SIBLING_TEST\n-        tag = 'blockquote'\n-        b = soup.find(tag, {'id' : 3})\n-        self.assertEquals(b.findPrevious(tag)['id'], '2.1')\n-        self.assertEquals(b.findPreviousSibling(tag)['id'], '2')\n-        self.assertEquals(b.findPreviousSibling(tag)['id'], '2')\n-        self.assertEquals(len(b.findPreviousSiblings(tag)), 2)\n-        self.assertEquals(len(b.findPreviousSiblings(tag, id=1)), 1)\n-\n-    def testTextNavigation(self):\n-        soup = BeautifulSoup('Foo<b>Bar</b><i id=\"1\"><b>Baz<br />Blee<hr id=\"1\"/></b></i>Blargh')\n-        baz = soup.find(text='Baz')\n-        self.assertEquals(baz.findParent(\"i\")['id'], '1')\n-        self.assertEquals(baz.findNext(text='Blee'), 'Blee')\n-        self.assertEquals(baz.findNextSibling(text='Blee'), 'Blee')\n-        self.assertEquals(baz.findNextSibling(text='Blargh'), None)\n-        self.assertEquals(baz.findNextSibling('hr')['id'], '1')\n-\n-class SiblingRivalry(SoupTest):\n-    \"Tests the nextSibling and previousSibling navigation.\"\n-\n-    def testSiblings(self):\n-        soup = BeautifulSoup(\"<ul><li>1<p>A</p>B<li>2<li>3</ul>\")\n-        secondLI = soup.find('li').nextSibling\n-        self.assert_(secondLI.name == 'li' and secondLI.string == '2')\n-        self.assertEquals(soup.find(text='1').nextSibling.name, 'p')\n-        self.assertEquals(soup.find('p').nextSibling, 'B')\n-        self.assertEquals(soup.find('p').nextSibling.previousSibling.nextSibling, 'B')\n-\n-class TagsAreObjectsToo(SoupTest):\n-    \"Tests the various built-in functions of Tag objects.\"\n-\n-    def testLen(self):\n-        soup = BeautifulSoup(\"<top>1<b>2</b>3</top>\")\n-        self.assertEquals(len(soup.top), 3)\n-\n-class StringEmUp(SoupTest):\n-    \"Tests the use of 'string' as an alias for a tag's only content.\"\n-\n-    def testString(self):\n-        s = BeautifulSoup(\"<b>foo</b>\")\n-        self.assertEquals(s.b.string, 'foo')\n-\n-    def testLackOfString(self):\n-        s = BeautifulSoup(\"<b>f<i>e</i>o</b>\")\n-        self.assert_(not s.b.string)\n-\n-    def testStringAssign(self):\n-        s = BeautifulSoup(\"<b></b>\")\n-        b = s.b\n-        b.string = \"foo\"\n-        string = b.string\n-        self.assertEquals(string, \"foo\")\n-        self.assert_(isinstance(string, NavigableString))\n-\n-class AllText(SoupTest):\n-    \"Tests the use of 'text' to get all of string content from the tag.\"\n-\n-    def testText(self):\n-        soup = BeautifulSoup(\"<ul><li>spam</li><li>eggs</li><li>cheese</li>\")\n-        self.assertEquals(soup.ul.text, \"spameggscheese\")\n-        self.assertEquals(soup.ul.getText('/'), \"spam/eggs/cheese\")\n-\n-class ThatsMyLimit(SoupTest):\n-    \"Tests the limit argument.\"\n-\n-    def testBasicLimits(self):\n-        s = BeautifulSoup('<br id=\"1\" /><br id=\"1\" /><br id=\"1\" /><br id=\"1\" />')\n-        self.assertEquals(len(s.findAll('br')), 4)\n-        self.assertEquals(len(s.findAll('br', limit=2)), 2)\n-        self.assertEquals(len(s('br', limit=2)), 2)\n-\n-class OnlyTheLonely(SoupTest):\n-    \"Tests the parseOnly argument to the constructor.\"\n-    def setUp(self):\n-        x = []\n-        for i in range(1,6):\n-            x.append('<a id=\"%s\">' % i)\n-            for j in range(100,103):\n-                x.append('<b id=\"%s.%s\">Content %s.%s</b>' % (i,j, i,j))\n-            x.append('</a>')\n-        self.x = ''.join(x)\n-\n-    def testOnly(self):\n-        strainer = SoupStrainer(\"b\")\n-        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)\n-        self.assertEquals(len(soup), 15)\n-\n-        strainer = SoupStrainer(id=re.compile(\"100.*\"))\n-        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)\n-        self.assertEquals(len(soup), 5)\n-\n-        strainer = SoupStrainer(text=re.compile(\"10[01].*\"))\n-        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)\n-        self.assertEquals(len(soup), 10)\n-\n-        strainer = SoupStrainer(text=lambda(x):x[8]=='3')\n-        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)\n-        self.assertEquals(len(soup), 3)\n-\n-class PickleMeThis(SoupTest):\n-    \"Testing features like pickle and deepcopy.\"\n-\n-    def setUp(self):\n-        self.page = \"\"\"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\n-\"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n-<html>\n-<head>\n-<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n-<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n-<link rev=\"made\" href=\"mailto:leonardr@segfault.org\">\n-<meta name=\"Description\" content=\"Beautiful Soup: an HTML parser optimized for screen-scraping.\">\n-<meta name=\"generator\" content=\"Markov Approximation 1.4 (module: leonardr)\">\n-<meta name=\"author\" content=\"Leonard Richardson\">\n-</head>\n-<body>\n-<a href=\"foo\">foo</a>\n-<a href=\"foo\"><b>bar</b></a>\n-</body>\n-</html>\"\"\"\n-\n-        self.soup = BeautifulSoup(self.page)\n-\n-    def testPickle(self):\n-        import pickle\n-        dumped = pickle.dumps(self.soup, 2)\n-        loaded = pickle.loads(dumped)\n-        self.assertEqual(loaded.__class__, BeautifulSoup)\n-        self.assertEqual(str(loaded), str(self.soup))\n-\n-    def testDeepcopy(self):\n-        from copy import deepcopy\n-        copied = deepcopy(self.soup)\n-        self.assertEqual(str(copied), str(self.soup))\n-\n-    def testUnicodePickle(self):\n-        import cPickle as pickle\n-        html = \"<b>\" + chr(0xc3) + \"</b>\"\n-        soup = BeautifulSoup(html)\n-        dumped = pickle.dumps(soup, pickle.HIGHEST_PROTOCOL)\n-        loaded = pickle.loads(dumped)\n-        self.assertEqual(str(loaded), str(soup))\n-\n-\n-class WriteOnlyCode(SoupTest):\n-    \"Testing the modification of the tree.\"\n-\n-    def testModifyAttributes(self):\n-        soup = BeautifulSoup('<a id=\"1\"></a>')\n-        soup.a['id'] = 2\n-        self.assertEqual(soup.renderContents(), '<a id=\"2\"></a>')\n-        del(soup.a['id'])\n-        self.assertEqual(soup.renderContents(), '<a></a>')\n-        soup.a['id2'] = 'foo'\n-        self.assertEqual(soup.renderContents(), '<a id2=\"foo\"></a>')\n-\n-    def testNewTagCreation(self):\n-        \"Makes sure tags don't step on each others' toes.\"\n-        soup = BeautifulSoup()\n-        a = Tag(soup, 'a')\n-        ol = Tag(soup, 'ol')\n-        a['href'] = 'http://foo.com/'\n-        self.assertRaises(KeyError, lambda : ol['href'])\n-\n-    def testNewTagWithAttributes(self):\n-        \"\"\"Makes sure new tags can be created complete with attributes.\"\"\"\n-        soup = BeautifulSoup()\n-        a = Tag(soup, 'a', [('href', 'foo')])\n-        b = Tag(soup, 'b', {'class':'bar'})\n-        soup.insert(0,a)\n-        soup.insert(1,b)\n-        self.assertEqual(soup.a['href'], 'foo')\n-        self.assertEqual(soup.b['class'], 'bar')\n-\n-    def testTagReplacement(self):\n-        # Make sure you can replace an element with itself.\n-        text = \"<a><b></b><c>Foo<d></d></c></a><a><e></e></a>\"\n-        soup = BeautifulSoup(text)\n-        c = soup.c\n-        soup.c.replaceWith(c)\n-        self.assertEquals(str(soup), text)\n-\n-        # A very simple case\n-        soup = BeautifulSoup(\"<b>Argh!</b>\")\n-        soup.find(text=\"Argh!\").replaceWith(\"Hooray!\")\n-        newText = soup.find(text=\"Hooray!\")\n-        b = soup.b\n-        self.assertEqual(newText.previous, b)\n-        self.assertEqual(newText.parent, b)\n-        self.assertEqual(newText.previous.next, newText)\n-        self.assertEqual(newText.next, None)\n-\n-        # A more complex case\n-        soup = BeautifulSoup(\"<a><b>Argh!</b><c></c><d></d></a>\")\n-        soup.b.insert(1, \"Hooray!\")\n-        newText = soup.find(text=\"Hooray!\")\n-        self.assertEqual(newText.previous, \"Argh!\")\n-        self.assertEqual(newText.previous.next, newText)\n-\n-        self.assertEqual(newText.previousSibling, \"Argh!\")\n-        self.assertEqual(newText.previousSibling.nextSibling, newText)\n-\n-        self.assertEqual(newText.nextSibling, None)\n-        self.assertEqual(newText.next, soup.c)\n-\n-        text = \"<html>There's <b>no</b> business like <b>show</b> business</html>\"\n-        soup = BeautifulSoup(text)\n-        no, show = soup.findAll('b')\n-        show.replaceWith(no)\n-        self.assertEquals(str(soup), \"<html>There's  business like <b>no</b> business</html>\")\n-\n-        # Even more complex\n-        soup = BeautifulSoup(\"<a><b>Find</b><c>lady!</c><d></d></a>\")\n-        tag = Tag(soup, 'magictag')\n-        tag.insert(0, \"the\")\n-        soup.a.insert(1, tag)\n-\n-        b = soup.b\n-        c = soup.c\n-        theText = tag.find(text=True)\n-        findText = b.find(text=\"Find\")\n-\n-        self.assertEqual(findText.next, tag)\n-        self.assertEqual(tag.previous, findText)\n-        self.assertEqual(b.nextSibling, tag)\n-        self.assertEqual(tag.previousSibling, b)\n-        self.assertEqual(tag.nextSibling, c)\n-        self.assertEqual(c.previousSibling, tag)\n-\n-        self.assertEqual(theText.next, c)\n-        self.assertEqual(c.previous, theText)\n-\n-        # Aand... incredibly complex.\n-        soup = BeautifulSoup(\"\"\"<a>We<b>reserve<c>the</c><d>right</d></b></a><e>to<f>refuse</f><g>service</g></e>\"\"\")\n-        f = soup.f\n-        a = soup.a\n-        c = soup.c\n-        e = soup.e\n-        weText = a.find(text=\"We\")\n-        soup.b.replaceWith(soup.f)\n-        self.assertEqual(str(soup), \"<a>We<f>refuse</f></a><e>to<g>service</g></e>\")\n-\n-        self.assertEqual(f.previous, weText)\n-        self.assertEqual(weText.next, f)\n-        self.assertEqual(f.previousSibling, weText)\n-        self.assertEqual(f.nextSibling, None)\n-        self.assertEqual(weText.nextSibling, f)\n-\n-    def testReplaceWithChildren(self):\n-        soup = BeautifulStoneSoup(\n-            \"<top><replace><child1/><child2/></replace></top>\",\n-            selfClosingTags=[\"child1\", \"child2\"])\n-        soup.replaceTag.replaceWithChildren()\n-        self.assertEqual(soup.top.contents[0].name, \"child1\")\n-        self.assertEqual(soup.top.contents[1].name, \"child2\")\n-\n-    def testAppend(self):\n-       doc = \"<p>Don't leave me <b>here</b>.</p> <p>Don't leave me.</p>\"\n-       soup = BeautifulSoup(doc)\n-       second_para = soup('p')[1]\n-       bold = soup.find('b')\n-       soup('p')[1].append(soup.find('b'))\n-       self.assertEqual(bold.parent, second_para)\n-       self.assertEqual(str(soup),\n-                        \"<p>Don't leave me .</p> \"\n-                        \"<p>Don't leave me.<b>here</b></p>\")\n-\n-    def testTagExtraction(self):\n-        # A very simple case\n-        text = '<html><div id=\"nav\">Nav crap</div>Real content here.</html>'\n-        soup = BeautifulSoup(text)\n-        extracted = soup.find(\"div\", id=\"nav\").extract()\n-        self.assertEqual(str(soup), \"<html>Real content here.</html>\")\n-        self.assertEqual(str(extracted), '<div id=\"nav\">Nav crap</div>')\n-\n-        # A simple case, a more complex test.\n-        text = \"<doc><a>1<b>2</b></a><a>i<b>ii</b></a><a>A<b>B</b></a></doc>\"\n-        soup = BeautifulStoneSoup(text)\n-        doc = soup.doc\n-        numbers, roman, letters = soup(\"a\")\n-\n-        self.assertEqual(roman.parent, doc)\n-        oldPrevious = roman.previous\n-        endOfThisTag = roman.nextSibling.previous\n-        self.assertEqual(oldPrevious, \"2\")\n-        self.assertEqual(roman.next, \"i\")\n-        self.assertEqual(endOfThisTag, \"ii\")\n-        self.assertEqual(roman.previousSibling, numbers)\n-        self.assertEqual(roman.nextSibling, letters)\n-\n-        roman.extract()\n-        self.assertEqual(roman.parent, None)\n-        self.assertEqual(roman.previous, None)\n-        self.assertEqual(roman.next, \"i\")\n-        self.assertEqual(letters.previous, '2')\n-        self.assertEqual(roman.previousSibling, None)\n-        self.assertEqual(roman.nextSibling, None)\n-        self.assertEqual(endOfThisTag.next, None)\n-        self.assertEqual(roman.b.contents[0].next, None)\n-        self.assertEqual(numbers.nextSibling, letters)\n-        self.assertEqual(letters.previousSibling, numbers)\n-        self.assertEqual(len(doc.contents), 2)\n-        self.assertEqual(doc.contents[0], numbers)\n-        self.assertEqual(doc.contents[1], letters)\n-\n-        # A more complex case.\n-        text = \"<a>1<b>2<c>Hollywood, baby!</c></b></a>3\"\n-        soup = BeautifulStoneSoup(text)\n-        one = soup.find(text=\"1\")\n-        three = soup.find(text=\"3\")\n-        toExtract = soup.b\n-        soup.b.extract()\n-        self.assertEqual(one.next, three)\n-        self.assertEqual(three.previous, one)\n-        self.assertEqual(one.parent.nextSibling, three)\n-        self.assertEqual(three.previousSibling, soup.a)\n-        \n-    def testClear(self):\n-        soup = BeautifulSoup(\"<ul><li></li><li></li></ul>\")\n-        soup.ul.clear()\n-        self.assertEqual(len(soup.ul.contents), 0)\n-\n-class TheManWithoutAttributes(SoupTest):\n-    \"Test attribute access\"\n-\n-    def testHasKey(self):\n-        text = \"<foo attr='bar'>\"\n-        self.assertEquals(BeautifulSoup(text).foo.has_key('attr'), True)\n-\n-class QuoteMeOnThat(SoupTest):\n-    \"Test quoting\"\n-    def testQuotedAttributeValues(self):\n-        self.assertSoupEquals(\"<foo attr='bar'></foo>\",\n-                              '<foo attr=\"bar\"></foo>')\n-\n-        text = \"\"\"<foo attr='bar \"brawls\" happen'>a</foo>\"\"\"\n-        soup = BeautifulSoup(text)\n-        self.assertEquals(soup.renderContents(), text)\n-\n-        soup.foo['attr'] = 'Brawls happen at \"Bob\\'s Bar\"'\n-        newText = \"\"\"<foo attr='Brawls happen at \"Bob&squot;s Bar\"'>a</foo>\"\"\"\n-        self.assertSoupEquals(soup.renderContents(), newText)\n-\n-        self.assertSoupEquals('<this is=\"really messed up & stuff\">',\n-                              '<this is=\"really messed up &amp; stuff\"></this>')\n-\n-        # This is not what the original author had in mind, but it's\n-        # a legitimate interpretation of what they wrote.\n-        self.assertSoupEquals(\"\"\"<a href=\"foo</a>, </a><a href=\"bar\">baz</a>\"\"\",\n-        '<a href=\"foo&lt;/a&gt;, &lt;/a&gt;&lt;a href=\"></a>, <a href=\"bar\">baz</a>')\n-\n-        # SGMLParser generates bogus parse events when attribute values\n-        # contain embedded brackets, but at least Beautiful Soup fixes\n-        # it up a little.\n-        self.assertSoupEquals('<a b=\"<a>\">', '<a b=\"&lt;a&gt;\"></a><a>\"&gt;</a>')\n-        self.assertSoupEquals('<a href=\"http://foo.com/<a> and blah and blah',\n-                              \"\"\"<a href='\"http://foo.com/'></a><a> and blah and blah</a>\"\"\")\n-\n-\n-\n-class YoureSoLiteral(SoupTest):\n-    \"Test literal mode.\"\n-    def testLiteralMode(self):\n-        text = \"<script>if (i<imgs.length)</script><b>Foo</b>\"\n-        soup = BeautifulSoup(text)\n-        self.assertEqual(soup.script.contents[0], \"if (i<imgs.length)\")\n-        self.assertEqual(soup.b.contents[0], \"Foo\")\n-\n-    def testTextArea(self):\n-        text = \"<textarea><b>This is an example of an HTML tag</b><&<&</textarea>\"\n-        soup = BeautifulSoup(text)\n-        self.assertEqual(soup.textarea.contents[0],\n-                         \"<b>This is an example of an HTML tag</b><&<&\")\n-\n-class OperatorOverload(SoupTest):\n-    \"Our operators do it all! Call now!\"\n-\n-    def testTagNameAsFind(self):\n-        \"Tests that referencing a tag name as a member delegates to find().\"\n-        soup = BeautifulSoup('<b id=\"1\">foo<i>bar</i></b><b>Red herring</b>')\n-        self.assertEqual(soup.b.i, soup.find('b').find('i'))\n-        self.assertEqual(soup.b.i.string, 'bar')\n-        self.assertEqual(soup.b['id'], '1')\n-        self.assertEqual(soup.b.contents[0], 'foo')\n-        self.assert_(not soup.a)\n-\n-        #Test the .fooTag variant of .foo.\n-        self.assertEqual(soup.bTag.iTag.string, 'bar')\n-        self.assertEqual(soup.b.iTag.string, 'bar')\n-        self.assertEqual(soup.find('b').find('i'), soup.bTag.iTag)\n-\n-class NestableEgg(SoupTest):\n-    \"\"\"Here we test tag nesting. TEST THE NEST, DUDE! X-TREME!\"\"\"\n-\n-    def testParaInsideBlockquote(self):\n-        soup = BeautifulSoup('<blockquote><p><b>Foo</blockquote><p>Bar')\n-        self.assertEqual(soup.blockquote.p.b.string, 'Foo')\n-        self.assertEqual(soup.blockquote.b.string, 'Foo')\n-        self.assertEqual(soup.find('p', recursive=False).string, 'Bar')\n-\n-    def testNestedTables(self):\n-        text = \"\"\"<table id=\"1\"><tr><td>Here's another table:\n-        <table id=\"2\"><tr><td>Juicy text</td></tr></table></td></tr></table>\"\"\"\n-        soup = BeautifulSoup(text)\n-        self.assertEquals(soup.table.table.td.string, 'Juicy text')\n-        self.assertEquals(len(soup.findAll('table')), 2)\n-        self.assertEquals(len(soup.table.findAll('table')), 1)\n-        self.assertEquals(soup.find('table', {'id' : 2}).parent.parent.parent.name,\n-                          'table')\n-\n-        text = \"<table><tr><td><div><table>Foo</table></div></td></tr></table>\"\n-        soup = BeautifulSoup(text)\n-        self.assertEquals(soup.table.tr.td.div.table.contents[0], \"Foo\")\n-\n-        text = \"\"\"<table><thead><tr>Foo</tr></thead><tbody><tr>Bar</tr></tbody>\n-        <tfoot><tr>Baz</tr></tfoot></table>\"\"\"\n-        soup = BeautifulSoup(text)\n-        self.assertEquals(soup.table.thead.tr.contents[0], \"Foo\")\n-\n-    def testBadNestedTables(self):\n-        soup = BeautifulSoup(\"<table><tr><table><tr id='nested'>\")\n-        self.assertEquals(soup.table.tr.table.tr['id'], 'nested')\n-\n-class CleanupOnAisleFour(SoupTest):\n-    \"\"\"Here we test cleanup of text that breaks SGMLParser or is just\n-    obnoxious.\"\"\"\n-\n-    def testSelfClosingtag(self):\n-        self.assertEqual(str(BeautifulSoup(\"Foo<br/>Bar\").find('br')),\n-                         '<br />')\n-\n-        self.assertSoupEquals('<p>test1<br/>test2</p>',\n-                              '<p>test1<br />test2</p>')\n-\n-        text = '<p>test1<selfclosing>test2'\n-        soup = BeautifulStoneSoup(text)\n-        self.assertEqual(str(soup),\n-                         '<p>test1<selfclosing>test2</selfclosing></p>')\n-\n-        soup = BeautifulStoneSoup(text, selfClosingTags='selfclosing')\n-        self.assertEqual(str(soup),\n-                         '<p>test1<selfclosing />test2</p>')\n-\n-    def testSelfClosingTagOrNot(self):\n-        text = \"<item><link>http://foo.com/</link></item>\"\n-        self.assertEqual(BeautifulStoneSoup(text).renderContents(), text)\n-        self.assertEqual(BeautifulSoup(text).renderContents(),\n-                         '<item><link />http://foo.com/</item>')\n-\n-    def testCData(self):\n-        xml = \"<root>foo<![CDATA[foobar]]>bar</root>\"\n-        self.assertSoupEquals(xml, xml)\n-        r = re.compile(\"foo.*bar\")\n-        soup = BeautifulSoup(xml)\n-        self.assertEquals(soup.find(text=r).string, \"foobar\")\n-        self.assertEquals(soup.find(text=r).__class__, CData)\n-\n-    def testComments(self):\n-        xml = \"foo<!--foobar-->baz\"\n-        self.assertSoupEquals(xml)\n-        r = re.compile(\"foo.*bar\")\n-        soup = BeautifulSoup(xml)\n-        self.assertEquals(soup.find(text=r).string, \"foobar\")\n-        self.assertEquals(soup.find(text=\"foobar\").__class__, Comment)\n-\n-    def testDeclaration(self):\n-        xml = \"foo<!DOCTYPE foobar>baz\"\n-        self.assertSoupEquals(xml)\n-        r = re.compile(\".*foo.*bar\")\n-        soup = BeautifulSoup(xml)\n-        text = \"DOCTYPE foobar\"\n-        self.assertEquals(soup.find(text=r).string, text)\n-        self.assertEquals(soup.find(text=text).__class__, Declaration)\n-\n-        namespaced_doctype = ('<!DOCTYPE xsl:stylesheet SYSTEM \"htmlent.dtd\">'\n-                              '<html>foo</html>')\n-        soup = BeautifulSoup(namespaced_doctype)\n-        self.assertEquals(soup.contents[0],\n-                          'DOCTYPE xsl:stylesheet SYSTEM \"htmlent.dtd\"')\n-        self.assertEquals(soup.html.contents[0], 'foo')\n-\n-    def testEntityConversions(self):\n-        text = \"&lt;&lt;sacr&eacute;&#32;bleu!&gt;&gt;\"\n-        soup = BeautifulStoneSoup(text)\n-        self.assertSoupEquals(text)\n-\n-        xmlEnt = BeautifulStoneSoup.XML_ENTITIES\n-        htmlEnt = BeautifulStoneSoup.HTML_ENTITIES\n-        xhtmlEnt = BeautifulStoneSoup.XHTML_ENTITIES\n-\n-        soup = BeautifulStoneSoup(text, convertEntities=xmlEnt)\n-        self.assertEquals(str(soup), \"&lt;&lt;sacr&eacute; bleu!&gt;&gt;\")\n-\n-        soup = BeautifulStoneSoup(text, convertEntities=htmlEnt)\n-        self.assertEquals(unicode(soup), u\"&lt;&lt;sacr\\xe9 bleu!&gt;&gt;\")\n-\n-        # Make sure the \"XML\", \"HTML\", and \"XHTML\" settings work.\n-        text = \"&lt;&trade;&apos;\"\n-        soup = BeautifulStoneSoup(text, convertEntities=xmlEnt)\n-        self.assertEquals(unicode(soup), u\"&lt;&trade;'\")\n-\n-        soup = BeautifulStoneSoup(text, convertEntities=htmlEnt)\n-        self.assertEquals(unicode(soup), u\"&lt;\\u2122&apos;\")\n-\n-        soup = BeautifulStoneSoup(text, convertEntities=xhtmlEnt)\n-        self.assertEquals(unicode(soup), u\"&lt;\\u2122'\")\n-\n-        invalidEntity = \"foo&#bar;baz\"\n-        soup = BeautifulStoneSoup\\\n-               (invalidEntity,\n-                convertEntities=htmlEnt)\n-        self.assertEquals(str(soup), \"foo&amp;#bar;baz\")\n-\n-        nonexistentEntity = \"foo&bar;baz\"\n-        soup = BeautifulStoneSoup\\\n-               (nonexistentEntity,\n-                convertEntities=\"xml\")\n-        self.assertEquals(str(soup), nonexistentEntity)\n-\n-\n-    def testNonBreakingSpaces(self):\n-        soup = BeautifulSoup(\"<a>&nbsp;&nbsp;</a>\",\n-                             convertEntities=BeautifulStoneSoup.HTML_ENTITIES)\n-        self.assertEquals(unicode(soup), u\"<a>\\xa0\\xa0</a>\")\n-\n-    def testWhitespaceInDeclaration(self):\n-        self.assertSoupEquals('<! DOCTYPE>', '<!DOCTYPE>')\n-\n-    def testJunkInDeclaration(self):\n-        self.assertSoupEquals('<! Foo = -8>a', '&lt;!Foo = -8&gt;a')\n-\n-    def testIncompleteDeclaration(self):\n-        self.assertSoupEquals('a<!b <p>c', 'a&lt;!b &lt;p&gt;c')\n-\n-    def testEntityReplacement(self):\n-        self.assertSoupEquals('<b>hello&nbsp;there</b>')\n-\n-    def testEntitiesInAttributeValues(self):\n-        self.assertSoupEquals('<x t=\"x&#241;\">', '<x t=\"x\\xc3\\xb1\"></x>')\n-        self.assertSoupEquals('<x t=\"x&#xf1;\">', '<x t=\"x\\xc3\\xb1\"></x>')\n-\n-        soup = BeautifulSoup('<x t=\"&gt;&trade;\">',\n-                             convertEntities=BeautifulStoneSoup.HTML_ENTITIES)\n-        self.assertEquals(unicode(soup), u'<x t=\"&gt;\\u2122\"></x>')\n-\n-        uri = \"http://crummy.com?sacr&eacute;&amp;bleu\"\n-        link = '<a href=\"%s\"></a>' % uri\n-        soup = BeautifulSoup(link)\n-        self.assertEquals(unicode(soup), link)\n-        #self.assertEquals(unicode(soup.a['href']), uri)\n-\n-        soup = BeautifulSoup(link, convertEntities=BeautifulSoup.HTML_ENTITIES)\n-        self.assertEquals(unicode(soup),\n-                          link.replace(\"&eacute;\", u\"\\xe9\"))\n-\n-        uri = \"http://crummy.com?sacr&eacute;&bleu\"\n-        link = '<a href=\"%s\"></a>' % uri\n-        soup = BeautifulSoup(link, convertEntities=BeautifulSoup.HTML_ENTITIES)\n-        self.assertEquals(unicode(soup.a['href']),\n-                          uri.replace(\"&eacute;\", u\"\\xe9\"))\n-\n-    def testNakedAmpersands(self):\n-        html = {'convertEntities':BeautifulStoneSoup.HTML_ENTITIES}\n-        soup = BeautifulStoneSoup(\"AT&T \", **html)\n-        self.assertEquals(str(soup), 'AT&amp;T ')\n-\n-        nakedAmpersandInASentence = \"AT&T was Ma Bell\"\n-        soup = BeautifulStoneSoup(nakedAmpersandInASentence,**html)\n-        self.assertEquals(str(soup), \\\n-               nakedAmpersandInASentence.replace('&','&amp;'))\n-\n-        invalidURL = '<a href=\"http://example.org?a=1&b=2;3\">foo</a>'\n-        validURL = invalidURL.replace('&','&amp;')\n-        soup = BeautifulStoneSoup(invalidURL)\n-        self.assertEquals(str(soup), validURL)\n-\n-        soup = BeautifulStoneSoup(validURL)\n-        self.assertEquals(str(soup), validURL)\n-\n-\n-class EncodeRed(SoupTest):\n-    \"\"\"Tests encoding conversion, Unicode conversion, and Microsoft\n-    smart quote fixes.\"\"\"\n-\n-    def testUnicodeDammitStandalone(self):\n-        markup = \"<foo>\\x92</foo>\"\n-        dammit = UnicodeDammit(markup)\n-        self.assertEquals(dammit.unicode, \"<foo>&#x2019;</foo>\")\n-\n-        hebrew = \"\\xed\\xe5\\xec\\xf9\"\n-        dammit = UnicodeDammit(hebrew, [\"iso-8859-8\"])\n-        self.assertEquals(dammit.unicode, u'\\u05dd\\u05d5\\u05dc\\u05e9')\n-        self.assertEquals(dammit.originalEncoding, 'iso-8859-8')\n-\n-    def testGarbageInGarbageOut(self):\n-        ascii = \"<foo>a</foo>\"\n-        asciiSoup = BeautifulStoneSoup(ascii)\n-        self.assertEquals(ascii, str(asciiSoup))\n-\n-        unicodeData = u\"<foo>\\u00FC</foo>\"\n-        utf8 = unicodeData.encode(\"utf-8\")\n-        self.assertEquals(utf8, '<foo>\\xc3\\xbc</foo>')\n-\n-        unicodeSoup = BeautifulStoneSoup(unicodeData)\n-        self.assertEquals(unicodeData, unicode(unicodeSoup))\n-        self.assertEquals(unicode(unicodeSoup.foo.string), u'\\u00FC')\n-\n-        utf8Soup = BeautifulStoneSoup(utf8, fromEncoding='utf-8')\n-        self.assertEquals(utf8, str(utf8Soup))\n-        self.assertEquals(utf8Soup.originalEncoding, \"utf-8\")\n-\n-        utf8Soup = BeautifulStoneSoup(unicodeData)\n-        self.assertEquals(utf8, str(utf8Soup))\n-        self.assertEquals(utf8Soup.originalEncoding, None)\n-\n-\n-    def testHandleInvalidCodec(self):\n-        for bad_encoding in ['.utf8', '...', 'utF---16.!']:\n-            soup = BeautifulSoup(\"R\u00e4ksm\u00f6rg\u00e5s\", fromEncoding=bad_encoding)\n-            self.assertEquals(soup.originalEncoding, 'utf-8')\n-\n-    def testUnicodeSearch(self):\n-        html = u'<html><body><h1>R\u00e4ksm\u00f6rg\u00e5s</h1></body></html>'\n-        soup = BeautifulSoup(html)\n-        self.assertEqual(soup.find(text=u'R\u00e4ksm\u00f6rg\u00e5s'),u'R\u00e4ksm\u00f6rg\u00e5s')\n-\n-    def testRewrittenXMLHeader(self):\n-        euc_jp = '<?xml version=\"1.0 encoding=\"euc-jp\"?>\\n<foo>\\n\\xa4\\xb3\\xa4\\xec\\xa4\\xcfEUC-JP\\xa4\\xc7\\xa5\\xb3\\xa1\\xbc\\xa5\\xc7\\xa5\\xa3\\xa5\\xf3\\xa5\\xb0\\xa4\\xb5\\xa4\\xec\\xa4\\xbf\\xc6\\xfc\\xcb\\xdc\\xb8\\xec\\xa4\\xce\\xa5\\xd5\\xa5\\xa1\\xa5\\xa4\\xa5\\xeb\\xa4\\xc7\\xa4\\xb9\\xa1\\xa3\\n</foo>\\n'\n-        utf8 = \"<?xml version='1.0' encoding='utf-8'?>\\n<foo>\\n\\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xafEUC-JP\\xe3\\x81\\xa7\\xe3\\x82\\xb3\\xe3\\x83\\xbc\\xe3\\x83\\x87\\xe3\\x82\\xa3\\xe3\\x83\\xb3\\xe3\\x82\\xb0\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x81\\x9f\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe8\\xaa\\x9e\\xe3\\x81\\xae\\xe3\\x83\\x95\\xe3\\x82\\xa1\\xe3\\x82\\xa4\\xe3\\x83\\xab\\xe3\\x81\\xa7\\xe3\\x81\\x99\\xe3\\x80\\x82\\n</foo>\\n\"\n-        soup = BeautifulStoneSoup(euc_jp)\n-        if soup.originalEncoding != \"euc-jp\":\n-            raise Exception(\"Test failed when parsing euc-jp document. \"\n-                            \"If you're running Python >=2.4, or you have \"\n-                            \"cjkcodecs installed, this is a real problem. \"\n-                            \"Otherwise, ignore it.\")\n-\n-        self.assertEquals(soup.originalEncoding, \"euc-jp\")\n-        self.assertEquals(str(soup), utf8)\n-\n-        old_text = \"<?xml encoding='windows-1252'><foo>\\x92</foo>\"\n-        new_text = \"<?xml version='1.0' encoding='utf-8'?><foo>&rsquo;</foo>\"\n-        self.assertSoupEquals(old_text, new_text)\n-\n-    def testRewrittenMetaTag(self):\n-        no_shift_jis_html = '''<html><head>\\n<meta http-equiv=\"Content-language\" content=\"ja\" /></head><body><pre>\\n\\x82\\xb1\\x82\\xea\\x82\\xcdShift-JIS\\x82\\xc5\\x83R\\x81[\\x83f\\x83B\\x83\\x93\\x83O\\x82\\xb3\\x82\\xea\\x82\\xbd\\x93\\xfa\\x96{\\x8c\\xea\\x82\\xcc\\x83t\\x83@\\x83C\\x83\\x8b\\x82\\xc5\\x82\\xb7\\x81B\\n</pre></body></html>'''\n-        soup = BeautifulSoup(no_shift_jis_html)\n-\n-        # Beautiful Soup used to try to rewrite the meta tag even if the\n-        # meta tag got filtered out by the strainer. This test makes\n-        # sure that doesn't happen.\n-        strainer = SoupStrainer('pre')\n-        soup = BeautifulSoup(no_shift_jis_html, parseOnlyThese=strainer)\n-        self.assertEquals(soup.contents[0].name, 'pre')\n-\n-        meta_tag = ('<meta content=\"text/html; charset=x-sjis\" '\n-                    'http-equiv=\"Content-type\" />')\n-        shift_jis_html = (\n-            '<html><head>\\n%s\\n'\n-            '<meta http-equiv=\"Content-language\" content=\"ja\" />'\n-            '</head><body><pre>\\n'\n-            '\\x82\\xb1\\x82\\xea\\x82\\xcdShift-JIS\\x82\\xc5\\x83R\\x81[\\x83f'\n-            '\\x83B\\x83\\x93\\x83O\\x82\\xb3\\x82\\xea\\x82\\xbd\\x93\\xfa\\x96{\\x8c'\n-            '\\xea\\x82\\xcc\\x83t\\x83@\\x83C\\x83\\x8b\\x82\\xc5\\x82\\xb7\\x81B\\n'\n-            '</pre></body></html>') % meta_tag\n-        soup = BeautifulSoup(shift_jis_html)\n-        if soup.originalEncoding != \"shift-jis\":\n-            raise Exception(\"Test failed when parsing shift-jis document \"\n-                            \"with meta tag '%s'.\"\n-                            \"If you're running Python >=2.4, or you have \"\n-                            \"cjkcodecs installed, this is a real problem. \"\n-                            \"Otherwise, ignore it.\" % meta_tag)\n-        self.assertEquals(soup.originalEncoding, \"shift-jis\")\n-\n-        content_type_tag = soup.meta['content']\n-        self.assertEquals(content_type_tag[content_type_tag.find('charset='):],\n-                          'charset=%SOUP-ENCODING%')\n-        content_type = str(soup.meta)\n-        index = content_type.find('charset=')\n-        self.assertEqual(content_type[index:index+len('charset=utf8')+1],\n-                         'charset=utf-8')\n-        content_type = soup.meta.__str__('shift-jis')\n-        index = content_type.find('charset=')\n-        self.assertEqual(content_type[index:index+len('charset=shift-jis')],\n-                         'charset=shift-jis')\n-\n-        self.assertEquals(str(soup), (\n-                '<html><head>\\n'\n-                '<meta content=\"text/html; charset=utf-8\" '\n-                'http-equiv=\"Content-type\" />\\n'\n-                '<meta http-equiv=\"Content-language\" content=\"ja\" />'\n-                '</head><body><pre>\\n'\n-                '\\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xafShift-JIS\\xe3\\x81\\xa7\\xe3'\n-                '\\x82\\xb3\\xe3\\x83\\xbc\\xe3\\x83\\x87\\xe3\\x82\\xa3\\xe3\\x83\\xb3\\xe3'\n-                '\\x82\\xb0\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x81\\x9f\\xe6\\x97\\xa5\\xe6'\n-                '\\x9c\\xac\\xe8\\xaa\\x9e\\xe3\\x81\\xae\\xe3\\x83\\x95\\xe3\\x82\\xa1\\xe3'\n-                '\\x82\\xa4\\xe3\\x83\\xab\\xe3\\x81\\xa7\\xe3\\x81\\x99\\xe3\\x80\\x82\\n'\n-                '</pre></body></html>'))\n-        self.assertEquals(soup.renderContents(\"shift-jis\"),\n-                          shift_jis_html.replace('x-sjis', 'shift-jis'))\n-\n-        isolatin =\"\"\"<html><meta http-equiv=\"Content-type\" content=\"text/html; charset=ISO-Latin-1\" />Sacr\\xe9 bleu!</html>\"\"\"\n-        soup = BeautifulSoup(isolatin)\n-        self.assertSoupEquals(soup.__str__(\"utf-8\"),\n-                              isolatin.replace(\"ISO-Latin-1\", \"utf-8\").replace(\"\\xe9\", \"\\xc3\\xa9\"))\n-\n-    def testHebrew(self):\n-        iso_8859_8= '<HEAD>\\n<TITLE>Hebrew (ISO 8859-8) in Visual Directionality</TITLE>\\n\\n\\n\\n</HEAD>\\n<BODY>\\n<H1>Hebrew (ISO 8859-8) in Visual Directionality</H1>\\n\\xed\\xe5\\xec\\xf9\\n</BODY>\\n'\n-        utf8 = '<head>\\n<title>Hebrew (ISO 8859-8) in Visual Directionality</title>\\n</head>\\n<body>\\n<h1>Hebrew (ISO 8859-8) in Visual Directionality</h1>\\n\\xd7\\x9d\\xd7\\x95\\xd7\\x9c\\xd7\\xa9\\n</body>\\n'\n-        soup = BeautifulStoneSoup(iso_8859_8, fromEncoding=\"iso-8859-8\")\n-        self.assertEquals(str(soup), utf8)\n-\n-    def testSmartQuotesNotSoSmartAnymore(self):\n-        self.assertSoupEquals(\"\\x91Foo\\x92 <!--blah-->\",\n-                              '&lsquo;Foo&rsquo; <!--blah-->')\n-\n-    def testDontConvertSmartQuotesWhenAlsoConvertingEntities(self):\n-        smartQuotes = \"Il a dit, \\x8BSacr&eacute; bl&#101;u!\\x9b\"\n-        soup = BeautifulSoup(smartQuotes)\n-        self.assertEquals(str(soup),\n-                          'Il a dit, &lsaquo;Sacr&eacute; bl&#101;u!&rsaquo;')\n-        soup = BeautifulSoup(smartQuotes, convertEntities=\"html\")\n-        self.assertEquals(str(soup),\n-                          'Il a dit, \\xe2\\x80\\xb9Sacr\\xc3\\xa9 bleu!\\xe2\\x80\\xba')\n-\n-    def testDontSeeSmartQuotesWhereThereAreNone(self):\n-        utf_8 = \"\\343\\202\\261\\343\\203\\274\\343\\202\\277\\343\\202\\244 Watch\"\n-        self.assertSoupEquals(utf_8)\n-\n-\n-class Whitewash(SoupTest):\n-    \"\"\"Test whitespace preservation.\"\"\"\n-\n-    def testPreservedWhitespace(self):\n-        self.assertSoupEquals(\"<pre>   </pre>\")\n-        self.assertSoupEquals(\"<pre> woo  </pre>\")\n-\n-    def testCollapsedWhitespace(self):\n-        self.assertSoupEquals(\"<p>   </p>\", \"<p> </p>\")\n-\n-\n-if __name__ == '__main__':\n-    unittest.main()"
            },
            {
                "sha": "6198f516e48a50a85826af93d0d1a85be38b5679",
                "filename": "github-analytics/libs/BeautifulSoup-3.2.2/README.md",
                "status": "removed",
                "additions": 0,
                "deletions": 18,
                "changes": 18,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/BeautifulSoup-3.2.2/README.md?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -1,18 +0,0 @@\n-Beautiful Soup is a library that makes it easy to scrape information\n-from web pages. It sits atop an HTML or XML parser, providing Pythonic\n-idioms for iterating, searching, and modifying the parse tree.\n-\n-# Discontinuation notice\n-\n-You should use the 'beautifulsoup4' package instead of this package.\n-\n-Development on the 3.x series of Beautiful Soup ended in 2011, and the\n-series will be discontinued on January 1, 2021, one year after the\n-Python 2 sunsetting date. At some point after that, the\n-'beautifulsoup' pip package will be updated to a recent version of\n-Beautiful Soup. This will free up the 'beautifulsoup' package name to\n-be used by a more recent release.\n-\n-If you're relying on version 3 of Beautiful Soup, you really ought to\n-port your code to Python 3. A relatively small part of this work will\n-be migrating your Beautiful Soup code to Beautiful Soup 4."
            },
            {
                "sha": "4413a904de5a5fa8d93a225de178e79ae9930227",
                "filename": "github-analytics/libs/BeautifulSoup-3.2.2/setup.py",
                "status": "removed",
                "additions": 0,
                "deletions": 36,
                "changes": 36,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/setup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/setup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/BeautifulSoup-3.2.2/setup.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -1,36 +0,0 @@\n-# The very first thing we do is give a useful error if someone is\n-# running this code under Python 3.\n-\"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"<>\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n-\n-from setuptools import (\n-    setup,\n-    find_packages,\n-)\n-from BeautifulSoup import __version__\n-\n-with open(\"README.md\", \"r\") as fh:\n-    long_description = fh.read()\n-\n-setup(\n-    name=\"BeautifulSoup\",\n-    version = __version__,\n-    author=\"Leonard Richardson\",\n-    author_email='leonardr@segfault.org',\n-    url=\"http://www.crummy.com/software/BeautifulSoup/\",\n-    download_url = \"http://www.crummy.com/software/BeautifulSoup/download/\",\n-    description=\"Screen-scraping library\",\n-    long_description=long_description,\n-    long_description_content_type=\"text/markdown\",\n-    license=\"MIT\",\n-    py_modules=['BeautifulSoup', 'BeautifulSoupTests'],\n-    classifiers=[\"Development Status :: 5 - Production/Stable\",\n-                 \"Intended Audience :: Developers\",\n-                 \"License :: OSI Approved :: Python Software Foundation License\",\n-                 \"Programming Language :: Python\",\n-                 \"Programming Language :: Python :: 2.7\",\n-                 \"Topic :: Text Processing :: Markup :: HTML\",\n-                 \"Topic :: Text Processing :: Markup :: XML\",\n-                 \"Topic :: Text Processing :: Markup :: SGML\",\n-                 \"Topic :: Software Development :: Libraries :: Python Modules\",\n-             ],\n-)"
            }
        ]
    },
    {
        "sha": "58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjU4ZDAzN2MwYzllMDM2NTFiMzIxNmFjMGEwZTUyNWFhODRiZmE5ZmQ=",
        "commit": {
            "author": {
                "name": "Shreyas Gopalakrishna",
                "email": "11889130+shreyas-gopalakrishna@users.noreply.github.com",
                "date": "2020-04-18T19:13:43Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-18T19:13:43Z"
            },
            "message": "Merge pull request #2 from CUBigDataClass/shreyas\n\nAnalytics on collaborators, issues, languages and contributions with multithreading.",
            "tree": {
                "sha": "ac434b4c56ec63c6b1c49114fc71b4f951387d08",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/ac434b4c56ec63c6b1c49114fc71b4f951387d08"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJem1FnCRBK7hj4Ov3rIwAAdHIIAGkLJWZ9tG50F1fFO5S7pbhX\nvdYl2s9hBAMZ9wLlTzCuQK6dwKj+mtohwTptVIls1eOU26YXShYgqtgPrG+G5vp8\nSA/Qer5a8OPN8CiOBH3ZvMdakznPHaMgNQwU2E4EwiZWQ+QYtJ1idPoeZQike5Rk\nZBPMD38PbUPLhKjJLGJvlNkr5GoZM2PT6SCcOD3nilwOdIgPiyGdpDbiHOFb9bxV\ne79YBiKCJlnxA022sxiKjOeQB89ZF21GulOgasg7tB8oGVBD8k2203DObNtCa1X4\nXQasoVrdaiZKGgCAZphkAoU+UaJgm1EFtpSPc+pbjrlypH1rhDpNN8fDjje5CyA=\n=LU4R\n-----END PGP SIGNATURE-----\n",
                "payload": "tree ac434b4c56ec63c6b1c49114fc71b4f951387d08\nparent bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14\nparent e500cf3ea83383cf4a9c6e8522a0710a042b2785\nauthor Shreyas Gopalakrishna <11889130+shreyas-gopalakrishna@users.noreply.github.com> 1587237223 -0600\ncommitter GitHub <noreply@github.com> 1587237223 -0600\n\nMerge pull request #2 from CUBigDataClass/shreyas\n\nAnalytics on collaborators, issues, languages and contributions with multithreading."
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14"
            },
            {
                "sha": "e500cf3ea83383cf4a9c6e8522a0710a042b2785",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/e500cf3ea83383cf4a9c6e8522a0710a042b2785",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/e500cf3ea83383cf4a9c6e8522a0710a042b2785"
            }
        ],
        "stats": {
            "total": 14187,
            "additions": 14169,
            "deletions": 18
        },
        "files": [
            {
                "sha": "4dcd3773728d9efe07d1417a28bae11c076a319e",
                "filename": "github-analytics/CassandraHelper/CassandraOrgData.py",
                "status": "added",
                "additions": 24,
                "deletions": 0,
                "changes": 24,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/CassandraHelper/CassandraOrgData.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/CassandraHelper/CassandraOrgData.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/CassandraOrgData.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,24 @@\n+class CassandraOrgData():\n+    def __init__(self, elasticOrgData):\n+        data = elasticOrgData['hits']['hits'][0]['_source']\n+        self.data = {\n+            \"avatar_url\": data['avatar_url'],\n+            \"description\": data['description'],\n+            \"email\": data['email'],\n+            \"followers\": data['followers'],\n+            \"following\": data['following'],\n+            \"html_url\": data['html_url'],\n+            \"id\": data['id'],\n+            \"issues_url\": data['issues_url'],\n+            \"location\": data['location'],\n+            \"login\": data['login'],\n+            \"members_url\": data['members_url'],\n+            \"name\": data['name'],\n+            \"node_id\": data['node_id'],\n+            \"public_members_url\": data['public_members_url'],\n+            \"public_repos\": data['public_repos'],\n+            \"repos_url\": data['repos_url'],\n+            \"type\": data['type'],\n+            \"updated_at\": data['updated_at'],\n+            \"url\": data['url']\n+        }\n\\ No newline at end of file"
            },
            {
                "sha": "a1ab0c3daca63c456c2f25214b2dcd67b4c6254b",
                "filename": "github-analytics/CassandraHelper/CassandraRepoData.py",
                "status": "added",
                "additions": 38,
                "deletions": 0,
                "changes": 38,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/CassandraHelper/CassandraRepoData.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/CassandraHelper/CassandraRepoData.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/CassandraRepoData.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,38 @@\n+from CassandraHelper import Utils as utils\n+from CassandraHelper import config\n+from multiprocessing import Pool\n+\n+class CassandraRepoData():\n+    def __init__(self, elasticOrgData):\n+        dataList = elasticOrgData['hits']['hits']\n+        threadPool = Pool(config.THREAD_COUNT)\n+        self.data = threadPool.map(self.processDataList, dataList)\n+\n+    def processDataList(self, repoDataItem):\n+        repoDataItem = repoDataItem['_source']\n+        repoData = {\n+            \"contributors\": utils.get(utils.getContributorsList, repoDataItem['contributors_url']),\n+            \"name\": repoDataItem['name'],\n+            \"commits\": utils.get(utils.getCommitsList, repoDataItem['commits_url'][:-6]),\n+            \"created_at\": repoDataItem['created_at'],\n+            \"issues\": utils.get(utils.getIssuesList, repoDataItem['issues_url'][:-9]),\n+            \"id\": repoDataItem['id'],\n+            \"watchers_count\": repoDataItem['watchers_count'],\n+            \"description\": repoDataItem['description'],\n+            \"forks_count\": repoDataItem['forks_count'],\n+            \"forks_url\": repoDataItem['forks_url'],\n+            \"full_name\": repoDataItem['full_name'],\n+            \"html_url\": repoDataItem['html_url'],\n+            \"languages\": utils.get(utils.getLanguages, repoDataItem['languages_url'], False),\n+            \"owner\": {\n+                \"name\": repoDataItem['owner']['login'],\n+                \"avatar_url\": repoDataItem['owner']['avatar_url'],\n+                \"html_url\": repoDataItem['owner']['html_url'],\n+                \"id\": repoDataItem['owner']['id'],\n+                \"organizations_url\": repoDataItem['owner']['organizations_url']\n+            },\n+            \"open_issues_count\": repoDataItem['open_issues_count'],\n+            \"tags_url\": repoDataItem['tags_url'],\n+            \"updated_at\": repoDataItem['updated_at']\n+        }\n+        return repoData\n\\ No newline at end of file"
            },
            {
                "sha": "7667996b0eeedf8911c2984ef82241821fc8958c",
                "filename": "github-analytics/CassandraHelper/Utils.py",
                "status": "added",
                "additions": 57,
                "deletions": 0,
                "changes": 57,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/CassandraHelper/Utils.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/CassandraHelper/Utils.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/Utils.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,57 @@\n+import requests\n+from requests.auth import HTTPBasicAuth\n+from CassandraHelper import config\n+import json\n+\n+# Http get request with callback\n+def get(function, url, isOutputlist=True):\n+    try:\n+        r = requests.get(url, auth=HTTPBasicAuth(config.GITHUB_USER, config.GITHUB_TOKEN))\n+        return function(r.json())\n+    except:\n+        if(not isOutputlist):\n+            return {}\n+        return []\n+\n+def getContributorsList(data):\n+    contributorsList = list()\n+    for i in range(len(data)):\n+        contributor = dict()\n+        contributor['name'] = data[i]['login']\n+        contributor['id'] = data[i]['id']\n+        contributor['avatar_url'] = data[i]['avatar_url']\n+        contributor['html_url'] = data[i]['html_url']\n+        contributor['contributions'] = data[i]['contributions']\n+        contributorsList.append(contributor)\n+    return contributorsList\n+\n+def getLanguages(data):\n+    return data\n+\n+def getCommitsList(data):\n+    commitsList = list()\n+    for i in range(len(data)):\n+        commit = dict()\n+        commit['sha'] = data[i]['sha']\n+        commit['message'] = data[i]['commit']['message']\n+        commit['date'] = data[i]['commit']['committer']['date']\n+        commit['commiter-name'] = data[i]['committer']['login']\n+        commit['commiter-id'] = data[i]['committer']['id']\n+        commitsList.append(commit)\n+    return commitsList\n+\n+def getIssuesList(data):\n+    issuesList = list()\n+    for i in range(len(data)):\n+        issue = dict()\n+        issue['id'] = data[i]['id']\n+        issue['number'] = data[i]['number']\n+        issue['title'] = data[i]['title']\n+        issue['body'] = data[i]['body']\n+        issue['state'] = data[i]['state']\n+        issue['created_at'] = data[i]['created_at']\n+        issue['updated_at'] = data[i]['updated_at']\n+        issue['closed_at'] = data[i]['closed_at']\n+        issue['labels'] = data[i]['labels']\n+        issuesList.append(issue)\n+    return issuesList\n\\ No newline at end of file"
            },
            {
                "sha": "8f55617b58c4563a6a5da005d1229eaffbdaee4d",
                "filename": "github-analytics/CassandraHelper/config.py",
                "status": "modified",
                "additions": 6,
                "deletions": 1,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/CassandraHelper/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/CassandraHelper/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/config.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -1,6 +1,11 @@\n+# Cassandra Config\n CASSANDRA_HOST = 'localhost'\n CASSANDRA_PORT = 5000\n \n CASSANDRA_URL = \"http://\" + CASSANDRA_HOST + \":\" + str(CASSANDRA_PORT) + \"/\"\n \n-CASSANDRA_URL_INSERT = CASSANDRA_URL + \"insert/\"\n\\ No newline at end of file\n+CASSANDRA_URL_INSERT = CASSANDRA_URL + \"insert/\"\n+\n+# Github Config\n+GITHUB_USER = \"shreyas-gopalakrishna\"\n+GITHUB_TOKEN = \"f02831928492a6bfe9f1bca49693f69d1d0e44b3\"\n\\ No newline at end of file"
            },
            {
                "sha": "0eda6e2c71d08288d68fa6edec8496bba73e6100",
                "filename": "github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "status": "modified",
                "additions": 7,
                "deletions": 9,
                "changes": 16,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -9,17 +9,15 @@ class ElasticSearchHelper():\n     def __init__(self):\n         pass\n \n-    def getOrgData(self,orgname):\n-        res = es.search(index=orgname, body={\"query\": {\"match_all\": {}}})\n-        print(res)\n+    def getOrgData(self, orgname):\n+        res = es.search(index='org1', body={\"query\": {\"match\": {\"login\":orgname}}, \"_source\": config.ELASTIC_ORG_DATA_FIELDS, 'size': 1000})\n         return res\n \n-    def getUserData(self,username):\n+    def getUserData(self, username):\n         res = es.search(index=username, body={\"query\": {\"match_all\": {}}})\n-        print(res)\n         return res\n \n-    def getRepoData(self,reponame):\n-        res = es.search(index=reponame, body={\"query\": {\"match_all\": {}}})\n-        print(res)\n-        return res\n\\ No newline at end of file\n+    def getRepoData(self, orgname):\n+        res = es.search(index='repos', body={\"query\": {\"match\": {\"owner.login\": orgname}}, \"_source\": config.ELASTIC_REPO_DATA_FIELDS, 'size': 1000})\n+        return res\n+"
            },
            {
                "sha": "caa2c54fb74dfc8565f153fae7cf1c4360113ae0",
                "filename": "github-analytics/ElasticSearchHelper/config.py",
                "status": "modified",
                "additions": 5,
                "deletions": 1,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/ElasticSearchHelper/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/ElasticSearchHelper/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/config.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -1,2 +1,6 @@\n ELASTIC_HOST = 'localhost'\n-ELASTIC_PORT = 9200\n\\ No newline at end of file\n+ELASTIC_PORT = 9200\n+\n+ELASTIC_ORG_DATA_FIELDS = [\"_id\",\"avatar_url\",\"description\",\"email\",\"followers\",\"following\",\"html_url\",\"id\",\"issues_url\",\"location\",\"members_url\",\"name\",\"node_id\",\"login\",\"url\",\"updated_at\",\"repos_url\",\"public_repos\",\"public_members_url\",\"type\"]\n+\n+ELASTIC_REPO_DATA_FIELDS = [\"contributors_url\",\"name\",\"owner.login\",\"owner.organizations_url\",\"commits_url\",\"created_at\",\"issues_url\",\"id\",\"watchers_count\",\"description\",\"forks_count\",\"forks_url\",\"full_name\",\"html_url\",\"languages_url\",\"open_issues_count\",\"owner.avatar_url\",\"owner.html_url\",\"owner.id\",\"tags_url\",\"updated_at\"]"
            },
            {
                "sha": "2482d9d60342674ac10f1e906bef0ea6edc22798",
                "filename": "github-analytics/app.py",
                "status": "modified",
                "additions": 23,
                "deletions": 7,
                "changes": 30,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/app.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -1,28 +1,44 @@\n from flask import Flask\n+from flask import jsonify\n \n from config import config\n from ElasticSearchHelper import ElasticSearchHelper as esh\n from CassandraHelper import CassandraHelper as ch\n+from CassandraHelper import CassandraOrgData as cod\n+from CassandraHelper import CassandraRepoData as crd\n+import json\n \n app = Flask(__name__)\n \n elasticSearchHelper = esh.ElasticSearchHelper()\n cassandraHelper = ch.CassandraHelper()\n \n+\n @app.route('/')\n def home():\n-    return 'Github Analytics - Use APIs to process data'\n+    return 'Github Analytics - Use APIs to process data. /org/{organization_name}'\n+\n \n @app.route('/org/<orgname>')\n-def org_retrive(orgname):\n-    return elasticSearchHelper.getOrgData(orgname)\n+def orgRetrieve(orgname):\n+    elasticOrgData = elasticSearchHelper.getOrgData(orgname)\n+    # return elasticOrgData\n+    cassandraOrgData = cod.CassandraOrgData(elasticOrgData)\n+    return cassandraOrgData.data\n+    # return cassandraHelper.insertOrgData(cassandraOrgData.data)\n+\n+\n+@app.route('/repo/<orgname>')\n+def repoRetrieve(orgname):\n+    elasticRepoData = elasticSearchHelper.getRepoData(orgname)\n+    # return elasticRepoData\n+    cassandraRepoData = crd.CassandraRepoData(elasticRepoData)\n+    print(\"Done Done!\")\n+    return jsonify(cassandraRepoData.data)\n \n-@app.route('/repo/<reponame>')\n-def repo_retrive(reponame):\n-    return elasticSearchHelper.getRepoData(reponame)\n \n @app.route('/user/<username>')\n-def user_retrive(username):\n+def userRetrieve(username):\n     return elasticSearchHelper.getUserData(username)\n \n "
            },
            {
                "sha": "f53fa24e39ce5b4abc2f3182231f90efbfec62a2",
                "filename": "github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoup.py",
                "status": "added",
                "additions": 2032,
                "deletions": 0,
                "changes": 2032,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoup.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,2032 @@\n+\"\"\"Beautiful Soup\n+Elixir and Tonic\n+\"The Screen-Scraper's Friend\"\n+http://www.crummy.com/software/BeautifulSoup/\n+\n+Beautiful Soup parses a (possibly invalid) XML or HTML document into a\n+tree representation. It provides methods and Pythonic idioms that make\n+it easy to navigate, search, and modify the tree.\n+\n+A well-formed XML/HTML document yields a well-formed data\n+structure. An ill-formed XML/HTML document yields a correspondingly\n+ill-formed data structure. If your document is only locally\n+well-formed, you can use this library to find and process the\n+well-formed part of it.\n+\n+Beautiful Soup works with Python 2.2 and up. It has no external\n+dependencies, but you'll have more success at converting data to UTF-8\n+if you also install these three packages:\n+\n+* chardet, for auto-detecting character encodings\n+  http://chardet.feedparser.org/\n+* cjkcodecs and iconv_codec, which add more encodings to the ones supported\n+  by stock Python.\n+  http://cjkpython.i18n.org/\n+\n+Beautiful Soup defines classes for two main parsing strategies:\n+\n+ * BeautifulStoneSoup, for parsing XML, SGML, or your domain-specific\n+   language that kind of looks like XML.\n+\n+ * BeautifulSoup, for parsing run-of-the-mill HTML code, be it valid\n+   or invalid. This class has web browser-like heuristics for\n+   obtaining a sensible parse tree in the face of common HTML errors.\n+\n+Beautiful Soup also defines a class (UnicodeDammit) for autodetecting\n+the encoding of an HTML or XML document, and converting it to\n+Unicode. Much of this code is taken from Mark Pilgrim's Universal Feed Parser.\n+\n+For more than you ever wanted to know about Beautiful Soup, see the\n+documentation:\n+http://www.crummy.com/software/BeautifulSoup/documentation.html\n+\n+Here, have some legalese:\n+\n+Copyright (c) 2004-2019, Leonard Richardson\n+\n+All rights reserved.\n+\n+Redistribution and use in source and binary forms, with or without\n+modification, are permitted provided that the following conditions are\n+met:\n+\n+  * Redistributions of source code must retain the above copyright\n+    notice, this list of conditions and the following disclaimer.\n+\n+  * Redistributions in binary form must reproduce the above\n+    copyright notice, this list of conditions and the following\n+    disclaimer in the documentation and/or other materials provided\n+    with the distribution.\n+\n+  * Neither the name of the the Beautiful Soup Consortium and All\n+    Night Kosher Bakery nor the names of its contributors may be\n+    used to endorse or promote products derived from this software\n+    without specific prior written permission.\n+\n+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n+\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n+CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n+EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n+PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n+PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n+LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n+NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE, DAMMIT.\n+\n+\"\"\"\n+from __future__ import generators\n+\n+__author__ = \"Leonard Richardson (leonardr@segfault.org)\"\n+__version__ = \"3.2.2\"\n+__copyright__ = \"Copyright (c) 2004-2019 Leonard Richardson\"\n+__license__ = \"New-style BSD\"\n+\n+from sgmllib import SGMLParser, SGMLParseError\n+import codecs\n+import markupbase\n+import types\n+import re\n+import sgmllib\n+try:\n+  from htmlentitydefs import name2codepoint\n+except ImportError:\n+  name2codepoint = {}\n+try:\n+    set\n+except NameError:\n+    from sets import Set as set\n+\n+# The very first thing we do is give a useful error if someone is\n+# running this code under Python 3.\n+\"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"<>\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n+\n+# Everyone else gets a deprecation warning.\n+import warnings\n+warnings.warn(\"\"\"You are using a very old release of Beautiful Soup, last updated in 2011. If you installed the 'beautifulsoup' package through pip, you should know the 'beautifulsoup' package name is about to be reclaimed by a more recent version of Beautiful Soup which is incompatible with this version.\n+\n+This will happen at some point after January 1, 2021.\n+\n+If you just started this project, this is easy to fix. Install the 'beautifulsoup4' package instead of 'beautifulsoup' and start using Beautiful Soup 4.\n+\n+If this is an existing project that depends on Beautiful Soup 3, the project maintainer (potentially you) needs to start the process of migrating to Beautiful Soup 4. This should be a relatively easy part of the Python 3 migration.\n+\"\"\")\n+\n+#These hacks make Beautiful Soup able to parse XML with namespaces\n+sgmllib.tagfind = re.compile('[a-zA-Z][-_.:a-zA-Z0-9]*')\n+markupbase._declname_match = re.compile(r'[a-zA-Z][-_.:a-zA-Z0-9]*\\s*').match\n+\n+DEFAULT_OUTPUT_ENCODING = \"utf-8\"\n+\n+def _match_css_class(str):\n+    \"\"\"Build a RE to match the given CSS class.\"\"\"\n+    return re.compile(r\"(^|.*\\s)%s($|\\s)\" % str)\n+\n+# First, the classes that represent markup elements.\n+\n+class PageElement(object):\n+    \"\"\"Contains the navigational information for some part of the page\n+    (either a tag or a piece of text)\"\"\"\n+\n+    def _invert(h):\n+        \"Cheap function to invert a hash.\"\n+        i = {}\n+        for k,v in h.items():\n+            i[v] = k\n+        return i\n+\n+    XML_ENTITIES_TO_SPECIAL_CHARS = { \"apos\" : \"'\",\n+                                      \"quot\" : '\"',\n+                                      \"amp\" : \"&\",\n+                                      \"lt\" : \"<\",\n+                                      \"gt\" : \">\" }\n+\n+    XML_SPECIAL_CHARS_TO_ENTITIES = _invert(XML_ENTITIES_TO_SPECIAL_CHARS)\n+\n+    def setup(self, parent=None, previous=None):\n+        \"\"\"Sets up the initial relations between this element and\n+        other elements.\"\"\"\n+        self.parent = parent\n+        self.previous = previous\n+        self.next = None\n+        self.previousSibling = None\n+        self.nextSibling = None\n+        if self.parent and self.parent.contents:\n+            self.previousSibling = self.parent.contents[-1]\n+            self.previousSibling.nextSibling = self\n+\n+    def replaceWith(self, replaceWith):\n+        oldParent = self.parent\n+        myIndex = self.parent.index(self)\n+        if hasattr(replaceWith, \"parent\")\\\n+                  and replaceWith.parent is self.parent:\n+            # We're replacing this element with one of its siblings.\n+            index = replaceWith.parent.index(replaceWith)\n+            if index and index < myIndex:\n+                # Furthermore, it comes before this element. That\n+                # means that when we extract it, the index of this\n+                # element will change.\n+                myIndex = myIndex - 1\n+        self.extract()\n+        oldParent.insert(myIndex, replaceWith)\n+\n+    def replaceWithChildren(self):\n+        myParent = self.parent\n+        myIndex = self.parent.index(self)\n+        self.extract()\n+        reversedChildren = list(self.contents)\n+        reversedChildren.reverse()\n+        for child in reversedChildren:\n+            myParent.insert(myIndex, child)\n+\n+    def extract(self):\n+        \"\"\"Destructively rips this element out of the tree.\"\"\"\n+        if self.parent:\n+            try:\n+                del self.parent.contents[self.parent.index(self)]\n+            except ValueError:\n+                pass\n+\n+        #Find the two elements that would be next to each other if\n+        #this element (and any children) hadn't been parsed. Connect\n+        #the two.\n+        lastChild = self._lastRecursiveChild()\n+        nextElement = lastChild.next\n+\n+        if self.previous:\n+            self.previous.next = nextElement\n+        if nextElement:\n+            nextElement.previous = self.previous\n+        self.previous = None\n+        lastChild.next = None\n+\n+        self.parent = None\n+        if self.previousSibling:\n+            self.previousSibling.nextSibling = self.nextSibling\n+        if self.nextSibling:\n+            self.nextSibling.previousSibling = self.previousSibling\n+        self.previousSibling = self.nextSibling = None\n+        return self\n+\n+    def _lastRecursiveChild(self):\n+        \"Finds the last element beneath this object to be parsed.\"\n+        lastChild = self\n+        while hasattr(lastChild, 'contents') and lastChild.contents:\n+            lastChild = lastChild.contents[-1]\n+        return lastChild\n+\n+    def insert(self, position, newChild):\n+        if isinstance(newChild, basestring) \\\n+            and not isinstance(newChild, NavigableString):\n+            newChild = NavigableString(newChild)\n+\n+        position =  min(position, len(self.contents))\n+        if hasattr(newChild, 'parent') and newChild.parent is not None:\n+            # We're 'inserting' an element that's already one\n+            # of this object's children.\n+            if newChild.parent is self:\n+                index = self.index(newChild)\n+                if index > position:\n+                    # Furthermore we're moving it further down the\n+                    # list of this object's children. That means that\n+                    # when we extract this element, our target index\n+                    # will jump down one.\n+                    position = position - 1\n+            newChild.extract()\n+\n+        newChild.parent = self\n+        previousChild = None\n+        if position == 0:\n+            newChild.previousSibling = None\n+            newChild.previous = self\n+        else:\n+            previousChild = self.contents[position-1]\n+            newChild.previousSibling = previousChild\n+            newChild.previousSibling.nextSibling = newChild\n+            newChild.previous = previousChild._lastRecursiveChild()\n+        if newChild.previous:\n+            newChild.previous.next = newChild\n+\n+        newChildsLastElement = newChild._lastRecursiveChild()\n+\n+        if position >= len(self.contents):\n+            newChild.nextSibling = None\n+\n+            parent = self\n+            parentsNextSibling = None\n+            while not parentsNextSibling:\n+                parentsNextSibling = parent.nextSibling\n+                parent = parent.parent\n+                if not parent: # This is the last element in the document.\n+                    break\n+            if parentsNextSibling:\n+                newChildsLastElement.next = parentsNextSibling\n+            else:\n+                newChildsLastElement.next = None\n+        else:\n+            nextChild = self.contents[position]\n+            newChild.nextSibling = nextChild\n+            if newChild.nextSibling:\n+                newChild.nextSibling.previousSibling = newChild\n+            newChildsLastElement.next = nextChild\n+\n+        if newChildsLastElement.next:\n+            newChildsLastElement.next.previous = newChildsLastElement\n+        self.contents.insert(position, newChild)\n+\n+    def append(self, tag):\n+        \"\"\"Appends the given tag to the contents of this tag.\"\"\"\n+        self.insert(len(self.contents), tag)\n+\n+    def findNext(self, name=None, attrs={}, text=None, **kwargs):\n+        \"\"\"Returns the first item that matches the given criteria and\n+        appears after this Tag in the document.\"\"\"\n+        return self._findOne(self.findAllNext, name, attrs, text, **kwargs)\n+\n+    def findAllNext(self, name=None, attrs={}, text=None, limit=None,\n+                    **kwargs):\n+        \"\"\"Returns all items that match the given criteria and appear\n+        after this Tag in the document.\"\"\"\n+        return self._findAll(name, attrs, text, limit, self.nextGenerator,\n+                             **kwargs)\n+\n+    def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n+        \"\"\"Returns the closest sibling to this Tag that matches the\n+        given criteria and appears after this Tag in the document.\"\"\"\n+        return self._findOne(self.findNextSiblings, name, attrs, text,\n+                             **kwargs)\n+\n+    def findNextSiblings(self, name=None, attrs={}, text=None, limit=None,\n+                         **kwargs):\n+        \"\"\"Returns the siblings of this Tag that match the given\n+        criteria and appear after this Tag in the document.\"\"\"\n+        return self._findAll(name, attrs, text, limit,\n+                             self.nextSiblingGenerator, **kwargs)\n+    fetchNextSiblings = findNextSiblings # Compatibility with pre-3.x\n+\n+    def findPrevious(self, name=None, attrs={}, text=None, **kwargs):\n+        \"\"\"Returns the first item that matches the given criteria and\n+        appears before this Tag in the document.\"\"\"\n+        return self._findOne(self.findAllPrevious, name, attrs, text, **kwargs)\n+\n+    def findAllPrevious(self, name=None, attrs={}, text=None, limit=None,\n+                        **kwargs):\n+        \"\"\"Returns all items that match the given criteria and appear\n+        before this Tag in the document.\"\"\"\n+        return self._findAll(name, attrs, text, limit, self.previousGenerator,\n+                           **kwargs)\n+    fetchPrevious = findAllPrevious # Compatibility with pre-3.x\n+\n+    def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):\n+        \"\"\"Returns the closest sibling to this Tag that matches the\n+        given criteria and appears before this Tag in the document.\"\"\"\n+        return self._findOne(self.findPreviousSiblings, name, attrs, text,\n+                             **kwargs)\n+\n+    def findPreviousSiblings(self, name=None, attrs={}, text=None,\n+                             limit=None, **kwargs):\n+        \"\"\"Returns the siblings of this Tag that match the given\n+        criteria and appear before this Tag in the document.\"\"\"\n+        return self._findAll(name, attrs, text, limit,\n+                             self.previousSiblingGenerator, **kwargs)\n+    fetchPreviousSiblings = findPreviousSiblings # Compatibility with pre-3.x\n+\n+    def findParent(self, name=None, attrs={}, **kwargs):\n+        \"\"\"Returns the closest parent of this Tag that matches the given\n+        criteria.\"\"\"\n+        # NOTE: We can't use _findOne because findParents takes a different\n+        # set of arguments.\n+        r = None\n+        l = self.findParents(name, attrs, 1)\n+        if l:\n+            r = l[0]\n+        return r\n+\n+    def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n+        \"\"\"Returns the parents of this Tag that match the given\n+        criteria.\"\"\"\n+\n+        return self._findAll(name, attrs, None, limit, self.parentGenerator,\n+                             **kwargs)\n+    fetchParents = findParents # Compatibility with pre-3.x\n+\n+    #These methods do the real heavy lifting.\n+\n+    def _findOne(self, method, name, attrs, text, **kwargs):\n+        r = None\n+        l = method(name, attrs, text, 1, **kwargs)\n+        if l:\n+            r = l[0]\n+        return r\n+\n+    def _findAll(self, name, attrs, text, limit, generator, **kwargs):\n+        \"Iterates over a generator looking for things that match.\"\n+\n+        if isinstance(name, SoupStrainer):\n+            strainer = name\n+        # (Possibly) special case some findAll*(...) searches\n+        elif text is None and not limit and not attrs and not kwargs:\n+            # findAll*(True)\n+            if name is True:\n+                return [element for element in generator()\n+                        if isinstance(element, Tag)]\n+            # findAll*('tag-name')\n+            elif isinstance(name, basestring):\n+                return [element for element in generator()\n+                        if isinstance(element, Tag) and\n+                        element.name == name]\n+            else:\n+                strainer = SoupStrainer(name, attrs, text, **kwargs)\n+        # Build a SoupStrainer\n+        else:\n+            strainer = SoupStrainer(name, attrs, text, **kwargs)\n+        results = ResultSet(strainer)\n+        g = generator()\n+        while True:\n+            try:\n+                i = g.next()\n+            except StopIteration:\n+                break\n+            if i:\n+                found = strainer.search(i)\n+                if found:\n+                    results.append(found)\n+                    if limit and len(results) >= limit:\n+                        break\n+        return results\n+\n+    #These Generators can be used to navigate starting from both\n+    #NavigableStrings and Tags.\n+    def nextGenerator(self):\n+        i = self\n+        while i is not None:\n+            i = i.next\n+            yield i\n+\n+    def nextSiblingGenerator(self):\n+        i = self\n+        while i is not None:\n+            i = i.nextSibling\n+            yield i\n+\n+    def previousGenerator(self):\n+        i = self\n+        while i is not None:\n+            i = i.previous\n+            yield i\n+\n+    def previousSiblingGenerator(self):\n+        i = self\n+        while i is not None:\n+            i = i.previousSibling\n+            yield i\n+\n+    def parentGenerator(self):\n+        i = self\n+        while i is not None:\n+            i = i.parent\n+            yield i\n+\n+    # Utility methods\n+    def substituteEncoding(self, str, encoding=None):\n+        encoding = encoding or \"utf-8\"\n+        return str.replace(\"%SOUP-ENCODING%\", encoding)\n+\n+    def toEncoding(self, s, encoding=None):\n+        \"\"\"Encodes an object to a string in some encoding, or to Unicode.\n+        .\"\"\"\n+        if isinstance(s, unicode):\n+            if encoding:\n+                s = s.encode(encoding)\n+        elif isinstance(s, str):\n+            if encoding:\n+                s = s.encode(encoding)\n+            else:\n+                s = unicode(s)\n+        else:\n+            if encoding:\n+                s  = self.toEncoding(str(s), encoding)\n+            else:\n+                s = unicode(s)\n+        return s\n+\n+    BARE_AMPERSAND_OR_BRACKET = re.compile(\"([<>]|\"\n+                                           + \"&(?!#\\d+;|#x[0-9a-fA-F]+;|\\w+;)\"\n+                                           + \")\")\n+\n+    def _sub_entity(self, x):\n+        \"\"\"Used with a regular expression to substitute the\n+        appropriate XML entity for an XML special character.\"\"\"\n+        return \"&\" + self.XML_SPECIAL_CHARS_TO_ENTITIES[x.group(0)[0]] + \";\"\n+\n+\n+class NavigableString(unicode, PageElement):\n+\n+    def __new__(cls, value):\n+        \"\"\"Create a new NavigableString.\n+\n+        When unpickling a NavigableString, this method is called with\n+        the string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be\n+        passed in to the superclass's __new__ or the superclass won't know\n+        how to handle non-ASCII characters.\n+        \"\"\"\n+        if isinstance(value, unicode):\n+            return unicode.__new__(cls, value)\n+        return unicode.__new__(cls, value, DEFAULT_OUTPUT_ENCODING)\n+\n+    def __getnewargs__(self):\n+        return (NavigableString.__str__(self),)\n+\n+    def __getattr__(self, attr):\n+        \"\"\"text.string gives you text. This is for backwards\n+        compatibility for Navigable*String, but for CData* it lets you\n+        get the string without the CData wrapper.\"\"\"\n+        if attr == 'string':\n+            return self\n+        else:\n+            raise AttributeError, \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, attr)\n+\n+    def __unicode__(self):\n+        return str(self).decode(DEFAULT_OUTPUT_ENCODING)\n+\n+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        # Substitute outgoing XML entities.\n+        data = self.BARE_AMPERSAND_OR_BRACKET.sub(self._sub_entity, self)\n+        if encoding:\n+            return data.encode(encoding)\n+        else:\n+            return data\n+\n+class CData(NavigableString):\n+\n+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        return \"<![CDATA[%s]]>\" % NavigableString.__str__(self, encoding)\n+\n+class ProcessingInstruction(NavigableString):\n+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        output = self\n+        if \"%SOUP-ENCODING%\" in output:\n+            output = self.substituteEncoding(output, encoding)\n+        return \"<?%s?>\" % self.toEncoding(output, encoding)\n+\n+class Comment(NavigableString):\n+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        return \"<!--%s-->\" % NavigableString.__str__(self, encoding)\n+\n+class Declaration(NavigableString):\n+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        return \"<!%s>\" % NavigableString.__str__(self, encoding)\n+\n+class Tag(PageElement):\n+\n+    \"\"\"Represents a found HTML tag with its attributes and contents.\"\"\"\n+\n+    def _convertEntities(self, match):\n+        \"\"\"Used in a call to re.sub to replace HTML, XML, and numeric\n+        entities with the appropriate Unicode characters. If HTML\n+        entities are being converted, any unrecognized entities are\n+        escaped.\"\"\"\n+        x = match.group(1)\n+        if self.convertHTMLEntities and x in name2codepoint:\n+            return unichr(name2codepoint[x])\n+        elif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:\n+            if self.convertXMLEntities:\n+                return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]\n+            else:\n+                return u'&%s;' % x\n+        elif len(x) > 0 and x[0] == '#':\n+            # Handle numeric entities\n+            if len(x) > 1 and x[1] == 'x':\n+                return unichr(int(x[2:], 16))\n+            else:\n+                return unichr(int(x[1:]))\n+\n+        elif self.escapeUnrecognizedEntities:\n+            return u'&amp;%s;' % x\n+        else:\n+            return u'&%s;' % x\n+\n+    def __init__(self, parser, name, attrs=None, parent=None,\n+                 previous=None):\n+        \"Basic constructor.\"\n+\n+        # We don't actually store the parser object: that lets extracted\n+        # chunks be garbage-collected\n+        self.parserClass = parser.__class__\n+        self.isSelfClosing = parser.isSelfClosingTag(name)\n+        self.name = name\n+        if attrs is None:\n+            attrs = []\n+        elif isinstance(attrs, dict):\n+            attrs = attrs.items()\n+        self.attrs = attrs\n+        self.contents = []\n+        self.setup(parent, previous)\n+        self.hidden = False\n+        self.containsSubstitutions = False\n+        self.convertHTMLEntities = parser.convertHTMLEntities\n+        self.convertXMLEntities = parser.convertXMLEntities\n+        self.escapeUnrecognizedEntities = parser.escapeUnrecognizedEntities\n+\n+        # Convert any HTML, XML, or numeric entities in the attribute values.\n+        convert = lambda(k, val): (k,\n+                                   re.sub(\"&(#\\d+|#x[0-9a-fA-F]+|\\w+);\",\n+                                          self._convertEntities,\n+                                          val))\n+        self.attrs = map(convert, self.attrs)\n+\n+    def getString(self):\n+        if (len(self.contents) == 1\n+            and isinstance(self.contents[0], NavigableString)):\n+            return self.contents[0]\n+\n+    def setString(self, string):\n+        \"\"\"Replace the contents of the tag with a string\"\"\"\n+        self.clear()\n+        self.append(string)\n+\n+    string = property(getString, setString)\n+\n+    def getText(self, separator=u\"\"):\n+        if not len(self.contents):\n+            return u\"\"\n+        stopNode = self._lastRecursiveChild().next\n+        strings = []\n+        current = self.contents[0]\n+        while current is not stopNode:\n+            if isinstance(current, NavigableString):\n+                strings.append(current.strip())\n+            current = current.next\n+        return separator.join(strings)\n+\n+    text = property(getText)\n+\n+    def get(self, key, default=None):\n+        \"\"\"Returns the value of the 'key' attribute for the tag, or\n+        the value given for 'default' if it doesn't have that\n+        attribute.\"\"\"\n+        return self._getAttrMap().get(key, default)\n+\n+    def clear(self):\n+        \"\"\"Extract all children.\"\"\"\n+        for child in self.contents[:]:\n+            child.extract()\n+\n+    def index(self, element):\n+        for i, child in enumerate(self.contents):\n+            if child is element:\n+                return i\n+        raise ValueError(\"Tag.index: element not in tag\")\n+\n+    def has_key(self, key):\n+        return self._getAttrMap().has_key(key)\n+\n+    def __getitem__(self, key):\n+        \"\"\"tag[key] returns the value of the 'key' attribute for the tag,\n+        and throws an exception if it's not there.\"\"\"\n+        return self._getAttrMap()[key]\n+\n+    def __iter__(self):\n+        \"Iterating over a tag iterates over its contents.\"\n+        return iter(self.contents)\n+\n+    def __len__(self):\n+        \"The length of a tag is the length of its list of contents.\"\n+        return len(self.contents)\n+\n+    def __contains__(self, x):\n+        return x in self.contents\n+\n+    def __nonzero__(self):\n+        \"A tag is non-None even if it has no contents.\"\n+        return True\n+\n+    def __setitem__(self, key, value):\n+        \"\"\"Setting tag[key] sets the value of the 'key' attribute for the\n+        tag.\"\"\"\n+        self._getAttrMap()\n+        self.attrMap[key] = value\n+        found = False\n+        for i in range(0, len(self.attrs)):\n+            if self.attrs[i][0] == key:\n+                self.attrs[i] = (key, value)\n+                found = True\n+        if not found:\n+            self.attrs.append((key, value))\n+        self._getAttrMap()[key] = value\n+\n+    def __delitem__(self, key):\n+        \"Deleting tag[key] deletes all 'key' attributes for the tag.\"\n+        for item in self.attrs:\n+            if item[0] == key:\n+                self.attrs.remove(item)\n+                #We don't break because bad HTML can define the same\n+                #attribute multiple times.\n+            self._getAttrMap()\n+            if self.attrMap.has_key(key):\n+                del self.attrMap[key]\n+\n+    def __call__(self, *args, **kwargs):\n+        \"\"\"Calling a tag like a function is the same as calling its\n+        findAll() method. Eg. tag('a') returns a list of all the A tags\n+        found within this tag.\"\"\"\n+        return apply(self.findAll, args, kwargs)\n+\n+    def __getattr__(self, tag):\n+        #print \"Getattr %s.%s\" % (self.__class__, tag)\n+        if len(tag) > 3 and tag.rfind('Tag') == len(tag)-3:\n+            return self.find(tag[:-3])\n+        elif tag.find('__') != 0:\n+            return self.find(tag)\n+        raise AttributeError, \"'%s' object has no attribute '%s'\" % (self.__class__, tag)\n+\n+    def __eq__(self, other):\n+        \"\"\"Returns true iff this tag has the same name, the same attributes,\n+        and the same contents (recursively) as the given tag.\n+\n+        NOTE: right now this will return false if two tags have the\n+        same attributes in a different order. Should this be fixed?\"\"\"\n+        if other is self:\n+            return True\n+        if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n+            return False\n+        for i in range(0, len(self.contents)):\n+            if self.contents[i] != other.contents[i]:\n+                return False\n+        return True\n+\n+    def __ne__(self, other):\n+        \"\"\"Returns true iff this tag is not identical to the other tag,\n+        as defined in __eq__.\"\"\"\n+        return not self == other\n+\n+    def __repr__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        \"\"\"Renders this tag as a string.\"\"\"\n+        return self.__str__(encoding)\n+\n+    def __unicode__(self):\n+        return self.__str__(None)\n+\n+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING,\n+                prettyPrint=False, indentLevel=0):\n+        \"\"\"Returns a string or Unicode representation of this tag and\n+        its contents. To get Unicode, pass None for encoding.\n+\n+        NOTE: since Python's HTML parser consumes whitespace, this\n+        method is not certain to reproduce the whitespace present in\n+        the original string.\"\"\"\n+\n+        encodedName = self.toEncoding(self.name, encoding)\n+\n+        attrs = []\n+        if self.attrs:\n+            for key, val in self.attrs:\n+                fmt = '%s=\"%s\"'\n+                if isinstance(val, basestring):\n+                    if self.containsSubstitutions and '%SOUP-ENCODING%' in val:\n+                        val = self.substituteEncoding(val, encoding)\n+\n+                    # The attribute value either:\n+                    #\n+                    # * Contains no embedded double quotes or single quotes.\n+                    #   No problem: we enclose it in double quotes.\n+                    # * Contains embedded single quotes. No problem:\n+                    #   double quotes work here too.\n+                    # * Contains embedded double quotes. No problem:\n+                    #   we enclose it in single quotes.\n+                    # * Embeds both single _and_ double quotes. This\n+                    #   can't happen naturally, but it can happen if\n+                    #   you modify an attribute value after parsing\n+                    #   the document. Now we have a bit of a\n+                    #   problem. We solve it by enclosing the\n+                    #   attribute in single quotes, and escaping any\n+                    #   embedded single quotes to XML entities.\n+                    if '\"' in val:\n+                        fmt = \"%s='%s'\"\n+                        if \"'\" in val:\n+                            # TODO: replace with apos when\n+                            # appropriate.\n+                            val = val.replace(\"'\", \"&squot;\")\n+\n+                    # Now we're okay w/r/t quotes. But the attribute\n+                    # value might also contain angle brackets, or\n+                    # ampersands that aren't part of entities. We need\n+                    # to escape those to XML entities too.\n+                    val = self.BARE_AMPERSAND_OR_BRACKET.sub(self._sub_entity, val)\n+\n+                attrs.append(fmt % (self.toEncoding(key, encoding),\n+                                    self.toEncoding(val, encoding)))\n+        close = ''\n+        closeTag = ''\n+        if self.isSelfClosing:\n+            close = ' /'\n+        else:\n+            closeTag = '</%s>' % encodedName\n+\n+        indentTag, indentContents = 0, 0\n+        if prettyPrint:\n+            indentTag = indentLevel\n+            space = (' ' * (indentTag-1))\n+            indentContents = indentTag + 1\n+        contents = self.renderContents(encoding, prettyPrint, indentContents)\n+        if self.hidden:\n+            s = contents\n+        else:\n+            s = []\n+            attributeString = ''\n+            if attrs:\n+                attributeString = ' ' + ' '.join(attrs)\n+            if prettyPrint:\n+                s.append(space)\n+            s.append('<%s%s%s>' % (encodedName, attributeString, close))\n+            if prettyPrint:\n+                s.append(\"\\n\")\n+            s.append(contents)\n+            if prettyPrint and contents and contents[-1] != \"\\n\":\n+                s.append(\"\\n\")\n+            if prettyPrint and closeTag:\n+                s.append(space)\n+            s.append(closeTag)\n+            if prettyPrint and closeTag and self.nextSibling:\n+                s.append(\"\\n\")\n+            s = ''.join(s)\n+        return s\n+\n+    def decompose(self):\n+        \"\"\"Recursively destroys the contents of this tree.\"\"\"\n+        self.extract()\n+        if len(self.contents) == 0:\n+            return\n+        current = self.contents[0]\n+        while current is not None:\n+            next = current.next\n+            if isinstance(current, Tag):\n+                del current.contents[:]\n+            current.parent = None\n+            current.previous = None\n+            current.previousSibling = None\n+            current.next = None\n+            current.nextSibling = None\n+            current = next\n+\n+    def prettify(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        return self.__str__(encoding, True)\n+\n+    def renderContents(self, encoding=DEFAULT_OUTPUT_ENCODING,\n+                       prettyPrint=False, indentLevel=0):\n+        \"\"\"Renders the contents of this tag as a string in the given\n+        encoding. If encoding is None, returns a Unicode string..\"\"\"\n+        s=[]\n+        for c in self:\n+            text = None\n+            if isinstance(c, NavigableString):\n+                text = c.__str__(encoding)\n+            elif isinstance(c, Tag):\n+                s.append(c.__str__(encoding, prettyPrint, indentLevel))\n+            if text and prettyPrint:\n+                text = text.strip()\n+            if text:\n+                if prettyPrint:\n+                    s.append(\" \" * (indentLevel-1))\n+                s.append(text)\n+                if prettyPrint:\n+                    s.append(\"\\n\")\n+        return ''.join(s)\n+\n+    #Soup methods\n+\n+    def find(self, name=None, attrs={}, recursive=True, text=None,\n+             **kwargs):\n+        \"\"\"Return only the first child of this Tag matching the given\n+        criteria.\"\"\"\n+        r = None\n+        l = self.findAll(name, attrs, recursive, text, 1, **kwargs)\n+        if l:\n+            r = l[0]\n+        return r\n+    findChild = find\n+\n+    def findAll(self, name=None, attrs={}, recursive=True, text=None,\n+                limit=None, **kwargs):\n+        \"\"\"Extracts a list of Tag objects that match the given\n+        criteria.  You can specify the name of the Tag and any\n+        attributes you want the Tag to have.\n+\n+        The value of a key-value pair in the 'attrs' map can be a\n+        string, a list of strings, a regular expression object, or a\n+        callable that takes a string and returns whether or not the\n+        string matches for some custom definition of 'matches'. The\n+        same is true of the tag name.\"\"\"\n+        generator = self.recursiveChildGenerator\n+        if not recursive:\n+            generator = self.childGenerator\n+        return self._findAll(name, attrs, text, limit, generator, **kwargs)\n+    findChildren = findAll\n+\n+    # Pre-3.x compatibility methods\n+    first = find\n+    fetch = findAll\n+\n+    def fetchText(self, text=None, recursive=True, limit=None):\n+        return self.findAll(text=text, recursive=recursive, limit=limit)\n+\n+    def firstText(self, text=None, recursive=True):\n+        return self.find(text=text, recursive=recursive)\n+\n+    #Private methods\n+\n+    def _getAttrMap(self):\n+        \"\"\"Initializes a map representation of this tag's attributes,\n+        if not already initialized.\"\"\"\n+        if not getattr(self, 'attrMap'):\n+            self.attrMap = {}\n+            for (key, value) in self.attrs:\n+                self.attrMap[key] = value\n+        return self.attrMap\n+\n+    #Generator methods\n+    def childGenerator(self):\n+        # Just use the iterator from the contents\n+        return iter(self.contents)\n+\n+    def recursiveChildGenerator(self):\n+        if not len(self.contents):\n+            raise StopIteration\n+        stopNode = self._lastRecursiveChild().next\n+        current = self.contents[0]\n+        while current is not stopNode:\n+            yield current\n+            current = current.next\n+\n+\n+# Next, a couple classes to represent queries and their results.\n+class SoupStrainer:\n+    \"\"\"Encapsulates a number of ways of matching a markup element (tag or\n+    text).\"\"\"\n+\n+    def __init__(self, name=None, attrs={}, text=None, **kwargs):\n+        self.name = name\n+        if isinstance(attrs, basestring):\n+            kwargs['class'] = _match_css_class(attrs)\n+            attrs = None\n+        if kwargs:\n+            if attrs:\n+                attrs = attrs.copy()\n+                attrs.update(kwargs)\n+            else:\n+                attrs = kwargs\n+        self.attrs = attrs\n+        self.text = text\n+\n+    def __str__(self):\n+        if self.text:\n+            return self.text\n+        else:\n+            return \"%s|%s\" % (self.name, self.attrs)\n+\n+    def searchTag(self, markupName=None, markupAttrs={}):\n+        found = None\n+        markup = None\n+        if isinstance(markupName, Tag):\n+            markup = markupName\n+            markupAttrs = markup\n+        callFunctionWithTagData = callable(self.name) \\\n+                                and not isinstance(markupName, Tag)\n+\n+        if (not self.name) \\\n+               or callFunctionWithTagData \\\n+               or (markup and self._matches(markup, self.name)) \\\n+               or (not markup and self._matches(markupName, self.name)):\n+            if callFunctionWithTagData:\n+                match = self.name(markupName, markupAttrs)\n+            else:\n+                match = True\n+                markupAttrMap = None\n+                for attr, matchAgainst in self.attrs.items():\n+                    if not markupAttrMap:\n+                         if hasattr(markupAttrs, 'get'):\n+                            markupAttrMap = markupAttrs\n+                         else:\n+                            markupAttrMap = {}\n+                            for k,v in markupAttrs:\n+                                markupAttrMap[k] = v\n+                    attrValue = markupAttrMap.get(attr)\n+                    if not self._matches(attrValue, matchAgainst):\n+                        match = False\n+                        break\n+            if match:\n+                if markup:\n+                    found = markup\n+                else:\n+                    found = markupName\n+        return found\n+\n+    def search(self, markup):\n+        #print 'looking for %s in %s' % (self, markup)\n+        found = None\n+        # If given a list of items, scan it for a text element that\n+        # matches.\n+        if hasattr(markup, \"__iter__\") \\\n+                and not isinstance(markup, Tag):\n+            for element in markup:\n+                if isinstance(element, NavigableString) \\\n+                       and self.search(element):\n+                    found = element\n+                    break\n+        # If it's a Tag, make sure its name or attributes match.\n+        # Don't bother with Tags if we're searching for text.\n+        elif isinstance(markup, Tag):\n+            if not self.text:\n+                found = self.searchTag(markup)\n+        # If it's text, make sure the text matches.\n+        elif isinstance(markup, NavigableString) or \\\n+                 isinstance(markup, basestring):\n+            if self._matches(markup, self.text):\n+                found = markup\n+        else:\n+            raise Exception, \"I don't know how to match against a %s\" \\\n+                  % markup.__class__\n+        return found\n+\n+    def _matches(self, markup, matchAgainst):\n+        #print \"Matching %s against %s\" % (markup, matchAgainst)\n+        result = False\n+        if matchAgainst is True:\n+            result = markup is not None\n+        elif callable(matchAgainst):\n+            result = matchAgainst(markup)\n+        else:\n+            #Custom match methods take the tag as an argument, but all\n+            #other ways of matching match the tag name as a string.\n+            if isinstance(markup, Tag):\n+                markup = markup.name\n+            if markup and not isinstance(markup, basestring):\n+                markup = unicode(markup)\n+            #Now we know that chunk is either a string, or None.\n+            if hasattr(matchAgainst, 'match'):\n+                # It's a regexp object.\n+                result = markup and matchAgainst.search(markup)\n+            elif hasattr(matchAgainst, '__iter__'): # list-like\n+                result = markup in matchAgainst\n+            elif hasattr(matchAgainst, 'items'):\n+                result = markup.has_key(matchAgainst)\n+            elif matchAgainst and isinstance(markup, basestring):\n+                if isinstance(markup, unicode):\n+                    matchAgainst = unicode(matchAgainst)\n+                else:\n+                    matchAgainst = str(matchAgainst)\n+\n+            if not result:\n+                result = matchAgainst == markup\n+        return result\n+\n+class ResultSet(list):\n+    \"\"\"A ResultSet is just a list that keeps track of the SoupStrainer\n+    that created it.\"\"\"\n+    def __init__(self, source):\n+        list.__init__([])\n+        self.source = source\n+\n+# Now, some helper functions.\n+\n+def buildTagMap(default, *args):\n+    \"\"\"Turns a list of maps, lists, or scalars into a single map.\n+    Used to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\n+    NESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n+    built = {}\n+    for portion in args:\n+        if hasattr(portion, 'items'):\n+            #It's a map. Merge it.\n+            for k,v in portion.items():\n+                built[k] = v\n+        elif hasattr(portion, '__iter__'): # is a list\n+            #It's a list. Map each item to the default.\n+            for k in portion:\n+                built[k] = default\n+        else:\n+            #It's a scalar. Map it to the default.\n+            built[portion] = default\n+    return built\n+\n+# Now, the parser classes.\n+\n+class BeautifulStoneSoup(Tag, SGMLParser):\n+\n+    \"\"\"This class contains the basic parser and search code. It defines\n+    a parser that knows nothing about tag behavior except for the\n+    following:\n+\n+      You can't close a tag without closing all the tags it encloses.\n+      That is, \"<foo><bar></foo>\" actually means\n+      \"<foo><bar></bar></foo>\".\n+\n+    [Another possible explanation is \"<foo><bar /></foo>\", but since\n+    this class defines no SELF_CLOSING_TAGS, it will never use that\n+    explanation.]\n+\n+    This class is useful for parsing XML or made-up markup languages,\n+    or when BeautifulSoup makes an assumption counter to what you were\n+    expecting.\"\"\"\n+\n+    SELF_CLOSING_TAGS = {}\n+    NESTABLE_TAGS = {}\n+    RESET_NESTING_TAGS = {}\n+    QUOTE_TAGS = {}\n+    PRESERVE_WHITESPACE_TAGS = []\n+\n+    MARKUP_MASSAGE = [(re.compile('(<[^<>]*)/>'),\n+                       lambda x: x.group(1) + ' />'),\n+                      (re.compile('<!\\s+([^<>]*)>'),\n+                       lambda x: '<!' + x.group(1) + '>')\n+                      ]\n+\n+    ROOT_TAG_NAME = u'[document]'\n+\n+    HTML_ENTITIES = \"html\"\n+    XML_ENTITIES = \"xml\"\n+    XHTML_ENTITIES = \"xhtml\"\n+    # TODO: This only exists for backwards-compatibility\n+    ALL_ENTITIES = XHTML_ENTITIES\n+\n+    # Used when determining whether a text node is all whitespace and\n+    # can be replaced with a single space. A text node that contains\n+    # fancy Unicode spaces (usually non-breaking) should be left\n+    # alone.\n+    STRIP_ASCII_SPACES = { 9: None, 10: None, 12: None, 13: None, 32: None, }\n+\n+    def __init__(self, markup=\"\", parseOnlyThese=None, fromEncoding=None,\n+                 markupMassage=True, smartQuotesTo=XML_ENTITIES,\n+                 convertEntities=None, selfClosingTags=None, isHTML=False):\n+        \"\"\"The Soup object is initialized as the 'root tag', and the\n+        provided markup (which can be a string or a file-like object)\n+        is fed into the underlying parser.\n+\n+        sgmllib will process most bad HTML, and the BeautifulSoup\n+        class has some tricks for dealing with some HTML that kills\n+        sgmllib, but Beautiful Soup can nonetheless choke or lose data\n+        if your data uses self-closing tags or declarations\n+        incorrectly.\n+\n+        By default, Beautiful Soup uses regexes to sanitize input,\n+        avoiding the vast majority of these problems. If the problems\n+        don't apply to you, pass in False for markupMassage, and\n+        you'll get better performance.\n+\n+        The default parser massage techniques fix the two most common\n+        instances of invalid HTML that choke sgmllib:\n+\n+         <br/> (No space between name of closing tag and tag close)\n+         <! --Comment--> (Extraneous whitespace in declaration)\n+\n+        You can pass in a custom list of (RE object, replace method)\n+        tuples to get Beautiful Soup to scrub your input the way you\n+        want.\"\"\"\n+\n+        self.parseOnlyThese = parseOnlyThese\n+        self.fromEncoding = fromEncoding\n+        self.smartQuotesTo = smartQuotesTo\n+        self.convertEntities = convertEntities\n+        # Set the rules for how we'll deal with the entities we\n+        # encounter\n+        if self.convertEntities:\n+            # It doesn't make sense to convert encoded characters to\n+            # entities even while you're converting entities to Unicode.\n+            # Just convert it all to Unicode.\n+            self.smartQuotesTo = None\n+            if convertEntities == self.HTML_ENTITIES:\n+                self.convertXMLEntities = False\n+                self.convertHTMLEntities = True\n+                self.escapeUnrecognizedEntities = True\n+            elif convertEntities == self.XHTML_ENTITIES:\n+                self.convertXMLEntities = True\n+                self.convertHTMLEntities = True\n+                self.escapeUnrecognizedEntities = False\n+            elif convertEntities == self.XML_ENTITIES:\n+                self.convertXMLEntities = True\n+                self.convertHTMLEntities = False\n+                self.escapeUnrecognizedEntities = False\n+        else:\n+            self.convertXMLEntities = False\n+            self.convertHTMLEntities = False\n+            self.escapeUnrecognizedEntities = False\n+\n+        self.instanceSelfClosingTags = buildTagMap(None, selfClosingTags)\n+        SGMLParser.__init__(self)\n+\n+        if hasattr(markup, 'read'):        # It's a file-type object.\n+            markup = markup.read()\n+        self.markup = markup\n+        self.markupMassage = markupMassage\n+        try:\n+            self._feed(isHTML=isHTML)\n+        except StopParsing:\n+            pass\n+        self.markup = None                 # The markup can now be GCed\n+\n+    def convert_charref(self, name):\n+        \"\"\"This method fixes a bug in Python's SGMLParser.\"\"\"\n+        try:\n+            n = int(name)\n+        except ValueError:\n+            return\n+        if not 0 <= n <= 127 : # ASCII ends at 127, not 255\n+            return\n+        return self.convert_codepoint(n)\n+\n+    def _feed(self, inDocumentEncoding=None, isHTML=False):\n+        # Convert the document to Unicode.\n+        markup = self.markup\n+        if isinstance(markup, unicode):\n+            if not hasattr(self, 'originalEncoding'):\n+                self.originalEncoding = None\n+        else:\n+            dammit = UnicodeDammit\\\n+                     (markup, [self.fromEncoding, inDocumentEncoding],\n+                      smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)\n+            markup = dammit.unicode\n+            self.originalEncoding = dammit.originalEncoding\n+            self.declaredHTMLEncoding = dammit.declaredHTMLEncoding\n+        if markup:\n+            if self.markupMassage:\n+                if not hasattr(self.markupMassage, \"__iter__\"):\n+                    self.markupMassage = self.MARKUP_MASSAGE\n+                for fix, m in self.markupMassage:\n+                    markup = fix.sub(m, markup)\n+                # TODO: We get rid of markupMassage so that the\n+                # soup object can be deepcopied later on. Some\n+                # Python installations can't copy regexes. If anyone\n+                # was relying on the existence of markupMassage, this\n+                # might cause problems.\n+                del(self.markupMassage)\n+        self.reset()\n+\n+        SGMLParser.feed(self, markup)\n+        # Close out any unfinished strings and close all the open tags.\n+        self.endData()\n+        while self.currentTag.name != self.ROOT_TAG_NAME:\n+            self.popTag()\n+\n+    def __getattr__(self, methodName):\n+        \"\"\"This method routes method call requests to either the SGMLParser\n+        superclass or the Tag superclass, depending on the method name.\"\"\"\n+        #print \"__getattr__ called on %s.%s\" % (self.__class__, methodName)\n+\n+        if methodName.startswith('start_') or methodName.startswith('end_') \\\n+               or methodName.startswith('do_'):\n+            return SGMLParser.__getattr__(self, methodName)\n+        elif not methodName.startswith('__'):\n+            return Tag.__getattr__(self, methodName)\n+        else:\n+            raise AttributeError\n+\n+    def isSelfClosingTag(self, name):\n+        \"\"\"Returns true iff the given string is the name of a\n+        self-closing tag according to this parser.\"\"\"\n+        return self.SELF_CLOSING_TAGS.has_key(name) \\\n+               or self.instanceSelfClosingTags.has_key(name)\n+\n+    def reset(self):\n+        Tag.__init__(self, self, self.ROOT_TAG_NAME)\n+        self.hidden = 1\n+        SGMLParser.reset(self)\n+        self.currentData = []\n+        self.currentTag = None\n+        self.tagStack = []\n+        self.quoteStack = []\n+        self.pushTag(self)\n+\n+    def popTag(self):\n+        tag = self.tagStack.pop()\n+\n+        #print \"Pop\", tag.name\n+        if self.tagStack:\n+            self.currentTag = self.tagStack[-1]\n+        return self.currentTag\n+\n+    def pushTag(self, tag):\n+        #print \"Push\", tag.name\n+        if self.currentTag:\n+            self.currentTag.contents.append(tag)\n+        self.tagStack.append(tag)\n+        self.currentTag = self.tagStack[-1]\n+\n+    def endData(self, containerClass=NavigableString):\n+        if self.currentData:\n+            currentData = u''.join(self.currentData)\n+            if (currentData.translate(self.STRIP_ASCII_SPACES) == '' and\n+                not set([tag.name for tag in self.tagStack]).intersection(\n+                    self.PRESERVE_WHITESPACE_TAGS)):\n+                if '\\n' in currentData:\n+                    currentData = '\\n'\n+                else:\n+                    currentData = ' '\n+            self.currentData = []\n+            if self.parseOnlyThese and len(self.tagStack) <= 1 and \\\n+                   (not self.parseOnlyThese.text or \\\n+                    not self.parseOnlyThese.search(currentData)):\n+                return\n+            o = containerClass(currentData)\n+            o.setup(self.currentTag, self.previous)\n+            if self.previous:\n+                self.previous.next = o\n+            self.previous = o\n+            self.currentTag.contents.append(o)\n+\n+\n+    def _popToTag(self, name, inclusivePop=True):\n+        \"\"\"Pops the tag stack up to and including the most recent\n+        instance of the given tag. If inclusivePop is false, pops the tag\n+        stack up to but *not* including the most recent instqance of\n+        the given tag.\"\"\"\n+        #print \"Popping to %s\" % name\n+        if name == self.ROOT_TAG_NAME:\n+            return\n+\n+        numPops = 0\n+        mostRecentTag = None\n+        for i in range(len(self.tagStack)-1, 0, -1):\n+            if name == self.tagStack[i].name:\n+                numPops = len(self.tagStack)-i\n+                break\n+        if not inclusivePop:\n+            numPops = numPops - 1\n+\n+        for i in range(0, numPops):\n+            mostRecentTag = self.popTag()\n+        return mostRecentTag\n+\n+    def _smartPop(self, name):\n+\n+        \"\"\"We need to pop up to the previous tag of this type, unless\n+        one of this tag's nesting reset triggers comes between this\n+        tag and the previous tag of this type, OR unless this tag is a\n+        generic nesting trigger and another generic nesting trigger\n+        comes between this tag and the previous tag of this type.\n+\n+        Examples:\n+         <p>Foo<b>Bar *<p>* should pop to 'p', not 'b'.\n+         <p>Foo<table>Bar *<p>* should pop to 'table', not 'p'.\n+         <p>Foo<table><tr>Bar *<p>* should pop to 'tr', not 'p'.\n+\n+         <li><ul><li> *<li>* should pop to 'ul', not the first 'li'.\n+         <tr><table><tr> *<tr>* should pop to 'table', not the first 'tr'\n+         <td><tr><td> *<td>* should pop to 'tr', not the first 'td'\n+        \"\"\"\n+\n+        nestingResetTriggers = self.NESTABLE_TAGS.get(name)\n+        isNestable = nestingResetTriggers != None\n+        isResetNesting = self.RESET_NESTING_TAGS.has_key(name)\n+        popTo = None\n+        inclusive = True\n+        for i in range(len(self.tagStack)-1, 0, -1):\n+            p = self.tagStack[i]\n+            if (not p or p.name == name) and not isNestable:\n+                #Non-nestable tags get popped to the top or to their\n+                #last occurance.\n+                popTo = name\n+                break\n+            if (nestingResetTriggers is not None\n+                and p.name in nestingResetTriggers) \\\n+                or (nestingResetTriggers is None and isResetNesting\n+                    and self.RESET_NESTING_TAGS.has_key(p.name)):\n+\n+                #If we encounter one of the nesting reset triggers\n+                #peculiar to this tag, or we encounter another tag\n+                #that causes nesting to reset, pop up to but not\n+                #including that tag.\n+                popTo = p.name\n+                inclusive = False\n+                break\n+            p = p.parent\n+        if popTo:\n+            self._popToTag(popTo, inclusive)\n+\n+    def unknown_starttag(self, name, attrs, selfClosing=0):\n+        #print \"Start tag %s: %s\" % (name, attrs)\n+        if self.quoteStack:\n+            #This is not a real tag.\n+            #print \"<%s> is not real!\" % name\n+            attrs = ''.join([' %s=\"%s\"' % (x, y) for x, y in attrs])\n+            self.handle_data('<%s%s>' % (name, attrs))\n+            return\n+        self.endData()\n+\n+        if not self.isSelfClosingTag(name) and not selfClosing:\n+            self._smartPop(name)\n+\n+        if self.parseOnlyThese and len(self.tagStack) <= 1 \\\n+               and (self.parseOnlyThese.text or not self.parseOnlyThese.searchTag(name, attrs)):\n+            return\n+\n+        tag = Tag(self, name, attrs, self.currentTag, self.previous)\n+        if self.previous:\n+            self.previous.next = tag\n+        self.previous = tag\n+        self.pushTag(tag)\n+        if selfClosing or self.isSelfClosingTag(name):\n+            self.popTag()\n+        if name in self.QUOTE_TAGS:\n+            #print \"Beginning quote (%s)\" % name\n+            self.quoteStack.append(name)\n+            self.literal = 1\n+        return tag\n+\n+    def unknown_endtag(self, name):\n+        #print \"End tag %s\" % name\n+        if self.quoteStack and self.quoteStack[-1] != name:\n+            #This is not a real end tag.\n+            #print \"</%s> is not real!\" % name\n+            self.handle_data('</%s>' % name)\n+            return\n+        self.endData()\n+        self._popToTag(name)\n+        if self.quoteStack and self.quoteStack[-1] == name:\n+            self.quoteStack.pop()\n+            self.literal = (len(self.quoteStack) > 0)\n+\n+    def handle_data(self, data):\n+        self.currentData.append(data)\n+\n+    def _toStringSubclass(self, text, subclass):\n+        \"\"\"Adds a certain piece of text to the tree as a NavigableString\n+        subclass.\"\"\"\n+        self.endData()\n+        self.handle_data(text)\n+        self.endData(subclass)\n+\n+    def handle_pi(self, text):\n+        \"\"\"Handle a processing instruction as a ProcessingInstruction\n+        object, possibly one with a %SOUP-ENCODING% slot into which an\n+        encoding will be plugged later.\"\"\"\n+        if text[:3] == \"xml\":\n+            text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\n+        self._toStringSubclass(text, ProcessingInstruction)\n+\n+    def handle_comment(self, text):\n+        \"Handle comments as Comment objects.\"\n+        self._toStringSubclass(text, Comment)\n+\n+    def handle_charref(self, ref):\n+        \"Handle character references as data.\"\n+        if self.convertEntities:\n+            data = unichr(int(ref))\n+        else:\n+            data = '&#%s;' % ref\n+        self.handle_data(data)\n+\n+    def handle_entityref(self, ref):\n+        \"\"\"Handle entity references as data, possibly converting known\n+        HTML and/or XML entity references to the corresponding Unicode\n+        characters.\"\"\"\n+        data = None\n+        if self.convertHTMLEntities:\n+            try:\n+                data = unichr(name2codepoint[ref])\n+            except KeyError:\n+                pass\n+\n+        if not data and self.convertXMLEntities:\n+                data = self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n+\n+        if not data and self.convertHTMLEntities and \\\n+            not self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n+                # TODO: We've got a problem here. We're told this is\n+                # an entity reference, but it's not an XML entity\n+                # reference or an HTML entity reference. Nonetheless,\n+                # the logical thing to do is to pass it through as an\n+                # unrecognized entity reference.\n+                #\n+                # Except: when the input is \"&carol;\" this function\n+                # will be called with input \"carol\". When the input is\n+                # \"AT&T\", this function will be called with input\n+                # \"T\". We have no way of knowing whether a semicolon\n+                # was present originally, so we don't know whether\n+                # this is an unknown entity or just a misplaced\n+                # ampersand.\n+                #\n+                # The more common case is a misplaced ampersand, so I\n+                # escape the ampersand and omit the trailing semicolon.\n+                data = \"&amp;%s\" % ref\n+        if not data:\n+            # This case is different from the one above, because we\n+            # haven't already gone through a supposedly comprehensive\n+            # mapping of entities to Unicode characters. We might not\n+            # have gone through any mapping at all. So the chances are\n+            # very high that this is a real entity, and not a\n+            # misplaced ampersand.\n+            data = \"&%s;\" % ref\n+        self.handle_data(data)\n+\n+    def handle_decl(self, data):\n+        \"Handle DOCTYPEs and the like as Declaration objects.\"\n+        self._toStringSubclass(data, Declaration)\n+\n+    def parse_declaration(self, i):\n+        \"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\n+        declaration as a CData object.\"\"\"\n+        j = None\n+        if self.rawdata[i:i+9] == '<![CDATA[':\n+             k = self.rawdata.find(']]>', i)\n+             if k == -1:\n+                 k = len(self.rawdata)\n+             data = self.rawdata[i+9:k]\n+             j = k+3\n+             self._toStringSubclass(data, CData)\n+        else:\n+            try:\n+                j = SGMLParser.parse_declaration(self, i)\n+            except SGMLParseError:\n+                toHandle = self.rawdata[i:]\n+                self.handle_data(toHandle)\n+                j = i + len(toHandle)\n+        return j\n+\n+class BeautifulSoup(BeautifulStoneSoup):\n+\n+    \"\"\"This parser knows the following facts about HTML:\n+\n+    * Some tags have no closing tag and should be interpreted as being\n+      closed as soon as they are encountered.\n+\n+    * The text inside some tags (ie. 'script') may contain tags which\n+      are not really part of the document and which should be parsed\n+      as text, not tags. If you want to parse the text as tags, you can\n+      always fetch it and parse it explicitly.\n+\n+    * Tag nesting rules:\n+\n+      Most tags can't be nested at all. For instance, the occurance of\n+      a <p> tag should implicitly close the previous <p> tag.\n+\n+       <p>Para1<p>Para2\n+        should be transformed into:\n+       <p>Para1</p><p>Para2\n+\n+      Some tags can be nested arbitrarily. For instance, the occurance\n+      of a <blockquote> tag should _not_ implicitly close the previous\n+      <blockquote> tag.\n+\n+       Alice said: <blockquote>Bob said: <blockquote>Blah\n+        should NOT be transformed into:\n+       Alice said: <blockquote>Bob said: </blockquote><blockquote>Blah\n+\n+      Some tags can be nested, but the nesting is reset by the\n+      interposition of other tags. For instance, a <tr> tag should\n+      implicitly close the previous <tr> tag within the same <table>,\n+      but not close a <tr> tag in another table.\n+\n+       <table><tr>Blah<tr>Blah\n+        should be transformed into:\n+       <table><tr>Blah</tr><tr>Blah\n+        but,\n+       <tr>Blah<table><tr>Blah\n+        should NOT be transformed into\n+       <tr>Blah<table></tr><tr>Blah\n+\n+    Differing assumptions about tag nesting rules are a major source\n+    of problems with the BeautifulSoup class. If BeautifulSoup is not\n+    treating as nestable a tag your page author treats as nestable,\n+    try ICantBelieveItsBeautifulSoup, MinimalSoup, or\n+    BeautifulStoneSoup before writing your own subclass.\"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        if not kwargs.has_key('smartQuotesTo'):\n+            kwargs['smartQuotesTo'] = self.HTML_ENTITIES\n+        kwargs['isHTML'] = True\n+        BeautifulStoneSoup.__init__(self, *args, **kwargs)\n+\n+    SELF_CLOSING_TAGS = buildTagMap(None,\n+                                    ('br' , 'hr', 'input', 'img', 'meta',\n+                                    'spacer', 'link', 'frame', 'base', 'col'))\n+\n+    PRESERVE_WHITESPACE_TAGS = set(['pre', 'textarea'])\n+\n+    QUOTE_TAGS = {'script' : None, 'textarea' : None}\n+\n+    #According to the HTML standard, each of these inline tags can\n+    #contain another tag of the same type. Furthermore, it's common\n+    #to actually use these tags this way.\n+    NESTABLE_INLINE_TAGS = ('span', 'font', 'q', 'object', 'bdo', 'sub', 'sup',\n+                            'center')\n+\n+    #According to the HTML standard, these block tags can contain\n+    #another tag of the same type. Furthermore, it's common\n+    #to actually use these tags this way.\n+    NESTABLE_BLOCK_TAGS = ('blockquote', 'div', 'fieldset', 'ins', 'del')\n+\n+    #Lists can contain other lists, but there are restrictions.\n+    NESTABLE_LIST_TAGS = { 'ol' : [],\n+                           'ul' : [],\n+                           'li' : ['ul', 'ol'],\n+                           'dl' : [],\n+                           'dd' : ['dl'],\n+                           'dt' : ['dl'] }\n+\n+    #Tables can contain other tables, but there are restrictions.\n+    NESTABLE_TABLE_TAGS = {'table' : [],\n+                           'tr' : ['table', 'tbody', 'tfoot', 'thead'],\n+                           'td' : ['tr'],\n+                           'th' : ['tr'],\n+                           'thead' : ['table'],\n+                           'tbody' : ['table'],\n+                           'tfoot' : ['table'],\n+                           }\n+\n+    NON_NESTABLE_BLOCK_TAGS = ('address', 'form', 'p', 'pre')\n+\n+    #If one of these tags is encountered, all tags up to the next tag of\n+    #this type are popped.\n+    RESET_NESTING_TAGS = buildTagMap(None, NESTABLE_BLOCK_TAGS, 'noscript',\n+                                     NON_NESTABLE_BLOCK_TAGS,\n+                                     NESTABLE_LIST_TAGS,\n+                                     NESTABLE_TABLE_TAGS)\n+\n+    NESTABLE_TAGS = buildTagMap([], NESTABLE_INLINE_TAGS, NESTABLE_BLOCK_TAGS,\n+                                NESTABLE_LIST_TAGS, NESTABLE_TABLE_TAGS)\n+\n+    # Used to detect the charset in a META tag; see start_meta\n+    CHARSET_RE = re.compile(\"((^|;)\\s*charset=)([^;]*)\", re.M)\n+\n+    def start_meta(self, attrs):\n+        \"\"\"Beautiful Soup can detect a charset included in a META tag,\n+        try to convert the document to that charset, and re-parse the\n+        document from the beginning.\"\"\"\n+        httpEquiv = None\n+        contentType = None\n+        contentTypeIndex = None\n+        tagNeedsEncodingSubstitution = False\n+\n+        for i in range(0, len(attrs)):\n+            key, value = attrs[i]\n+            key = key.lower()\n+            if key == 'http-equiv':\n+                httpEquiv = value\n+            elif key == 'content':\n+                contentType = value\n+                contentTypeIndex = i\n+\n+        if httpEquiv and contentType: # It's an interesting meta tag.\n+            match = self.CHARSET_RE.search(contentType)\n+            if match:\n+                if (self.declaredHTMLEncoding is not None or\n+                    self.originalEncoding == self.fromEncoding):\n+                    # An HTML encoding was sniffed while converting\n+                    # the document to Unicode, or an HTML encoding was\n+                    # sniffed during a previous pass through the\n+                    # document, or an encoding was specified\n+                    # explicitly and it worked. Rewrite the meta tag.\n+                    def rewrite(match):\n+                        return match.group(1) + \"%SOUP-ENCODING%\"\n+                    newAttr = self.CHARSET_RE.sub(rewrite, contentType)\n+                    attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\n+                                               newAttr)\n+                    tagNeedsEncodingSubstitution = True\n+                else:\n+                    # This is our first pass through the document.\n+                    # Go through it again with the encoding information.\n+                    newCharset = match.group(3)\n+                    if newCharset and newCharset != self.originalEncoding:\n+                        self.declaredHTMLEncoding = newCharset\n+                        self._feed(self.declaredHTMLEncoding)\n+                        raise StopParsing\n+                    pass\n+        tag = self.unknown_starttag(\"meta\", attrs)\n+        if tag and tagNeedsEncodingSubstitution:\n+            tag.containsSubstitutions = True\n+\n+class StopParsing(Exception):\n+    pass\n+\n+class ICantBelieveItsBeautifulSoup(BeautifulSoup):\n+\n+    \"\"\"The BeautifulSoup class is oriented towards skipping over\n+    common HTML errors like unclosed tags. However, sometimes it makes\n+    errors of its own. For instance, consider this fragment:\n+\n+     <b>Foo<b>Bar</b></b>\n+\n+    This is perfectly valid (if bizarre) HTML. However, the\n+    BeautifulSoup class will implicitly close the first b tag when it\n+    encounters the second 'b'. It will think the author wrote\n+    \"<b>Foo<b>Bar\", and didn't close the first 'b' tag, because\n+    there's no real-world reason to bold something that's already\n+    bold. When it encounters '</b></b>' it will close two more 'b'\n+    tags, for a grand total of three tags closed instead of two. This\n+    can throw off the rest of your document structure. The same is\n+    true of a number of other tags, listed below.\n+\n+    It's much more common for someone to forget to close a 'b' tag\n+    than to actually use nested 'b' tags, and the BeautifulSoup class\n+    handles the common case. This class handles the not-co-common\n+    case: where you can't believe someone wrote what they did, but\n+    it's valid HTML and BeautifulSoup screwed up by assuming it\n+    wouldn't be.\"\"\"\n+\n+    I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS = \\\n+     ('em', 'big', 'i', 'small', 'tt', 'abbr', 'acronym', 'strong',\n+      'cite', 'code', 'dfn', 'kbd', 'samp', 'strong', 'var', 'b',\n+      'big')\n+\n+    I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS = ('noscript',)\n+\n+    NESTABLE_TAGS = buildTagMap([], BeautifulSoup.NESTABLE_TAGS,\n+                                I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS,\n+                                I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS)\n+\n+class MinimalSoup(BeautifulSoup):\n+    \"\"\"The MinimalSoup class is for parsing HTML that contains\n+    pathologically bad markup. It makes no assumptions about tag\n+    nesting, but it does know which tags are self-closing, that\n+    <script> tags contain Javascript and should not be parsed, that\n+    META tags may contain encoding information, and so on.\n+\n+    This also makes it better for subclassing than BeautifulStoneSoup\n+    or BeautifulSoup.\"\"\"\n+\n+    RESET_NESTING_TAGS = buildTagMap('noscript')\n+    NESTABLE_TAGS = {}\n+\n+class BeautifulSOAP(BeautifulStoneSoup):\n+    \"\"\"This class will push a tag with only a single string child into\n+    the tag's parent as an attribute. The attribute's name is the tag\n+    name, and the value is the string child. An example should give\n+    the flavor of the change:\n+\n+    <foo><bar>baz</bar></foo>\n+     =>\n+    <foo bar=\"baz\"><bar>baz</bar></foo>\n+\n+    You can then access fooTag['bar'] instead of fooTag.barTag.string.\n+\n+    This is, of course, useful for scraping structures that tend to\n+    use subelements instead of attributes, such as SOAP messages. Note\n+    that it modifies its input, so don't print the modified version\n+    out.\n+\n+    I'm not sure how many people really want to use this class; let me\n+    know if you do. Mainly I like the name.\"\"\"\n+\n+    def popTag(self):\n+        if len(self.tagStack) > 1:\n+            tag = self.tagStack[-1]\n+            parent = self.tagStack[-2]\n+            parent._getAttrMap()\n+            if (isinstance(tag, Tag) and len(tag.contents) == 1 and\n+                isinstance(tag.contents[0], NavigableString) and\n+                not parent.attrMap.has_key(tag.name)):\n+                parent[tag.name] = tag.contents[0]\n+        BeautifulStoneSoup.popTag(self)\n+\n+#Enterprise class names! It has come to our attention that some people\n+#think the names of the Beautiful Soup parser classes are too silly\n+#and \"unprofessional\" for use in enterprise screen-scraping. We feel\n+#your pain! For such-minded folk, the Beautiful Soup Consortium And\n+#All-Night Kosher Bakery recommends renaming this file to\n+#\"RobustParser.py\" (or, in cases of extreme enterprisiness,\n+#\"RobustParserBeanInterface.class\") and using the following\n+#enterprise-friendly class aliases:\n+class RobustXMLParser(BeautifulStoneSoup):\n+    pass\n+class RobustHTMLParser(BeautifulSoup):\n+    pass\n+class RobustWackAssHTMLParser(ICantBelieveItsBeautifulSoup):\n+    pass\n+class RobustInsanelyWackAssHTMLParser(MinimalSoup):\n+    pass\n+class SimplifyingSOAPParser(BeautifulSOAP):\n+    pass\n+\n+######################################################\n+#\n+# Bonus library: Unicode, Dammit\n+#\n+# This class forces XML data into a standard format (usually to UTF-8\n+# or Unicode).  It is heavily based on code from Mark Pilgrim's\n+# Universal Feed Parser. It does not rewrite the XML or HTML to\n+# reflect a new encoding: that happens in BeautifulStoneSoup.handle_pi\n+# (XML) and BeautifulSoup.start_meta (HTML).\n+\n+# Autodetects character encodings.\n+# Download from http://chardet.feedparser.org/\n+try:\n+    import chardet\n+#    import chardet.constants\n+#    chardet.constants._debug = 1\n+except ImportError:\n+    chardet = None\n+\n+# cjkcodecs and iconv_codec make Python know about more character encodings.\n+# Both are available from http://cjkpython.i18n.org/\n+# They're built in if you use Python 2.4.\n+try:\n+    import cjkcodecs.aliases\n+except ImportError:\n+    pass\n+try:\n+    import iconv_codec\n+except ImportError:\n+    pass\n+\n+class UnicodeDammit:\n+    \"\"\"A class for detecting the encoding of a *ML document and\n+    converting it to a Unicode string. If the source encoding is\n+    windows-1252, can replace MS smart quotes with their HTML or XML\n+    equivalents.\"\"\"\n+\n+    # This dictionary maps commonly seen values for \"charset\" in HTML\n+    # meta tags to the corresponding Python codec names. It only covers\n+    # values that aren't in Python's aliases and can't be determined\n+    # by the heuristics in find_codec.\n+    CHARSET_ALIASES = { \"macintosh\" : \"mac-roman\",\n+                        \"x-sjis\" : \"shift-jis\" }\n+\n+    def __init__(self, markup, overrideEncodings=[],\n+                 smartQuotesTo='xml', isHTML=False):\n+        self.declaredHTMLEncoding = None\n+        self.markup, documentEncoding, sniffedEncoding = \\\n+                     self._detectEncoding(markup, isHTML)\n+        self.smartQuotesTo = smartQuotesTo\n+        self.triedEncodings = []\n+        if markup == '' or isinstance(markup, unicode):\n+            self.originalEncoding = None\n+            self.unicode = unicode(markup)\n+            return\n+\n+        u = None\n+        for proposedEncoding in overrideEncodings:\n+            u = self._convertFrom(proposedEncoding)\n+            if u: break\n+        if not u:\n+            for proposedEncoding in (documentEncoding, sniffedEncoding):\n+                u = self._convertFrom(proposedEncoding)\n+                if u: break\n+\n+        # If no luck and we have auto-detection library, try that:\n+        if not u and chardet and not isinstance(self.markup, unicode):\n+            u = self._convertFrom(chardet.detect(self.markup)['encoding'])\n+\n+        # As a last resort, try utf-8 and windows-1252:\n+        if not u:\n+            for proposed_encoding in (\"utf-8\", \"windows-1252\"):\n+                u = self._convertFrom(proposed_encoding)\n+                if u: break\n+\n+        self.unicode = u\n+        if not u: self.originalEncoding = None\n+\n+    def _subMSChar(self, orig):\n+        \"\"\"Changes a MS smart quote character to an XML or HTML\n+        entity.\"\"\"\n+        sub = self.MS_CHARS.get(orig)\n+        if isinstance(sub, tuple):\n+            if self.smartQuotesTo == 'xml':\n+                sub = '&#x%s;' % sub[1]\n+            else:\n+                sub = '&%s;' % sub[0]\n+        return sub\n+\n+    def _convertFrom(self, proposed):\n+        proposed = self.find_codec(proposed)\n+        if not proposed or proposed in self.triedEncodings:\n+            return None\n+        self.triedEncodings.append(proposed)\n+        markup = self.markup\n+\n+        # Convert smart quotes to HTML if coming from an encoding\n+        # that might have them.\n+        if self.smartQuotesTo and proposed.lower() in(\"windows-1252\",\n+                                                      \"iso-8859-1\",\n+                                                      \"iso-8859-2\"):\n+            markup = re.compile(\"([\\x80-\\x9f])\").sub \\\n+                     (lambda(x): self._subMSChar(x.group(1)),\n+                      markup)\n+\n+        try:\n+            # print \"Trying to convert document to %s\" % proposed\n+            u = self._toUnicode(markup, proposed)\n+            self.markup = u\n+            self.originalEncoding = proposed\n+        except Exception, e:\n+            # print \"That didn't work!\"\n+            # print e\n+            return None\n+        #print \"Correct encoding: %s\" % proposed\n+        return self.markup\n+\n+    def _toUnicode(self, data, encoding):\n+        '''Given a string and its encoding, decodes the string into Unicode.\n+        %encoding is a string recognized by encodings.aliases'''\n+\n+        # strip Byte Order Mark (if present)\n+        if (len(data) >= 4) and (data[:2] == '\\xfe\\xff') \\\n+               and (data[2:4] != '\\x00\\x00'):\n+            encoding = 'utf-16be'\n+            data = data[2:]\n+        elif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') \\\n+                 and (data[2:4] != '\\x00\\x00'):\n+            encoding = 'utf-16le'\n+            data = data[2:]\n+        elif data[:3] == '\\xef\\xbb\\xbf':\n+            encoding = 'utf-8'\n+            data = data[3:]\n+        elif data[:4] == '\\x00\\x00\\xfe\\xff':\n+            encoding = 'utf-32be'\n+            data = data[4:]\n+        elif data[:4] == '\\xff\\xfe\\x00\\x00':\n+            encoding = 'utf-32le'\n+            data = data[4:]\n+        newdata = unicode(data, encoding)\n+        return newdata\n+\n+    def _detectEncoding(self, xml_data, isHTML=False):\n+        \"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n+        xml_encoding = sniffed_xml_encoding = None\n+        try:\n+            if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n+                # EBCDIC\n+                xml_data = self._ebcdic_to_ascii(xml_data)\n+            elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n+                # UTF-16BE\n+                sniffed_xml_encoding = 'utf-16be'\n+                xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n+            elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n+                     and (xml_data[2:4] != '\\x00\\x00'):\n+                # UTF-16BE with BOM\n+                sniffed_xml_encoding = 'utf-16be'\n+                xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n+            elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n+                # UTF-16LE\n+                sniffed_xml_encoding = 'utf-16le'\n+                xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n+            elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n+                     (xml_data[2:4] != '\\x00\\x00'):\n+                # UTF-16LE with BOM\n+                sniffed_xml_encoding = 'utf-16le'\n+                xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n+            elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n+                # UTF-32BE\n+                sniffed_xml_encoding = 'utf-32be'\n+                xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n+            elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n+                # UTF-32LE\n+                sniffed_xml_encoding = 'utf-32le'\n+                xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n+            elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n+                # UTF-32BE with BOM\n+                sniffed_xml_encoding = 'utf-32be'\n+                xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n+            elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n+                # UTF-32LE with BOM\n+                sniffed_xml_encoding = 'utf-32le'\n+                xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n+            elif xml_data[:3] == '\\xef\\xbb\\xbf':\n+                # UTF-8 with BOM\n+                sniffed_xml_encoding = 'utf-8'\n+                xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n+            else:\n+                sniffed_xml_encoding = 'ascii'\n+                pass\n+        except:\n+            xml_encoding_match = None\n+        xml_encoding_match = re.compile(\n+            '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\n+        if not xml_encoding_match and isHTML:\n+            regexp = re.compile('<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]', re.I)\n+            xml_encoding_match = regexp.search(xml_data)\n+        if xml_encoding_match is not None:\n+            xml_encoding = xml_encoding_match.groups()[0].lower()\n+            if isHTML:\n+                self.declaredHTMLEncoding = xml_encoding\n+            if sniffed_xml_encoding and \\\n+               (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n+                                 'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n+                                 'utf-16', 'utf-32', 'utf_16', 'utf_32',\n+                                 'utf16', 'u16')):\n+                xml_encoding = sniffed_xml_encoding\n+        return xml_data, xml_encoding, sniffed_xml_encoding\n+\n+\n+    def find_codec(self, charset):\n+        return self._codec(self.CHARSET_ALIASES.get(charset, charset)) \\\n+               or (charset and self._codec(charset.replace(\"-\", \"\"))) \\\n+               or (charset and self._codec(charset.replace(\"-\", \"_\"))) \\\n+               or charset\n+\n+    def _codec(self, charset):\n+        if not charset: return charset\n+        codec = None\n+        try:\n+            codecs.lookup(charset)\n+            codec = charset\n+        except (LookupError, ValueError):\n+            pass\n+        return codec\n+\n+    EBCDIC_TO_ASCII_MAP = None\n+    def _ebcdic_to_ascii(self, s):\n+        c = self.__class__\n+        if not c.EBCDIC_TO_ASCII_MAP:\n+            emap = (0,1,2,3,156,9,134,127,151,141,142,11,12,13,14,15,\n+                    16,17,18,19,157,133,8,135,24,25,146,143,28,29,30,31,\n+                    128,129,130,131,132,10,23,27,136,137,138,139,140,5,6,7,\n+                    144,145,22,147,148,149,150,4,152,153,154,155,20,21,158,26,\n+                    32,160,161,162,163,164,165,166,167,168,91,46,60,40,43,33,\n+                    38,169,170,171,172,173,174,175,176,177,93,36,42,41,59,94,\n+                    45,47,178,179,180,181,182,183,184,185,124,44,37,95,62,63,\n+                    186,187,188,189,190,191,192,193,194,96,58,35,64,39,61,34,\n+                    195,97,98,99,100,101,102,103,104,105,196,197,198,199,200,\n+                    201,202,106,107,108,109,110,111,112,113,114,203,204,205,\n+                    206,207,208,209,126,115,116,117,118,119,120,121,122,210,\n+                    211,212,213,214,215,216,217,218,219,220,221,222,223,224,\n+                    225,226,227,228,229,230,231,123,65,66,67,68,69,70,71,72,\n+                    73,232,233,234,235,236,237,125,74,75,76,77,78,79,80,81,\n+                    82,238,239,240,241,242,243,92,159,83,84,85,86,87,88,89,\n+                    90,244,245,246,247,248,249,48,49,50,51,52,53,54,55,56,57,\n+                    250,251,252,253,254,255)\n+            import string\n+            c.EBCDIC_TO_ASCII_MAP = string.maketrans( \\\n+            ''.join(map(chr, range(256))), ''.join(map(chr, emap)))\n+        return s.translate(c.EBCDIC_TO_ASCII_MAP)\n+\n+    MS_CHARS = { '\\x80' : ('euro', '20AC'),\n+                 '\\x81' : ' ',\n+                 '\\x82' : ('sbquo', '201A'),\n+                 '\\x83' : ('fnof', '192'),\n+                 '\\x84' : ('bdquo', '201E'),\n+                 '\\x85' : ('hellip', '2026'),\n+                 '\\x86' : ('dagger', '2020'),\n+                 '\\x87' : ('Dagger', '2021'),\n+                 '\\x88' : ('circ', '2C6'),\n+                 '\\x89' : ('permil', '2030'),\n+                 '\\x8A' : ('Scaron', '160'),\n+                 '\\x8B' : ('lsaquo', '2039'),\n+                 '\\x8C' : ('OElig', '152'),\n+                 '\\x8D' : '?',\n+                 '\\x8E' : ('#x17D', '17D'),\n+                 '\\x8F' : '?',\n+                 '\\x90' : '?',\n+                 '\\x91' : ('lsquo', '2018'),\n+                 '\\x92' : ('rsquo', '2019'),\n+                 '\\x93' : ('ldquo', '201C'),\n+                 '\\x94' : ('rdquo', '201D'),\n+                 '\\x95' : ('bull', '2022'),\n+                 '\\x96' : ('ndash', '2013'),\n+                 '\\x97' : ('mdash', '2014'),\n+                 '\\x98' : ('tilde', '2DC'),\n+                 '\\x99' : ('trade', '2122'),\n+                 '\\x9a' : ('scaron', '161'),\n+                 '\\x9b' : ('rsaquo', '203A'),\n+                 '\\x9c' : ('oelig', '153'),\n+                 '\\x9d' : '?',\n+                 '\\x9e' : ('#x17E', '17E'),\n+                 '\\x9f' : ('Yuml', ''),}\n+\n+#######################################################################\n+\n+\n+#By default, act as an HTML pretty-printer.\n+if __name__ == '__main__':\n+    import sys\n+    soup = BeautifulSoup(sys.stdin)\n+    print soup.prettify()"
            },
            {
                "sha": "d3b35368514cbdb649b314329ee1b1973ca1e750",
                "filename": "github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoupTests.py",
                "status": "added",
                "additions": 906,
                "deletions": 0,
                "changes": 906,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoupTests.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoupTests.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoupTests.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,906 @@\n+# -*- coding: utf-8 -*-\n+\"\"\"Unit tests for Beautiful Soup.\n+\n+These tests make sure the Beautiful Soup works as it should. If you\n+find a bug in Beautiful Soup, the best way to express it is as a test\n+case like this that fails.\"\"\"\n+\n+# The very first thing we do is give a useful error if someone is\n+# running this code under Python 3.\n+\"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"<>\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n+\n+import unittest\n+from BeautifulSoup import *\n+\n+class SoupTest(unittest.TestCase):\n+\n+    def assertSoupEquals(self, toParse, rep=None, c=BeautifulSoup):\n+        \"\"\"Parse the given text and make sure its string rep is the other\n+        given text.\"\"\"\n+        if rep == None:\n+            rep = toParse\n+        self.assertEqual(str(c(toParse)), rep)\n+\n+\n+class FollowThatTag(SoupTest):\n+\n+    \"Tests the various ways of fetching tags from a soup.\"\n+\n+    def setUp(self):\n+        ml = \"\"\"\n+        <a id=\"x\">1</a>\n+        <A id=\"a\">2</a>\n+        <b id=\"b\">3</a>\n+        <b href=\"foo\" id=\"x\">4</a>\n+        <ac width=100>4</ac>\"\"\"\n+        self.soup = BeautifulStoneSoup(ml)\n+\n+    def testFindAllByName(self):\n+        matching = self.soup('a')\n+        self.assertEqual(len(matching), 2)\n+        self.assertEqual(matching[0].name, 'a')\n+        self.assertEqual(matching, self.soup.findAll('a'))\n+        self.assertEqual(matching, self.soup.findAll(SoupStrainer('a')))\n+\n+    def testFindAllByAttribute(self):\n+        matching = self.soup.findAll(id='x')\n+        self.assertEqual(len(matching), 2)\n+        self.assertEqual(matching[0].name, 'a')\n+        self.assertEqual(matching[1].name, 'b')\n+\n+        matching2 = self.soup.findAll(attrs={'id' : 'x'})\n+        self.assertEqual(matching, matching2)\n+\n+        strainer = SoupStrainer(attrs={'id' : 'x'})\n+        self.assertEqual(matching, self.soup.findAll(strainer))\n+\n+        self.assertEqual(len(self.soup.findAll(id=None)), 1)\n+\n+        self.assertEqual(len(self.soup.findAll(width=100)), 1)\n+        self.assertEqual(len(self.soup.findAll(junk=None)), 5)\n+        self.assertEqual(len(self.soup.findAll(junk=[1, None])), 5)\n+\n+        self.assertEqual(len(self.soup.findAll(junk=re.compile('.*'))), 0)\n+        self.assertEqual(len(self.soup.findAll(junk=True)), 0)\n+\n+        self.assertEqual(len(self.soup.findAll(junk=True)), 0)\n+        self.assertEqual(len(self.soup.findAll(href=True)), 1)\n+\n+    def testFindallByClass(self):\n+        soup = BeautifulSoup('<b class=\"foo\">Foo</b><a class=\"1 23 4\">Bar</a>')\n+        self.assertEqual(soup.find(attrs='foo').string, \"Foo\")\n+        self.assertEqual(soup.find('a', '1').string, \"Bar\")\n+        self.assertEqual(soup.find('a', '23').string, \"Bar\")\n+        self.assertEqual(soup.find('a', '4').string, \"Bar\")\n+\n+        self.assertEqual(soup.find('a', '2'), None)\n+\n+    def testFindAllByList(self):\n+        matching = self.soup(['a', 'ac'])\n+        self.assertEqual(len(matching), 3)\n+\n+    def testFindAllByHash(self):\n+        matching = self.soup({'a' : True, 'b' : True})\n+        self.assertEqual(len(matching), 4)\n+\n+    def testFindAllText(self):\n+        soup = BeautifulSoup(\"<html>\\xbb</html>\")\n+        self.assertEqual(soup.findAll(text=re.compile('.*')),\n+                         [u'\\xbb'])\n+\n+    def testFindAllByRE(self):\n+        import re\n+        r = re.compile('a.*')\n+        self.assertEqual(len(self.soup(r)), 3)\n+\n+    def testFindAllByMethod(self):\n+        def matchTagWhereIDMatchesName(tag):\n+            return tag.name == tag.get('id')\n+\n+        matching = self.soup.findAll(matchTagWhereIDMatchesName)\n+        self.assertEqual(len(matching), 2)\n+        self.assertEqual(matching[0].name, 'a')\n+\n+    def testFindByIndex(self):\n+        \"\"\"For when you have the tag and you want to know where it is.\"\"\"\n+        tag = self.soup.find('a', id=\"a\")\n+        self.assertEqual(self.soup.index(tag), 3)\n+\n+        # It works for NavigableStrings as well.\n+        s = tag.string\n+        self.assertEqual(tag.index(s), 0)\n+\n+        # If the tag isn't present, a ValueError is raised.\n+        soup2 = BeautifulSoup(\"<b></b>\")\n+        tag2 = soup2.find('b')\n+        self.assertRaises(ValueError, self.soup.index, tag2)\n+\n+    def testConflictingFindArguments(self):\n+        \"\"\"The 'text' argument takes precedence.\"\"\"\n+        soup = BeautifulSoup('Foo<b>Bar</b>Baz')\n+        self.assertEqual(soup.find('b', text='Baz'), 'Baz')\n+        self.assertEqual(soup.findAll('b', text='Baz'), ['Baz'])\n+\n+        self.assertEqual(soup.find(True, text='Baz'), 'Baz')\n+        self.assertEqual(soup.findAll(True, text='Baz'), ['Baz'])\n+\n+    def testParents(self):\n+        soup = BeautifulSoup('<ul id=\"foo\"></ul><ul id=\"foo\"><ul><ul id=\"foo\" a=\"b\"><b>Blah')\n+        b = soup.b\n+        self.assertEquals(len(b.findParents('ul', {'id' : 'foo'})), 2)\n+        self.assertEquals(b.findParent('ul')['a'], 'b')\n+\n+    PROXIMITY_TEST = BeautifulSoup('<b id=\"1\"><b id=\"2\"><b id=\"3\"><b id=\"4\">')\n+\n+    def testNext(self):\n+        soup = self.PROXIMITY_TEST\n+        b = soup.find('b', {'id' : 2})\n+        self.assertEquals(b.findNext('b')['id'], '3')\n+        self.assertEquals(b.findNext('b')['id'], '3')\n+        self.assertEquals(len(b.findAllNext('b')), 2)\n+        self.assertEquals(len(b.findAllNext('b', {'id' : 4})), 1)\n+\n+    def testPrevious(self):\n+        soup = self.PROXIMITY_TEST\n+        b = soup.find('b', {'id' : 3})\n+        self.assertEquals(b.findPrevious('b')['id'], '2')\n+        self.assertEquals(b.findPrevious('b')['id'], '2')\n+        self.assertEquals(len(b.findAllPrevious('b')), 2)\n+        self.assertEquals(len(b.findAllPrevious('b', {'id' : 2})), 1)\n+\n+\n+    SIBLING_TEST = BeautifulSoup('<blockquote id=\"1\"><blockquote id=\"1.1\"></blockquote></blockquote><blockquote id=\"2\"><blockquote id=\"2.1\"></blockquote></blockquote><blockquote id=\"3\"><blockquote id=\"3.1\"></blockquote></blockquote><blockquote id=\"4\">')\n+\n+    def testNextSibling(self):\n+        soup = self.SIBLING_TEST\n+        tag = 'blockquote'\n+        b = soup.find(tag, {'id' : 2})\n+        self.assertEquals(b.findNext(tag)['id'], '2.1')\n+        self.assertEquals(b.findNextSibling(tag)['id'], '3')\n+        self.assertEquals(b.findNextSibling(tag)['id'], '3')\n+        self.assertEquals(len(b.findNextSiblings(tag)), 2)\n+        self.assertEquals(len(b.findNextSiblings(tag, {'id' : 4})), 1)\n+\n+    def testPreviousSibling(self):\n+        soup = self.SIBLING_TEST\n+        tag = 'blockquote'\n+        b = soup.find(tag, {'id' : 3})\n+        self.assertEquals(b.findPrevious(tag)['id'], '2.1')\n+        self.assertEquals(b.findPreviousSibling(tag)['id'], '2')\n+        self.assertEquals(b.findPreviousSibling(tag)['id'], '2')\n+        self.assertEquals(len(b.findPreviousSiblings(tag)), 2)\n+        self.assertEquals(len(b.findPreviousSiblings(tag, id=1)), 1)\n+\n+    def testTextNavigation(self):\n+        soup = BeautifulSoup('Foo<b>Bar</b><i id=\"1\"><b>Baz<br />Blee<hr id=\"1\"/></b></i>Blargh')\n+        baz = soup.find(text='Baz')\n+        self.assertEquals(baz.findParent(\"i\")['id'], '1')\n+        self.assertEquals(baz.findNext(text='Blee'), 'Blee')\n+        self.assertEquals(baz.findNextSibling(text='Blee'), 'Blee')\n+        self.assertEquals(baz.findNextSibling(text='Blargh'), None)\n+        self.assertEquals(baz.findNextSibling('hr')['id'], '1')\n+\n+class SiblingRivalry(SoupTest):\n+    \"Tests the nextSibling and previousSibling navigation.\"\n+\n+    def testSiblings(self):\n+        soup = BeautifulSoup(\"<ul><li>1<p>A</p>B<li>2<li>3</ul>\")\n+        secondLI = soup.find('li').nextSibling\n+        self.assert_(secondLI.name == 'li' and secondLI.string == '2')\n+        self.assertEquals(soup.find(text='1').nextSibling.name, 'p')\n+        self.assertEquals(soup.find('p').nextSibling, 'B')\n+        self.assertEquals(soup.find('p').nextSibling.previousSibling.nextSibling, 'B')\n+\n+class TagsAreObjectsToo(SoupTest):\n+    \"Tests the various built-in functions of Tag objects.\"\n+\n+    def testLen(self):\n+        soup = BeautifulSoup(\"<top>1<b>2</b>3</top>\")\n+        self.assertEquals(len(soup.top), 3)\n+\n+class StringEmUp(SoupTest):\n+    \"Tests the use of 'string' as an alias for a tag's only content.\"\n+\n+    def testString(self):\n+        s = BeautifulSoup(\"<b>foo</b>\")\n+        self.assertEquals(s.b.string, 'foo')\n+\n+    def testLackOfString(self):\n+        s = BeautifulSoup(\"<b>f<i>e</i>o</b>\")\n+        self.assert_(not s.b.string)\n+\n+    def testStringAssign(self):\n+        s = BeautifulSoup(\"<b></b>\")\n+        b = s.b\n+        b.string = \"foo\"\n+        string = b.string\n+        self.assertEquals(string, \"foo\")\n+        self.assert_(isinstance(string, NavigableString))\n+\n+class AllText(SoupTest):\n+    \"Tests the use of 'text' to get all of string content from the tag.\"\n+\n+    def testText(self):\n+        soup = BeautifulSoup(\"<ul><li>spam</li><li>eggs</li><li>cheese</li>\")\n+        self.assertEquals(soup.ul.text, \"spameggscheese\")\n+        self.assertEquals(soup.ul.getText('/'), \"spam/eggs/cheese\")\n+\n+class ThatsMyLimit(SoupTest):\n+    \"Tests the limit argument.\"\n+\n+    def testBasicLimits(self):\n+        s = BeautifulSoup('<br id=\"1\" /><br id=\"1\" /><br id=\"1\" /><br id=\"1\" />')\n+        self.assertEquals(len(s.findAll('br')), 4)\n+        self.assertEquals(len(s.findAll('br', limit=2)), 2)\n+        self.assertEquals(len(s('br', limit=2)), 2)\n+\n+class OnlyTheLonely(SoupTest):\n+    \"Tests the parseOnly argument to the constructor.\"\n+    def setUp(self):\n+        x = []\n+        for i in range(1,6):\n+            x.append('<a id=\"%s\">' % i)\n+            for j in range(100,103):\n+                x.append('<b id=\"%s.%s\">Content %s.%s</b>' % (i,j, i,j))\n+            x.append('</a>')\n+        self.x = ''.join(x)\n+\n+    def testOnly(self):\n+        strainer = SoupStrainer(\"b\")\n+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)\n+        self.assertEquals(len(soup), 15)\n+\n+        strainer = SoupStrainer(id=re.compile(\"100.*\"))\n+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)\n+        self.assertEquals(len(soup), 5)\n+\n+        strainer = SoupStrainer(text=re.compile(\"10[01].*\"))\n+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)\n+        self.assertEquals(len(soup), 10)\n+\n+        strainer = SoupStrainer(text=lambda(x):x[8]=='3')\n+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)\n+        self.assertEquals(len(soup), 3)\n+\n+class PickleMeThis(SoupTest):\n+    \"Testing features like pickle and deepcopy.\"\n+\n+    def setUp(self):\n+        self.page = \"\"\"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\n+\"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n+<html>\n+<head>\n+<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n+<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n+<link rev=\"made\" href=\"mailto:leonardr@segfault.org\">\n+<meta name=\"Description\" content=\"Beautiful Soup: an HTML parser optimized for screen-scraping.\">\n+<meta name=\"generator\" content=\"Markov Approximation 1.4 (module: leonardr)\">\n+<meta name=\"author\" content=\"Leonard Richardson\">\n+</head>\n+<body>\n+<a href=\"foo\">foo</a>\n+<a href=\"foo\"><b>bar</b></a>\n+</body>\n+</html>\"\"\"\n+\n+        self.soup = BeautifulSoup(self.page)\n+\n+    def testPickle(self):\n+        import pickle\n+        dumped = pickle.dumps(self.soup, 2)\n+        loaded = pickle.loads(dumped)\n+        self.assertEqual(loaded.__class__, BeautifulSoup)\n+        self.assertEqual(str(loaded), str(self.soup))\n+\n+    def testDeepcopy(self):\n+        from copy import deepcopy\n+        copied = deepcopy(self.soup)\n+        self.assertEqual(str(copied), str(self.soup))\n+\n+    def testUnicodePickle(self):\n+        import cPickle as pickle\n+        html = \"<b>\" + chr(0xc3) + \"</b>\"\n+        soup = BeautifulSoup(html)\n+        dumped = pickle.dumps(soup, pickle.HIGHEST_PROTOCOL)\n+        loaded = pickle.loads(dumped)\n+        self.assertEqual(str(loaded), str(soup))\n+\n+\n+class WriteOnlyCode(SoupTest):\n+    \"Testing the modification of the tree.\"\n+\n+    def testModifyAttributes(self):\n+        soup = BeautifulSoup('<a id=\"1\"></a>')\n+        soup.a['id'] = 2\n+        self.assertEqual(soup.renderContents(), '<a id=\"2\"></a>')\n+        del(soup.a['id'])\n+        self.assertEqual(soup.renderContents(), '<a></a>')\n+        soup.a['id2'] = 'foo'\n+        self.assertEqual(soup.renderContents(), '<a id2=\"foo\"></a>')\n+\n+    def testNewTagCreation(self):\n+        \"Makes sure tags don't step on each others' toes.\"\n+        soup = BeautifulSoup()\n+        a = Tag(soup, 'a')\n+        ol = Tag(soup, 'ol')\n+        a['href'] = 'http://foo.com/'\n+        self.assertRaises(KeyError, lambda : ol['href'])\n+\n+    def testNewTagWithAttributes(self):\n+        \"\"\"Makes sure new tags can be created complete with attributes.\"\"\"\n+        soup = BeautifulSoup()\n+        a = Tag(soup, 'a', [('href', 'foo')])\n+        b = Tag(soup, 'b', {'class':'bar'})\n+        soup.insert(0,a)\n+        soup.insert(1,b)\n+        self.assertEqual(soup.a['href'], 'foo')\n+        self.assertEqual(soup.b['class'], 'bar')\n+\n+    def testTagReplacement(self):\n+        # Make sure you can replace an element with itself.\n+        text = \"<a><b></b><c>Foo<d></d></c></a><a><e></e></a>\"\n+        soup = BeautifulSoup(text)\n+        c = soup.c\n+        soup.c.replaceWith(c)\n+        self.assertEquals(str(soup), text)\n+\n+        # A very simple case\n+        soup = BeautifulSoup(\"<b>Argh!</b>\")\n+        soup.find(text=\"Argh!\").replaceWith(\"Hooray!\")\n+        newText = soup.find(text=\"Hooray!\")\n+        b = soup.b\n+        self.assertEqual(newText.previous, b)\n+        self.assertEqual(newText.parent, b)\n+        self.assertEqual(newText.previous.next, newText)\n+        self.assertEqual(newText.next, None)\n+\n+        # A more complex case\n+        soup = BeautifulSoup(\"<a><b>Argh!</b><c></c><d></d></a>\")\n+        soup.b.insert(1, \"Hooray!\")\n+        newText = soup.find(text=\"Hooray!\")\n+        self.assertEqual(newText.previous, \"Argh!\")\n+        self.assertEqual(newText.previous.next, newText)\n+\n+        self.assertEqual(newText.previousSibling, \"Argh!\")\n+        self.assertEqual(newText.previousSibling.nextSibling, newText)\n+\n+        self.assertEqual(newText.nextSibling, None)\n+        self.assertEqual(newText.next, soup.c)\n+\n+        text = \"<html>There's <b>no</b> business like <b>show</b> business</html>\"\n+        soup = BeautifulSoup(text)\n+        no, show = soup.findAll('b')\n+        show.replaceWith(no)\n+        self.assertEquals(str(soup), \"<html>There's  business like <b>no</b> business</html>\")\n+\n+        # Even more complex\n+        soup = BeautifulSoup(\"<a><b>Find</b><c>lady!</c><d></d></a>\")\n+        tag = Tag(soup, 'magictag')\n+        tag.insert(0, \"the\")\n+        soup.a.insert(1, tag)\n+\n+        b = soup.b\n+        c = soup.c\n+        theText = tag.find(text=True)\n+        findText = b.find(text=\"Find\")\n+\n+        self.assertEqual(findText.next, tag)\n+        self.assertEqual(tag.previous, findText)\n+        self.assertEqual(b.nextSibling, tag)\n+        self.assertEqual(tag.previousSibling, b)\n+        self.assertEqual(tag.nextSibling, c)\n+        self.assertEqual(c.previousSibling, tag)\n+\n+        self.assertEqual(theText.next, c)\n+        self.assertEqual(c.previous, theText)\n+\n+        # Aand... incredibly complex.\n+        soup = BeautifulSoup(\"\"\"<a>We<b>reserve<c>the</c><d>right</d></b></a><e>to<f>refuse</f><g>service</g></e>\"\"\")\n+        f = soup.f\n+        a = soup.a\n+        c = soup.c\n+        e = soup.e\n+        weText = a.find(text=\"We\")\n+        soup.b.replaceWith(soup.f)\n+        self.assertEqual(str(soup), \"<a>We<f>refuse</f></a><e>to<g>service</g></e>\")\n+\n+        self.assertEqual(f.previous, weText)\n+        self.assertEqual(weText.next, f)\n+        self.assertEqual(f.previousSibling, weText)\n+        self.assertEqual(f.nextSibling, None)\n+        self.assertEqual(weText.nextSibling, f)\n+\n+    def testReplaceWithChildren(self):\n+        soup = BeautifulStoneSoup(\n+            \"<top><replace><child1/><child2/></replace></top>\",\n+            selfClosingTags=[\"child1\", \"child2\"])\n+        soup.replaceTag.replaceWithChildren()\n+        self.assertEqual(soup.top.contents[0].name, \"child1\")\n+        self.assertEqual(soup.top.contents[1].name, \"child2\")\n+\n+    def testAppend(self):\n+       doc = \"<p>Don't leave me <b>here</b>.</p> <p>Don't leave me.</p>\"\n+       soup = BeautifulSoup(doc)\n+       second_para = soup('p')[1]\n+       bold = soup.find('b')\n+       soup('p')[1].append(soup.find('b'))\n+       self.assertEqual(bold.parent, second_para)\n+       self.assertEqual(str(soup),\n+                        \"<p>Don't leave me .</p> \"\n+                        \"<p>Don't leave me.<b>here</b></p>\")\n+\n+    def testTagExtraction(self):\n+        # A very simple case\n+        text = '<html><div id=\"nav\">Nav crap</div>Real content here.</html>'\n+        soup = BeautifulSoup(text)\n+        extracted = soup.find(\"div\", id=\"nav\").extract()\n+        self.assertEqual(str(soup), \"<html>Real content here.</html>\")\n+        self.assertEqual(str(extracted), '<div id=\"nav\">Nav crap</div>')\n+\n+        # A simple case, a more complex test.\n+        text = \"<doc><a>1<b>2</b></a><a>i<b>ii</b></a><a>A<b>B</b></a></doc>\"\n+        soup = BeautifulStoneSoup(text)\n+        doc = soup.doc\n+        numbers, roman, letters = soup(\"a\")\n+\n+        self.assertEqual(roman.parent, doc)\n+        oldPrevious = roman.previous\n+        endOfThisTag = roman.nextSibling.previous\n+        self.assertEqual(oldPrevious, \"2\")\n+        self.assertEqual(roman.next, \"i\")\n+        self.assertEqual(endOfThisTag, \"ii\")\n+        self.assertEqual(roman.previousSibling, numbers)\n+        self.assertEqual(roman.nextSibling, letters)\n+\n+        roman.extract()\n+        self.assertEqual(roman.parent, None)\n+        self.assertEqual(roman.previous, None)\n+        self.assertEqual(roman.next, \"i\")\n+        self.assertEqual(letters.previous, '2')\n+        self.assertEqual(roman.previousSibling, None)\n+        self.assertEqual(roman.nextSibling, None)\n+        self.assertEqual(endOfThisTag.next, None)\n+        self.assertEqual(roman.b.contents[0].next, None)\n+        self.assertEqual(numbers.nextSibling, letters)\n+        self.assertEqual(letters.previousSibling, numbers)\n+        self.assertEqual(len(doc.contents), 2)\n+        self.assertEqual(doc.contents[0], numbers)\n+        self.assertEqual(doc.contents[1], letters)\n+\n+        # A more complex case.\n+        text = \"<a>1<b>2<c>Hollywood, baby!</c></b></a>3\"\n+        soup = BeautifulStoneSoup(text)\n+        one = soup.find(text=\"1\")\n+        three = soup.find(text=\"3\")\n+        toExtract = soup.b\n+        soup.b.extract()\n+        self.assertEqual(one.next, three)\n+        self.assertEqual(three.previous, one)\n+        self.assertEqual(one.parent.nextSibling, three)\n+        self.assertEqual(three.previousSibling, soup.a)\n+        \n+    def testClear(self):\n+        soup = BeautifulSoup(\"<ul><li></li><li></li></ul>\")\n+        soup.ul.clear()\n+        self.assertEqual(len(soup.ul.contents), 0)\n+\n+class TheManWithoutAttributes(SoupTest):\n+    \"Test attribute access\"\n+\n+    def testHasKey(self):\n+        text = \"<foo attr='bar'>\"\n+        self.assertEquals(BeautifulSoup(text).foo.has_key('attr'), True)\n+\n+class QuoteMeOnThat(SoupTest):\n+    \"Test quoting\"\n+    def testQuotedAttributeValues(self):\n+        self.assertSoupEquals(\"<foo attr='bar'></foo>\",\n+                              '<foo attr=\"bar\"></foo>')\n+\n+        text = \"\"\"<foo attr='bar \"brawls\" happen'>a</foo>\"\"\"\n+        soup = BeautifulSoup(text)\n+        self.assertEquals(soup.renderContents(), text)\n+\n+        soup.foo['attr'] = 'Brawls happen at \"Bob\\'s Bar\"'\n+        newText = \"\"\"<foo attr='Brawls happen at \"Bob&squot;s Bar\"'>a</foo>\"\"\"\n+        self.assertSoupEquals(soup.renderContents(), newText)\n+\n+        self.assertSoupEquals('<this is=\"really messed up & stuff\">',\n+                              '<this is=\"really messed up &amp; stuff\"></this>')\n+\n+        # This is not what the original author had in mind, but it's\n+        # a legitimate interpretation of what they wrote.\n+        self.assertSoupEquals(\"\"\"<a href=\"foo</a>, </a><a href=\"bar\">baz</a>\"\"\",\n+        '<a href=\"foo&lt;/a&gt;, &lt;/a&gt;&lt;a href=\"></a>, <a href=\"bar\">baz</a>')\n+\n+        # SGMLParser generates bogus parse events when attribute values\n+        # contain embedded brackets, but at least Beautiful Soup fixes\n+        # it up a little.\n+        self.assertSoupEquals('<a b=\"<a>\">', '<a b=\"&lt;a&gt;\"></a><a>\"&gt;</a>')\n+        self.assertSoupEquals('<a href=\"http://foo.com/<a> and blah and blah',\n+                              \"\"\"<a href='\"http://foo.com/'></a><a> and blah and blah</a>\"\"\")\n+\n+\n+\n+class YoureSoLiteral(SoupTest):\n+    \"Test literal mode.\"\n+    def testLiteralMode(self):\n+        text = \"<script>if (i<imgs.length)</script><b>Foo</b>\"\n+        soup = BeautifulSoup(text)\n+        self.assertEqual(soup.script.contents[0], \"if (i<imgs.length)\")\n+        self.assertEqual(soup.b.contents[0], \"Foo\")\n+\n+    def testTextArea(self):\n+        text = \"<textarea><b>This is an example of an HTML tag</b><&<&</textarea>\"\n+        soup = BeautifulSoup(text)\n+        self.assertEqual(soup.textarea.contents[0],\n+                         \"<b>This is an example of an HTML tag</b><&<&\")\n+\n+class OperatorOverload(SoupTest):\n+    \"Our operators do it all! Call now!\"\n+\n+    def testTagNameAsFind(self):\n+        \"Tests that referencing a tag name as a member delegates to find().\"\n+        soup = BeautifulSoup('<b id=\"1\">foo<i>bar</i></b><b>Red herring</b>')\n+        self.assertEqual(soup.b.i, soup.find('b').find('i'))\n+        self.assertEqual(soup.b.i.string, 'bar')\n+        self.assertEqual(soup.b['id'], '1')\n+        self.assertEqual(soup.b.contents[0], 'foo')\n+        self.assert_(not soup.a)\n+\n+        #Test the .fooTag variant of .foo.\n+        self.assertEqual(soup.bTag.iTag.string, 'bar')\n+        self.assertEqual(soup.b.iTag.string, 'bar')\n+        self.assertEqual(soup.find('b').find('i'), soup.bTag.iTag)\n+\n+class NestableEgg(SoupTest):\n+    \"\"\"Here we test tag nesting. TEST THE NEST, DUDE! X-TREME!\"\"\"\n+\n+    def testParaInsideBlockquote(self):\n+        soup = BeautifulSoup('<blockquote><p><b>Foo</blockquote><p>Bar')\n+        self.assertEqual(soup.blockquote.p.b.string, 'Foo')\n+        self.assertEqual(soup.blockquote.b.string, 'Foo')\n+        self.assertEqual(soup.find('p', recursive=False).string, 'Bar')\n+\n+    def testNestedTables(self):\n+        text = \"\"\"<table id=\"1\"><tr><td>Here's another table:\n+        <table id=\"2\"><tr><td>Juicy text</td></tr></table></td></tr></table>\"\"\"\n+        soup = BeautifulSoup(text)\n+        self.assertEquals(soup.table.table.td.string, 'Juicy text')\n+        self.assertEquals(len(soup.findAll('table')), 2)\n+        self.assertEquals(len(soup.table.findAll('table')), 1)\n+        self.assertEquals(soup.find('table', {'id' : 2}).parent.parent.parent.name,\n+                          'table')\n+\n+        text = \"<table><tr><td><div><table>Foo</table></div></td></tr></table>\"\n+        soup = BeautifulSoup(text)\n+        self.assertEquals(soup.table.tr.td.div.table.contents[0], \"Foo\")\n+\n+        text = \"\"\"<table><thead><tr>Foo</tr></thead><tbody><tr>Bar</tr></tbody>\n+        <tfoot><tr>Baz</tr></tfoot></table>\"\"\"\n+        soup = BeautifulSoup(text)\n+        self.assertEquals(soup.table.thead.tr.contents[0], \"Foo\")\n+\n+    def testBadNestedTables(self):\n+        soup = BeautifulSoup(\"<table><tr><table><tr id='nested'>\")\n+        self.assertEquals(soup.table.tr.table.tr['id'], 'nested')\n+\n+class CleanupOnAisleFour(SoupTest):\n+    \"\"\"Here we test cleanup of text that breaks SGMLParser or is just\n+    obnoxious.\"\"\"\n+\n+    def testSelfClosingtag(self):\n+        self.assertEqual(str(BeautifulSoup(\"Foo<br/>Bar\").find('br')),\n+                         '<br />')\n+\n+        self.assertSoupEquals('<p>test1<br/>test2</p>',\n+                              '<p>test1<br />test2</p>')\n+\n+        text = '<p>test1<selfclosing>test2'\n+        soup = BeautifulStoneSoup(text)\n+        self.assertEqual(str(soup),\n+                         '<p>test1<selfclosing>test2</selfclosing></p>')\n+\n+        soup = BeautifulStoneSoup(text, selfClosingTags='selfclosing')\n+        self.assertEqual(str(soup),\n+                         '<p>test1<selfclosing />test2</p>')\n+\n+    def testSelfClosingTagOrNot(self):\n+        text = \"<item><link>http://foo.com/</link></item>\"\n+        self.assertEqual(BeautifulStoneSoup(text).renderContents(), text)\n+        self.assertEqual(BeautifulSoup(text).renderContents(),\n+                         '<item><link />http://foo.com/</item>')\n+\n+    def testCData(self):\n+        xml = \"<root>foo<![CDATA[foobar]]>bar</root>\"\n+        self.assertSoupEquals(xml, xml)\n+        r = re.compile(\"foo.*bar\")\n+        soup = BeautifulSoup(xml)\n+        self.assertEquals(soup.find(text=r).string, \"foobar\")\n+        self.assertEquals(soup.find(text=r).__class__, CData)\n+\n+    def testComments(self):\n+        xml = \"foo<!--foobar-->baz\"\n+        self.assertSoupEquals(xml)\n+        r = re.compile(\"foo.*bar\")\n+        soup = BeautifulSoup(xml)\n+        self.assertEquals(soup.find(text=r).string, \"foobar\")\n+        self.assertEquals(soup.find(text=\"foobar\").__class__, Comment)\n+\n+    def testDeclaration(self):\n+        xml = \"foo<!DOCTYPE foobar>baz\"\n+        self.assertSoupEquals(xml)\n+        r = re.compile(\".*foo.*bar\")\n+        soup = BeautifulSoup(xml)\n+        text = \"DOCTYPE foobar\"\n+        self.assertEquals(soup.find(text=r).string, text)\n+        self.assertEquals(soup.find(text=text).__class__, Declaration)\n+\n+        namespaced_doctype = ('<!DOCTYPE xsl:stylesheet SYSTEM \"htmlent.dtd\">'\n+                              '<html>foo</html>')\n+        soup = BeautifulSoup(namespaced_doctype)\n+        self.assertEquals(soup.contents[0],\n+                          'DOCTYPE xsl:stylesheet SYSTEM \"htmlent.dtd\"')\n+        self.assertEquals(soup.html.contents[0], 'foo')\n+\n+    def testEntityConversions(self):\n+        text = \"&lt;&lt;sacr&eacute;&#32;bleu!&gt;&gt;\"\n+        soup = BeautifulStoneSoup(text)\n+        self.assertSoupEquals(text)\n+\n+        xmlEnt = BeautifulStoneSoup.XML_ENTITIES\n+        htmlEnt = BeautifulStoneSoup.HTML_ENTITIES\n+        xhtmlEnt = BeautifulStoneSoup.XHTML_ENTITIES\n+\n+        soup = BeautifulStoneSoup(text, convertEntities=xmlEnt)\n+        self.assertEquals(str(soup), \"&lt;&lt;sacr&eacute; bleu!&gt;&gt;\")\n+\n+        soup = BeautifulStoneSoup(text, convertEntities=htmlEnt)\n+        self.assertEquals(unicode(soup), u\"&lt;&lt;sacr\\xe9 bleu!&gt;&gt;\")\n+\n+        # Make sure the \"XML\", \"HTML\", and \"XHTML\" settings work.\n+        text = \"&lt;&trade;&apos;\"\n+        soup = BeautifulStoneSoup(text, convertEntities=xmlEnt)\n+        self.assertEquals(unicode(soup), u\"&lt;&trade;'\")\n+\n+        soup = BeautifulStoneSoup(text, convertEntities=htmlEnt)\n+        self.assertEquals(unicode(soup), u\"&lt;\\u2122&apos;\")\n+\n+        soup = BeautifulStoneSoup(text, convertEntities=xhtmlEnt)\n+        self.assertEquals(unicode(soup), u\"&lt;\\u2122'\")\n+\n+        invalidEntity = \"foo&#bar;baz\"\n+        soup = BeautifulStoneSoup\\\n+               (invalidEntity,\n+                convertEntities=htmlEnt)\n+        self.assertEquals(str(soup), \"foo&amp;#bar;baz\")\n+\n+        nonexistentEntity = \"foo&bar;baz\"\n+        soup = BeautifulStoneSoup\\\n+               (nonexistentEntity,\n+                convertEntities=\"xml\")\n+        self.assertEquals(str(soup), nonexistentEntity)\n+\n+\n+    def testNonBreakingSpaces(self):\n+        soup = BeautifulSoup(\"<a>&nbsp;&nbsp;</a>\",\n+                             convertEntities=BeautifulStoneSoup.HTML_ENTITIES)\n+        self.assertEquals(unicode(soup), u\"<a>\\xa0\\xa0</a>\")\n+\n+    def testWhitespaceInDeclaration(self):\n+        self.assertSoupEquals('<! DOCTYPE>', '<!DOCTYPE>')\n+\n+    def testJunkInDeclaration(self):\n+        self.assertSoupEquals('<! Foo = -8>a', '&lt;!Foo = -8&gt;a')\n+\n+    def testIncompleteDeclaration(self):\n+        self.assertSoupEquals('a<!b <p>c', 'a&lt;!b &lt;p&gt;c')\n+\n+    def testEntityReplacement(self):\n+        self.assertSoupEquals('<b>hello&nbsp;there</b>')\n+\n+    def testEntitiesInAttributeValues(self):\n+        self.assertSoupEquals('<x t=\"x&#241;\">', '<x t=\"x\\xc3\\xb1\"></x>')\n+        self.assertSoupEquals('<x t=\"x&#xf1;\">', '<x t=\"x\\xc3\\xb1\"></x>')\n+\n+        soup = BeautifulSoup('<x t=\"&gt;&trade;\">',\n+                             convertEntities=BeautifulStoneSoup.HTML_ENTITIES)\n+        self.assertEquals(unicode(soup), u'<x t=\"&gt;\\u2122\"></x>')\n+\n+        uri = \"http://crummy.com?sacr&eacute;&amp;bleu\"\n+        link = '<a href=\"%s\"></a>' % uri\n+        soup = BeautifulSoup(link)\n+        self.assertEquals(unicode(soup), link)\n+        #self.assertEquals(unicode(soup.a['href']), uri)\n+\n+        soup = BeautifulSoup(link, convertEntities=BeautifulSoup.HTML_ENTITIES)\n+        self.assertEquals(unicode(soup),\n+                          link.replace(\"&eacute;\", u\"\\xe9\"))\n+\n+        uri = \"http://crummy.com?sacr&eacute;&bleu\"\n+        link = '<a href=\"%s\"></a>' % uri\n+        soup = BeautifulSoup(link, convertEntities=BeautifulSoup.HTML_ENTITIES)\n+        self.assertEquals(unicode(soup.a['href']),\n+                          uri.replace(\"&eacute;\", u\"\\xe9\"))\n+\n+    def testNakedAmpersands(self):\n+        html = {'convertEntities':BeautifulStoneSoup.HTML_ENTITIES}\n+        soup = BeautifulStoneSoup(\"AT&T \", **html)\n+        self.assertEquals(str(soup), 'AT&amp;T ')\n+\n+        nakedAmpersandInASentence = \"AT&T was Ma Bell\"\n+        soup = BeautifulStoneSoup(nakedAmpersandInASentence,**html)\n+        self.assertEquals(str(soup), \\\n+               nakedAmpersandInASentence.replace('&','&amp;'))\n+\n+        invalidURL = '<a href=\"http://example.org?a=1&b=2;3\">foo</a>'\n+        validURL = invalidURL.replace('&','&amp;')\n+        soup = BeautifulStoneSoup(invalidURL)\n+        self.assertEquals(str(soup), validURL)\n+\n+        soup = BeautifulStoneSoup(validURL)\n+        self.assertEquals(str(soup), validURL)\n+\n+\n+class EncodeRed(SoupTest):\n+    \"\"\"Tests encoding conversion, Unicode conversion, and Microsoft\n+    smart quote fixes.\"\"\"\n+\n+    def testUnicodeDammitStandalone(self):\n+        markup = \"<foo>\\x92</foo>\"\n+        dammit = UnicodeDammit(markup)\n+        self.assertEquals(dammit.unicode, \"<foo>&#x2019;</foo>\")\n+\n+        hebrew = \"\\xed\\xe5\\xec\\xf9\"\n+        dammit = UnicodeDammit(hebrew, [\"iso-8859-8\"])\n+        self.assertEquals(dammit.unicode, u'\\u05dd\\u05d5\\u05dc\\u05e9')\n+        self.assertEquals(dammit.originalEncoding, 'iso-8859-8')\n+\n+    def testGarbageInGarbageOut(self):\n+        ascii = \"<foo>a</foo>\"\n+        asciiSoup = BeautifulStoneSoup(ascii)\n+        self.assertEquals(ascii, str(asciiSoup))\n+\n+        unicodeData = u\"<foo>\\u00FC</foo>\"\n+        utf8 = unicodeData.encode(\"utf-8\")\n+        self.assertEquals(utf8, '<foo>\\xc3\\xbc</foo>')\n+\n+        unicodeSoup = BeautifulStoneSoup(unicodeData)\n+        self.assertEquals(unicodeData, unicode(unicodeSoup))\n+        self.assertEquals(unicode(unicodeSoup.foo.string), u'\\u00FC')\n+\n+        utf8Soup = BeautifulStoneSoup(utf8, fromEncoding='utf-8')\n+        self.assertEquals(utf8, str(utf8Soup))\n+        self.assertEquals(utf8Soup.originalEncoding, \"utf-8\")\n+\n+        utf8Soup = BeautifulStoneSoup(unicodeData)\n+        self.assertEquals(utf8, str(utf8Soup))\n+        self.assertEquals(utf8Soup.originalEncoding, None)\n+\n+\n+    def testHandleInvalidCodec(self):\n+        for bad_encoding in ['.utf8', '...', 'utF---16.!']:\n+            soup = BeautifulSoup(\"R\u00e4ksm\u00f6rg\u00e5s\", fromEncoding=bad_encoding)\n+            self.assertEquals(soup.originalEncoding, 'utf-8')\n+\n+    def testUnicodeSearch(self):\n+        html = u'<html><body><h1>R\u00e4ksm\u00f6rg\u00e5s</h1></body></html>'\n+        soup = BeautifulSoup(html)\n+        self.assertEqual(soup.find(text=u'R\u00e4ksm\u00f6rg\u00e5s'),u'R\u00e4ksm\u00f6rg\u00e5s')\n+\n+    def testRewrittenXMLHeader(self):\n+        euc_jp = '<?xml version=\"1.0 encoding=\"euc-jp\"?>\\n<foo>\\n\\xa4\\xb3\\xa4\\xec\\xa4\\xcfEUC-JP\\xa4\\xc7\\xa5\\xb3\\xa1\\xbc\\xa5\\xc7\\xa5\\xa3\\xa5\\xf3\\xa5\\xb0\\xa4\\xb5\\xa4\\xec\\xa4\\xbf\\xc6\\xfc\\xcb\\xdc\\xb8\\xec\\xa4\\xce\\xa5\\xd5\\xa5\\xa1\\xa5\\xa4\\xa5\\xeb\\xa4\\xc7\\xa4\\xb9\\xa1\\xa3\\n</foo>\\n'\n+        utf8 = \"<?xml version='1.0' encoding='utf-8'?>\\n<foo>\\n\\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xafEUC-JP\\xe3\\x81\\xa7\\xe3\\x82\\xb3\\xe3\\x83\\xbc\\xe3\\x83\\x87\\xe3\\x82\\xa3\\xe3\\x83\\xb3\\xe3\\x82\\xb0\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x81\\x9f\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe8\\xaa\\x9e\\xe3\\x81\\xae\\xe3\\x83\\x95\\xe3\\x82\\xa1\\xe3\\x82\\xa4\\xe3\\x83\\xab\\xe3\\x81\\xa7\\xe3\\x81\\x99\\xe3\\x80\\x82\\n</foo>\\n\"\n+        soup = BeautifulStoneSoup(euc_jp)\n+        if soup.originalEncoding != \"euc-jp\":\n+            raise Exception(\"Test failed when parsing euc-jp document. \"\n+                            \"If you're running Python >=2.4, or you have \"\n+                            \"cjkcodecs installed, this is a real problem. \"\n+                            \"Otherwise, ignore it.\")\n+\n+        self.assertEquals(soup.originalEncoding, \"euc-jp\")\n+        self.assertEquals(str(soup), utf8)\n+\n+        old_text = \"<?xml encoding='windows-1252'><foo>\\x92</foo>\"\n+        new_text = \"<?xml version='1.0' encoding='utf-8'?><foo>&rsquo;</foo>\"\n+        self.assertSoupEquals(old_text, new_text)\n+\n+    def testRewrittenMetaTag(self):\n+        no_shift_jis_html = '''<html><head>\\n<meta http-equiv=\"Content-language\" content=\"ja\" /></head><body><pre>\\n\\x82\\xb1\\x82\\xea\\x82\\xcdShift-JIS\\x82\\xc5\\x83R\\x81[\\x83f\\x83B\\x83\\x93\\x83O\\x82\\xb3\\x82\\xea\\x82\\xbd\\x93\\xfa\\x96{\\x8c\\xea\\x82\\xcc\\x83t\\x83@\\x83C\\x83\\x8b\\x82\\xc5\\x82\\xb7\\x81B\\n</pre></body></html>'''\n+        soup = BeautifulSoup(no_shift_jis_html)\n+\n+        # Beautiful Soup used to try to rewrite the meta tag even if the\n+        # meta tag got filtered out by the strainer. This test makes\n+        # sure that doesn't happen.\n+        strainer = SoupStrainer('pre')\n+        soup = BeautifulSoup(no_shift_jis_html, parseOnlyThese=strainer)\n+        self.assertEquals(soup.contents[0].name, 'pre')\n+\n+        meta_tag = ('<meta content=\"text/html; charset=x-sjis\" '\n+                    'http-equiv=\"Content-type\" />')\n+        shift_jis_html = (\n+            '<html><head>\\n%s\\n'\n+            '<meta http-equiv=\"Content-language\" content=\"ja\" />'\n+            '</head><body><pre>\\n'\n+            '\\x82\\xb1\\x82\\xea\\x82\\xcdShift-JIS\\x82\\xc5\\x83R\\x81[\\x83f'\n+            '\\x83B\\x83\\x93\\x83O\\x82\\xb3\\x82\\xea\\x82\\xbd\\x93\\xfa\\x96{\\x8c'\n+            '\\xea\\x82\\xcc\\x83t\\x83@\\x83C\\x83\\x8b\\x82\\xc5\\x82\\xb7\\x81B\\n'\n+            '</pre></body></html>') % meta_tag\n+        soup = BeautifulSoup(shift_jis_html)\n+        if soup.originalEncoding != \"shift-jis\":\n+            raise Exception(\"Test failed when parsing shift-jis document \"\n+                            \"with meta tag '%s'.\"\n+                            \"If you're running Python >=2.4, or you have \"\n+                            \"cjkcodecs installed, this is a real problem. \"\n+                            \"Otherwise, ignore it.\" % meta_tag)\n+        self.assertEquals(soup.originalEncoding, \"shift-jis\")\n+\n+        content_type_tag = soup.meta['content']\n+        self.assertEquals(content_type_tag[content_type_tag.find('charset='):],\n+                          'charset=%SOUP-ENCODING%')\n+        content_type = str(soup.meta)\n+        index = content_type.find('charset=')\n+        self.assertEqual(content_type[index:index+len('charset=utf8')+1],\n+                         'charset=utf-8')\n+        content_type = soup.meta.__str__('shift-jis')\n+        index = content_type.find('charset=')\n+        self.assertEqual(content_type[index:index+len('charset=shift-jis')],\n+                         'charset=shift-jis')\n+\n+        self.assertEquals(str(soup), (\n+                '<html><head>\\n'\n+                '<meta content=\"text/html; charset=utf-8\" '\n+                'http-equiv=\"Content-type\" />\\n'\n+                '<meta http-equiv=\"Content-language\" content=\"ja\" />'\n+                '</head><body><pre>\\n'\n+                '\\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xafShift-JIS\\xe3\\x81\\xa7\\xe3'\n+                '\\x82\\xb3\\xe3\\x83\\xbc\\xe3\\x83\\x87\\xe3\\x82\\xa3\\xe3\\x83\\xb3\\xe3'\n+                '\\x82\\xb0\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x81\\x9f\\xe6\\x97\\xa5\\xe6'\n+                '\\x9c\\xac\\xe8\\xaa\\x9e\\xe3\\x81\\xae\\xe3\\x83\\x95\\xe3\\x82\\xa1\\xe3'\n+                '\\x82\\xa4\\xe3\\x83\\xab\\xe3\\x81\\xa7\\xe3\\x81\\x99\\xe3\\x80\\x82\\n'\n+                '</pre></body></html>'))\n+        self.assertEquals(soup.renderContents(\"shift-jis\"),\n+                          shift_jis_html.replace('x-sjis', 'shift-jis'))\n+\n+        isolatin =\"\"\"<html><meta http-equiv=\"Content-type\" content=\"text/html; charset=ISO-Latin-1\" />Sacr\\xe9 bleu!</html>\"\"\"\n+        soup = BeautifulSoup(isolatin)\n+        self.assertSoupEquals(soup.__str__(\"utf-8\"),\n+                              isolatin.replace(\"ISO-Latin-1\", \"utf-8\").replace(\"\\xe9\", \"\\xc3\\xa9\"))\n+\n+    def testHebrew(self):\n+        iso_8859_8= '<HEAD>\\n<TITLE>Hebrew (ISO 8859-8) in Visual Directionality</TITLE>\\n\\n\\n\\n</HEAD>\\n<BODY>\\n<H1>Hebrew (ISO 8859-8) in Visual Directionality</H1>\\n\\xed\\xe5\\xec\\xf9\\n</BODY>\\n'\n+        utf8 = '<head>\\n<title>Hebrew (ISO 8859-8) in Visual Directionality</title>\\n</head>\\n<body>\\n<h1>Hebrew (ISO 8859-8) in Visual Directionality</h1>\\n\\xd7\\x9d\\xd7\\x95\\xd7\\x9c\\xd7\\xa9\\n</body>\\n'\n+        soup = BeautifulStoneSoup(iso_8859_8, fromEncoding=\"iso-8859-8\")\n+        self.assertEquals(str(soup), utf8)\n+\n+    def testSmartQuotesNotSoSmartAnymore(self):\n+        self.assertSoupEquals(\"\\x91Foo\\x92 <!--blah-->\",\n+                              '&lsquo;Foo&rsquo; <!--blah-->')\n+\n+    def testDontConvertSmartQuotesWhenAlsoConvertingEntities(self):\n+        smartQuotes = \"Il a dit, \\x8BSacr&eacute; bl&#101;u!\\x9b\"\n+        soup = BeautifulSoup(smartQuotes)\n+        self.assertEquals(str(soup),\n+                          'Il a dit, &lsaquo;Sacr&eacute; bl&#101;u!&rsaquo;')\n+        soup = BeautifulSoup(smartQuotes, convertEntities=\"html\")\n+        self.assertEquals(str(soup),\n+                          'Il a dit, \\xe2\\x80\\xb9Sacr\\xc3\\xa9 bleu!\\xe2\\x80\\xba')\n+\n+    def testDontSeeSmartQuotesWhereThereAreNone(self):\n+        utf_8 = \"\\343\\202\\261\\343\\203\\274\\343\\202\\277\\343\\202\\244 Watch\"\n+        self.assertSoupEquals(utf_8)\n+\n+\n+class Whitewash(SoupTest):\n+    \"\"\"Test whitespace preservation.\"\"\"\n+\n+    def testPreservedWhitespace(self):\n+        self.assertSoupEquals(\"<pre>   </pre>\")\n+        self.assertSoupEquals(\"<pre> woo  </pre>\")\n+\n+    def testCollapsedWhitespace(self):\n+        self.assertSoupEquals(\"<p>   </p>\", \"<p> </p>\")\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()"
            },
            {
                "sha": "6198f516e48a50a85826af93d0d1a85be38b5679",
                "filename": "github-analytics/libs/BeautifulSoup-3.2.2/README.md",
                "status": "added",
                "additions": 18,
                "deletions": 0,
                "changes": 18,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/BeautifulSoup-3.2.2/README.md?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,18 @@\n+Beautiful Soup is a library that makes it easy to scrape information\n+from web pages. It sits atop an HTML or XML parser, providing Pythonic\n+idioms for iterating, searching, and modifying the parse tree.\n+\n+# Discontinuation notice\n+\n+You should use the 'beautifulsoup4' package instead of this package.\n+\n+Development on the 3.x series of Beautiful Soup ended in 2011, and the\n+series will be discontinued on January 1, 2021, one year after the\n+Python 2 sunsetting date. At some point after that, the\n+'beautifulsoup' pip package will be updated to a recent version of\n+Beautiful Soup. This will free up the 'beautifulsoup' package name to\n+be used by a more recent release.\n+\n+If you're relying on version 3 of Beautiful Soup, you really ought to\n+port your code to Python 3. A relatively small part of this work will\n+be migrating your Beautiful Soup code to Beautiful Soup 4."
            },
            {
                "sha": "4413a904de5a5fa8d93a225de178e79ae9930227",
                "filename": "github-analytics/libs/BeautifulSoup-3.2.2/setup.py",
                "status": "added",
                "additions": 36,
                "deletions": 0,
                "changes": 36,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/setup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/BeautifulSoup-3.2.2/setup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/BeautifulSoup-3.2.2/setup.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,36 @@\n+# The very first thing we do is give a useful error if someone is\n+# running this code under Python 3.\n+\"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"<>\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n+\n+from setuptools import (\n+    setup,\n+    find_packages,\n+)\n+from BeautifulSoup import __version__\n+\n+with open(\"README.md\", \"r\") as fh:\n+    long_description = fh.read()\n+\n+setup(\n+    name=\"BeautifulSoup\",\n+    version = __version__,\n+    author=\"Leonard Richardson\",\n+    author_email='leonardr@segfault.org',\n+    url=\"http://www.crummy.com/software/BeautifulSoup/\",\n+    download_url = \"http://www.crummy.com/software/BeautifulSoup/download/\",\n+    description=\"Screen-scraping library\",\n+    long_description=long_description,\n+    long_description_content_type=\"text/markdown\",\n+    license=\"MIT\",\n+    py_modules=['BeautifulSoup', 'BeautifulSoupTests'],\n+    classifiers=[\"Development Status :: 5 - Production/Stable\",\n+                 \"Intended Audience :: Developers\",\n+                 \"License :: OSI Approved :: Python Software Foundation License\",\n+                 \"Programming Language :: Python\",\n+                 \"Programming Language :: Python :: 2.7\",\n+                 \"Topic :: Text Processing :: Markup :: HTML\",\n+                 \"Topic :: Text Processing :: Markup :: XML\",\n+                 \"Topic :: Text Processing :: Markup :: SGML\",\n+                 \"Topic :: Software Development :: Libraries :: Python Modules\",\n+             ],\n+)"
            },
            {
                "sha": "afd39cff3bc2d9d449d2674f27a2a0d3e450c8d9",
                "filename": "github-analytics/libs/Flask-Cassandra-0.14/PKG-INFO",
                "status": "added",
                "additions": 25,
                "deletions": 0,
                "changes": 25,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/Flask-Cassandra-0.14/PKG-INFO",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/Flask-Cassandra-0.14/PKG-INFO",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/Flask-Cassandra-0.14/PKG-INFO?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,25 @@\n+Metadata-Version: 1.1\n+Name: Flask-Cassandra\n+Version: 0.14\n+Summary: Provides a connection to a Cassandra cluster in a Flask app\n+Home-page: http://terbiumlabs.com/flask-cassandra/\n+Author: Michael Moore\n+Author-email: michael@terbiumlabs.com\n+License: BSD\n+Description: \n+        Flask-Cassandra\n+        -------------\n+        \n+        Flask-Cassandra provides an application-level connection\n+        to an Apache Cassandra database. This connection can be\n+        used to interact with a Cassandra cluster.\n+        \n+        \n+Platform: any\n+Classifier: Environment :: Web Environment\n+Classifier: Intended Audience :: Developers\n+Classifier: License :: OSI Approved :: BSD License\n+Classifier: Operating System :: OS Independent\n+Classifier: Programming Language :: Python\n+Classifier: Topic :: Internet :: WWW/HTTP :: Dynamic Content\n+Classifier: Topic :: Software Development :: Libraries :: Python Modules"
            },
            {
                "sha": "b6e4a39fcab36d79b5e1403d80830c0ccc000e11",
                "filename": "github-analytics/libs/Flask-Cassandra-0.14/flask_cassandra.py",
                "status": "added",
                "additions": 77,
                "deletions": 0,
                "changes": 77,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/Flask-Cassandra-0.14/flask_cassandra.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/Flask-Cassandra-0.14/flask_cassandra.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/Flask-Cassandra-0.14/flask_cassandra.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,77 @@\n+# -*- coding: utf-8 -*-\n+'''\n+    flask-cassandra\n+    ---------------\n+    Flask-Cassandra provides an application-level connection\n+    to an Apache Cassandra database. This connection can be\n+    used to interact with a Cassandra cluster.\n+\n+    :copyright: (c) 2015 by Terbium Labs.\n+    :license: BSD, see LICENSE for more details.\n+'''\n+\n+__version_info__ = ('0', '1', '4')\n+__version__ = '.'.join(__version_info__)\n+__author__ = 'Michael Moore'\n+__license__ = 'BSD'\n+__copyright__ = '(c) 2015 by TerbiumLabs'\n+\n+from cassandra.cluster import Cluster\n+import logging\n+\n+from flask import current_app\n+\n+log = logging.getLogger(__name__)\n+\n+try:\n+    from flask import _app_ctx_stack as stack\n+except ImportError:\n+    from flask import _request_ctx_stack as stack\n+\n+\n+try:\n+    unicode\n+except NameError:  # Python3\n+    unicode = str\n+\n+\n+class CassandraCluster(object):\n+\n+    def __init__(self, app=None):\n+        self.app = app\n+        self.cluster = None\n+        if app is not None:\n+            self.init_app(app)\n+\n+    def init_app(self, app):\n+        app.config.setdefault('CASSANDRA_CLUSTER', ':memory:')\n+        if hasattr(app, 'teardown_appcontext'):\n+            app.teardown_appcontext(self.teardown)\n+        else:\n+            app.teardown_request(self.teardown)\n+\n+    def connect(self):\n+        log.debug(\"Connecting to CASSANDRA NODES {}\".format(current_app.config['CASSANDRA_NODES']))\n+        if self.cluster is None:\n+            if isinstance(current_app.config['CASSANDRA_NODES'], (list, tuple)):\n+                self.cluster = Cluster(current_app.config['CASSANDRA_NODES'])\n+            elif isinstance(current_app.config['CASSANDRA_NODES'], (str, unicode)):\n+                self.cluster = Cluster([current_app.config['CASSANDRA_NODES']])\n+            else:\n+                raise TypeError(\"CASSANDRA_NODES must be defined as a list, tuple, string, or unicode object.\")\n+\n+        online_cluster = self.cluster.connect()\n+        return online_cluster\n+\n+    def teardown(self, exception):\n+        ctx = stack.top\n+        if hasattr(ctx, 'cassandra_cluster'):\n+            ctx.cassandra_cluster.shutdown()\n+\n+    @property\n+    def connection(self):\n+        ctx = stack.top\n+        if ctx is not None:\n+            if not hasattr(ctx, 'cassandra_cluster'):\n+                ctx.cassandra_cluster = self.connect()\n+            return ctx.cassandra_cluster"
            },
            {
                "sha": "861a9f554263efb088d8636c4f17a30696e495ad",
                "filename": "github-analytics/libs/Flask-Cassandra-0.14/setup.cfg",
                "status": "added",
                "additions": 5,
                "deletions": 0,
                "changes": 5,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/Flask-Cassandra-0.14/setup.cfg",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/Flask-Cassandra-0.14/setup.cfg",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/Flask-Cassandra-0.14/setup.cfg?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,5 @@\n+[egg_info]\n+tag_build = \n+tag_date = 0\n+tag_svn_revision = 0\n+"
            },
            {
                "sha": "9538d34fc73f63d9f53c4a34c231a99ca41579d4",
                "filename": "github-analytics/libs/Flask-Cassandra-0.14/setup.py",
                "status": "added",
                "additions": 39,
                "deletions": 0,
                "changes": 39,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/Flask-Cassandra-0.14/setup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/Flask-Cassandra-0.14/setup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/Flask-Cassandra-0.14/setup.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,39 @@\n+\"\"\"\n+Flask-Cassandra\n+-------------\n+\n+Flask-Cassandra provides an application-level connection\n+to an Apache Cassandra database. This connection can be\n+used to interact with a Cassandra cluster.\n+\n+\"\"\"\n+from setuptools import setup\n+\n+\n+setup(\n+    name='Flask-Cassandra',\n+    version='0.14',\n+    url='http://terbiumlabs.com/flask-cassandra/',\n+    license='BSD',\n+    author='Michael Moore',\n+    author_email='michael@terbiumlabs.com',\n+    description='Provides a connection to a Cassandra cluster in a Flask app',\n+    long_description=__doc__,\n+    py_modules=['flask_cassandra'],\n+    zip_safe=False,\n+    include_package_data=True,\n+    platforms='any',\n+    install_requires=[\n+        'Flask',\n+        'cassandra-driver'\n+    ],\n+    classifiers=[\n+        'Environment :: Web Environment',\n+        'Intended Audience :: Developers',\n+        'License :: OSI Approved :: BSD License',\n+        'Operating System :: OS Independent',\n+        'Programming Language :: Python',\n+        'Topic :: Internet :: WWW/HTTP :: Dynamic Content',\n+        'Topic :: Software Development :: Libraries :: Python Modules'\n+    ]\n+)"
            },
            {
                "sha": "68c771a099958211169377d766a7389422f5573d",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/LICENSE",
                "status": "added",
                "additions": 176,
                "deletions": 0,
                "changes": 176,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/LICENSE",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/LICENSE",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/LICENSE?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,176 @@\n+\n+                                 Apache License\n+                           Version 2.0, January 2004\n+                        http://www.apache.org/licenses/\n+\n+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n+\n+   1. Definitions.\n+\n+      \"License\" shall mean the terms and conditions for use, reproduction,\n+      and distribution as defined by Sections 1 through 9 of this document.\n+\n+      \"Licensor\" shall mean the copyright owner or entity authorized by\n+      the copyright owner that is granting the License.\n+\n+      \"Legal Entity\" shall mean the union of the acting entity and all\n+      other entities that control, are controlled by, or are under common\n+      control with that entity. For the purposes of this definition,\n+      \"control\" means (i) the power, direct or indirect, to cause the\n+      direction or management of such entity, whether by contract or\n+      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n+      outstanding shares, or (iii) beneficial ownership of such entity.\n+\n+      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n+      exercising permissions granted by this License.\n+\n+      \"Source\" form shall mean the preferred form for making modifications,\n+      including but not limited to software source code, documentation\n+      source, and configuration files.\n+\n+      \"Object\" form shall mean any form resulting from mechanical\n+      transformation or translation of a Source form, including but\n+      not limited to compiled object code, generated documentation,\n+      and conversions to other media types.\n+\n+      \"Work\" shall mean the work of authorship, whether in Source or\n+      Object form, made available under the License, as indicated by a\n+      copyright notice that is included in or attached to the work\n+      (an example is provided in the Appendix below).\n+\n+      \"Derivative Works\" shall mean any work, whether in Source or Object\n+      form, that is based on (or derived from) the Work and for which the\n+      editorial revisions, annotations, elaborations, or other modifications\n+      represent, as a whole, an original work of authorship. For the purposes\n+      of this License, Derivative Works shall not include works that remain\n+      separable from, or merely link (or bind by name) to the interfaces of,\n+      the Work and Derivative Works thereof.\n+\n+      \"Contribution\" shall mean any work of authorship, including\n+      the original version of the Work and any modifications or additions\n+      to that Work or Derivative Works thereof, that is intentionally\n+      submitted to Licensor for inclusion in the Work by the copyright owner\n+      or by an individual or Legal Entity authorized to submit on behalf of\n+      the copyright owner. For the purposes of this definition, \"submitted\"\n+      means any form of electronic, verbal, or written communication sent\n+      to the Licensor or its representatives, including but not limited to\n+      communication on electronic mailing lists, source code control systems,\n+      and issue tracking systems that are managed by, or on behalf of, the\n+      Licensor for the purpose of discussing and improving the Work, but\n+      excluding communication that is conspicuously marked or otherwise\n+      designated in writing by the copyright owner as \"Not a Contribution.\"\n+\n+      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n+      on behalf of whom a Contribution has been received by Licensor and\n+      subsequently incorporated within the Work.\n+\n+   2. Grant of Copyright License. Subject to the terms and conditions of\n+      this License, each Contributor hereby grants to You a perpetual,\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n+      copyright license to reproduce, prepare Derivative Works of,\n+      publicly display, publicly perform, sublicense, and distribute the\n+      Work and such Derivative Works in Source or Object form.\n+\n+   3. Grant of Patent License. Subject to the terms and conditions of\n+      this License, each Contributor hereby grants to You a perpetual,\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n+      (except as stated in this section) patent license to make, have made,\n+      use, offer to sell, sell, import, and otherwise transfer the Work,\n+      where such license applies only to those patent claims licensable\n+      by such Contributor that are necessarily infringed by their\n+      Contribution(s) alone or by combination of their Contribution(s)\n+      with the Work to which such Contribution(s) was submitted. If You\n+      institute patent litigation against any entity (including a\n+      cross-claim or counterclaim in a lawsuit) alleging that the Work\n+      or a Contribution incorporated within the Work constitutes direct\n+      or contributory patent infringement, then any patent licenses\n+      granted to You under this License for that Work shall terminate\n+      as of the date such litigation is filed.\n+\n+   4. Redistribution. You may reproduce and distribute copies of the\n+      Work or Derivative Works thereof in any medium, with or without\n+      modifications, and in Source or Object form, provided that You\n+      meet the following conditions:\n+\n+      (a) You must give any other recipients of the Work or\n+          Derivative Works a copy of this License; and\n+\n+      (b) You must cause any modified files to carry prominent notices\n+          stating that You changed the files; and\n+\n+      (c) You must retain, in the Source form of any Derivative Works\n+          that You distribute, all copyright, patent, trademark, and\n+          attribution notices from the Source form of the Work,\n+          excluding those notices that do not pertain to any part of\n+          the Derivative Works; and\n+\n+      (d) If the Work includes a \"NOTICE\" text file as part of its\n+          distribution, then any Derivative Works that You distribute must\n+          include a readable copy of the attribution notices contained\n+          within such NOTICE file, excluding those notices that do not\n+          pertain to any part of the Derivative Works, in at least one\n+          of the following places: within a NOTICE text file distributed\n+          as part of the Derivative Works; within the Source form or\n+          documentation, if provided along with the Derivative Works; or,\n+          within a display generated by the Derivative Works, if and\n+          wherever such third-party notices normally appear. The contents\n+          of the NOTICE file are for informational purposes only and\n+          do not modify the License. You may add Your own attribution\n+          notices within Derivative Works that You distribute, alongside\n+          or as an addendum to the NOTICE text from the Work, provided\n+          that such additional attribution notices cannot be construed\n+          as modifying the License.\n+\n+      You may add Your own copyright statement to Your modifications and\n+      may provide additional or different license terms and conditions\n+      for use, reproduction, or distribution of Your modifications, or\n+      for any such Derivative Works as a whole, provided Your use,\n+      reproduction, and distribution of the Work otherwise complies with\n+      the conditions stated in this License.\n+\n+   5. Submission of Contributions. Unless You explicitly state otherwise,\n+      any Contribution intentionally submitted for inclusion in the Work\n+      by You to the Licensor shall be under the terms and conditions of\n+      this License, without any additional terms or conditions.\n+      Notwithstanding the above, nothing herein shall supersede or modify\n+      the terms of any separate license agreement you may have executed\n+      with Licensor regarding such Contributions.\n+\n+   6. Trademarks. This License does not grant permission to use the trade\n+      names, trademarks, service marks, or product names of the Licensor,\n+      except as required for reasonable and customary use in describing the\n+      origin of the Work and reproducing the content of the NOTICE file.\n+\n+   7. Disclaimer of Warranty. Unless required by applicable law or\n+      agreed to in writing, Licensor provides the Work (and each\n+      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+      implied, including, without limitation, any warranties or conditions\n+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n+      PARTICULAR PURPOSE. You are solely responsible for determining the\n+      appropriateness of using or redistributing the Work and assume any\n+      risks associated with Your exercise of permissions under this License.\n+\n+   8. Limitation of Liability. In no event and under no legal theory,\n+      whether in tort (including negligence), contract, or otherwise,\n+      unless required by applicable law (such as deliberate and grossly\n+      negligent acts) or agreed to in writing, shall any Contributor be\n+      liable to You for damages, including any direct, indirect, special,\n+      incidental, or consequential damages of any character arising as a\n+      result of this License or out of the use or inability to use the\n+      Work (including but not limited to damages for loss of goodwill,\n+      work stoppage, computer failure or malfunction, or any and all\n+      other commercial damages or losses), even if such Contributor\n+      has been advised of the possibility of such damages.\n+\n+   9. Accepting Warranty or Additional Liability. While redistributing\n+      the Work or Derivative Works thereof, You may choose to offer,\n+      and charge a fee for, acceptance of support, warranty, indemnity,\n+      or other liability obligations and/or rights consistent with this\n+      License. However, in accepting such obligations, You may act only\n+      on Your own behalf and on Your sole responsibility, not on behalf\n+      of any other Contributor, and only if You agree to indemnify,\n+      defend, and hold each Contributor harmless for any liability\n+      incurred by, or claims asserted against, such Contributor by reason\n+      of your accepting any such warranty or additional liability.\n+"
            },
            {
                "sha": "b5bad501e41ffb70d9cacbaf0b860b6ef0ede6e7",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/README",
                "status": "added",
                "additions": 161,
                "deletions": 0,
                "changes": 161,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/README",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/README",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/README?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,161 @@\n+Python Elasticsearch Client\n+===========================\n+\n+Official low-level client for Elasticsearch. Its goal is to provide common\n+ground for all Elasticsearch-related code in Python; because of this it tries\n+to be opinion-free and very extendable.\n+\n+For a more high level client library with more limited scope, have a look at\n+`elasticsearch-dsl`_ - a more pythonic library sitting on top of\n+``elasticsearch-py``.\n+\n+It provides a more convenient and idiomatic way to write and manipulate\n+`queries`_. It stays close to the Elasticsearch JSON DSL, mirroring its\n+terminology and structure while exposing the whole range of the DSL from Python\n+either directly using defined classes or a queryset-like expressions.\n+\n+It also provides an optional `persistence layer`_ for working with documents as\n+Python objects in an ORM-like fashion: defining mappings, retrieving and saving\n+documents, wrapping the document data in user-defined classes.\n+\n+.. _elasticsearch-dsl: https://elasticsearch-dsl.readthedocs.io/\n+.. _queries: https://elasticsearch-dsl.readthedocs.io/en/latest/search_dsl.html\n+.. _persistence layer: https://elasticsearch-dsl.readthedocs.io/en/latest/persistence.html#doctype\n+\n+Compatibility\n+-------------\n+\n+The library is compatible with all Elasticsearch versions since ``0.90.x`` but you\n+**have to use a matching major version**:\n+\n+For **Elasticsearch 7.0** and later, use the major version 7 (``7.x.y``) of the\n+library.\n+\n+For **Elasticsearch 6.0** and later, use the major version 6 (``6.x.y``) of the\n+library.\n+\n+For **Elasticsearch 5.0** and later, use the major version 5 (``5.x.y``) of the\n+library.\n+\n+For **Elasticsearch 2.0** and later, use the major version 2 (``2.x.y``) of the\n+library, and so on.\n+\n+The recommended way to set your requirements in your `setup.py` or\n+`requirements.txt` is::\n+\n+    # Elasticsearch 7.x\n+    elasticsearch>=7.0.0,<8.0.0\n+\n+    # Elasticsearch 6.x\n+    elasticsearch>=6.0.0,<7.0.0\n+\n+    # Elasticsearch 5.x\n+    elasticsearch>=5.0.0,<6.0.0\n+\n+    # Elasticsearch 2.x\n+    elasticsearch>=2.0.0,<3.0.0\n+\n+If you have a need to have multiple versions installed at the same time older\n+versions are also released as ``elasticsearch2`` and ``elasticsearch5``.\n+\n+Installation\n+------------\n+\n+Install the ``elasticsearch`` package with `pip\n+<https://pypi.python.org/pypi/elasticsearch>`_::\n+\n+    pip install elasticsearch\n+\n+\n+Example use\n+-----------\n+\n+Simple use-case::\n+\n+    >>> from datetime import datetime\n+    >>> from elasticsearch import Elasticsearch\n+\n+    # by default we connect to localhost:9200\n+    >>> es = Elasticsearch()\n+\n+    # create an index in elasticsearch, ignore status code 400 (index already exists)\n+    >>> es.indices.create(index='my-index', ignore=400)\n+    {'acknowledged': True, 'shards_acknowledged': True, 'index': 'my-index'}\n+\n+    # datetimes will be serialized\n+    >>> es.index(index=\"my-index\", id=42, body={\"any\": \"data\", \"timestamp\": datetime.now()})\n+    {'_index': 'my-index',\n+     '_type': '_doc',\n+     '_id': '42',\n+     '_version': 1,\n+     'result': 'created',\n+     '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n+     '_seq_no': 0,\n+     '_primary_term': 1}\n+\n+    # but not deserialized\n+    >>> es.get(index=\"my-index\", id=42)['_source']\n+    {'any': 'data', 'timestamp': '2019-05-17T17:28:10.329598'}\n+\n+`Full documentation`_.\n+\n+.. _Full documentation: https://elasticsearch-py.readthedocs.io/\n+\n+Elastic Cloud (and SSL) use-case::\n+\n+    >>> from elasticsearch import Elasticsearch\n+    >>> es = Elasticsearch(cloud_id=\"<some_long_cloud_id>\", http_auth=('elastic','yourpassword'))\n+    >>> es.info()\n+\n+Using SSL Context with a self-signed cert use-case::\n+\n+    >>> from elasticsearch import Elasticsearch\n+    >>> from ssl import create_default_context\n+\n+    >>> context = create_default_context(cafile=\"path/to/cafile.pem\")\n+    >>> es = Elasticsearch(\"https://elasticsearch.url:port\", ssl_context=context, http_auth=('elastic','yourpassword'))\n+    >>> es.info()\n+\n+\n+\n+Features\n+--------\n+\n+The client's features include:\n+\n+ * translating basic Python data types to and from json (datetimes are not\n+   decoded for performance reasons)\n+ * configurable automatic discovery of cluster nodes\n+ * persistent connections\n+ * load balancing (with pluggable selection strategy) across all available nodes\n+ * failed connection penalization (time based - failed connections won't be\n+   retried until a timeout is reached)\n+ * support for ssl and http authentication\n+ * thread safety\n+ * pluggable architecture\n+\n+\n+License\n+-------\n+\n+Copyright 2019 Elasticsearch\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+Build status\n+------------\n+.. image:: https://readthedocs.org/projects/elasticsearch-py/badge/?version=latest&style=flat\n+   :target: https://elasticsearch-py.readthedocs.io/en/master/\n+\n+.. image:: https://clients-ci.elastic.co/job/elastic+elasticsearch-py+master/badge/icon\n+   :target: https://clients-ci.elastic.co/job/elastic+elasticsearch-py+master/"
            },
            {
                "sha": "a4f42e5aebdc6e6e72f4a78c68b336e2b836049d",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/__init__.py",
                "status": "added",
                "additions": 29,
                "deletions": 0,
                "changes": 29,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/__init__.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,29 @@\n+# flake8: noqa\n+from __future__ import absolute_import\n+\n+VERSION = (7, 6, 0)\n+__version__ = VERSION\n+__versionstr__ = \".\".join(map(str, VERSION))\n+\n+import logging\n+\n+try:  # Python 2.7+\n+    from logging import NullHandler\n+except ImportError:\n+\n+    class NullHandler(logging.Handler):\n+        def emit(self, record):\n+            pass\n+\n+\n+import sys\n+\n+logger = logging.getLogger(\"elasticsearch7\")\n+logger.addHandler(logging.NullHandler())\n+\n+from .client import Elasticsearch\n+from .transport import Transport\n+from .connection_pool import ConnectionPool, ConnectionSelector, RoundRobinSelector\n+from .serializer import JSONSerializer\n+from .connection import Connection, RequestsHttpConnection, Urllib3HttpConnection\n+from .exceptions import *"
            },
            {
                "sha": "387440aa393a134c496ca0fb93e66b083af7dc9e",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/__init__.py",
                "status": "added",
                "additions": 2034,
                "deletions": 0,
                "changes": 2034,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/__init__.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,2034 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+import logging\n+\n+from ..transport import Transport\n+from ..exceptions import TransportError\n+from ..compat import string_types, urlparse, unquote\n+from .indices import IndicesClient\n+from .ingest import IngestClient\n+from .cluster import ClusterClient\n+from .cat import CatClient\n+from .nodes import NodesClient\n+from .remote import RemoteClient\n+from .snapshot import SnapshotClient\n+from .tasks import TasksClient\n+from .xpack import XPackClient\n+from .utils import query_params, _make_path, SKIP_IN_PATH, _bulk_body\n+\n+# xpack APIs\n+from .ccr import CcrClient\n+from .data_frame import Data_FrameClient\n+from .deprecation import DeprecationClient\n+from .graph import GraphClient\n+from .ilm import IlmClient\n+from .license import LicenseClient\n+from .migration import MigrationClient\n+from .ml import MlClient\n+from .monitoring import MonitoringClient\n+from .rollup import RollupClient\n+from .security import SecurityClient\n+from .sql import SqlClient\n+from .ssl import SslClient\n+from .watcher import WatcherClient\n+from .enrich import EnrichClient\n+from .slm import SlmClient\n+from .transform import TransformClient\n+\n+\n+logger = logging.getLogger(\"elasticsearch7\")\n+\n+\n+def _normalize_hosts(hosts):\n+    \"\"\"\n+    Helper function to transform hosts argument to\n+    :class:`~elasticsearch7.Elasticsearch` to a list of dicts.\n+    \"\"\"\n+    # if hosts are empty, just defer to defaults down the line\n+    if hosts is None:\n+        return [{}]\n+\n+    # passed in just one string\n+    if isinstance(hosts, string_types):\n+        hosts = [hosts]\n+\n+    out = []\n+    # normalize hosts to dicts\n+    for host in hosts:\n+        if isinstance(host, string_types):\n+            if \"://\" not in host:\n+                host = \"//%s\" % host\n+\n+            parsed_url = urlparse(host)\n+            h = {\"host\": parsed_url.hostname}\n+\n+            if parsed_url.port:\n+                h[\"port\"] = parsed_url.port\n+\n+            if parsed_url.scheme == \"https\":\n+                h[\"port\"] = parsed_url.port or 443\n+                h[\"use_ssl\"] = True\n+\n+            if parsed_url.username or parsed_url.password:\n+                h[\"http_auth\"] = \"%s:%s\" % (\n+                    unquote(parsed_url.username),\n+                    unquote(parsed_url.password),\n+                )\n+\n+            if parsed_url.path and parsed_url.path != \"/\":\n+                h[\"url_prefix\"] = parsed_url.path\n+\n+            out.append(h)\n+        else:\n+            out.append(host)\n+    return out\n+\n+\n+class Elasticsearch(object):\n+    \"\"\"\n+    Elasticsearch low-level client. Provides a straightforward mapping from\n+    Python to ES REST endpoints.\n+\n+    The instance has attributes ``cat``, ``cluster``, ``indices``, ``ingest``,\n+    ``nodes``, ``snapshot`` and ``tasks`` that provide access to instances of\n+    :class:`~elasticsearch7.client.CatClient`,\n+    :class:`~elasticsearchelasticsearch7.client.ClusterClient`,\n+    :class:`~elasticsearch7.client.IndicesClient`,\n+    :class:`~elasticsearch7.client.IngestClient`,\n+    :class:`~elasticsearch7.client.NodesClient`,\n+    :class:`~elasticsearch7.client.SnapshotClient` and\n+    :class:`~elasticsearch7.client.TasksClient` respectively. This is the\n+    preferred (and only supported) way to get access to those classes and their\n+    methods.\n+\n+    You can specify your own connection class which should be used by providing\n+    the ``connection_class`` parameter::\n+\n+        # create connection to localhost using the ThriftConnection\n+        es = Elasticsearch(connection_class=ThriftConnection)\n+\n+    If you want to turn on :ref:`sniffing` you have several options (described\n+    in :class:`~elasticsearch7.Transport`)::\n+\n+        # create connection that will automatically inspect the cluster to get\n+        # the list of active nodes. Start with nodes running on 'esnode1' and\n+        # 'esnode2'\n+        es = Elasticsearch(\n+            ['esnode1', 'esnode2'],\n+            # sniff before doing anything\n+            sniff_on_start=True,\n+            # refresh nodes after a node fails to respond\n+            sniff_on_connection_fail=True,\n+            # and also every 60 seconds\n+            sniffer_timeout=60\n+        )\n+\n+    Different hosts can have different parameters, use a dictionary per node to\n+    specify those::\n+\n+        # connect to localhost directly and another node using SSL on port 443\n+        # and an url_prefix. Note that ``port`` needs to be an int.\n+        es = Elasticsearch([\n+            {'host': 'localhost'},\n+            {'host': 'othernode', 'port': 443, 'url_prefix': 'es', 'use_ssl': True},\n+        ])\n+\n+    If using SSL, there are several parameters that control how we deal with\n+    certificates (see :class:`~elasticsearch7.Urllib3HttpConnection` for\n+    detailed description of the options)::\n+\n+        es = Elasticsearch(\n+            ['localhost:443', 'other_host:443'],\n+            # turn on SSL\n+            use_ssl=True,\n+            # make sure we verify SSL certificates\n+            verify_certs=True,\n+            # provide a path to CA certs on disk\n+            ca_certs='/path/to/CA_certs'\n+        )\n+\n+    If using SSL, but don't verify the certs, a warning message is showed\n+    optionally (see :class:`~elasticsearch7.Urllib3HttpConnection` for\n+    detailed description of the options)::\n+\n+        es = Elasticsearch(\n+            ['localhost:443', 'other_host:443'],\n+            # turn on SSL\n+            use_ssl=True,\n+            # no verify SSL certificates\n+            verify_certs=False,\n+            # don't show warnings about ssl certs verification\n+            ssl_show_warn=False\n+        )\n+\n+    SSL client authentication is supported\n+    (see :class:`~elasticsearch7.Urllib3HttpConnection` for\n+    detailed description of the options)::\n+\n+        es = Elasticsearch(\n+            ['localhost:443', 'other_host:443'],\n+            # turn on SSL\n+            use_ssl=True,\n+            # make sure we verify SSL certificates\n+            verify_certs=True,\n+            # provide a path to CA certs on disk\n+            ca_certs='/path/to/CA_certs',\n+            # PEM formatted SSL client certificate\n+            client_cert='/path/to/clientcert.pem',\n+            # PEM formatted SSL client key\n+            client_key='/path/to/clientkey.pem'\n+        )\n+\n+    Alternatively you can use RFC-1738 formatted URLs, as long as they are not\n+    in conflict with other options::\n+\n+        es = Elasticsearch(\n+            [\n+                'http://user:secret@localhost:9200/',\n+                'https://user:secret@other_host:443/production'\n+            ],\n+            verify_certs=True\n+        )\n+\n+    By default, `JSONSerializer\n+    <https://github.com/elastic/elasticsearch/blob/master/elasticsearch/serializer.py#L24>`_\n+    is used to encode all outgoing requests.\n+    However, you can implement your own custom serializer::\n+\n+        from elasticsearch7.serializer import JSONSerializer\n+\n+        class SetEncoder(JSONSerializer):\n+            def default(self, obj):\n+                if isinstance(obj, set):\n+                    return list(obj)\n+                if isinstance(obj, Something):\n+                    return 'CustomSomethingRepresentation'\n+                return JSONSerializer.default(self, obj)\n+\n+        es = Elasticsearch(serializer=SetEncoder())\n+\n+    \"\"\"\n+\n+    def __init__(self, hosts=None, transport_class=Transport, **kwargs):\n+        \"\"\"\n+        :arg hosts: list of nodes, or a single node, we should connect to.\n+            Node should be a dictionary ({\"host\": \"localhost\", \"port\": 9200}),\n+            the entire dictionary will be passed to the :class:`~elasticsearch7.Connection`\n+            class as kwargs, or a string in the format of ``host[:port]`` which will be\n+            translated to a dictionary automatically.  If no value is given the\n+            :class:`~elasticsearch7.Connection` class defaults will be used.\n+\n+        :arg transport_class: :class:`~elasticsearch7.Transport` subclass to use.\n+\n+        :arg kwargs: any additional arguments will be passed on to the\n+            :class:`~elasticsearch7.Transport` class and, subsequently, to the\n+            :class:`~elasticsearch7.Connection` instances.\n+        \"\"\"\n+        self.transport = transport_class(_normalize_hosts(hosts), **kwargs)\n+\n+        # namespaced clients for compatibility with API names\n+        self.indices = IndicesClient(self)\n+        self.ingest = IngestClient(self)\n+        self.cluster = ClusterClient(self)\n+        self.cat = CatClient(self)\n+        self.nodes = NodesClient(self)\n+        self.remote = RemoteClient(self)\n+        self.snapshot = SnapshotClient(self)\n+        self.tasks = TasksClient(self)\n+\n+        self.xpack = XPackClient(self)\n+        self.ccr = CcrClient(self)\n+        self.data_frame = Data_FrameClient(self)\n+        self.deprecation = DeprecationClient(self)\n+        self.graph = GraphClient(self)\n+        self.ilm = IlmClient(self)\n+        self.indices = IndicesClient(self)\n+        self.license = LicenseClient(self)\n+        self.migration = MigrationClient(self)\n+        self.ml = MlClient(self)\n+        self.monitoring = MonitoringClient(self)\n+        self.rollup = RollupClient(self)\n+        self.security = SecurityClient(self)\n+        self.sql = SqlClient(self)\n+        self.ssl = SslClient(self)\n+        self.watcher = WatcherClient(self)\n+        self.enrich = EnrichClient(self)\n+        self.slm = SlmClient(self)\n+        self.transform = TransformClient(self)\n+\n+    def __repr__(self):\n+        try:\n+            # get a list of all connections\n+            cons = self.transport.hosts\n+            # truncate to 5 if there are too many\n+            if len(cons) > 5:\n+                cons = cons[:5] + [\"...\"]\n+            return \"<{cls}({cons})>\".format(cls=self.__class__.__name__, cons=cons)\n+        except Exception:\n+            # probably operating on custom transport and connection_pool, ignore\n+            return super(Elasticsearch, self).__repr__()\n+\n+    # AUTO-GENERATED-API-DEFINITIONS #\n+    @query_params()\n+    def ping(self, params=None, headers=None):\n+        \"\"\"\n+        Returns whether the cluster is running.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html>`_\n+\n+        \"\"\"\n+        try:\n+            return self.transport.perform_request(\n+                \"HEAD\", \"/\", params=params, headers=headers\n+            )\n+        except TransportError:\n+            return False\n+\n+    @query_params()\n+    def info(self, params=None, headers=None):\n+        \"\"\"\n+        Returns basic information about the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/\", params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"pipeline\",\n+        \"refresh\",\n+        \"routing\",\n+        \"timeout\",\n+        \"version\",\n+        \"version_type\",\n+        \"wait_for_active_shards\",\n+    )\n+    def create(self, index, id, body, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Creates a new document in the index.  Returns a 409 response when a document\n+        with a same ID already exists in the index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-index_.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: Document ID\n+        :arg body: The document\n+        :arg doc_type: The type of the document\n+        :arg pipeline: The pipeline id to preprocess incoming documents\n+            with\n+        :arg refresh: If `true` then refresh the affected shards to make\n+            this operation visible to search, if `wait_for` then wait for a refresh\n+            to make this operation visible to search, if `false` (the default) then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        :arg routing: Specific routing value\n+        :arg timeout: Explicit operation timeout\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the index operation. Defaults\n+            to 1, meaning the primary shard only. Set to `all` for all shard copies,\n+            otherwise set to any non-negative value less than or equal to the total\n+            number of copies for the shard (number of replicas + 1)\n+        \"\"\"\n+        for param in (index, id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, doc_type, id, \"_create\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"if_primary_term\",\n+        \"if_seq_no\",\n+        \"op_type\",\n+        \"pipeline\",\n+        \"refresh\",\n+        \"routing\",\n+        \"timeout\",\n+        \"version\",\n+        \"version_type\",\n+        \"wait_for_active_shards\",\n+    )\n+    def index(self, index, body, doc_type=None, id=None, params=None, headers=None):\n+        \"\"\"\n+        Creates or updates a document in an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-index_.html>`_\n+\n+        :arg index: The name of the index\n+        :arg body: The document\n+        :arg doc_type: The type of the document\n+        :arg id: Document ID\n+        :arg if_primary_term: only perform the index operation if the\n+            last operation that has changed the document has the specified primary\n+            term\n+        :arg if_seq_no: only perform the index operation if the last\n+            operation that has changed the document has the specified sequence\n+            number\n+        :arg op_type: Explicit operation type. Defaults to `index` for\n+            requests with an explicit document ID, and to `create`for requests\n+            without an explicit document ID  Valid choices: index, create\n+        :arg pipeline: The pipeline id to preprocess incoming documents\n+            with\n+        :arg refresh: If `true` then refresh the affected shards to make\n+            this operation visible to search, if `wait_for` then wait for a refresh\n+            to make this operation visible to search, if `false` (the default) then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        :arg routing: Specific routing value\n+        :arg timeout: Explicit operation timeout\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the index operation. Defaults\n+            to 1, meaning the primary shard only. Set to `all` for all shard copies,\n+            otherwise set to any non-negative value less than or equal to the total\n+            number of copies for the shard (number of replicas + 1)\n+        \"\"\"\n+        for param in (index, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type is None:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"POST\" if id in SKIP_IN_PATH else \"PUT\",\n+            _make_path(index, doc_type, id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"pipeline\",\n+        \"refresh\",\n+        \"routing\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def bulk(self, body, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to perform multiple index/update/delete operations in a single request.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-bulk.html>`_\n+\n+        :arg body: The operation definition and data (action-data\n+            pairs), separated by newlines\n+        :arg index: Default index for items which don't provide one\n+        :arg doc_type: Default document type for items which don't\n+            provide one\n+        :arg _source: True or false to return the _source field or not,\n+            or default list of fields to return, can be overridden on each sub-\n+            request\n+        :arg _source_excludes: Default list of fields to exclude from\n+            the returned _source field, can be overridden on each sub-request\n+        :arg _source_includes: Default list of fields to extract and\n+            return from the _source field, can be overridden on each sub-request\n+        :arg doc_type: Default document type for items which don't\n+            provide one\n+        :arg pipeline: The pipeline id to preprocess incoming documents\n+            with\n+        :arg refresh: If `true` then refresh the affected shards to make\n+            this operation visible to search, if `wait_for` then wait for a refresh\n+            to make this operation visible to search, if `false` (the default) then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        :arg routing: Specific routing value\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the bulk operation. Defaults\n+            to 1, meaning the primary shard only. Set to `all` for all shard copies,\n+            otherwise set to any non-negative value less than or equal to the total\n+            number of copies for the shard (number of replicas + 1)\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        body = _bulk_body(self.transport.serializer, body)\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_bulk\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def clear_scroll(self, body=None, scroll_id=None, params=None, headers=None):\n+        \"\"\"\n+        Explicitly clears the search context for a scroll.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-request-body.html#_clear_scroll_api>`_\n+\n+        :arg body: A comma-separated list of scroll IDs to clear if none\n+            was specified via the scroll_id parameter\n+        :arg scroll_id: A comma-separated list of scroll IDs to clear\n+        \"\"\"\n+        if scroll_id in SKIP_IN_PATH and body in SKIP_IN_PATH:\n+            raise ValueError(\"You need to supply scroll_id or body.\")\n+        elif scroll_id and not body:\n+            body = {\"scroll_id\": [scroll_id]}\n+        elif scroll_id:\n+            params[\"scroll_id\"] = scroll_id\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", \"/_search/scroll\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"analyze_wildcard\",\n+        \"analyzer\",\n+        \"default_operator\",\n+        \"df\",\n+        \"expand_wildcards\",\n+        \"ignore_throttled\",\n+        \"ignore_unavailable\",\n+        \"lenient\",\n+        \"min_score\",\n+        \"preference\",\n+        \"q\",\n+        \"routing\",\n+        \"terminate_after\",\n+    )\n+    def count(self, body=None, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns number of documents matching a query.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-count.html>`_\n+\n+        :arg body: A query to restrict the results specified with the\n+            Query DSL (optional)\n+        :arg index: A comma-separated list of indices to restrict the\n+            results\n+        :arg doc_type: A comma-separated list of types to restrict the\n+            results\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg analyze_wildcard: Specify whether wildcard and prefix\n+            queries should be analyzed (default: false)\n+        :arg analyzer: The analyzer to use for the query string\n+        :arg default_operator: The default operator for query string\n+            query (AND or OR)  Valid choices: AND, OR  Default: OR\n+        :arg df: The field to use as default where no field prefix is\n+            given in the query string\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_throttled: Whether specified concrete, expanded or\n+            aliased indices should be ignored when throttled\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg lenient: Specify whether format-based query failures (such\n+            as providing text to a numeric field) should be ignored\n+        :arg min_score: Include only documents with a specific `_score`\n+            value in the result\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg q: Query in the Lucene query string syntax\n+        :arg routing: A comma-separated list of specific routing values\n+        :arg terminate_after: The maximum count for each shard, upon\n+            reaching which the query execution will terminate early\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_count\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"if_primary_term\",\n+        \"if_seq_no\",\n+        \"refresh\",\n+        \"routing\",\n+        \"timeout\",\n+        \"version\",\n+        \"version_type\",\n+        \"wait_for_active_shards\",\n+    )\n+    def delete(self, index, id, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Removes a document from the index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-delete.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: The document ID\n+        :arg doc_type: The type of the document\n+        :arg if_primary_term: only perform the delete operation if the\n+            last operation that has changed the document has the specified primary\n+            term\n+        :arg if_seq_no: only perform the delete operation if the last\n+            operation that has changed the document has the specified sequence\n+            number\n+        :arg refresh: If `true` then refresh the affected shards to make\n+            this operation visible to search, if `wait_for` then wait for a refresh\n+            to make this operation visible to search, if `false` (the default) then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        :arg routing: Specific routing value\n+        :arg timeout: Explicit operation timeout\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the delete operation.\n+            Defaults to 1, meaning the primary shard only. Set to `all` for all\n+            shard copies, otherwise set to any non-negative value less than or equal\n+            to the total number of copies for the shard (number of replicas + 1)\n+        \"\"\"\n+        for param in (index, id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", _make_path(index, doc_type, id), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"allow_no_indices\",\n+        \"analyze_wildcard\",\n+        \"analyzer\",\n+        \"conflicts\",\n+        \"default_operator\",\n+        \"df\",\n+        \"expand_wildcards\",\n+        \"from_\",\n+        \"ignore_unavailable\",\n+        \"lenient\",\n+        \"max_docs\",\n+        \"preference\",\n+        \"q\",\n+        \"refresh\",\n+        \"request_cache\",\n+        \"requests_per_second\",\n+        \"routing\",\n+        \"scroll\",\n+        \"scroll_size\",\n+        \"search_timeout\",\n+        \"search_type\",\n+        \"size\",\n+        \"slices\",\n+        \"sort\",\n+        \"stats\",\n+        \"terminate_after\",\n+        \"timeout\",\n+        \"version\",\n+        \"wait_for_active_shards\",\n+        \"wait_for_completion\",\n+    )\n+    def delete_by_query(self, index, body, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Deletes documents matching the provided query.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-delete-by-query.html>`_\n+\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg body: The search definition using the Query DSL\n+        :arg doc_type: A comma-separated list of document types to\n+            search; leave empty to perform the operation on all types\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg analyze_wildcard: Specify whether wildcard and prefix\n+            queries should be analyzed (default: false)\n+        :arg analyzer: The analyzer to use for the query string\n+        :arg conflicts: What to do when the delete by query hits version\n+            conflicts?  Valid choices: abort, proceed  Default: abort\n+        :arg default_operator: The default operator for query string\n+            query (AND or OR)  Valid choices: AND, OR  Default: OR\n+        :arg df: The field to use as default where no field prefix is\n+            given in the query string\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg from_: Starting offset (default: 0)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg lenient: Specify whether format-based query failures (such\n+            as providing text to a numeric field) should be ignored\n+        :arg max_docs: Maximum number of documents to process (default:\n+            all documents)\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg q: Query in the Lucene query string syntax\n+        :arg refresh: Should the effected indexes be refreshed?\n+        :arg request_cache: Specify if request cache should be used for\n+            this request or not, defaults to index level setting\n+        :arg requests_per_second: The throttle for this request in sub-\n+            requests per second. -1 means no throttle.\n+        :arg routing: A comma-separated list of specific routing values\n+        :arg scroll: Specify how long a consistent view of the index\n+            should be maintained for scrolled search\n+        :arg scroll_size: Size on the scroll request powering the delete\n+            by query\n+        :arg search_timeout: Explicit timeout for each search request.\n+            Defaults to no timeout.\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, dfs_query_then_fetch\n+        :arg size: Deprecated, please use `max_docs` instead\n+        :arg slices: The number of slices this task should be divided\n+            into. Defaults to 1 meaning the task isn't sliced into subtasks.\n+            Default: 1\n+        :arg sort: A comma-separated list of <field>:<direction> pairs\n+        :arg stats: Specific 'tag' of the request for logging and\n+            statistical purposes\n+        :arg terminate_after: The maximum number of documents to collect\n+            for each shard, upon reaching which the query execution will terminate\n+            early.\n+        :arg timeout: Time each individual bulk request should wait for\n+            shards that are unavailable.  Default: 1m\n+        :arg version: Specify whether to return document version as part\n+            of a hit\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the delete by query\n+            operation. Defaults to 1, meaning the primary shard only. Set to `all`\n+            for all shard copies, otherwise set to any non-negative value less than\n+            or equal to the total number of copies for the shard (number of replicas\n+            + 1)\n+        :arg wait_for_completion: Should the request should block until\n+            the delete by query is complete.  Default: True\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        for param in (index, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_delete_by_query\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"requests_per_second\")\n+    def delete_by_query_rethrottle(self, task_id, params=None, headers=None):\n+        \"\"\"\n+        Changes the number of requests per second for a particular Delete By Query\n+        operation.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-delete-by-query.html>`_\n+\n+        :arg task_id: The task id to rethrottle\n+        :arg requests_per_second: The throttle to set on this request in\n+            floating sub-requests per second. -1 means set no throttle.\n+        \"\"\"\n+        if task_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'task_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_delete_by_query\", task_id, \"_rethrottle\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def delete_script(self, id, params=None, headers=None):\n+        \"\"\"\n+        Deletes a script.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting.html>`_\n+\n+        :arg id: Script ID\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", _make_path(\"_scripts\", id), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"preference\",\n+        \"realtime\",\n+        \"refresh\",\n+        \"routing\",\n+        \"stored_fields\",\n+        \"version\",\n+        \"version_type\",\n+    )\n+    def exists(self, index, id, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about whether a document exists in an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-get.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: The document ID\n+        :arg doc_type: The type of the document (use `_all` to fetch the\n+            first document matching the ID across all types)\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg realtime: Specify whether to perform the operation in\n+            realtime or search mode\n+        :arg refresh: Refresh the shard containing the document before\n+            performing the operation\n+        :arg routing: Specific routing value\n+        :arg stored_fields: A comma-separated list of stored fields to\n+            return in the response\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        \"\"\"\n+        for param in (index, id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"HEAD\", _make_path(index, doc_type, id), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"preference\",\n+        \"realtime\",\n+        \"refresh\",\n+        \"routing\",\n+        \"version\",\n+        \"version_type\",\n+    )\n+    def exists_source(self, index, id, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about whether a document source exists in an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-get.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: The document ID\n+        :arg doc_type: The type of the document; deprecated and optional\n+            starting with 7.0\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg realtime: Specify whether to perform the operation in\n+            realtime or search mode\n+        :arg refresh: Refresh the shard containing the document before\n+            performing the operation\n+        :arg routing: Specific routing value\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        \"\"\"\n+        for param in (index, id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"HEAD\",\n+            _make_path(index, doc_type, id, \"_source\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"analyze_wildcard\",\n+        \"analyzer\",\n+        \"default_operator\",\n+        \"df\",\n+        \"lenient\",\n+        \"preference\",\n+        \"q\",\n+        \"routing\",\n+        \"stored_fields\",\n+    )\n+    def explain(self, index, id, body=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about why a specific matches (or doesn't match) a query.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-explain.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: The document ID\n+        :arg body: The query definition using the Query DSL\n+        :arg doc_type: The type of the document\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg analyze_wildcard: Specify whether wildcards and prefix\n+            queries in the query string query should be analyzed (default: false)\n+        :arg analyzer: The analyzer for the query string query\n+        :arg default_operator: The default operator for query string\n+            query (AND or OR)  Valid choices: AND, OR  Default: OR\n+        :arg df: The default field for query string query (default:\n+            _all)\n+        :arg lenient: Specify whether format-based query failures (such\n+            as providing text to a numeric field) should be ignored\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg q: Query in the Lucene query string syntax\n+        :arg routing: Specific routing value\n+        :arg stored_fields: A comma-separated list of stored fields to\n+            return in the response\n+        \"\"\"\n+        for param in (index, id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, id, \"_explain\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"fields\",\n+        \"ignore_unavailable\",\n+        \"include_unmapped\",\n+    )\n+    def field_caps(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns the information about the capabilities of fields among multiple\n+        indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-field-caps.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg fields: A comma-separated list of field names\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg include_unmapped: Indicates whether unmapped fields should\n+            be included in the response.\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_field_caps\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"preference\",\n+        \"realtime\",\n+        \"refresh\",\n+        \"routing\",\n+        \"stored_fields\",\n+        \"version\",\n+        \"version_type\",\n+    )\n+    def get(self, index, id, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns a document.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-get.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: The document ID\n+        :arg doc_type: The type of the document (use `_all` to fetch the\n+            first document matching the ID across all types)\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg realtime: Specify whether to perform the operation in\n+            realtime or search mode\n+        :arg refresh: Refresh the shard containing the document before\n+            performing the operation\n+        :arg routing: Specific routing value\n+        :arg stored_fields: A comma-separated list of stored fields to\n+            return in the response\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        \"\"\"\n+        for param in (index, id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, doc_type, id), params=params, headers=headers\n+        )\n+\n+    @query_params(\"master_timeout\")\n+    def get_script(self, id, params=None, headers=None):\n+        \"\"\"\n+        Returns a script.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting.html>`_\n+\n+        :arg id: Script ID\n+        :arg master_timeout: Specify timeout for connection to master\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_scripts\", id), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"preference\",\n+        \"realtime\",\n+        \"refresh\",\n+        \"routing\",\n+        \"version\",\n+        \"version_type\",\n+    )\n+    def get_source(self, index, id, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns the source of a document.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-get.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: The document ID\n+        :arg doc_type: The type of the document; deprecated and optional\n+            starting with 7.0\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg realtime: Specify whether to perform the operation in\n+            realtime or search mode\n+        :arg refresh: Refresh the shard containing the document before\n+            performing the operation\n+        :arg routing: Specific routing value\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        \"\"\"\n+        for param in (index, id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(index, doc_type, id, \"_source\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"preference\",\n+        \"realtime\",\n+        \"refresh\",\n+        \"routing\",\n+        \"stored_fields\",\n+    )\n+    def mget(self, body, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to get multiple documents in one request.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-multi-get.html>`_\n+\n+        :arg body: Document identifiers; can be either `docs`\n+            (containing full document information) or `ids` (when index and type is\n+            provided in the URL.\n+        :arg index: The name of the index\n+        :arg doc_type: The type of the document\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg realtime: Specify whether to perform the operation in\n+            realtime or search mode\n+        :arg refresh: Refresh the shard containing the document before\n+            performing the operation\n+        :arg routing: Specific routing value\n+        :arg stored_fields: A comma-separated list of stored fields to\n+            return in the response\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_mget\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"ccs_minimize_roundtrips\",\n+        \"max_concurrent_searches\",\n+        \"max_concurrent_shard_requests\",\n+        \"pre_filter_shard_size\",\n+        \"rest_total_hits_as_int\",\n+        \"search_type\",\n+        \"typed_keys\",\n+    )\n+    def msearch(self, body, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to execute several search operations in one request.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-multi-search.html>`_\n+\n+        :arg body: The request definitions (metadata-search request\n+            definition pairs), separated by newlines\n+        :arg index: A comma-separated list of index names to use as\n+            default\n+        :arg doc_type: A comma-separated list of document types to use\n+            as default\n+        :arg ccs_minimize_roundtrips: Indicates whether network round-\n+            trips should be minimized as part of cross-cluster search requests\n+            execution  Default: true\n+        :arg max_concurrent_searches: Controls the maximum number of\n+            concurrent searches the multi search api will execute\n+        :arg max_concurrent_shard_requests: The number of concurrent\n+            shard requests each sub search executes concurrently per node. This\n+            value should be used to limit the impact of the search on the cluster in\n+            order to limit the number of concurrent shard requests  Default: 5\n+        :arg pre_filter_shard_size: A threshold that enforces a pre-\n+            filter roundtrip to prefilter search shards based on query rewriting if\n+            the\u00a0number of shards the search request expands to exceeds the\n+            threshold. This filter roundtrip can limit the number of shards\n+            significantly if for instance a shard can not match any documents based\n+            on it's rewrite method ie. if date filters are mandatory to match but\n+            the shard bounds and the query are disjoint.  Default: 128\n+        :arg rest_total_hits_as_int: Indicates whether hits.total should\n+            be rendered as an integer or an object in the rest search response\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, query_and_fetch, dfs_query_then_fetch,\n+            dfs_query_and_fetch\n+        :arg typed_keys: Specify whether aggregation and suggester names\n+            should be prefixed by their respective types in the response\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        body = _bulk_body(self.transport.serializer, body)\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_msearch\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"ccs_minimize_roundtrips\",\n+        \"max_concurrent_searches\",\n+        \"rest_total_hits_as_int\",\n+        \"search_type\",\n+        \"typed_keys\",\n+    )\n+    def msearch_template(\n+        self, body, index=None, doc_type=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Allows to execute several search template operations in one request.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/search-multi-search.html>`_\n+\n+        :arg body: The request definitions (metadata-search request\n+            definition pairs), separated by newlines\n+        :arg index: A comma-separated list of index names to use as\n+            default\n+        :arg doc_type: A comma-separated list of document types to use\n+            as default\n+        :arg ccs_minimize_roundtrips: Indicates whether network round-\n+            trips should be minimized as part of cross-cluster search requests\n+            execution  Default: true\n+        :arg max_concurrent_searches: Controls the maximum number of\n+            concurrent searches the multi search api will execute\n+        :arg rest_total_hits_as_int: Indicates whether hits.total should\n+            be rendered as an integer or an object in the rest search response\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, query_and_fetch, dfs_query_then_fetch,\n+            dfs_query_and_fetch\n+        :arg typed_keys: Specify whether aggregation and suggester names\n+            should be prefixed by their respective types in the response\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        body = _bulk_body(self.transport.serializer, body)\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_msearch\", \"template\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"field_statistics\",\n+        \"fields\",\n+        \"ids\",\n+        \"offsets\",\n+        \"payloads\",\n+        \"positions\",\n+        \"preference\",\n+        \"realtime\",\n+        \"routing\",\n+        \"term_statistics\",\n+        \"version\",\n+        \"version_type\",\n+    )\n+    def mtermvectors(\n+        self, body=None, index=None, doc_type=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Returns multiple termvectors in one request.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-multi-termvectors.html>`_\n+\n+        :arg body: Define ids, documents, parameters or a list of\n+            parameters per document here. You must at least provide a list of\n+            document ids. See documentation.\n+        :arg index: The index in which the document resides.\n+        :arg doc_type: The type of the document.\n+        :arg field_statistics: Specifies if document count, sum of\n+            document frequencies and sum of total term frequencies should be\n+            returned. Applies to all returned documents unless otherwise specified\n+            in body \"params\" or \"docs\".  Default: True\n+        :arg fields: A comma-separated list of fields to return. Applies\n+            to all returned documents unless otherwise specified in body \"params\" or\n+            \"docs\".\n+        :arg ids: A comma-separated list of documents ids. You must\n+            define ids as parameter or set \"ids\" or \"docs\" in the request body\n+        :arg offsets: Specifies if term offsets should be returned.\n+            Applies to all returned documents unless otherwise specified in body\n+            \"params\" or \"docs\".  Default: True\n+        :arg payloads: Specifies if term payloads should be returned.\n+            Applies to all returned documents unless otherwise specified in body\n+            \"params\" or \"docs\".  Default: True\n+        :arg positions: Specifies if term positions should be returned.\n+            Applies to all returned documents unless otherwise specified in body\n+            \"params\" or \"docs\".  Default: True\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random) .Applies to all returned documents\n+            unless otherwise specified in body \"params\" or \"docs\".\n+        :arg realtime: Specifies if requests are real-time as opposed to\n+            near-real-time (default: true).\n+        :arg routing: Specific routing value. Applies to all returned\n+            documents unless otherwise specified in body \"params\" or \"docs\".\n+        :arg term_statistics: Specifies if total term frequency and\n+            document frequency should be returned. Applies to all returned documents\n+            unless otherwise specified in body \"params\" or \"docs\".\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_mtermvectors\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def put_script(self, id, body, context=None, params=None, headers=None):\n+        \"\"\"\n+        Creates or updates a script.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting.html>`_\n+\n+        :arg id: Script ID\n+        :arg body: The document\n+        :arg context: Script context\n+        :arg context: Context name to compile script against\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        for param in (id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_scripts\", id, context),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\", \"search_type\"\n+    )\n+    def rank_eval(self, body, index=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to evaluate the quality of ranked search results over a set of typical\n+        search queries\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-rank-eval.html>`_\n+\n+        :arg body: The ranking evaluation search definition, including\n+            search requests, document ratings and ranking metric definition.\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, dfs_query_then_fetch\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_rank_eval\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"max_docs\",\n+        \"refresh\",\n+        \"requests_per_second\",\n+        \"scroll\",\n+        \"slices\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+        \"wait_for_completion\",\n+    )\n+    def reindex(self, body, params=None, headers=None):\n+        \"\"\"\n+        Allows to copy documents from one index to another, optionally filtering the\n+        source documents by a query, changing the destination index settings, or\n+        fetching the documents from a remote cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-reindex.html>`_\n+\n+        :arg body: The search definition using the Query DSL and the\n+            prototype for the index request.\n+        :arg max_docs: Maximum number of documents to process (default:\n+            all documents)\n+        :arg refresh: Should the affected indexes be refreshed?\n+        :arg requests_per_second: The throttle to set on this request in\n+            sub-requests per second. -1 means no throttle.\n+        :arg scroll: Control how long to keep the search context alive\n+            Default: 5m\n+        :arg slices: The number of slices this task should be divided\n+            into. Defaults to 1 meaning the task isn't sliced into subtasks.\n+            Default: 1\n+        :arg timeout: Time each individual bulk request should wait for\n+            shards that are unavailable.  Default: 1m\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the reindex operation.\n+            Defaults to 1, meaning the primary shard only. Set to `all` for all\n+            shard copies, otherwise set to any non-negative value less than or equal\n+            to the total number of copies for the shard (number of replicas + 1)\n+        :arg wait_for_completion: Should the request should block until\n+            the reindex is complete.  Default: True\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_reindex\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"requests_per_second\")\n+    def reindex_rethrottle(self, task_id, params=None, headers=None):\n+        \"\"\"\n+        Changes the number of requests per second for a particular Reindex operation.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-reindex.html>`_\n+\n+        :arg task_id: The task id to rethrottle\n+        :arg requests_per_second: The throttle to set on this request in\n+            floating sub-requests per second. -1 means set no throttle.\n+        \"\"\"\n+        if task_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'task_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_reindex\", task_id, \"_rethrottle\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def render_search_template(self, body=None, id=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to use the Mustache language to pre-render a search definition.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html#_validating_templates>`_\n+\n+        :arg body: The search definition template and its params\n+        :arg id: The id of the stored search template\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_render\", \"template\", id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def scripts_painless_execute(self, body=None, params=None, headers=None):\n+        \"\"\"\n+        Allows an arbitrary script to be executed and a result to be returned\n+        `<https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-execute-api.html>`_\n+\n+        :arg body: The script to execute\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_scripts/painless/_execute\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"rest_total_hits_as_int\", \"scroll\")\n+    def scroll(self, body=None, scroll_id=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to retrieve a large numbers of results from a single search request.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-request-body.html#request-body-search-scroll>`_\n+\n+        :arg body: The scroll ID if not passed by URL or query\n+            parameter.\n+        :arg scroll_id: The scroll ID\n+        :arg rest_total_hits_as_int: Indicates whether hits.total should\n+            be rendered as an integer or an object in the rest search response\n+        :arg scroll: Specify how long a consistent view of the index\n+            should be maintained for scrolled search\n+        :arg scroll_id: The scroll ID for scrolled search\n+        \"\"\"\n+        if scroll_id in SKIP_IN_PATH and body in SKIP_IN_PATH:\n+            raise ValueError(\"You need to supply scroll_id or body.\")\n+        elif scroll_id and not body:\n+            body = {\"scroll_id\": scroll_id}\n+        elif scroll_id:\n+            params[\"scroll_id\"] = scroll_id\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_search/scroll\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"allow_no_indices\",\n+        \"allow_partial_search_results\",\n+        \"analyze_wildcard\",\n+        \"analyzer\",\n+        \"batched_reduce_size\",\n+        \"ccs_minimize_roundtrips\",\n+        \"default_operator\",\n+        \"df\",\n+        \"docvalue_fields\",\n+        \"expand_wildcards\",\n+        \"explain\",\n+        \"from_\",\n+        \"ignore_throttled\",\n+        \"ignore_unavailable\",\n+        \"lenient\",\n+        \"max_concurrent_shard_requests\",\n+        \"pre_filter_shard_size\",\n+        \"preference\",\n+        \"q\",\n+        \"request_cache\",\n+        \"rest_total_hits_as_int\",\n+        \"routing\",\n+        \"scroll\",\n+        \"search_type\",\n+        \"seq_no_primary_term\",\n+        \"size\",\n+        \"sort\",\n+        \"stats\",\n+        \"stored_fields\",\n+        \"suggest_field\",\n+        \"suggest_mode\",\n+        \"suggest_size\",\n+        \"suggest_text\",\n+        \"terminate_after\",\n+        \"timeout\",\n+        \"track_scores\",\n+        \"track_total_hits\",\n+        \"typed_keys\",\n+        \"version\",\n+    )\n+    def search(self, body=None, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns results matching a query.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-search.html>`_\n+\n+        :arg body: The search definition using the Query DSL\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg doc_type: A comma-separated list of document types to\n+            search; leave empty to perform the operation on all types\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg allow_partial_search_results: Indicate if an error should\n+            be returned if there is a partial search failure or timeout  Default:\n+            True\n+        :arg analyze_wildcard: Specify whether wildcard and prefix\n+            queries should be analyzed (default: false)\n+        :arg analyzer: The analyzer to use for the query string\n+        :arg batched_reduce_size: The number of shard results that\n+            should be reduced at once on the coordinating node. This value should be\n+            used as a protection mechanism to reduce the memory overhead per search\n+            request if the potential number of shards in the request can be large.\n+            Default: 512\n+        :arg ccs_minimize_roundtrips: Indicates whether network round-\n+            trips should be minimized as part of cross-cluster search requests\n+            execution  Default: true\n+        :arg default_operator: The default operator for query string\n+            query (AND or OR)  Valid choices: AND, OR  Default: OR\n+        :arg df: The field to use as default where no field prefix is\n+            given in the query string\n+        :arg docvalue_fields: A comma-separated list of fields to return\n+            as the docvalue representation of a field for each hit\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg explain: Specify whether to return detailed information\n+            about score computation as part of a hit\n+        :arg from_: Starting offset (default: 0)\n+        :arg ignore_throttled: Whether specified concrete, expanded or\n+            aliased indices should be ignored when throttled\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg lenient: Specify whether format-based query failures (such\n+            as providing text to a numeric field) should be ignored\n+        :arg max_concurrent_shard_requests: The number of concurrent\n+            shard requests per node this search executes concurrently. This value\n+            should be used to limit the impact of the search on the cluster in order\n+            to limit the number of concurrent shard requests  Default: 5\n+        :arg pre_filter_shard_size: A threshold that enforces a pre-\n+            filter roundtrip to prefilter search shards based on query rewriting if\n+            the\u00a0number of shards the search request expands to exceeds the\n+            threshold. This filter roundtrip can limit the number of shards\n+            significantly if for instance a shard can not match any documents based\n+            on it's rewrite method ie. if date filters are mandatory to match but\n+            the shard bounds and the query are disjoint.  Default: 128\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg q: Query in the Lucene query string syntax\n+        :arg request_cache: Specify if request cache should be used for\n+            this request or not, defaults to index level setting\n+        :arg rest_total_hits_as_int: Indicates whether hits.total should\n+            be rendered as an integer or an object in the rest search response\n+        :arg routing: A comma-separated list of specific routing values\n+        :arg scroll: Specify how long a consistent view of the index\n+            should be maintained for scrolled search\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, dfs_query_then_fetch\n+        :arg seq_no_primary_term: Specify whether to return sequence\n+            number and primary term of the last modification of each hit\n+        :arg size: Number of hits to return (default: 10)\n+        :arg sort: A comma-separated list of <field>:<direction> pairs\n+        :arg stats: Specific 'tag' of the request for logging and\n+            statistical purposes\n+        :arg stored_fields: A comma-separated list of stored fields to\n+            return as part of a hit\n+        :arg suggest_field: Specify which field to use for suggestions\n+        :arg suggest_mode: Specify suggest mode  Valid choices: missing,\n+            popular, always  Default: missing\n+        :arg suggest_size: How many suggestions to return in response\n+        :arg suggest_text: The source text for which the suggestions\n+            should be returned\n+        :arg terminate_after: The maximum number of documents to collect\n+            for each shard, upon reaching which the query execution will terminate\n+            early.\n+        :arg timeout: Explicit operation timeout\n+        :arg track_scores: Whether to calculate and return scores even\n+            if they are not used for sorting\n+        :arg track_total_hits: Indicate if the number of documents that\n+            match the query should be tracked\n+        :arg typed_keys: Specify whether aggregation and suggester names\n+            should be prefixed by their respective types in the response\n+        :arg version: Specify whether to return document version as part\n+            of a hit\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_search\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"local\",\n+        \"preference\",\n+        \"routing\",\n+    )\n+    def search_shards(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about the indices and shards that a search request would be\n+        executed against.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-shards.html>`_\n+\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg routing: Specific routing value\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_search_shards\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"ccs_minimize_roundtrips\",\n+        \"expand_wildcards\",\n+        \"explain\",\n+        \"ignore_throttled\",\n+        \"ignore_unavailable\",\n+        \"preference\",\n+        \"profile\",\n+        \"rest_total_hits_as_int\",\n+        \"routing\",\n+        \"scroll\",\n+        \"search_type\",\n+        \"typed_keys\",\n+    )\n+    def search_template(\n+        self, body, index=None, doc_type=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Allows to use the Mustache language to pre-render a search definition.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html>`_\n+\n+        :arg body: The search definition template and its params\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg doc_type: A comma-separated list of document types to\n+            search; leave empty to perform the operation on all types\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg ccs_minimize_roundtrips: Indicates whether network round-\n+            trips should be minimized as part of cross-cluster search requests\n+            execution  Default: true\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg explain: Specify whether to return detailed information\n+            about score computation as part of a hit\n+        :arg ignore_throttled: Whether specified concrete, expanded or\n+            aliased indices should be ignored when throttled\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg profile: Specify whether to profile the query execution\n+        :arg rest_total_hits_as_int: Indicates whether hits.total should\n+            be rendered as an integer or an object in the rest search response\n+        :arg routing: A comma-separated list of specific routing values\n+        :arg scroll: Specify how long a consistent view of the index\n+            should be maintained for scrolled search\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, query_and_fetch, dfs_query_then_fetch,\n+            dfs_query_and_fetch\n+        :arg typed_keys: Specify whether aggregation and suggester names\n+            should be prefixed by their respective types in the response\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_search\", \"template\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"field_statistics\",\n+        \"fields\",\n+        \"offsets\",\n+        \"payloads\",\n+        \"positions\",\n+        \"preference\",\n+        \"realtime\",\n+        \"routing\",\n+        \"term_statistics\",\n+        \"version\",\n+        \"version_type\",\n+    )\n+    def termvectors(\n+        self, index, body=None, doc_type=None, id=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Returns information and statistics about terms in the fields of a particular\n+        document.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-termvectors.html>`_\n+\n+        :arg index: The index in which the document resides.\n+        :arg body: Define parameters and or supply a document to get\n+            termvectors for. See documentation.\n+        :arg doc_type: The type of the document.\n+        :arg id: The id of the document, when not specified a doc param\n+            should be supplied.\n+        :arg field_statistics: Specifies if document count, sum of\n+            document frequencies and sum of total term frequencies should be\n+            returned.  Default: True\n+        :arg fields: A comma-separated list of fields to return.\n+        :arg offsets: Specifies if term offsets should be returned.\n+            Default: True\n+        :arg payloads: Specifies if term payloads should be returned.\n+            Default: True\n+        :arg positions: Specifies if term positions should be returned.\n+            Default: True\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random).\n+        :arg realtime: Specifies if request is real-time as opposed to\n+            near-real-time (default: true).\n+        :arg routing: Specific routing value.\n+        :arg term_statistics: Specifies if total term frequency and\n+            document frequency should be returned.\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, id, \"_termvectors\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"if_primary_term\",\n+        \"if_seq_no\",\n+        \"lang\",\n+        \"refresh\",\n+        \"retry_on_conflict\",\n+        \"routing\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def update(self, index, id, body, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Updates a document with a script or partial document.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-update.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: Document ID\n+        :arg body: The request definition requires either `script` or\n+            partial `doc`\n+        :arg doc_type: The type of the document\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg if_primary_term: only perform the update operation if the\n+            last operation that has changed the document has the specified primary\n+            term\n+        :arg if_seq_no: only perform the update operation if the last\n+            operation that has changed the document has the specified sequence\n+            number\n+        :arg lang: The script language (default: painless)\n+        :arg refresh: If `true` then refresh the affected shards to make\n+            this operation visible to search, if `wait_for` then wait for a refresh\n+            to make this operation visible to search, if `false` (the default) then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        :arg retry_on_conflict: Specify how many times should the\n+            operation be retried when a conflict occurs (default: 0)\n+        :arg routing: Specific routing value\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the update operation.\n+            Defaults to 1, meaning the primary shard only. Set to `all` for all\n+            shard copies, otherwise set to any non-negative value less than or equal\n+            to the total number of copies for the shard (number of replicas + 1)\n+        \"\"\"\n+        for param in (index, id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, id, \"_update\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"allow_no_indices\",\n+        \"analyze_wildcard\",\n+        \"analyzer\",\n+        \"conflicts\",\n+        \"default_operator\",\n+        \"df\",\n+        \"expand_wildcards\",\n+        \"from_\",\n+        \"ignore_unavailable\",\n+        \"lenient\",\n+        \"max_docs\",\n+        \"pipeline\",\n+        \"preference\",\n+        \"q\",\n+        \"refresh\",\n+        \"request_cache\",\n+        \"requests_per_second\",\n+        \"routing\",\n+        \"scroll\",\n+        \"scroll_size\",\n+        \"search_timeout\",\n+        \"search_type\",\n+        \"size\",\n+        \"slices\",\n+        \"sort\",\n+        \"stats\",\n+        \"terminate_after\",\n+        \"timeout\",\n+        \"version\",\n+        \"version_type\",\n+        \"wait_for_active_shards\",\n+        \"wait_for_completion\",\n+    )\n+    def update_by_query(\n+        self, index, body=None, doc_type=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Performs an update on every document in the index without changing the source,\n+        for example to pick up a mapping change.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-update-by-query.html>`_\n+\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg body: The search definition using the Query DSL\n+        :arg doc_type: A comma-separated list of document types to\n+            search; leave empty to perform the operation on all types\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg analyze_wildcard: Specify whether wildcard and prefix\n+            queries should be analyzed (default: false)\n+        :arg analyzer: The analyzer to use for the query string\n+        :arg conflicts: What to do when the update by query hits version\n+            conflicts?  Valid choices: abort, proceed  Default: abort\n+        :arg default_operator: The default operator for query string\n+            query (AND or OR)  Valid choices: AND, OR  Default: OR\n+        :arg df: The field to use as default where no field prefix is\n+            given in the query string\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg from_: Starting offset (default: 0)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg lenient: Specify whether format-based query failures (such\n+            as providing text to a numeric field) should be ignored\n+        :arg max_docs: Maximum number of documents to process (default:\n+            all documents)\n+        :arg pipeline: Ingest pipeline to set on index requests made by\n+            this action. (default: none)\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg q: Query in the Lucene query string syntax\n+        :arg refresh: Should the affected indexes be refreshed?\n+        :arg request_cache: Specify if request cache should be used for\n+            this request or not, defaults to index level setting\n+        :arg requests_per_second: The throttle to set on this request in\n+            sub-requests per second. -1 means no throttle.\n+        :arg routing: A comma-separated list of specific routing values\n+        :arg scroll: Specify how long a consistent view of the index\n+            should be maintained for scrolled search\n+        :arg scroll_size: Size on the scroll request powering the update\n+            by query\n+        :arg search_timeout: Explicit timeout for each search request.\n+            Defaults to no timeout.\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, dfs_query_then_fetch\n+        :arg size: Deprecated, please use `max_docs` instead\n+        :arg slices: The number of slices this task should be divided\n+            into. Defaults to 1 meaning the task isn't sliced into subtasks.\n+            Default: 1\n+        :arg sort: A comma-separated list of <field>:<direction> pairs\n+        :arg stats: Specific 'tag' of the request for logging and\n+            statistical purposes\n+        :arg terminate_after: The maximum number of documents to collect\n+            for each shard, upon reaching which the query execution will terminate\n+            early.\n+        :arg timeout: Time each individual bulk request should wait for\n+            shards that are unavailable.  Default: 1m\n+        :arg version: Specify whether to return document version as part\n+            of a hit\n+        :arg version_type: Should the document increment the version\n+            number (internal) on hit or not (reindex)\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the update by query\n+            operation. Defaults to 1, meaning the primary shard only. Set to `all`\n+            for all shard copies, otherwise set to any non-negative value less than\n+            or equal to the total number of copies for the shard (number of replicas\n+            + 1)\n+        :arg wait_for_completion: Should the request should block until\n+            the update by query operation is complete.  Default: True\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_update_by_query\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"requests_per_second\")\n+    def update_by_query_rethrottle(self, task_id, params=None, headers=None):\n+        \"\"\"\n+        Changes the number of requests per second for a particular Update By Query\n+        operation.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update-by-query.html>`_\n+\n+        :arg task_id: The task id to rethrottle\n+        :arg requests_per_second: The throttle to set on this request in\n+            floating sub-requests per second. -1 means set no throttle.\n+        \"\"\"\n+        if task_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'task_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_update_by_query\", task_id, \"_rethrottle\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_script_context(self, params=None, headers=None):\n+        \"\"\"\n+        Returns all script contexts.\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_script_context\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_script_languages(self, params=None, headers=None):\n+        \"\"\"\n+        Returns available script types, languages and contexts\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_script_language\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "6c4ae7606733655e31ecb4af0aa90143058dce70",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cat.py",
                "status": "added",
                "additions": 556,
                "deletions": 0,
                "changes": 556,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cat.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cat.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cat.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,556 @@\n+from .utils import NamespacedClient, query_params, _make_path\n+\n+\n+class CatClient(NamespacedClient):\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"s\", \"v\")\n+    def aliases(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        Shows information about currently configured aliases to indices including\n+        filter and routing infos.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-alias.html>`_\n+\n+        :arg name: A comma-separated list of alias names to return\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"aliases\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\"bytes\", \"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"v\")\n+    def allocation(self, node_id=None, params=None, headers=None):\n+        \"\"\"\n+        Provides a snapshot of how many shards are allocated to each data node and how\n+        much disk space they are using.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-allocation.html>`_\n+\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_cat\", \"allocation\", node_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"s\", \"v\")\n+    def count(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Provides quick access to the document count of the entire cluster, or\n+        individual indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-count.html>`_\n+\n+        :arg index: A comma-separated list of index names to limit the\n+            returned information\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"count\", index), params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"s\", \"time\", \"ts\", \"v\")\n+    def health(self, params=None, headers=None):\n+        \"\"\"\n+        Returns a concise representation of the cluster health.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-health.html>`_\n+\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg ts: Set to false to disable timestamping  Default: True\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/health\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"help\", \"s\")\n+    def help(self, params=None, headers=None):\n+        \"\"\"\n+        Returns help for the Cat APIs.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat.html>`_\n+\n+        :arg help: Return help information\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat\", params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"bytes\",\n+        \"format\",\n+        \"h\",\n+        \"health\",\n+        \"help\",\n+        \"include_unloaded_segments\",\n+        \"local\",\n+        \"master_timeout\",\n+        \"pri\",\n+        \"s\",\n+        \"time\",\n+        \"v\",\n+    )\n+    def indices(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about indices: number of primaries and replicas, document\n+        counts, disk size, ...\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-indices.html>`_\n+\n+        :arg index: A comma-separated list of index names to limit the\n+            returned information\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg health: A health status (\"green\", \"yellow\", or \"red\" to\n+            filter only indices matching the specified health status  Valid choices:\n+            green, yellow, red\n+        :arg help: Return help information\n+        :arg include_unloaded_segments: If set to true segment stats\n+            will include stats for segments that are not currently loaded into\n+            memory\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg pri: Set to true to return stats only for primary shards\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"indices\", index), params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"v\")\n+    def master(self, params=None, headers=None):\n+        \"\"\"\n+        Returns information about the master node.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-master.html>`_\n+\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/master\", params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"bytes\",\n+        \"format\",\n+        \"full_id\",\n+        \"h\",\n+        \"help\",\n+        \"local\",\n+        \"master_timeout\",\n+        \"s\",\n+        \"time\",\n+        \"v\",\n+    )\n+    def nodes(self, params=None, headers=None):\n+        \"\"\"\n+        Returns basic statistics about performance of cluster nodes.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-nodes.html>`_\n+\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg full_id: Return the full node ID instead of the shortened\n+            version (default: false)\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Calculate the selected nodes using the local cluster\n+            state rather than the state from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/nodes\", params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"active_only\", \"bytes\", \"detailed\", \"format\", \"h\", \"help\", \"s\", \"time\", \"v\"\n+    )\n+    def recovery(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about index shard recoveries, both on-going completed.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-recovery.html>`_\n+\n+        :arg index: Comma-separated list or wildcard expression of index\n+            names to limit the returned information\n+        :arg active_only: If `true`, the response only includes ongoing\n+            shard recoveries\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg detailed: If `true`, the response includes detailed\n+            information about shard recoveries\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg index: Comma-separated list or wildcard expression of index\n+            names to limit the returned information\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"recovery\", index), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"bytes\", \"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"time\", \"v\"\n+    )\n+    def shards(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Provides a detailed view of shard allocation on nodes.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-shards.html>`_\n+\n+        :arg index: A comma-separated list of index names to limit the\n+            returned information\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"shards\", index), params=params, headers=headers\n+        )\n+\n+    @query_params(\"bytes\", \"format\", \"h\", \"help\", \"s\", \"v\")\n+    def segments(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Provides low-level information about the segments in the shards of an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-segments.html>`_\n+\n+        :arg index: A comma-separated list of index names to limit the\n+            returned information\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"segments\", index), params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"time\", \"v\")\n+    def pending_tasks(self, params=None, headers=None):\n+        \"\"\"\n+        Returns a concise representation of the cluster pending tasks.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-pending-tasks.html>`_\n+\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/pending_tasks\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"size\", \"v\")\n+    def thread_pool(self, thread_pool_patterns=None, params=None, headers=None):\n+        \"\"\"\n+        Returns cluster-wide thread pool statistics per node. By default the active,\n+        queue and rejected statistics are returned for all thread pools.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-thread-pool.html>`_\n+\n+        :arg thread_pool_patterns: A comma-separated list of regular-\n+            expressions to filter the thread pools in the output\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg size: The multiplier in which to display values  Valid\n+            choices: , k, m, g, t, p\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_cat\", \"thread_pool\", thread_pool_patterns),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"bytes\", \"format\", \"h\", \"help\", \"s\", \"v\")\n+    def fielddata(self, fields=None, params=None, headers=None):\n+        \"\"\"\n+        Shows how much heap memory is currently being used by fielddata on every data\n+        node in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-fielddata.html>`_\n+\n+        :arg fields: A comma-separated list of fields to return the\n+            fielddata size\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg fields: A comma-separated list of fields to return in the\n+            output\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_cat\", \"fielddata\", fields),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"v\")\n+    def plugins(self, params=None, headers=None):\n+        \"\"\"\n+        Returns information about installed plugins across nodes node.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-plugins.html>`_\n+\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/plugins\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"v\")\n+    def nodeattrs(self, params=None, headers=None):\n+        \"\"\"\n+        Returns information about custom node attributes.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-nodeattrs.html>`_\n+\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/nodeattrs\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"v\")\n+    def repositories(self, params=None, headers=None):\n+        \"\"\"\n+        Returns information about snapshot repositories registered in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-repositories.html>`_\n+\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/repositories\", params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"format\", \"h\", \"help\", \"ignore_unavailable\", \"master_timeout\", \"s\", \"time\", \"v\"\n+    )\n+    def snapshots(self, repository=None, params=None, headers=None):\n+        \"\"\"\n+        Returns all snapshots in a specific repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-snapshots.html>`_\n+\n+        :arg repository: Name of repository from which to fetch the\n+            snapshot information\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg ignore_unavailable: Set to true to ignore unavailable\n+            snapshots\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_cat\", \"snapshots\", repository),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"actions\",\n+        \"detailed\",\n+        \"format\",\n+        \"h\",\n+        \"help\",\n+        \"node_id\",\n+        \"parent_task\",\n+        \"s\",\n+        \"time\",\n+        \"v\",\n+    )\n+    def tasks(self, params=None, headers=None):\n+        \"\"\"\n+        Returns information about the tasks currently executing on one or more nodes in\n+        the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/tasks.html>`_\n+\n+        :arg actions: A comma-separated list of actions that should be\n+            returned. Leave empty to return all.\n+        :arg detailed: Return detailed task information (default: false)\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information; use `_local` to return information from\n+            the node you're connecting to, leave empty to get information from all\n+            nodes\n+        :arg parent_task: Return tasks with specified parent task id.\n+            Set to -1 to return all.\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/tasks\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"v\")\n+    def templates(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about existing templates.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-templates.html>`_\n+\n+        :arg name: A pattern that returned template names must match\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"templates\", name), params=params, headers=headers\n+        )"
            },
            {
                "sha": "4f44c6e3bcec484fbe7a9f58f96a6150f6c4a5f5",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ccr.py",
                "status": "added",
                "additions": 236,
                "deletions": 0,
                "changes": 236,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ccr.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ccr.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ccr.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,236 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class CcrClient(NamespacedClient):\n+    @query_params()\n+    def delete_auto_follow_pattern(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-delete-auto-follow-pattern.html>`_\n+\n+        :arg name: The name of the auto follow pattern.\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ccr\", \"auto_follow\", name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"wait_for_active_shards\")\n+    def follow(self, index, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-put-follow.html>`_\n+\n+        :arg index: The name of the follower index\n+        :arg body: The name of the leader index and other optional ccr\n+            related parameters\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before returning. Defaults to 0. Set to `all` for\n+            all shard copies, otherwise set to any non-negative value less than or\n+            equal to the total number of copies for the shard (number of replicas +\n+            1)  Default: 0\n+        \"\"\"\n+        for param in (index, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, \"_ccr\", \"follow\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def follow_info(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-follow-info.html>`_\n+\n+        :arg index: A comma-separated list of index patterns; use `_all`\n+            to perform the operation on all indices\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_ccr\", \"info\"), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def follow_stats(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-follow-stats.html>`_\n+\n+        :arg index: A comma-separated list of index patterns; use `_all`\n+            to perform the operation on all indices\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_ccr\", \"stats\"), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def forget_follower(self, index, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current>`_\n+\n+        :arg index: the name of the leader index for which specified\n+            follower retention leases should be removed\n+        :arg body: the name and UUID of the follower index, the name of\n+            the cluster containing the follower index, and the alias from the\n+            perspective of that cluster for the remote cluster containing the leader\n+            index\n+        \"\"\"\n+        for param in (index, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_ccr\", \"forget_follower\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def get_auto_follow_pattern(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-auto-follow-pattern.html>`_\n+\n+        :arg name: The name of the auto follow pattern.\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ccr\", \"auto_follow\", name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def pause_follow(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-post-pause-follow.html>`_\n+\n+        :arg index: The name of the follower index that should pause\n+            following its leader index.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_ccr\", \"pause_follow\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def put_auto_follow_pattern(self, name, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-put-auto-follow-pattern.html>`_\n+\n+        :arg name: The name of the auto follow pattern.\n+        :arg body: The specification of the auto follow pattern\n+        \"\"\"\n+        for param in (name, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ccr\", \"auto_follow\", name),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def resume_follow(self, index, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-post-resume-follow.html>`_\n+\n+        :arg index: The name of the follow index to resume following.\n+        :arg body: The name of the leader index and other optional ccr\n+            related parameters\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_ccr\", \"resume_follow\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def stats(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-stats.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_ccr/stats\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def unfollow(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current>`_\n+\n+        :arg index: The name of the follower index that should be turned\n+            into a regular index.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_ccr\", \"unfollow\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def pause_auto_follow_pattern(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-pause-auto-follow-pattern.html>`_\n+\n+        :arg name: The name of the auto follow pattern that should pause\n+            discovering new indices to follow.\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ccr\", \"auto_follow\", name, \"pause\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def resume_auto_follow_pattern(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-resume-auto-follow-pattern.html>`_\n+\n+        :arg name: The name of the auto follow pattern to resume\n+            discovering new indices to follow.\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ccr\", \"auto_follow\", name, \"resume\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "4cbcddc7139168c0a2e0541bfa7a86c674154e11",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cluster.py",
                "status": "added",
                "additions": 237,
                "deletions": 0,
                "changes": 237,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cluster.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cluster.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cluster.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,237 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class ClusterClient(NamespacedClient):\n+    @query_params(\n+        \"expand_wildcards\",\n+        \"level\",\n+        \"local\",\n+        \"master_timeout\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+        \"wait_for_events\",\n+        \"wait_for_no_initializing_shards\",\n+        \"wait_for_no_relocating_shards\",\n+        \"wait_for_nodes\",\n+        \"wait_for_status\",\n+    )\n+    def health(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns basic information about the health of the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-health.html>`_\n+\n+        :arg index: Limit the information returned to a specific index\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: all\n+        :arg level: Specify the level of detail for returned information\n+            Valid choices: cluster, indices, shards  Default: cluster\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Wait until the specified number of\n+            shards is active\n+        :arg wait_for_events: Wait until all currently queued events\n+            with the given priority are processed  Valid choices: immediate, urgent,\n+            high, normal, low, languid\n+        :arg wait_for_no_initializing_shards: Whether to wait until\n+            there are no initializing shards in the cluster\n+        :arg wait_for_no_relocating_shards: Whether to wait until there\n+            are no relocating shards in the cluster\n+        :arg wait_for_nodes: Wait until the specified number of nodes is\n+            available\n+        :arg wait_for_status: Wait until cluster is in a specific state\n+            Valid choices: green, yellow, red\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_cluster\", \"health\", index),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"local\", \"master_timeout\")\n+    def pending_tasks(self, params=None, headers=None):\n+        \"\"\"\n+        Returns a list of any cluster-level changes (e.g. create index, update mapping,\n+        allocate or fail shard) which have not yet been executed.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-pending.html>`_\n+\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cluster/pending_tasks\", params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"flat_settings\",\n+        \"ignore_unavailable\",\n+        \"local\",\n+        \"master_timeout\",\n+        \"wait_for_metadata_version\",\n+        \"wait_for_timeout\",\n+    )\n+    def state(self, metric=None, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns a comprehensive information about the state of the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-state.html>`_\n+\n+        :arg metric: Limit the information returned to the specified\n+            metrics  Valid choices: _all, blocks, metadata, nodes, routing_table,\n+            routing_nodes, master_node, version\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg wait_for_metadata_version: Wait for the metadata version to\n+            be equal or greater than the specified metadata version\n+        :arg wait_for_timeout: The maximum time to wait for\n+            wait_for_metadata_version before timing out\n+        \"\"\"\n+        if index and metric in SKIP_IN_PATH:\n+            metric = \"_all\"\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_cluster\", \"state\", metric, index),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"flat_settings\", \"timeout\")\n+    def stats(self, node_id=None, params=None, headers=None):\n+        \"\"\"\n+        Returns high-level overview of cluster statistics.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-stats.html>`_\n+\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information; use `_local` to return information from\n+            the node you're connecting to, leave empty to get information from all\n+            nodes\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            \"/_cluster/stats\"\n+            if node_id in SKIP_IN_PATH\n+            else _make_path(\"_cluster\", \"stats\", \"nodes\", node_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"dry_run\", \"explain\", \"master_timeout\", \"metric\", \"retry_failed\", \"timeout\"\n+    )\n+    def reroute(self, body=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to manually change the allocation of individual shards in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-reroute.html>`_\n+\n+        :arg body: The definition of `commands` to perform (`move`,\n+            `cancel`, `allocate`)\n+        :arg dry_run: Simulate the operation only and return the\n+            resulting state\n+        :arg explain: Return an explanation of why the commands can or\n+            cannot be executed\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg metric: Limit the information returned to the specified\n+            metrics. Defaults to all but metadata  Valid choices: _all, blocks,\n+            metadata, nodes, routing_table, master_node, version\n+        :arg retry_failed: Retries allocation of shards that are blocked\n+            due to too many subsequent allocation failures\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_cluster/reroute\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"flat_settings\", \"include_defaults\", \"master_timeout\", \"timeout\")\n+    def get_settings(self, params=None, headers=None):\n+        \"\"\"\n+        Returns cluster settings.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-update-settings.html>`_\n+\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg include_defaults: Whether to return all default clusters\n+            setting.\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cluster/settings\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"flat_settings\", \"master_timeout\", \"timeout\")\n+    def put_settings(self, body, params=None, headers=None):\n+        \"\"\"\n+        Updates the cluster settings.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-update-settings.html>`_\n+\n+        :arg body: The settings to be updated. Can be either `transient`\n+            or `persistent` (survives cluster restart).\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\", \"/_cluster/settings\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params()\n+    def remote_info(self, params=None, headers=None):\n+        \"\"\"\n+        Returns the information about configured remote clusters.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-remote-info.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_remote/info\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"include_disk_info\", \"include_yes_decisions\")\n+    def allocation_explain(self, body=None, params=None, headers=None):\n+        \"\"\"\n+        Provides explanations for shard allocations in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-allocation-explain.html>`_\n+\n+        :arg body: The index, shard, and primary flag to explain. Empty\n+            means 'explain the first unassigned shard'\n+        :arg include_disk_info: Return information about disk usage and\n+            shard sizes (default: false)\n+        :arg include_yes_decisions: Return 'YES' decisions in\n+            explanation (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_cluster/allocation/explain\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )"
            },
            {
                "sha": "e7ff49a565d55bd3b233b44aa5f00eeb9ee0fcf5",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/data_frame.py",
                "status": "added",
                "additions": 131,
                "deletions": 0,
                "changes": 131,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/data_frame.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/data_frame.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/data_frame.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,131 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class Data_FrameClient(NamespacedClient):\n+    @query_params()\n+    def delete_data_frame_transform(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/delete-data-frame-transform.html>`_\n+\n+        :arg transform_id: The id of the transform to delete\n+        \"\"\"\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_data_frame\", \"transforms\", transform_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"from_\", \"size\")\n+    def get_data_frame_transform(self, transform_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-data-frame-transform.html>`_\n+\n+        :arg transform_id: The id or comma delimited list of id expressions of\n+            the transforms to get, '_all' or '*' implies get all transforms\n+        :arg from_: skips a number of transform configs, defaults to 0\n+        :arg size: specifies a max number of transforms to get, defaults to 100\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_data_frame\", \"transforms\", transform_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_data_frame_transform_stats(\n+        self, transform_id=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-data-frame-transform-stats.html>`_\n+\n+        :arg transform_id: The id of the transform for which to get stats.\n+            '_all' or '*' implies all transforms\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_data_frame\", \"transforms\", transform_id, \"_stats\"),\n+            params=params,\n+        )\n+\n+    @query_params()\n+    def preview_data_frame_transform(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/preview-data-frame-transform.html>`_\n+\n+        :arg body: The definition for the data_frame transform to preview\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_data_frame/transforms/_preview\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def put_data_frame_transform(self, transform_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/put-data-frame-transform.html>`_\n+\n+        :arg transform_id: The id of the new transform.\n+        :arg body: The data frame transform definition\n+        \"\"\"\n+        for param in (transform_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_data_frame\", \"transforms\", transform_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"timeout\")\n+    def start_data_frame_transform(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/start-data-frame-transform.html>`_\n+\n+        :arg transform_id: The id of the transform to start\n+        :arg timeout: Controls the time to wait for the transform to start\n+        \"\"\"\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_data_frame\", \"transforms\", transform_id, \"_start\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"timeout\", \"wait_for_completion\")\n+    def stop_data_frame_transform(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/stop-data-frame-transform.html>`_\n+\n+        :arg transform_id: The id of the transform to stop\n+        :arg timeout: Controls the time to wait until the transform has stopped.\n+            Default to 30 seconds\n+        :arg wait_for_completion: Whether to wait for the transform to fully\n+            stop before returning or not. Default to false\n+        \"\"\"\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_data_frame\", \"transforms\", transform_id, \"_stop\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "8b0e9d70e39797179d77955b7ba9de13430fdd3e",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/deprecation.py",
                "status": "added",
                "additions": 17,
                "deletions": 0,
                "changes": 17,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/deprecation.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/deprecation.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/deprecation.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,17 @@\n+from .utils import NamespacedClient, query_params, _make_path\n+\n+\n+class DeprecationClient(NamespacedClient):\n+    @query_params()\n+    def info(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        `<http://www.elastic.co/guide/en/migration/current/migration-api-deprecation.html>`_\n+\n+        :arg index: Index pattern\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(index, \"_xpack\", \"migration\", \"deprecations\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "48f30902571a200762de8ab88759f1ecb7d00631",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/enrich.py",
                "status": "added",
                "additions": 80,
                "deletions": 0,
                "changes": 80,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/enrich.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/enrich.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/enrich.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,80 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class EnrichClient(NamespacedClient):\n+    @query_params()\n+    def delete_policy(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/enrich-delete-policy.html>`_\n+\n+        :arg name: The name of the enrich policy\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_enrich\", \"policy\", name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"wait_for_completion\")\n+    def execute_policy(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/enrich-execute-policy.html>`_\n+\n+        :arg name: The name of the enrich policy\n+        :arg wait_for_completion: Should the request should block until\n+            the execution is complete.  Default: True\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_enrich\", \"policy\", name, \"_execute\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_policy(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/enrich-get-policy.html>`_\n+\n+        :arg name: A comma-separated list of enrich policy names\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_enrich\", \"policy\", name), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def put_policy(self, name, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/enrich-put-policy.html>`_\n+\n+        :arg name: The name of the enrich policy\n+        :arg body: The enrich policy to register\n+        \"\"\"\n+        for param in (name, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_enrich\", \"policy\", name),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def stats(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/enrich-stats.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_enrich/_stats\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "526d1d273fcbebb38fdce6652d8d14dd290b3f33",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/graph.py",
                "status": "added",
                "additions": 27,
                "deletions": 0,
                "changes": 27,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/graph.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/graph.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/graph.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,27 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class GraphClient(NamespacedClient):\n+    @query_params(\"routing\", \"timeout\")\n+    def explore(self, index, body=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/graph-explore-api.html>`_\n+\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg body: Graph Query DSL\n+        :arg doc_type: A comma-separated list of document types to\n+            search; leave empty to perform the operation on all types\n+        :arg routing: Specific routing value\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_graph\", \"explore\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )"
            },
            {
                "sha": "a7a147005ed6e4c84624c3fc491d8aaec9c210b3",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ilm.py",
                "status": "added",
                "additions": 147,
                "deletions": 0,
                "changes": 147,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ilm.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ilm.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ilm.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,147 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class IlmClient(NamespacedClient):\n+    @query_params()\n+    def delete_lifecycle(self, policy, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-delete-lifecycle.html>`_\n+\n+        :arg policy: The name of the index lifecycle policy\n+        \"\"\"\n+        if policy in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'policy'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ilm\", \"policy\", policy),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"only_errors\", \"only_managed\")\n+    def explain_lifecycle(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-explain-lifecycle.html>`_\n+\n+        :arg index: The name of the index to explain\n+        :arg only_errors: filters the indices included in the response\n+            to ones in an ILM error state, implies only_managed\n+        :arg only_managed: filters the indices included in the response\n+            to ones managed by ILM\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_ilm\", \"explain\"), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_lifecycle(self, policy=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-get-lifecycle.html>`_\n+\n+        :arg policy: The name of the index lifecycle policy\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_ilm\", \"policy\", policy), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_status(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-get-status.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_ilm/status\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def move_to_step(self, index, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-move-to-step.html>`_\n+\n+        :arg index: The name of the index whose lifecycle step is to\n+            change\n+        :arg body: The new lifecycle step to move to\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ilm\", \"move\", index),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def put_lifecycle(self, policy, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-put-lifecycle.html>`_\n+\n+        :arg policy: The name of the index lifecycle policy\n+        :arg body: The lifecycle policy definition to register\n+        \"\"\"\n+        if policy in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'policy'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ilm\", \"policy\", policy),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def remove_policy(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-remove-policy.html>`_\n+\n+        :arg index: The name of the index to remove policy on\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_ilm\", \"remove\"), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def retry(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-retry-policy.html>`_\n+\n+        :arg index: The name of the indices (comma-separated) whose\n+            failed lifecycle step is to be retry\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_ilm\", \"retry\"), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def start(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-start.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_ilm/start\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def stop(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-stop.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_ilm/stop\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "4e42e6da6614028057829e3d926a1b0f4adf4121",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/indices.py",
                "status": "added",
                "additions": 1276,
                "deletions": 0,
                "changes": 1276,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/indices.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/indices.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/indices.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,1276 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class IndicesClient(NamespacedClient):\n+    @query_params()\n+    def analyze(self, body=None, index=None, params=None, headers=None):\n+        \"\"\"\n+        Performs the analysis process on a text and return the tokens breakdown of the\n+        text.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-analyze.html>`_\n+\n+        :arg body: Define analyzer/tokenizer parameters and the text on\n+            which the analysis should be performed\n+        :arg index: The name of the index to scope the operation\n+        :arg index: The name of the index to scope the operation\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_analyze\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\")\n+    def refresh(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Performs the refresh operation in one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-refresh.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_refresh\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"force\",\n+        \"ignore_unavailable\",\n+        \"wait_if_ongoing\",\n+    )\n+    def flush(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Performs the flush operation on one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-flush.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string for all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg force: Whether a flush should be forced even if it is not\n+            necessarily needed ie. if no changes will be committed to the index.\n+            This is useful if transaction log IDs should be incremented even if no\n+            uncommitted changes are present. (This setting can be considered as\n+            internal)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg wait_if_ongoing: If set to true the flush operation will\n+            block until the flush can be executed if another flush operation is\n+            already executing. The default is true. If set to false the flush will\n+            be skipped iff if another flush operation is already running.\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_flush\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"include_type_name\", \"master_timeout\", \"timeout\", \"wait_for_active_shards\"\n+    )\n+    def create(self, index, body=None, params=None, headers=None):\n+        \"\"\"\n+        Creates an index with optional settings and mappings.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-create-index.html>`_\n+\n+        :arg index: The name of the index\n+        :arg body: The configuration for the index (`settings` and\n+            `mappings`)\n+        :arg include_type_name: Whether a type should be expected in the\n+            body of the mappings.\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Set the number of active shards to\n+            wait for before the operation returns.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\", _make_path(index), params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\", \"wait_for_active_shards\")\n+    def clone(self, index, target, body=None, params=None, headers=None):\n+        \"\"\"\n+        Clones an index\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-clone-index.html>`_\n+\n+        :arg index: The name of the source index to clone\n+        :arg target: The name of the target index to clone into\n+        :arg body: The configuration for the target index (`settings`\n+            and `aliases`)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Set the number of active shards to\n+            wait for on the cloned index before the operation returns.\n+        \"\"\"\n+        for param in (index, target):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, \"_clone\", target),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"flat_settings\",\n+        \"ignore_unavailable\",\n+        \"include_defaults\",\n+        \"include_type_name\",\n+        \"local\",\n+        \"master_timeout\",\n+    )\n+    def get(self, index, params=None, headers=None):\n+        \"\"\"\n+        Returns information about one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-get-index.html>`_\n+\n+        :arg index: A comma-separated list of index names\n+        :arg allow_no_indices: Ignore if a wildcard expression resolves\n+            to no concrete indices (default: false)\n+        :arg expand_wildcards: Whether wildcard expressions should get\n+            expanded to open or closed indices (default: open)  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg ignore_unavailable: Ignore unavailable indexes (default:\n+            false)\n+        :arg include_defaults: Whether to return all default setting for\n+            each of the indices.\n+        :arg include_type_name: Whether to add the type name to the\n+            response (default: false)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"master_timeout\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def open(self, index, params=None, headers=None):\n+        \"\"\"\n+        Opens an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-open-close.html>`_\n+\n+        :arg index: A comma separated list of indices to open\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: closed\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Sets the number of active shards to\n+            wait for before the operation returns.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_open\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"master_timeout\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def close(self, index, params=None, headers=None):\n+        \"\"\"\n+        Closes an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-open-close.html>`_\n+\n+        :arg index: A comma separated list of indices to close\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Sets the number of active shards to\n+            wait for before the operation returns.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_close\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"master_timeout\",\n+        \"timeout\",\n+    )\n+    def delete(self, index, params=None, headers=None):\n+        \"\"\"\n+        Deletes an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-delete-index.html>`_\n+\n+        :arg index: A comma-separated list of indices to delete; use\n+            `_all` or `*` string to delete all indices\n+        :arg allow_no_indices: Ignore if a wildcard expression resolves\n+            to no concrete indices (default: false)\n+        :arg expand_wildcards: Whether wildcard expressions should get\n+            expanded to open or closed indices (default: open)  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Ignore unavailable indexes (default:\n+            false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", _make_path(index), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"flat_settings\",\n+        \"ignore_unavailable\",\n+        \"include_defaults\",\n+        \"local\",\n+    )\n+    def exists(self, index, params=None, headers=None):\n+        \"\"\"\n+        Returns information about whether a particular index exists.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-exists.html>`_\n+\n+        :arg index: A comma-separated list of index names\n+        :arg allow_no_indices: Ignore if a wildcard expression resolves\n+            to no concrete indices (default: false)\n+        :arg expand_wildcards: Whether wildcard expressions should get\n+            expanded to open or closed indices (default: open)  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg ignore_unavailable: Ignore unavailable indexes (default:\n+            false)\n+        :arg include_defaults: Whether to return all default setting for\n+            each of the indices.\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"HEAD\", _make_path(index), params=params, headers=headers\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\", \"local\")\n+    def exists_type(self, index, doc_type, params=None, headers=None):\n+        \"\"\"\n+        Returns information about whether a particular document type exists.\n+        (DEPRECATED)\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-types-exists.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` to\n+            check the types across all indices\n+        :arg doc_type: A comma-separated list of document types to check\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        \"\"\"\n+        for param in (index, doc_type):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"HEAD\",\n+            _make_path(index, \"_mapping\", doc_type),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"include_type_name\",\n+        \"master_timeout\",\n+        \"timeout\",\n+    )\n+    def put_mapping(self, body, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Updates the index mappings.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-put-mapping.html>`_\n+\n+        :arg body: The mapping definition\n+        :arg index: A comma-separated list of index names the mapping\n+            should be added to (supports wildcards); use `_all` or omit to add the\n+            mapping on all indices.\n+        :arg doc_type: The name of the document type\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg include_type_name: Whether a type should be expected in the\n+            body of the mappings.\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        if doc_type not in SKIP_IN_PATH and index in SKIP_IN_PATH:\n+            index = \"_all\"\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, doc_type, \"_mapping\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"include_type_name\",\n+        \"local\",\n+        \"master_timeout\",\n+    )\n+    def get_mapping(self, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns mappings for one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-get-mapping.html>`_\n+\n+        :arg index: A comma-separated list of index names\n+        :arg doc_type: A comma-separated list of document types\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg include_type_name: Whether to add the type name to the\n+            response (default: false)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(index, \"_mapping\", doc_type),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"include_defaults\",\n+        \"include_type_name\",\n+        \"local\",\n+    )\n+    def get_field_mapping(\n+        self, fields, index=None, doc_type=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Returns mapping for one or more fields.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-get-field-mapping.html>`_\n+\n+        :arg fields: A comma-separated list of fields\n+        :arg index: A comma-separated list of index names\n+        :arg doc_type: A comma-separated list of document types\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg include_defaults: Whether the default mapping values should\n+            be returned as well\n+        :arg include_type_name: Whether a type should be returned in the\n+            body of the mappings.\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        \"\"\"\n+        if fields in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'fields'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(index, \"_mapping\", doc_type, \"field\", fields),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def put_alias(self, index, name, body=None, params=None, headers=None):\n+        \"\"\"\n+        Creates or updates an alias.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-aliases.html>`_\n+\n+        :arg index: A comma-separated list of index names the alias\n+            should point to (supports wildcards); use `_all` to perform the\n+            operation on all indices.\n+        :arg name: The name of the alias to be created or updated\n+        :arg body: The settings for the alias, such as `routing` or\n+            `filter`\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit timestamp for the document\n+        \"\"\"\n+        for param in (index, name):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, \"_alias\", name),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\", \"local\")\n+    def exists_alias(self, name, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about whether a particular alias exists.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-aliases.html>`_\n+\n+        :arg name: A comma-separated list of alias names to return\n+        :arg index: A comma-separated list of index names to filter\n+            aliases\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: all\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"HEAD\", _make_path(index, \"_alias\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\", \"local\")\n+    def get_alias(self, index=None, name=None, params=None, headers=None):\n+        \"\"\"\n+        Returns an alias.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-aliases.html>`_\n+\n+        :arg index: A comma-separated list of index names to filter\n+            aliases\n+        :arg name: A comma-separated list of alias names to return\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: all\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_alias\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def update_aliases(self, body, params=None, headers=None):\n+        \"\"\"\n+        Updates index aliases.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-aliases.html>`_\n+\n+        :arg body: The definition of `actions` to perform\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Request timeout\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_aliases\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def delete_alias(self, index, name, params=None, headers=None):\n+        \"\"\"\n+        Deletes an alias.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-aliases.html>`_\n+\n+        :arg index: A comma-separated list of index names (supports\n+            wildcards); use `_all` for all indices\n+        :arg name: A comma-separated list of aliases to delete (supports\n+            wildcards); use `_all` to delete all aliases for the specified indices.\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit timestamp for the document\n+        \"\"\"\n+        for param in (index, name):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", _make_path(index, \"_alias\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"create\",\n+        \"flat_settings\",\n+        \"include_type_name\",\n+        \"master_timeout\",\n+        \"order\",\n+        \"timeout\",\n+    )\n+    def put_template(self, name, body, params=None, headers=None):\n+        \"\"\"\n+        Creates or updates an index template.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-templates.html>`_\n+\n+        :arg name: The name of the template\n+        :arg body: The template definition\n+        :arg create: Whether the index template should only be added if\n+            new or can also replace an existing one\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg include_type_name: Whether a type should be returned in the\n+            body of the mappings.\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg order: The order for this template when merging multiple\n+            matching ones (higher numbers are merged later, overriding the lower\n+            numbers)\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        for param in (name, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_template\", name),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"flat_settings\", \"local\", \"master_timeout\")\n+    def exists_template(self, name, params=None, headers=None):\n+        \"\"\"\n+        Returns information about whether a particular index template exists.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-templates.html>`_\n+\n+        :arg name: The comma separated names of the index templates\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"HEAD\", _make_path(\"_template\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\"flat_settings\", \"include_type_name\", \"local\", \"master_timeout\")\n+    def get_template(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        Returns an index template.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-templates.html>`_\n+\n+        :arg name: The comma separated names of the index templates\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg include_type_name: Whether a type should be returned in the\n+            body of the mappings.\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_template\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def delete_template(self, name, params=None, headers=None):\n+        \"\"\"\n+        Deletes an index template.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-templates.html>`_\n+\n+        :arg name: The name of the template\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", _make_path(\"_template\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"flat_settings\",\n+        \"ignore_unavailable\",\n+        \"include_defaults\",\n+        \"local\",\n+        \"master_timeout\",\n+    )\n+    def get_settings(self, index=None, name=None, params=None, headers=None):\n+        \"\"\"\n+        Returns settings for one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-get-settings.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg name: The name of the settings that should be included\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: ['open', 'closed']\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg include_defaults: Whether to return all default setting for\n+            each of the indices.\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_settings\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"flat_settings\",\n+        \"ignore_unavailable\",\n+        \"master_timeout\",\n+        \"preserve_existing\",\n+        \"timeout\",\n+    )\n+    def put_settings(self, body, index=None, params=None, headers=None):\n+        \"\"\"\n+        Updates the index settings.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-update-settings.html>`_\n+\n+        :arg body: The index settings to be updated\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg preserve_existing: Whether to update existing settings. If\n+            set to `true` existing settings on an index remain unchanged, the\n+            default is `false`\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, \"_settings\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"completion_fields\",\n+        \"expand_wildcards\",\n+        \"fielddata_fields\",\n+        \"fields\",\n+        \"forbid_closed_indices\",\n+        \"groups\",\n+        \"include_segment_file_sizes\",\n+        \"include_unloaded_segments\",\n+        \"level\",\n+        \"types\",\n+    )\n+    def stats(self, index=None, metric=None, params=None, headers=None):\n+        \"\"\"\n+        Provides statistics on operations happening in an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-stats.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg metric: Limit the information returned the specific\n+            metrics.  Valid choices: _all, completion, docs, fielddata, query_cache,\n+            flush, get, indexing, merge, request_cache, refresh, search, segments,\n+            store, warmer, suggest\n+        :arg completion_fields: A comma-separated list of fields for\n+            `fielddata` and `suggest` index metric (supports wildcards)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg fielddata_fields: A comma-separated list of fields for\n+            `fielddata` index metric (supports wildcards)\n+        :arg fields: A comma-separated list of fields for `fielddata`\n+            and `completion` index metric (supports wildcards)\n+        :arg forbid_closed_indices: If set to false stats will also\n+            collected from closed indices if explicitly specified or if\n+            expand_wildcards expands to closed indices  Default: True\n+        :arg groups: A comma-separated list of search groups for\n+            `search` index metric\n+        :arg include_segment_file_sizes: Whether to report the\n+            aggregated disk usage of each one of the Lucene index files (only\n+            applies if segment stats are requested)\n+        :arg include_unloaded_segments: If set to true segment stats\n+            will include stats for segments that are not currently loaded into\n+            memory\n+        :arg level: Return stats aggregated at cluster, index or shard\n+            level  Valid choices: cluster, indices, shards  Default: indices\n+        :arg types: A comma-separated list of document types for the\n+            `indexing` index metric\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_stats\", metric), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\", \"verbose\"\n+    )\n+    def segments(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Provides low-level information about segments in a Lucene index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-segments.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg verbose: Includes detailed memory usage by Lucene.\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_segments\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"all_shards\",\n+        \"allow_no_indices\",\n+        \"analyze_wildcard\",\n+        \"analyzer\",\n+        \"default_operator\",\n+        \"df\",\n+        \"expand_wildcards\",\n+        \"explain\",\n+        \"ignore_unavailable\",\n+        \"lenient\",\n+        \"q\",\n+        \"rewrite\",\n+    )\n+    def validate_query(\n+        self, body=None, index=None, doc_type=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Allows a user to validate a potentially expensive query without executing it.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-validate.html>`_\n+\n+        :arg body: The query definition specified with the Query DSL\n+        :arg index: A comma-separated list of index names to restrict\n+            the operation; use `_all` or empty string to perform the operation on\n+            all indices\n+        :arg doc_type: A comma-separated list of document types to\n+            restrict the operation; leave empty to perform the operation on all\n+            types\n+        :arg all_shards: Execute validation on all shards instead of one\n+            random shard per index\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg analyze_wildcard: Specify whether wildcard and prefix\n+            queries should be analyzed (default: false)\n+        :arg analyzer: The analyzer to use for the query string\n+        :arg default_operator: The default operator for query string\n+            query (AND or OR)  Valid choices: AND, OR  Default: OR\n+        :arg df: The field to use as default where no field prefix is\n+            given in the query string\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg explain: Return detailed information about the error\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg lenient: Specify whether format-based query failures (such\n+            as providing text to a numeric field) should be ignored\n+        :arg q: Query in the Lucene query string syntax\n+        :arg rewrite: Provide a more detailed explanation showing the\n+            actual Lucene query that will be executed.\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_validate\", \"query\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"fielddata\",\n+        \"fields\",\n+        \"ignore_unavailable\",\n+        \"query\",\n+        \"request\",\n+    )\n+    def clear_cache(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Clears all or specific caches for one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-clearcache.html>`_\n+\n+        :arg index: A comma-separated list of index name to limit the\n+            operation\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg fielddata: Clear field data\n+        :arg fields: A comma-separated list of fields to clear when\n+            using the `fielddata` parameter (default: all)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg index: A comma-separated list of index name to limit the\n+            operation\n+        :arg query: Clear query caches\n+        :arg request: Clear request cache\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_cache\", \"clear\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\"active_only\", \"detailed\")\n+    def recovery(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about ongoing index shard recoveries.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-recovery.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg active_only: Display only those recoveries that are\n+            currently on-going\n+        :arg detailed: Whether to display detailed information about\n+            shard recovery\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_recovery\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"only_ancient_segments\",\n+        \"wait_for_completion\",\n+    )\n+    def upgrade(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        The _upgrade API is no longer useful and will be removed.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-upgrade.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg only_ancient_segments: If true, only ancient (an older\n+            Lucene major release) segments will be upgraded\n+        :arg wait_for_completion: Specify whether the request should\n+            block until the all segments are upgraded (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_upgrade\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\")\n+    def get_upgrade(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        The _upgrade API is no longer useful and will be removed.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-upgrade.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_upgrade\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\")\n+    def flush_synced(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Performs a synced flush operation on one or more indices. Synced flush is\n+        deprecated and will be removed in 8.0. Use flush instead\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-synced-flush-api.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string for all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_flush\", \"synced\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\", \"status\"\n+    )\n+    def shard_stores(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Provides store information for shard copies of indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-shards-stores.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg status: A comma-separated list of statuses used to filter\n+            on shards to get store information for  Valid choices: green, yellow,\n+            red, all\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_shard_stores\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"flush\",\n+        \"ignore_unavailable\",\n+        \"max_num_segments\",\n+        \"only_expunge_deletes\",\n+    )\n+    def forcemerge(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Performs the force merge operation on one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-forcemerge.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg flush: Specify whether the index should be flushed after\n+            performing the operation (default: true)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg max_num_segments: The number of segments the index should\n+            be merged into (default: dynamic)\n+        :arg only_expunge_deletes: Specify whether the operation should\n+            only expunge deleted documents\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_forcemerge\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"copy_settings\", \"master_timeout\", \"timeout\", \"wait_for_active_shards\"\n+    )\n+    def shrink(self, index, target, body=None, params=None, headers=None):\n+        \"\"\"\n+        Allow to shrink an existing index into a new index with fewer primary shards.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-shrink-index.html>`_\n+\n+        :arg index: The name of the source index to shrink\n+        :arg target: The name of the target index to shrink into\n+        :arg body: The configuration for the target index (`settings`\n+            and `aliases`)\n+        :arg copy_settings: whether or not to copy settings from the\n+            source index (defaults to false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Set the number of active shards to\n+            wait for on the shrunken index before the operation returns.\n+        \"\"\"\n+        for param in (index, target):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, \"_shrink\", target),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"copy_settings\", \"master_timeout\", \"timeout\", \"wait_for_active_shards\"\n+    )\n+    def split(self, index, target, body=None, params=None, headers=None):\n+        \"\"\"\n+        Allows you to split an existing index into a new index with more primary\n+        shards.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-split-index.html>`_\n+\n+        :arg index: The name of the source index to split\n+        :arg target: The name of the target index to split into\n+        :arg body: The configuration for the target index (`settings`\n+            and `aliases`)\n+        :arg copy_settings: whether or not to copy settings from the\n+            source index (defaults to false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Set the number of active shards to\n+            wait for on the shrunken index before the operation returns.\n+        \"\"\"\n+        for param in (index, target):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, \"_split\", target),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"dry_run\",\n+        \"include_type_name\",\n+        \"master_timeout\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def rollover(self, alias, body=None, new_index=None, params=None, headers=None):\n+        \"\"\"\n+        Updates an alias to point to a new index when the existing index is considered\n+        to be too large or too old.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-rollover-index.html>`_\n+\n+        :arg alias: The name of the alias to rollover\n+        :arg body: The conditions that needs to be met for executing\n+            rollover\n+        :arg new_index: The name of the rollover index\n+        :arg dry_run: If set to true the rollover action will only be\n+            validated but not actually performed even if a condition matches. The\n+            default is false\n+        :arg include_type_name: Whether a type should be included in the\n+            body of the mappings.\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Set the number of active shards to\n+            wait for on the newly created rollover index before the operation\n+            returns.\n+        \"\"\"\n+        if alias in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'alias'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(alias, \"_rollover\", new_index),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"master_timeout\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def freeze(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/frozen.html>`_\n+\n+        :arg index: The name of the index to freeze\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: closed\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Sets the number of active shards to\n+            wait for before the operation returns.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_freeze\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"master_timeout\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def unfreeze(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/frozen.html>`_\n+\n+        :arg index: The name of the index to unfreeze\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: closed\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Sets the number of active shards to\n+            wait for before the operation returns.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_unfreeze\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\")\n+    def reload_search_analyzers(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-reload-analyzers.html>`_\n+\n+        :arg index: A comma-separated list of index names to reload\n+            analyzers for\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(index, \"_reload_search_analyzers\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "50bd8831ab546ad66aa31e1f94e3b8ba8f57e52e",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ingest.py",
                "status": "added",
                "additions": 96,
                "deletions": 0,
                "changes": 96,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ingest.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ingest.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ingest.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,96 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class IngestClient(NamespacedClient):\n+    @query_params(\"master_timeout\")\n+    def get_pipeline(self, id=None, params=None, headers=None):\n+        \"\"\"\n+        Returns a pipeline.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/get-pipeline-api.html>`_\n+\n+        :arg id: Comma separated list of pipeline ids. Wildcards\n+            supported\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_ingest\", \"pipeline\", id), params=params, headers=headers\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def put_pipeline(self, id, body, params=None, headers=None):\n+        \"\"\"\n+        Creates or updates a pipeline.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/put-pipeline-api.html>`_\n+\n+        :arg id: Pipeline ID\n+        :arg body: The ingest definition\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        for param in (id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ingest\", \"pipeline\", id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def delete_pipeline(self, id, params=None, headers=None):\n+        \"\"\"\n+        Deletes a pipeline.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-pipeline-api.html>`_\n+\n+        :arg id: Pipeline ID\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ingest\", \"pipeline\", id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"verbose\")\n+    def simulate(self, body, id=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to simulate a pipeline with example documents.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/simulate-pipeline-api.html>`_\n+\n+        :arg body: The simulate definition\n+        :arg id: Pipeline ID\n+        :arg verbose: Verbose mode. Display data output for each\n+            processor in executed pipeline\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ingest\", \"pipeline\", id, \"_simulate\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def processor_grok(self, params=None, headers=None):\n+        \"\"\"\n+        Returns a list of the built-in patterns.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/grok-processor.html#grok-processor-rest-get>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_ingest/processor/grok\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "dc2d52fe07db0e247a68de8c60981cd829f60297",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/license.py",
                "status": "added",
                "additions": 90,
                "deletions": 0,
                "changes": 90,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/license.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/license.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/license.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,90 @@\n+from .utils import NamespacedClient, query_params\n+\n+\n+class LicenseClient(NamespacedClient):\n+    @query_params()\n+    def delete(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-license.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"DELETE\", \"/_license\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"accept_enterprise\", \"local\")\n+    def get(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/get-license.html>`_\n+\n+        :arg accept_enterprise: If the active license is an enterprise\n+            license, return type as 'enterprise' (default: false)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_license\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_basic_status(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/get-basic-status.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_license/basic_status\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_trial_status(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/get-trial-status.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_license/trial_status\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"acknowledge\")\n+    def post(self, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/update-license.html>`_\n+\n+        :arg body: licenses to be installed\n+        :arg acknowledge: whether the user has acknowledged acknowledge\n+            messages (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"PUT\", \"/_license\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"acknowledge\")\n+    def post_start_basic(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/start-basic.html>`_\n+\n+        :arg acknowledge: whether the user has acknowledged acknowledge\n+            messages (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_license/start_basic\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"acknowledge\", \"doc_type\")\n+    def post_start_trial(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/start-trial.html>`_\n+\n+        :arg acknowledge: whether the user has acknowledged acknowledge\n+            messages (default: false)\n+        :arg doc_type: The type of trial license to generate (default:\n+            \"trial\")\n+        \"\"\"\n+        # type is a reserved word so it cannot be used, use doc_type instead\n+        if \"doc_type\" in params:\n+            params[\"type\"] = params.pop(\"doc_type\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_license/start_trial\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "1ab4e41c55ceec36f5e5eff4479df96d91e8e568",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/migration.py",
                "status": "added",
                "additions": 17,
                "deletions": 0,
                "changes": 17,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/migration.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/migration.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/migration.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,17 @@\n+from .utils import NamespacedClient, query_params, _make_path\n+\n+\n+class MigrationClient(NamespacedClient):\n+    @query_params()\n+    def deprecations(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/migration-api-deprecation.html>`_\n+\n+        :arg index: Index pattern\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(index, \"_migration\", \"deprecations\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "340c31ce4839698e93aced2be77746bb72a4cae8",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ml.py",
                "status": "added",
                "additions": 1353,
                "deletions": 0,
                "changes": 1353,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ml.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ml.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ml.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,1353 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH, _bulk_body\n+\n+\n+class MlClient(NamespacedClient):\n+    @query_params(\"allow_no_jobs\", \"force\", \"timeout\")\n+    def close_job(self, job_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-close-job.html>`_\n+\n+        :arg job_id: The name of the job to close\n+        :arg body: The URL params optionally sent in the body\n+        :arg allow_no_jobs: Whether to ignore if a wildcard expression\n+            matches no jobs. (This includes `_all` string or when no jobs have been\n+            specified)\n+        :arg force: True if the job should be forcefully closed\n+        :arg timeout: Controls the time to wait until a job has closed.\n+            Default to 30 minutes\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_close\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def delete_calendar(self, calendar_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar to delete\n+        \"\"\"\n+        if calendar_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'calendar_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def delete_calendar_event(self, calendar_id, event_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar to modify\n+        :arg event_id: The ID of the event to remove from the calendar\n+        \"\"\"\n+        for param in (calendar_id, event_id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id, \"events\", event_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def delete_calendar_job(self, calendar_id, job_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar to modify\n+        :arg job_id: The ID of the job to remove from the calendar\n+        \"\"\"\n+        for param in (calendar_id, job_id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id, \"jobs\", job_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"force\")\n+    def delete_datafeed(self, datafeed_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeed to delete\n+        :arg force: True if the datafeed should be forcefully deleted\n+        \"\"\"\n+        if datafeed_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'datafeed_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def delete_expired_data(self, params=None, headers=None):\n+        \"\"\"\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"DELETE\", \"/_ml/_delete_expired_data\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def delete_filter(self, filter_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg filter_id: The ID of the filter to delete\n+        \"\"\"\n+        if filter_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'filter_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"filters\", filter_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_forecasts\", \"timeout\")\n+    def delete_forecast(self, job_id, forecast_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-forecast.html>`_\n+\n+        :arg job_id: The ID of the job from which to delete forecasts\n+        :arg forecast_id: The ID of the forecast to delete, can be comma\n+            delimited list. Leaving blank implies `_all`\n+        :arg allow_no_forecasts: Whether to ignore if `_all` matches no\n+            forecasts\n+        :arg timeout: Controls the time to wait until the forecast(s)\n+            are deleted. Default to 30 seconds\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_forecast\", forecast_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"force\", \"wait_for_completion\")\n+    def delete_job(self, job_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-job.html>`_\n+\n+        :arg job_id: The ID of the job to delete\n+        :arg force: True if the job should be forcefully deleted\n+        :arg wait_for_completion: Should this request wait until the\n+            operation has completed before returning  Default: True\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def delete_model_snapshot(self, job_id, snapshot_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-snapshot.html>`_\n+\n+        :arg job_id: The ID of the job to fetch\n+        :arg snapshot_id: The ID of the snapshot to delete\n+        \"\"\"\n+        for param in (job_id, snapshot_id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\n+                \"_ml\", \"anomaly_detectors\", job_id, \"model_snapshots\", snapshot_id\n+            ),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"charset\",\n+        \"column_names\",\n+        \"delimiter\",\n+        \"explain\",\n+        \"format\",\n+        \"grok_pattern\",\n+        \"has_header_row\",\n+        \"line_merge_size_limit\",\n+        \"lines_to_sample\",\n+        \"quote\",\n+        \"should_trim_fields\",\n+        \"timeout\",\n+        \"timestamp_field\",\n+        \"timestamp_format\",\n+    )\n+    def find_file_structure(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-find-file-structure.html>`_\n+\n+        :arg body: The contents of the file to be analyzed\n+        :arg charset: Optional parameter to specify the character set of\n+            the file\n+        :arg column_names: Optional parameter containing a comma\n+            separated list of the column names for a delimited file\n+        :arg delimiter: Optional parameter to specify the delimiter\n+            character for a delimited file - must be a single character\n+        :arg explain: Whether to include a commentary on how the\n+            structure was derived\n+        :arg format: Optional parameter to specify the high level file\n+            format  Valid choices: ndjson, xml, delimited, semi_structured_text\n+        :arg grok_pattern: Optional parameter to specify the Grok\n+            pattern that should be used to extract fields from messages in a semi-\n+            structured text file\n+        :arg has_header_row: Optional parameter to specify whether a\n+            delimited file includes the column names in its first row\n+        :arg line_merge_size_limit: Maximum number of characters\n+            permitted in a single message when lines are merged to create messages.\n+            Default: 10000\n+        :arg lines_to_sample: How many lines of the file should be\n+            included in the analysis  Default: 1000\n+        :arg quote: Optional parameter to specify the quote character\n+            for a delimited file - must be a single character\n+        :arg should_trim_fields: Optional parameter to specify whether\n+            the values between delimiters in a delimited file should have whitespace\n+            trimmed from them\n+        :arg timeout: Timeout after which the analysis will be aborted\n+            Default: 25s\n+        :arg timestamp_field: Optional parameter to specify the\n+            timestamp field in the file\n+        :arg timestamp_format: Optional parameter to specify the\n+            timestamp format in the file - may be either a Joda or Java time format\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        body = _bulk_body(self.transport.serializer, body)\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_ml/find_file_structure\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"advance_time\", \"calc_interim\", \"end\", \"skip_time\", \"start\")\n+    def flush_job(self, job_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-flush-job.html>`_\n+\n+        :arg job_id: The name of the job to flush\n+        :arg body: Flush parameters\n+        :arg advance_time: Advances time to the given value generating\n+            results and updating the model for the advanced interval\n+        :arg calc_interim: Calculates interim results for the most\n+            recent bucket or all buckets within the latency period\n+        :arg end: When used in conjunction with calc_interim, specifies\n+            the range of buckets on which to calculate interim results\n+        :arg skip_time: Skips time to the given value without generating\n+            results or updating the model for the skipped interval\n+        :arg start: When used in conjunction with calc_interim,\n+            specifies the range of buckets on which to calculate interim results\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_flush\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"duration\", \"expires_in\")\n+    def forecast(self, job_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg job_id: The ID of the job to forecast for\n+        :arg duration: The duration of the forecast\n+        :arg expires_in: The time interval after which the forecast\n+            expires. Expired forecasts will be deleted at the first opportunity.\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_forecast\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"anomaly_score\",\n+        \"desc\",\n+        \"end\",\n+        \"exclude_interim\",\n+        \"expand\",\n+        \"from_\",\n+        \"size\",\n+        \"sort\",\n+        \"start\",\n+    )\n+    def get_buckets(self, job_id, body=None, timestamp=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-bucket.html>`_\n+\n+        :arg job_id: ID of the job to get bucket results from\n+        :arg body: Bucket selection details if not provided in URI\n+        :arg timestamp: The timestamp of the desired single bucket\n+            result\n+        :arg anomaly_score: Filter for the most anomalous buckets\n+        :arg desc: Set the sort direction\n+        :arg end: End time filter for buckets\n+        :arg exclude_interim: Exclude interim results\n+        :arg expand: Include anomaly records\n+        :arg from_: skips a number of buckets\n+        :arg size: specifies a max number of buckets to get\n+        :arg sort: Sort buckets by a particular field\n+        :arg start: Start time filter for buckets\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\n+                \"_ml\", \"anomaly_detectors\", job_id, \"results\", \"buckets\", timestamp\n+            ),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"end\", \"from_\", \"job_id\", \"size\", \"start\")\n+    def get_calendar_events(self, calendar_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar containing the events\n+        :arg end: Get events before this time\n+        :arg from_: Skips a number of events\n+        :arg job_id: Get events for the job. When this option is used\n+            calendar_id must be '_all'\n+        :arg size: Specifies a max number of events to get\n+        :arg start: Get events after this time\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if calendar_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'calendar_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id, \"events\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"from_\", \"size\")\n+    def get_calendars(self, body=None, calendar_id=None, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: The from and size parameters optionally sent in the\n+            body\n+        :arg calendar_id: The ID of the calendar to fetch\n+        :arg from_: skips a number of calendars\n+        :arg size: specifies a max number of calendars to get\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"from_\", \"size\")\n+    def get_categories(\n+        self, job_id, body=None, category_id=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-category.html>`_\n+\n+        :arg job_id: The name of the job\n+        :arg body: Category selection details if not provided in URI\n+        :arg category_id: The identifier of the category definition of\n+            interest\n+        :arg from_: skips a number of categories\n+        :arg size: specifies a max number of categories to get\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\n+                \"_ml\", \"anomaly_detectors\", job_id, \"results\", \"categories\", category_id\n+            ),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_datafeeds\")\n+    def get_datafeed_stats(self, datafeed_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-datafeed-stats.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeeds stats to fetch\n+        :arg allow_no_datafeeds: Whether to ignore if a wildcard\n+            expression matches no datafeeds. (This includes `_all` string or when no\n+            datafeeds have been specified)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_stats\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_datafeeds\")\n+    def get_datafeeds(self, datafeed_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeeds to fetch\n+        :arg allow_no_datafeeds: Whether to ignore if a wildcard\n+            expression matches no datafeeds. (This includes `_all` string or when no\n+            datafeeds have been specified)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"from_\", \"size\")\n+    def get_filters(self, filter_id=None, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg filter_id: The ID of the filter to fetch\n+        :arg from_: skips a number of filters\n+        :arg size: specifies a max number of filters to get\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"filters\", filter_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"desc\",\n+        \"end\",\n+        \"exclude_interim\",\n+        \"from_\",\n+        \"influencer_score\",\n+        \"size\",\n+        \"sort\",\n+        \"start\",\n+    )\n+    def get_influencers(self, job_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-influencer.html>`_\n+\n+        :arg job_id:\n+        :arg body: Influencer selection criteria\n+        :arg desc: whether the results should be sorted in decending\n+            order\n+        :arg end: end timestamp for the requested influencers\n+        :arg exclude_interim: Exclude interim results\n+        :arg from_: skips a number of influencers\n+        :arg influencer_score: influencer score threshold for the\n+            requested influencers\n+        :arg size: specifies a max number of influencers to get\n+        :arg sort: sort field for the requested influencers\n+        :arg start: start timestamp for the requested influencers\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"results\", \"influencers\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_jobs\")\n+    def get_job_stats(self, job_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-job-stats.html>`_\n+\n+        :arg job_id: The ID of the jobs stats to fetch\n+        :arg allow_no_jobs: Whether to ignore if a wildcard expression\n+            matches no jobs. (This includes `_all` string or when no jobs have been\n+            specified)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_stats\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_jobs\")\n+    def get_jobs(self, job_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-job.html>`_\n+\n+        :arg job_id: The ID of the jobs to fetch\n+        :arg allow_no_jobs: Whether to ignore if a wildcard expression\n+            matches no jobs. (This includes `_all` string or when no jobs have been\n+            specified)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"desc\", \"end\", \"from_\", \"size\", \"sort\", \"start\")\n+    def get_model_snapshots(\n+        self, job_id, body=None, snapshot_id=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-snapshot.html>`_\n+\n+        :arg job_id: The ID of the job to fetch\n+        :arg body: Model snapshot selection criteria\n+        :arg snapshot_id: The ID of the snapshot to fetch\n+        :arg desc: True if the results should be sorted in descending\n+            order\n+        :arg end: The filter 'end' query parameter\n+        :arg from_: Skips a number of documents\n+        :arg size: The default number of documents returned in queries\n+            as a string.\n+        :arg sort: Name of the field to sort on\n+        :arg start: The filter 'start' query parameter\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\n+                \"_ml\", \"anomaly_detectors\", job_id, \"model_snapshots\", snapshot_id\n+            ),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_jobs\",\n+        \"bucket_span\",\n+        \"end\",\n+        \"exclude_interim\",\n+        \"overall_score\",\n+        \"start\",\n+        \"top_n\",\n+    )\n+    def get_overall_buckets(self, job_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-overall-buckets.html>`_\n+\n+        :arg job_id: The job IDs for which to calculate overall bucket\n+            results\n+        :arg body: Overall bucket selection details if not provided in\n+            URI\n+        :arg allow_no_jobs: Whether to ignore if a wildcard expression\n+            matches no jobs. (This includes `_all` string or when no jobs have been\n+            specified)\n+        :arg bucket_span: The span of the overall buckets. Defaults to\n+            the longest job bucket_span\n+        :arg end: Returns overall buckets with timestamps earlier than\n+            this time\n+        :arg exclude_interim: If true overall buckets that include\n+            interim buckets will be excluded\n+        :arg overall_score: Returns overall buckets with overall scores\n+            higher than this value\n+        :arg start: Returns overall buckets with timestamps after this\n+            time\n+        :arg top_n: The number of top job bucket scores to be used in\n+            the overall_score calculation\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\n+                \"_ml\", \"anomaly_detectors\", job_id, \"results\", \"overall_buckets\"\n+            ),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"desc\",\n+        \"end\",\n+        \"exclude_interim\",\n+        \"from_\",\n+        \"record_score\",\n+        \"size\",\n+        \"sort\",\n+        \"start\",\n+    )\n+    def get_records(self, job_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-record.html>`_\n+\n+        :arg job_id:\n+        :arg body: Record selection criteria\n+        :arg desc: Set the sort direction\n+        :arg end: End time filter for records\n+        :arg exclude_interim: Exclude interim results\n+        :arg from_: skips a number of records\n+        :arg record_score:\n+        :arg size: specifies a max number of records to get\n+        :arg sort: Sort records by a particular field\n+        :arg start: Start time filter for records\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"results\", \"records\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def info(self, params=None, headers=None):\n+        \"\"\"\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_ml/info\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def open_job(self, job_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-open-job.html>`_\n+\n+        :arg job_id: The ID of the job to open\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_open\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def post_calendar_events(self, calendar_id, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar to modify\n+        :arg body: A list of events\n+        \"\"\"\n+        for param in (calendar_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id, \"events\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"reset_end\", \"reset_start\")\n+    def post_data(self, job_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-post-data.html>`_\n+\n+        :arg job_id: The name of the job receiving the data\n+        :arg body: The data to process\n+        :arg reset_end: Optional parameter to specify the end of the\n+            bucket resetting range\n+        :arg reset_start: Optional parameter to specify the start of the\n+            bucket resetting range\n+        \"\"\"\n+        for param in (job_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        body = _bulk_body(self.transport.serializer, body)\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_data\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def preview_datafeed(self, datafeed_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-preview-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeed to preview\n+        \"\"\"\n+        if datafeed_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'datafeed_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_preview\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def put_calendar(self, calendar_id, body=None, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar to create\n+        :arg body: The calendar details\n+        \"\"\"\n+        if calendar_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'calendar_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def put_calendar_job(self, calendar_id, job_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar to modify\n+        :arg job_id: The ID of the job to add to the calendar\n+        \"\"\"\n+        for param in (calendar_id, job_id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id, \"jobs\", job_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def put_datafeed(self, datafeed_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeed to create\n+        :arg body: The datafeed config\n+        \"\"\"\n+        for param in (datafeed_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def put_filter(self, filter_id, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg filter_id: The ID of the filter to create\n+        :arg body: The filter details\n+        \"\"\"\n+        for param in (filter_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"filters\", filter_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def put_job(self, job_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-job.html>`_\n+\n+        :arg job_id: The ID of the job to create\n+        :arg body: The job\n+        \"\"\"\n+        for param in (job_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"delete_intervening_results\")\n+    def revert_model_snapshot(\n+        self, job_id, snapshot_id, body=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-revert-snapshot.html>`_\n+\n+        :arg job_id: The ID of the job to fetch\n+        :arg snapshot_id: The ID of the snapshot to revert to\n+        :arg body: Reversion options\n+        :arg delete_intervening_results: Should we reset the results\n+            back to the time of the snapshot?\n+        \"\"\"\n+        for param in (job_id, snapshot_id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\n+                \"_ml\",\n+                \"anomaly_detectors\",\n+                job_id,\n+                \"model_snapshots\",\n+                snapshot_id,\n+                \"_revert\",\n+            ),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"enabled\", \"timeout\")\n+    def set_upgrade_mode(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-set-upgrade-mode.html>`_\n+\n+        :arg enabled: Whether to enable upgrade_mode ML setting or not.\n+            Defaults to false.\n+        :arg timeout: Controls the time to wait before action times out.\n+            Defaults to 30 seconds\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_ml/set_upgrade_mode\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"end\", \"start\", \"timeout\")\n+    def start_datafeed(self, datafeed_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-start-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeed to start\n+        :arg body: The start datafeed parameters\n+        :arg end: The end time when the datafeed should stop. When not\n+            set, the datafeed continues in real time\n+        :arg start: The start time from where the datafeed should begin\n+        :arg timeout: Controls the time to wait until a datafeed has\n+            started. Default to 20 seconds\n+        \"\"\"\n+        if datafeed_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'datafeed_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_start\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_datafeeds\", \"force\", \"timeout\")\n+    def stop_datafeed(self, datafeed_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-stop-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeed to stop\n+        :arg allow_no_datafeeds: Whether to ignore if a wildcard\n+            expression matches no datafeeds. (This includes `_all` string or when no\n+            datafeeds have been specified)\n+        :arg force: True if the datafeed should be forcefully stopped.\n+        :arg timeout: Controls the time to wait until a datafeed has\n+            stopped. Default to 20 seconds\n+        \"\"\"\n+        if datafeed_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'datafeed_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_stop\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def update_datafeed(self, datafeed_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeed to update\n+        :arg body: The datafeed update settings\n+        \"\"\"\n+        for param in (datafeed_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_update\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def update_filter(self, filter_id, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg filter_id: The ID of the filter to update\n+        :arg body: The filter update\n+        \"\"\"\n+        for param in (filter_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"filters\", filter_id, \"_update\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def update_job(self, job_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-job.html>`_\n+\n+        :arg job_id: The ID of the job to create\n+        :arg body: The job update settings\n+        \"\"\"\n+        for param in (job_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_update\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def update_model_snapshot(\n+        self, job_id, snapshot_id, body, params=None, headers=None\n+    ):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-snapshot.html>`_\n+\n+        :arg job_id: The ID of the job to fetch\n+        :arg snapshot_id: The ID of the snapshot to update\n+        :arg body: The model snapshot properties to update\n+        \"\"\"\n+        for param in (job_id, snapshot_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\n+                \"_ml\",\n+                \"anomaly_detectors\",\n+                job_id,\n+                \"model_snapshots\",\n+                snapshot_id,\n+                \"_update\",\n+            ),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def validate(self, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: The job config\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_ml/anomaly_detectors/_validate\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def validate_detector(self, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: The detector\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_ml/anomaly_detectors/_validate/detector\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"force\")\n+    def delete_data_frame_analytics(self, id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/delete-dfanalytics.html>`_\n+\n+        :arg id: The ID of the data frame analytics to delete\n+        :arg force: True if the job should be forcefully deleted\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def evaluate_data_frame(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/evaluate-dfanalytics.html>`_\n+\n+        :arg body: The evaluation definition\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_ml/data_frame/_evaluate\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_match\", \"from_\", \"size\")\n+    def get_data_frame_analytics(self, id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-dfanalytics.html>`_\n+\n+        :arg id: The ID of the data frame analytics to fetch\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no data frame analytics. (This includes `_all` string or when no\n+            data frame analytics have been specified)  Default: True\n+        :arg from_: skips a number of analytics\n+        :arg size: specifies a max number of analytics to get  Default:\n+            100\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_match\", \"from_\", \"size\")\n+    def get_data_frame_analytics_stats(self, id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-dfanalytics-stats.html>`_\n+\n+        :arg id: The ID of the data frame analytics stats to fetch\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no data frame analytics. (This includes `_all` string or when no\n+            data frame analytics have been specified)  Default: True\n+        :arg from_: skips a number of analytics\n+        :arg size: specifies a max number of analytics to get  Default:\n+            100\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id, \"_stats\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def put_data_frame_analytics(self, id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/put-dfanalytics.html>`_\n+\n+        :arg id: The ID of the data frame analytics to create\n+        :arg body: The data frame analytics configuration\n+        \"\"\"\n+        for param in (id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"timeout\")\n+    def start_data_frame_analytics(self, id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/start-dfanalytics.html>`_\n+\n+        :arg id: The ID of the data frame analytics to start\n+        :arg body: The start data frame analytics parameters\n+        :arg timeout: Controls the time to wait until the task has\n+            started. Defaults to 20 seconds\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id, \"_start\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_match\", \"force\", \"timeout\")\n+    def stop_data_frame_analytics(self, id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/stop-dfanalytics.html>`_\n+\n+        :arg id: The ID of the data frame analytics to stop\n+        :arg body: The stop data frame analytics parameters\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no data frame analytics. (This includes `_all` string or when no\n+            data frame analytics have been specified)\n+        :arg force: True if the data frame analytics should be\n+            forcefully stopped\n+        :arg timeout: Controls the time to wait until the task has\n+            stopped. Defaults to 20 seconds\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id, \"_stop\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def delete_trained_model(self, model_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/delete-inference.html>`_\n+\n+        :arg model_id: The ID of the trained model to delete\n+        \"\"\"\n+        if model_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'model_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"inference\", model_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def explain_data_frame_analytics(\n+        self, body=None, id=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/explain-dfanalytics.html>`_\n+\n+        :arg body: The data frame analytics config to explain\n+        :arg id: The ID of the data frame analytics to explain\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id, \"_explain\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_match\",\n+        \"decompress_definition\",\n+        \"from_\",\n+        \"include_model_definition\",\n+        \"size\",\n+    )\n+    def get_trained_models(self, model_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-inference.html>`_\n+\n+        :arg model_id: The ID of the trained models to fetch\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no trained models. (This includes `_all` string or when no\n+            trained models have been specified)  Default: True\n+        :arg decompress_definition: Should the model definition be\n+            decompressed into valid JSON or returned in a custom compressed format.\n+            Defaults to true.  Default: True\n+        :arg from_: skips a number of trained models\n+        :arg include_model_definition: Should the full model definition\n+            be included in the results. These definitions can be large. So be\n+            cautious when including them. Defaults to false.\n+        :arg size: specifies a max number of trained models to get\n+            Default: 100\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"inference\", model_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_match\", \"from_\", \"size\")\n+    def get_trained_models_stats(self, model_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-inference-stats.html>`_\n+\n+        :arg model_id: The ID of the trained models stats to fetch\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no trained models. (This includes `_all` string or when no\n+            trained models have been specified)  Default: True\n+        :arg from_: skips a number of trained models\n+        :arg size: specifies a max number of trained models to get\n+            Default: 100\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"inference\", model_id, \"_stats\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def put_trained_model(self, model_id, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg model_id: The ID of the trained models to store\n+        :arg body: The trained model configuration\n+        \"\"\"\n+        for param in (model_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"inference\", model_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )"
            },
            {
                "sha": "c5bd2066c894cddcce53af2c38fc0c744f3ab64c",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/monitoring.py",
                "status": "added",
                "additions": 29,
                "deletions": 0,
                "changes": 29,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/monitoring.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/monitoring.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/monitoring.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,29 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH, _bulk_body\n+\n+\n+class MonitoringClient(NamespacedClient):\n+    @query_params(\"interval\", \"system_api_version\", \"system_id\")\n+    def bulk(self, body, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/monitor-elasticsearch-cluster.html>`_\n+\n+        :arg body: The operation definition and data (action-data\n+            pairs), separated by newlines\n+        :arg doc_type: Default document type for items which don't\n+            provide one\n+        :arg interval: Collection interval (e.g., '10s' or '10000ms') of\n+            the payload\n+        :arg system_api_version: API Version of the monitored system\n+        :arg system_id: Identifier of the monitored system\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        body = _bulk_body(self.transport.serializer, body)\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_monitoring\", doc_type, \"bulk\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )"
            },
            {
                "sha": "61b01a6e28ee900a2205bcf1966e814110d60b24",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/nodes.py",
                "status": "added",
                "additions": 151,
                "deletions": 0,
                "changes": 151,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/nodes.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/nodes.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/nodes.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,151 @@\n+from .utils import NamespacedClient, query_params, _make_path\n+\n+\n+class NodesClient(NamespacedClient):\n+    @query_params(\"timeout\")\n+    def reload_secure_settings(self, node_id=None, params=None, headers=None):\n+        \"\"\"\n+        Reloads secure settings.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/secure-settings.html#reloadable-secure-settings>`_\n+\n+        :arg node_id: A comma-separated list of node IDs to span the\n+            reload/reinit call. Should stay empty because reloading usually involves\n+            all cluster nodes.\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_nodes\", node_id, \"reload_secure_settings\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"flat_settings\", \"timeout\")\n+    def info(self, node_id=None, metric=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about nodes in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-nodes-info.html>`_\n+\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information; use `_local` to return information from\n+            the node you're connecting to, leave empty to get information from all\n+            nodes\n+        :arg metric: A comma-separated list of metrics you wish\n+            returned. Leave empty to return all.  Valid choices: settings, os,\n+            process, jvm, thread_pool, transport, http, plugins, ingest\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_nodes\", node_id, metric), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"completion_fields\",\n+        \"fielddata_fields\",\n+        \"fields\",\n+        \"groups\",\n+        \"include_segment_file_sizes\",\n+        \"level\",\n+        \"timeout\",\n+        \"types\",\n+    )\n+    def stats(\n+        self, node_id=None, metric=None, index_metric=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Returns statistical information about nodes in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-nodes-stats.html>`_\n+\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information; use `_local` to return information from\n+            the node you're connecting to, leave empty to get information from all\n+            nodes\n+        :arg metric: Limit the information returned to the specified\n+            metrics  Valid choices: _all, breaker, fs, http, indices, jvm, os,\n+            process, thread_pool, transport, discovery\n+        :arg index_metric: Limit the information returned for `indices`\n+            metric to the specific index metrics. Isn't used if `indices` (or `all`)\n+            metric isn't specified.  Valid choices: _all, completion, docs,\n+            fielddata, query_cache, flush, get, indexing, merge, request_cache,\n+            refresh, search, segments, store, warmer, suggest\n+        :arg completion_fields: A comma-separated list of fields for\n+            `fielddata` and `suggest` index metric (supports wildcards)\n+        :arg fielddata_fields: A comma-separated list of fields for\n+            `fielddata` index metric (supports wildcards)\n+        :arg fields: A comma-separated list of fields for `fielddata`\n+            and `completion` index metric (supports wildcards)\n+        :arg groups: A comma-separated list of search groups for\n+            `search` index metric\n+        :arg include_segment_file_sizes: Whether to report the\n+            aggregated disk usage of each one of the Lucene index files (only\n+            applies if segment stats are requested)\n+        :arg level: Return indices stats aggregated at index, node or\n+            shard level  Valid choices: indices, node, shards  Default: node\n+        :arg timeout: Explicit operation timeout\n+        :arg types: A comma-separated list of document types for the\n+            `indexing` index metric\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_nodes\", node_id, \"stats\", metric, index_metric),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"doc_type\", \"ignore_idle_threads\", \"interval\", \"snapshots\", \"threads\", \"timeout\"\n+    )\n+    def hot_threads(self, node_id=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about hot threads on each node in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-nodes-hot-threads.html>`_\n+\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information; use `_local` to return information from\n+            the node you're connecting to, leave empty to get information from all\n+            nodes\n+        :arg doc_type: The type to sample (default: cpu)  Valid choices:\n+            cpu, wait, block\n+        :arg ignore_idle_threads: Don't show threads that are in known-\n+            idle places, such as waiting on a socket select or pulling from an empty\n+            task queue (default: true)\n+        :arg interval: The interval for the second sampling of threads\n+        :arg snapshots: Number of samples of thread stacktrace (default:\n+            10)\n+        :arg threads: Specify the number of threads to provide\n+            information for (default: 3)\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        # type is a reserved word so it cannot be used, use doc_type instead\n+        if \"doc_type\" in params:\n+            params[\"type\"] = params.pop(\"doc_type\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_nodes\", node_id, \"hot_threads\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"timeout\")\n+    def usage(self, node_id=None, metric=None, params=None, headers=None):\n+        \"\"\"\n+        Returns low-level information about REST actions usage on nodes.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-nodes-usage.html>`_\n+\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information; use `_local` to return information from\n+            the node you're connecting to, leave empty to get information from all\n+            nodes\n+        :arg metric: Limit the information returned to the specified\n+            metrics  Valid choices: _all, rest_actions\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_nodes\", node_id, \"usage\", metric),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "8590313a7e179b1383fbd5b1fb488d2639145b5b",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/remote.py",
                "status": "added",
                "additions": 12,
                "deletions": 0,
                "changes": 12,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/remote.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/remote.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/remote.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,12 @@\n+from .utils import NamespacedClient, query_params\n+\n+\n+class RemoteClient(NamespacedClient):\n+    @query_params()\n+    def info(self, params=None, headers=None):\n+        \"\"\"\n+        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-remote-info.html>`_\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_remote/info\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "e26288c6a813405480d74f9561b927a33130e583",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/rollup.py",
                "status": "added",
                "additions": 133,
                "deletions": 0,
                "changes": 133,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/rollup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/rollup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/rollup.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,133 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class RollupClient(NamespacedClient):\n+    @query_params()\n+    def delete_job(self, id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg id: The ID of the job to delete\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", _make_path(\"_rollup\", \"job\", id), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_jobs(self, id=None, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg id: The ID of the job(s) to fetch. Accepts glob patterns,\n+            or left blank for all jobs\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_rollup\", \"job\", id), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_rollup_caps(self, id=None, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg id: The ID of the index to check rollup capabilities on, or\n+            left blank for all jobs\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_rollup\", \"data\", id), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_rollup_index_caps(self, index, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg index: The rollup index or index pattern to obtain rollup\n+            capabilities from.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_rollup\", \"data\"), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def put_job(self, id, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg id: The ID of the job to create\n+        :arg body: The job configuration\n+        \"\"\"\n+        for param in (id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_rollup\", \"job\", id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"rest_total_hits_as_int\", \"typed_keys\")\n+    def rollup_search(self, index, body, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg index: The indices or index-pattern(s) (containing rollup\n+            or regular data) that should be searched\n+        :arg body: The search request body\n+        :arg doc_type: The doc type inside the index\n+        :arg rest_total_hits_as_int: Indicates whether hits.total should\n+            be rendered as an integer or an object in the rest search response\n+        :arg typed_keys: Specify whether aggregation and suggester names\n+            should be prefixed by their respective types in the response\n+        \"\"\"\n+        for param in (index, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_rollup_search\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def start_job(self, id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg id: The ID of the job to start\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_rollup\", \"job\", id, \"_start\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"timeout\", \"wait_for_completion\")\n+    def stop_job(self, id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg id: The ID of the job to stop\n+        :arg timeout: Block for (at maximum) the specified duration\n+            while waiting for the job to stop.  Defaults to 30s.\n+        :arg wait_for_completion: True if the API should block until the\n+            job has fully stopped, false if should be executed async. Defaults to\n+            false.\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_rollup\", \"job\", id, \"_stop\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "eaf8e4673d5c5b5c0814731ed7a49657ca5951d8",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/security.py",
                "status": "added",
                "additions": 464,
                "deletions": 0,
                "changes": 464,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/security.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/security.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/security.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,464 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class SecurityClient(NamespacedClient):\n+    @query_params()\n+    def authenticate(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-authenticate.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_security/_authenticate\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"refresh\")\n+    def change_password(self, body, username=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-change-password.html>`_\n+\n+        :arg body: the new password for the user\n+        :arg username: The username of the user to change the password\n+            for\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_security\", \"user\", username, \"_password\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"usernames\")\n+    def clear_cached_realms(self, realms, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-cache.html>`_\n+\n+        :arg realms: Comma-separated list of realms to clear\n+        :arg usernames: Comma-separated list of usernames to clear from\n+            the cache\n+        \"\"\"\n+        if realms in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'realms'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_security\", \"realm\", realms, \"_clear_cache\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def clear_cached_roles(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-role-cache.html>`_\n+\n+        :arg name: Role name\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_security\", \"role\", name, \"_clear_cache\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def create_api_key(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html>`_\n+\n+        :arg body: The api key request to create an API key\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\", \"/_security/api_key\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"refresh\")\n+    def delete_privileges(self, application, name, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg application: Application name\n+        :arg name: Privilege name\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        for param in (application, name):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_security\", \"privilege\", application, name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def delete_role(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-role.html>`_\n+\n+        :arg name: Role name\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_security\", \"role\", name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def delete_role_mapping(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-role-mapping.html>`_\n+\n+        :arg name: Role-mapping name\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_security\", \"role_mapping\", name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def delete_user(self, username, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-user.html>`_\n+\n+        :arg username: username\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if username in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'username'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_security\", \"user\", username),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def disable_user(self, username, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-disable-user.html>`_\n+\n+        :arg username: The username of the user to disable\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if username in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'username'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_security\", \"user\", username, \"_disable\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def enable_user(self, username, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-enable-user.html>`_\n+\n+        :arg username: The username of the user to enable\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if username in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'username'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_security\", \"user\", username, \"_enable\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"id\", \"name\", \"owner\", \"realm_name\", \"username\")\n+    def get_api_key(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-api-key.html>`_\n+\n+        :arg id: API key id of the API key to be retrieved\n+        :arg name: API key name of the API key to be retrieved\n+        :arg owner: flag to query API keys owned by the currently\n+            authenticated user\n+        :arg realm_name: realm name of the user who created this API key\n+            to be retrieved\n+        :arg username: user name of the user who created this API key to\n+            be retrieved\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_security/api_key\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_privileges(self, application=None, name=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-privileges.html>`_\n+\n+        :arg application: Application name\n+        :arg name: Privilege name\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_security\", \"privilege\", application, name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_role(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role.html>`_\n+\n+        :arg name: Role name\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_security\", \"role\", name), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_role_mapping(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role-mapping.html>`_\n+\n+        :arg name: Role-Mapping name\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_security\", \"role_mapping\", name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_token(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-token.html>`_\n+\n+        :arg body: The token request to get\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_security/oauth2/token\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params()\n+    def get_user(self, username=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-user.html>`_\n+\n+        :arg username: A comma-separated list of usernames\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_security\", \"user\", username),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_user_privileges(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-privileges.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_security/user/_privileges\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def has_privileges(self, body, user=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-has-privileges.html>`_\n+\n+        :arg body: The privileges to test\n+        :arg user: Username\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_security\", \"user\", user, \"_has_privileges\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def invalidate_api_key(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-invalidate-api-key.html>`_\n+\n+        :arg body: The api key request to invalidate API key(s)\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", \"/_security/api_key\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params()\n+    def invalidate_token(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-invalidate-token.html>`_\n+\n+        :arg body: The token to invalidate\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            \"/_security/oauth2/token\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def put_privileges(self, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: The privilege(s) to add\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\", \"/_security/privilege/\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"refresh\")\n+    def put_role(self, name, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-role.html>`_\n+\n+        :arg name: Role name\n+        :arg body: The role to add\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        for param in (name, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_security\", \"role\", name),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def put_role_mapping(self, name, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-role-mapping.html>`_\n+\n+        :arg name: Role-mapping name\n+        :arg body: The role mapping to add\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        for param in (name, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_security\", \"role_mapping\", name),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def put_user(self, username, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-user.html>`_\n+\n+        :arg username: The username of the User\n+        :arg body: The user to add\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        for param in (username, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_security\", \"user\", username),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def get_builtin_privileges(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-builtin-privileges.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_security/privilege/_builtin\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "a2c0a0cab48b64ff2bf960c475f8794135890ebc",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/slm.py",
                "status": "added",
                "additions": 123,
                "deletions": 0,
                "changes": 123,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/slm.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/slm.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/slm.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,123 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class SlmClient(NamespacedClient):\n+    @query_params()\n+    def delete_lifecycle(self, policy_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-delete-policy.html>`_\n+\n+        :arg policy_id: The id of the snapshot lifecycle policy to\n+            remove\n+        \"\"\"\n+        if policy_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'policy_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_slm\", \"policy\", policy_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def execute_lifecycle(self, policy_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-execute-lifecycle.html>`_\n+\n+        :arg policy_id: The id of the snapshot lifecycle policy to be\n+            executed\n+        \"\"\"\n+        if policy_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'policy_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_slm\", \"policy\", policy_id, \"_execute\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def execute_retention(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-execute-retention.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_slm/_execute_retention\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_lifecycle(self, policy_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-get-policy.html>`_\n+\n+        :arg policy_id: Comma-separated list of snapshot lifecycle\n+            policies to retrieve\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_slm\", \"policy\", policy_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_stats(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/slm-api-get-stats.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_slm/stats\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def put_lifecycle(self, policy_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-put-policy.html>`_\n+\n+        :arg policy_id: The id of the snapshot lifecycle policy\n+        :arg body: The snapshot lifecycle policy definition to register\n+        \"\"\"\n+        if policy_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'policy_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_slm\", \"policy\", policy_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def get_status(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-get-status.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_slm/status\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def start(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-start.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_slm/start\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def stop(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-stop.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_slm/stop\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "9fc2a41159d8e8e6581ad8e858572dfef9bdb938",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/snapshot.py",
                "status": "added",
                "additions": 229,
                "deletions": 0,
                "changes": 229,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/snapshot.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/snapshot.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/snapshot.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,229 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class SnapshotClient(NamespacedClient):\n+    @query_params(\"master_timeout\", \"wait_for_completion\")\n+    def create(self, repository, snapshot, body=None, params=None, headers=None):\n+        \"\"\"\n+        Creates a snapshot in a repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg snapshot: A snapshot name\n+        :arg body: The snapshot definition\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg wait_for_completion: Should this request wait until the\n+            operation has completed before returning\n+        \"\"\"\n+        for param in (repository, snapshot):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_snapshot\", repository, snapshot),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"master_timeout\")\n+    def delete(self, repository, snapshot, params=None, headers=None):\n+        \"\"\"\n+        Deletes a snapshot.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg snapshot: A snapshot name\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        \"\"\"\n+        for param in (repository, snapshot):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_snapshot\", repository, snapshot),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"ignore_unavailable\", \"master_timeout\", \"verbose\")\n+    def get(self, repository, snapshot, params=None, headers=None):\n+        \"\"\"\n+        Returns information about a snapshot.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg snapshot: A comma-separated list of snapshot names\n+        :arg ignore_unavailable: Whether to ignore unavailable\n+            snapshots, defaults to false which means a SnapshotMissingException is\n+            thrown\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg verbose: Whether to show verbose snapshot info or only show\n+            the basic info found in the repository index blob\n+        \"\"\"\n+        for param in (repository, snapshot):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_snapshot\", repository, snapshot),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def delete_repository(self, repository, params=None, headers=None):\n+        \"\"\"\n+        Deletes a repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A comma-separated list of repository names\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if repository in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'repository'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_snapshot\", repository),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"local\", \"master_timeout\")\n+    def get_repository(self, repository=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about a repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A comma-separated list of repository names\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_snapshot\", repository), params=params, headers=headers\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\", \"verify\")\n+    def create_repository(self, repository, body, params=None, headers=None):\n+        \"\"\"\n+        Creates a repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg body: The repository definition\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        :arg verify: Whether to verify the repository after creation\n+        \"\"\"\n+        for param in (repository, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_snapshot\", repository),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"master_timeout\", \"wait_for_completion\")\n+    def restore(self, repository, snapshot, body=None, params=None, headers=None):\n+        \"\"\"\n+        Restores a snapshot.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg snapshot: A snapshot name\n+        :arg body: Details of what to restore\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg wait_for_completion: Should this request wait until the\n+            operation has completed before returning\n+        \"\"\"\n+        for param in (repository, snapshot):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_snapshot\", repository, snapshot, \"_restore\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"ignore_unavailable\", \"master_timeout\")\n+    def status(self, repository=None, snapshot=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about the status of a snapshot.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg snapshot: A comma-separated list of snapshot names\n+        :arg ignore_unavailable: Whether to ignore unavailable\n+            snapshots, defaults to false which means a SnapshotMissingException is\n+            thrown\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_snapshot\", repository, snapshot, \"_status\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def verify_repository(self, repository, params=None, headers=None):\n+        \"\"\"\n+        Verifies a repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if repository in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'repository'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_snapshot\", repository, \"_verify\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def cleanup_repository(self, repository, params=None, headers=None):\n+        \"\"\"\n+        Removes stale data from repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if repository in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'repository'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_snapshot\", repository, \"_cleanup\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "103f6865dcade4ea08e779eda3a4655b9e7b2fed",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/sql.py",
                "status": "added",
                "additions": 46,
                "deletions": 0,
                "changes": 46,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/sql.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/sql.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/sql.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,46 @@\n+from .utils import NamespacedClient, query_params, SKIP_IN_PATH\n+\n+\n+class SqlClient(NamespacedClient):\n+    @query_params()\n+    def clear_cursor(self, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: Specify the cursor value in the `cursor` element to\n+            clean the cursor.\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_sql/close\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"format\")\n+    def query(self, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: Use the `query` element to start a query. Use the\n+            `cursor` element to continue a query.\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_sql\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params()\n+    def translate(self, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: Specify the query in the `query` element.\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_sql/translate\", params=params, headers=headers, body=body\n+        )"
            },
            {
                "sha": "66ec2855e916ff4b7ea208c1dce362a3dca07417",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ssl.py",
                "status": "added",
                "additions": 13,
                "deletions": 0,
                "changes": 13,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ssl.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ssl.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ssl.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,13 @@\n+from .utils import NamespacedClient, query_params\n+\n+\n+class SslClient(NamespacedClient):\n+    @query_params()\n+    def certificates(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-ssl.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_ssl/certificates\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "4c4f6d1fa638ca4ad604336ad22c82c83100e50a",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/tasks.py",
                "status": "added",
                "additions": 82,
                "deletions": 0,
                "changes": 82,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/tasks.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/tasks.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/tasks.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,82 @@\n+import warnings\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class TasksClient(NamespacedClient):\n+    @query_params(\n+        \"actions\",\n+        \"detailed\",\n+        \"group_by\",\n+        \"nodes\",\n+        \"parent_task_id\",\n+        \"timeout\",\n+        \"wait_for_completion\",\n+    )\n+    def list(self, params=None, headers=None):\n+        \"\"\"\n+        Returns a list of tasks.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/tasks.html>`_\n+\n+        :arg actions: A comma-separated list of actions that should be\n+            returned. Leave empty to return all.\n+        :arg detailed: Return detailed task information (default: false)\n+        :arg group_by: Group tasks by nodes or parent/child\n+            relationships  Valid choices: nodes, parents, none  Default: nodes\n+        :arg nodes: A comma-separated list of node IDs or names to limit\n+            the returned information; use `_local` to return information from the\n+            node you're connecting to, leave empty to get information from all nodes\n+        :arg parent_task_id: Return tasks with specified parent task id\n+            (node_id:task_number). Set to -1 to return all.\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_completion: Wait for the matching tasks to\n+            complete (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_tasks\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"actions\", \"nodes\", \"parent_task_id\")\n+    def cancel(self, task_id=None, params=None, headers=None):\n+        \"\"\"\n+        Cancels a task, if it can be cancelled through an API.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/tasks.html>`_\n+\n+        :arg task_id: Cancel the task with specified task id\n+            (node_id:task_number)\n+        :arg actions: A comma-separated list of actions that should be\n+            cancelled. Leave empty to cancel all.\n+        :arg nodes: A comma-separated list of node IDs or names to limit\n+            the returned information; use `_local` to return information from the\n+            node you're connecting to, leave empty to get information from all nodes\n+        :arg parent_task_id: Cancel tasks with specified parent task id\n+            (node_id:task_number). Set to -1 to cancel all.\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_tasks\", task_id, \"_cancel\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"timeout\", \"wait_for_completion\")\n+    def get(self, task_id=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about a task.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/tasks.html>`_\n+\n+        :arg task_id: Return the task with specified id\n+            (node_id:task_number)\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_completion: Wait for the matching tasks to\n+            complete (default: false)\n+        \"\"\"\n+        if task_id in SKIP_IN_PATH:\n+            warnings.warn(\n+                \"Calling client.tasks.get() without a task_id is deprecated \"\n+                \"and will be removed in v8.0. Use client.tasks.list() instead.\",\n+                DeprecationWarning,\n+            )\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_tasks\", task_id), params=params, headers=headers\n+        )"
            },
            {
                "sha": "6cab5377984e1a305d7f15c34a1edd315d14751f",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/transform.py",
                "status": "added",
                "additions": 196,
                "deletions": 0,
                "changes": 196,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/transform.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/transform.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/transform.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,196 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class TransformClient(NamespacedClient):\n+    @query_params(\"force\")\n+    def delete_transform(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/delete-transform.html>`_\n+\n+        :arg transform_id: The id of the transform to delete\n+        :arg force: When `true`, the transform is deleted regardless of\n+            its current state. The default value is `false`, meaning that the\n+            transform must be `stopped` before it can be deleted.\n+        \"\"\"\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_transform\", transform_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_match\", \"from_\", \"size\")\n+    def get_transform(self, transform_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-transform.html>`_\n+\n+        :arg transform_id: The id or comma delimited list of id\n+            expressions of the transforms to get, '_all' or '*' implies get all\n+            transforms\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no transforms. (This includes `_all` string or when no\n+            transforms have been specified)\n+        :arg from_: skips a number of transform configs, defaults to 0\n+        :arg size: specifies a max number of transforms to get, defaults\n+            to 100\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_transform\", transform_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_match\", \"from_\", \"size\")\n+    def get_transform_stats(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-transform-stats.html>`_\n+\n+        :arg transform_id: The id of the transform for which to get\n+            stats. '_all' or '*' implies all transforms\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no transforms. (This includes `_all` string or when no\n+            transforms have been specified)\n+        :arg from_: skips a number of transform stats, defaults to 0\n+        :arg size: specifies a max number of transform stats to get,\n+            defaults to 100\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_transform\", transform_id, \"_stats\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def preview_transform(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/preview-transform.html>`_\n+\n+        :arg body: The definition for the transform to preview\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_transform/_preview\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"defer_validation\")\n+    def put_transform(self, transform_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/put-transform.html>`_\n+\n+        :arg transform_id: The id of the new transform.\n+        :arg body: The transform definition\n+        :arg defer_validation: If validations should be deferred until\n+            transform starts, defaults to false.\n+        \"\"\"\n+        for param in (transform_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_transform\", transform_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"timeout\")\n+    def start_transform(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/start-transform.html>`_\n+\n+        :arg transform_id: The id of the transform to start\n+        :arg timeout: Controls the time to wait for the transform to\n+            start\n+        \"\"\"\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_transform\", transform_id, \"_start\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"allow_no_match\",\n+        \"force\",\n+        \"timeout\",\n+        \"wait_for_checkpoint\",\n+        \"wait_for_completion\",\n+    )\n+    def stop_transform(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/stop-transform.html>`_\n+\n+        :arg transform_id: The id of the transform to stop\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no transforms. (This includes `_all` string or when no\n+            transforms have been specified)\n+        :arg force: Whether to force stop a failed transform or not.\n+            Default to false\n+        :arg timeout: Controls the time to wait until the transform has\n+            stopped. Default to 30 seconds\n+        :arg wait_for_checkpoint: Whether to wait for the transform to\n+            reach a checkpoint before stopping. Default to false\n+        :arg wait_for_completion: Whether to wait for the transform to\n+            fully stop before returning or not. Default to false\n+        \"\"\"\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_transform\", transform_id, \"_stop\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"defer_validation\")\n+    def update_transform(self, transform_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/update-transform.html>`_\n+\n+        :arg transform_id: The id of the transform.\n+        :arg body: The update transform definition\n+        :arg defer_validation: If validations should be deferred until\n+            transform starts, defaults to false.\n+        \"\"\"\n+        for param in (transform_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_transform\", transform_id, \"_update\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )"
            },
            {
                "sha": "3baa7666b321b08d5d24b8de7a2d237766d130c5",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/utils.py",
                "status": "added",
                "additions": 125,
                "deletions": 0,
                "changes": 125,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/utils.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/utils.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/utils.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,125 @@\n+from __future__ import unicode_literals\n+\n+import weakref\n+from datetime import date, datetime\n+from functools import wraps\n+from ..compat import string_types, quote, PY2\n+\n+# parts of URL to be omitted\n+SKIP_IN_PATH = (None, \"\", b\"\", [], ())\n+\n+\n+def _escape(value):\n+    \"\"\"\n+    Escape a single value of a URL string or a query parameter. If it is a list\n+    or tuple, turn it into a comma-separated string first.\n+    \"\"\"\n+\n+    # make sequences into comma-separated stings\n+    if isinstance(value, (list, tuple)):\n+        value = \",\".join(value)\n+\n+    # dates and datetimes into isoformat\n+    elif isinstance(value, (date, datetime)):\n+        value = value.isoformat()\n+\n+    # make bools into true/false strings\n+    elif isinstance(value, bool):\n+        value = str(value).lower()\n+\n+    # don't decode bytestrings\n+    elif isinstance(value, bytes):\n+        return value\n+\n+    # encode strings to utf-8\n+    if isinstance(value, string_types):\n+        if PY2 and isinstance(value, unicode):  # noqa: F821\n+            return value.encode(\"utf-8\")\n+        if not PY2 and isinstance(value, str):\n+            return value.encode(\"utf-8\")\n+\n+    return str(value)\n+\n+\n+def _make_path(*parts):\n+    \"\"\"\n+    Create a URL string from parts, omit all `None` values and empty strings.\n+    Convert lists and tuples to comma separated values.\n+    \"\"\"\n+    # TODO: maybe only allow some parts to be lists/tuples ?\n+    return \"/\" + \"/\".join(\n+        # preserve ',' and '*' in url for nicer URLs in logs\n+        quote(_escape(p), b\",*\")\n+        for p in parts\n+        if p not in SKIP_IN_PATH\n+    )\n+\n+\n+# parameters that apply to all methods\n+GLOBAL_PARAMS = (\"pretty\", \"human\", \"error_trace\", \"format\", \"filter_path\")\n+\n+\n+def query_params(*es_query_params):\n+    \"\"\"\n+    Decorator that pops all accepted parameters from method's kwargs and puts\n+    them in the params argument.\n+    \"\"\"\n+\n+    def _wrapper(func):\n+        @wraps(func)\n+        def _wrapped(*args, **kwargs):\n+            params = {}\n+            headers = {}\n+            if \"params\" in kwargs:\n+                params = kwargs.pop(\"params\").copy()\n+            if \"headers\" in kwargs:\n+                headers = {\n+                    k.lower(): v for k, v in (kwargs.pop(\"headers\") or {}).items()\n+                }\n+            if \"opaque_id\" in kwargs:\n+                headers[\"x-opaque-id\"] = kwargs.pop(\"opaque_id\")\n+\n+            for p in es_query_params + GLOBAL_PARAMS:\n+                if p in kwargs:\n+                    v = kwargs.pop(p)\n+                    if v is not None:\n+                        params[p] = _escape(v)\n+\n+            # don't treat ignore, request_timeout, and opaque_id as other params to avoid escaping\n+            for p in (\"ignore\", \"request_timeout\"):\n+                if p in kwargs:\n+                    params[p] = kwargs.pop(p)\n+            return func(*args, params=params, headers=headers, **kwargs)\n+\n+        return _wrapped\n+\n+    return _wrapper\n+\n+\n+def _bulk_body(serializer, body):\n+    # if not passed in a string, serialize items and join by newline\n+    if not isinstance(body, string_types):\n+        body = \"\\n\".join(map(serializer.dumps, body))\n+\n+    # bulk body must end with a newline\n+    if not body.endswith(\"\\n\"):\n+        body += \"\\n\"\n+\n+    return body\n+\n+\n+class NamespacedClient(object):\n+    def __init__(self, client):\n+        self.client = client\n+\n+    @property\n+    def transport(self):\n+        return self.client.transport\n+\n+\n+class AddonClient(NamespacedClient):\n+    @classmethod\n+    def infect_client(cls, client):\n+        addon = cls(weakref.proxy(client))\n+        setattr(client, cls.namespace, addon)\n+        return client"
            },
            {
                "sha": "591d0abc9c149071036bfbc97bf94f5787289fe2",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/watcher.py",
                "status": "added",
                "additions": 171,
                "deletions": 0,
                "changes": 171,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/watcher.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/watcher.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/watcher.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,171 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class WatcherClient(NamespacedClient):\n+    @query_params()\n+    def ack_watch(self, watch_id, action_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-ack-watch.html>`_\n+\n+        :arg watch_id: Watch ID\n+        :arg action_id: A comma-separated list of the action ids to be\n+            acked\n+        \"\"\"\n+        if watch_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'watch_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_watcher\", \"watch\", watch_id, \"_ack\", action_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def activate_watch(self, watch_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-activate-watch.html>`_\n+\n+        :arg watch_id: Watch ID\n+        \"\"\"\n+        if watch_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'watch_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_watcher\", \"watch\", watch_id, \"_activate\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def deactivate_watch(self, watch_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-deactivate-watch.html>`_\n+\n+        :arg watch_id: Watch ID\n+        \"\"\"\n+        if watch_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'watch_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_watcher\", \"watch\", watch_id, \"_deactivate\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def delete_watch(self, id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-delete-watch.html>`_\n+\n+        :arg id: Watch ID\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_watcher\", \"watch\", id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"debug\")\n+    def execute_watch(self, body=None, id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-execute-watch.html>`_\n+\n+        :arg body: Execution control\n+        :arg id: Watch ID\n+        :arg debug: indicates whether the watch should execute in debug\n+            mode\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_watcher\", \"watch\", id, \"_execute\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def get_watch(self, id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-get-watch.html>`_\n+\n+        :arg id: Watch ID\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_watcher\", \"watch\", id), params=params, headers=headers\n+        )\n+\n+    @query_params(\"active\", \"if_primary_term\", \"if_seq_no\", \"version\")\n+    def put_watch(self, id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-put-watch.html>`_\n+\n+        :arg id: Watch ID\n+        :arg body: The watch\n+        :arg active: Specify whether the watch is in/active by default\n+        :arg if_primary_term: only update the watch if the last\n+            operation that has changed the watch has the specified primary term\n+        :arg if_seq_no: only update the watch if the last operation that\n+            has changed the watch has the specified sequence number\n+        :arg version: Explicit version number for concurrency control\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_watcher\", \"watch\", id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def start(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-start.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_watcher/_start\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"emit_stacktraces\")\n+    def stats(self, metric=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-stats.html>`_\n+\n+        :arg metric: Controls what additional stat metrics should be\n+            include in the response  Valid choices: _all, queued_watches,\n+            current_watches, pending_watches\n+        :arg emit_stacktraces: Emits stack traces of currently running\n+            watches\n+        :arg metric: Controls what additional stat metrics should be\n+            include in the response  Valid choices: _all, queued_watches,\n+            current_watches, pending_watches\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_watcher\", \"stats\", metric),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def stop(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-stop.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_watcher/_stop\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "769e681775124e13cd7fa4f0b55f82ce19a805ad",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/xpack.py",
                "status": "added",
                "additions": 30,
                "deletions": 0,
                "changes": 30,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/xpack.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/xpack.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/xpack.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,30 @@\n+from .utils import NamespacedClient, query_params\n+\n+\n+class XPackClient(NamespacedClient):\n+    def __getattr__(self, attr_name):\n+        return getattr(self.client, attr_name)\n+\n+    # AUTO-GENERATED-API-DEFINITIONS #\n+    @query_params(\"categories\")\n+    def info(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/info-api.html>`_\n+\n+        :arg categories: Comma-separated list of info categories. Can be\n+            any of: build, license, features\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_xpack\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"master_timeout\")\n+    def usage(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/usage-api.html>`_\n+\n+        :arg master_timeout: Specify timeout for watch write operation\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_xpack/usage\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "aba63ea739416eabc6713368f277c38764ddf192",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/compat.py",
                "status": "added",
                "additions": 27,
                "deletions": 0,
                "changes": 27,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/compat.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/compat.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/compat.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,27 @@\n+import sys\n+\n+PY2 = sys.version_info[0] == 2\n+\n+if PY2:\n+    string_types = (basestring,)  # noqa: F821\n+    from urllib import quote_plus, quote, urlencode, unquote\n+    from urlparse import urlparse\n+    from itertools import imap as map\n+    from Queue import Queue\n+else:\n+    string_types = str, bytes\n+    from urllib.parse import quote, quote_plus, urlencode, urlparse, unquote\n+\n+    map = map\n+    from queue import Queue\n+\n+__all__ = [\n+    \"string_types\",\n+    \"quote_plus\",\n+    \"quote\",\n+    \"urlencode\",\n+    \"unquote\",\n+    \"urlparse\",\n+    \"map\",\n+    \"Queue\",\n+]"
            },
            {
                "sha": "e56e541d5d5e99357e23e5e8018682c89832f496",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/__init__.py",
                "status": "added",
                "additions": 10,
                "deletions": 0,
                "changes": 10,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/__init__.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,10 @@\n+from .base import Connection\n+from .http_requests import RequestsHttpConnection\n+from .http_urllib3 import Urllib3HttpConnection, create_ssl_context\n+\n+__all__ = [\n+    \"Connection\",\n+    \"RequestsHttpConnection\",\n+    \"Urllib3HttpConnection\",\n+    \"create_ssl_context\",\n+]"
            },
            {
                "sha": "bc78fc918044e072de48badb72d4dc2d33bdcc3a",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/base.py",
                "status": "added",
                "additions": 259,
                "deletions": 0,
                "changes": 259,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/base.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/base.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/base.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,259 @@\n+import logging\n+import binascii\n+import gzip\n+import io\n+from platform import python_version\n+\n+try:\n+    import simplejson as json\n+except ImportError:\n+    import json\n+\n+from ..exceptions import TransportError, ImproperlyConfigured, HTTP_EXCEPTIONS\n+from .. import __versionstr__\n+\n+logger = logging.getLogger(\"elasticsearch7\")\n+\n+# create the elasticsearch7.trace logger, but only set propagate to False if the\n+# logger hasn't already been configured\n+_tracer_already_configured = \"elasticsearch7.trace\" in logging.Logger.manager.loggerDict\n+tracer = logging.getLogger(\"elasticsearch7.trace\")\n+if not _tracer_already_configured:\n+    tracer.propagate = False\n+\n+\n+class Connection(object):\n+    \"\"\"\n+    Class responsible for maintaining a connection to an Elasticsearch node. It\n+    holds persistent connection pool to it and it's main interface\n+    (`perform_request`) is thread-safe.\n+\n+    Also responsible for logging.\n+\n+    :arg host: hostname of the node (default: localhost)\n+    :arg port: port to use (integer, default: 9200)\n+    :arg use_ssl: use ssl for the connection if `True`\n+    :arg url_prefix: optional url prefix for elasticsearch7\n+    :arg timeout: default timeout in seconds (float, default: 10)\n+    :arg http_compress: Use gzip compression\n+    :arg cloud_id: The Cloud ID from ElasticCloud. Convenient way to connect to cloud instances.\n+    :arg opaque_id: Send this value in the 'X-Opaque-Id' HTTP header\n+        For tracing all requests made by this transport.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        host=\"localhost\",\n+        port=None,\n+        use_ssl=False,\n+        url_prefix=\"\",\n+        timeout=10,\n+        headers=None,\n+        http_compress=None,\n+        cloud_id=None,\n+        api_key=None,\n+        **kwargs\n+    ):\n+\n+        if cloud_id:\n+            try:\n+                _, cloud_id = cloud_id.split(\":\")\n+                parent_dn, es_uuid = (\n+                    binascii.a2b_base64(cloud_id.encode(\"utf-8\"))\n+                    .decode(\"utf-8\")\n+                    .split(\"$\")[:2]\n+                )\n+                if \":\" in parent_dn:\n+                    parent_dn, _, parent_port = parent_dn.rpartition(\":\")\n+                    if port is None and parent_port != \"443\":\n+                        port = int(parent_port)\n+            except (ValueError, IndexError):\n+                raise ImproperlyConfigured(\"'cloud_id' is not properly formatted\")\n+\n+            host = \"%s.%s\" % (es_uuid, parent_dn)\n+            use_ssl = True\n+            if http_compress is None:\n+                http_compress = True\n+\n+        # If cloud_id isn't set and port is default then use 9200.\n+        # Cloud should use '443' by default via the 'https' scheme.\n+        elif port is None:\n+            port = 9200\n+\n+        # Work-around if the implementing class doesn't\n+        # define the headers property before calling super().__init__()\n+        if not hasattr(self, \"headers\"):\n+            self.headers = {}\n+\n+        headers = headers or {}\n+        for key in headers:\n+            self.headers[key.lower()] = headers[key]\n+\n+        self.headers.setdefault(\"content-type\", \"application/json\")\n+        self.headers.setdefault(\"user-agent\", self._get_default_user_agent())\n+\n+        if api_key is not None:\n+            self.headers[\"authorization\"] = self._get_api_key_header_val(api_key)\n+\n+        if http_compress:\n+            self.headers[\"accept-encoding\"] = \"gzip,deflate\"\n+\n+        scheme = kwargs.get(\"scheme\", \"http\")\n+        if use_ssl or scheme == \"https\":\n+            scheme = \"https\"\n+            use_ssl = True\n+        self.use_ssl = use_ssl\n+        self.http_compress = http_compress or False\n+\n+        self.hostname = host\n+        self.port = port\n+        self.host = \"%s://%s\" % (scheme, host)\n+        if self.port is not None:\n+            self.host += \":%s\" % self.port\n+        if url_prefix:\n+            url_prefix = \"/\" + url_prefix.strip(\"/\")\n+        self.url_prefix = url_prefix\n+        self.timeout = timeout\n+\n+    def __repr__(self):\n+        return \"<%s: %s>\" % (self.__class__.__name__, self.host)\n+\n+    def __eq__(self, other):\n+        if not isinstance(other, Connection):\n+            raise TypeError(\"Unsupported equality check for %s and %s\" % (self, other))\n+        return self.__hash__() == other.__hash__()\n+\n+    def __hash__(self):\n+        return id(self)\n+\n+    def _gzip_compress(self, body):\n+        buf = io.BytesIO()\n+        with gzip.GzipFile(fileobj=buf, mode=\"wb\") as f:\n+            f.write(body)\n+        return buf.getvalue()\n+\n+    def _pretty_json(self, data):\n+        # pretty JSON in tracer curl logs\n+        try:\n+            return json.dumps(\n+                json.loads(data), sort_keys=True, indent=2, separators=(\",\", \": \")\n+            ).replace(\"'\", r\"\\u0027\")\n+        except (ValueError, TypeError):\n+            # non-json data or a bulk request\n+            return data\n+\n+    def _log_trace(self, method, path, body, status_code, response, duration):\n+        if not tracer.isEnabledFor(logging.INFO) or not tracer.handlers:\n+            return\n+\n+        # include pretty in trace curls\n+        path = path.replace(\"?\", \"?pretty&\", 1) if \"?\" in path else path + \"?pretty\"\n+        if self.url_prefix:\n+            path = path.replace(self.url_prefix, \"\", 1)\n+        tracer.info(\n+            \"curl %s-X%s 'http://localhost:9200%s' -d '%s'\",\n+            \"-H 'Content-Type: application/json' \" if body else \"\",\n+            method,\n+            path,\n+            self._pretty_json(body) if body else \"\",\n+        )\n+\n+        if tracer.isEnabledFor(logging.DEBUG):\n+            tracer.debug(\n+                \"#[%s] (%.3fs)\\n#%s\",\n+                status_code,\n+                duration,\n+                self._pretty_json(response).replace(\"\\n\", \"\\n#\") if response else \"\",\n+            )\n+\n+    def log_request_success(\n+        self, method, full_url, path, body, status_code, response, duration\n+    ):\n+        \"\"\" Log a successful API call.  \"\"\"\n+        #  TODO: optionally pass in params instead of full_url and do urlencode only when needed\n+\n+        # body has already been serialized to utf-8, deserialize it for logging\n+        # TODO: find a better way to avoid (de)encoding the body back and forth\n+        if body:\n+            try:\n+                body = body.decode(\"utf-8\", \"ignore\")\n+            except AttributeError:\n+                pass\n+\n+        logger.info(\n+            \"%s %s [status:%s request:%.3fs]\", method, full_url, status_code, duration\n+        )\n+        logger.debug(\"> %s\", body)\n+        logger.debug(\"< %s\", response)\n+\n+        self._log_trace(method, path, body, status_code, response, duration)\n+\n+    def log_request_fail(\n+        self,\n+        method,\n+        full_url,\n+        path,\n+        body,\n+        duration,\n+        status_code=None,\n+        response=None,\n+        exception=None,\n+    ):\n+        \"\"\" Log an unsuccessful API call.  \"\"\"\n+        # do not log 404s on HEAD requests\n+        if method == \"HEAD\" and status_code == 404:\n+            return\n+        logger.warning(\n+            \"%s %s [status:%s request:%.3fs]\",\n+            method,\n+            full_url,\n+            status_code or \"N/A\",\n+            duration,\n+            exc_info=exception is not None,\n+        )\n+\n+        # body has already been serialized to utf-8, deserialize it for logging\n+        # TODO: find a better way to avoid (de)encoding the body back and forth\n+        if body:\n+            try:\n+                body = body.decode(\"utf-8\", \"ignore\")\n+            except AttributeError:\n+                pass\n+\n+        logger.debug(\"> %s\", body)\n+\n+        self._log_trace(method, path, body, status_code, response, duration)\n+\n+        if response is not None:\n+            logger.debug(\"< %s\", response)\n+\n+    def _raise_error(self, status_code, raw_data):\n+        \"\"\" Locate appropriate exception and raise it. \"\"\"\n+        error_message = raw_data\n+        additional_info = None\n+        try:\n+            if raw_data:\n+                additional_info = json.loads(raw_data)\n+                error_message = additional_info.get(\"error\", error_message)\n+                if isinstance(error_message, dict) and \"type\" in error_message:\n+                    error_message = error_message[\"type\"]\n+        except (ValueError, TypeError) as err:\n+            logger.warning(\"Undecodable raw error response from server: %s\", err)\n+\n+        raise HTTP_EXCEPTIONS.get(status_code, TransportError)(\n+            status_code, error_message, additional_info\n+        )\n+\n+    def _get_default_user_agent(self):\n+        return \"elasticsearch-py/%s (Python %s)\" % (__versionstr__, python_version())\n+\n+    def _get_api_key_header_val(self, api_key):\n+        \"\"\"\n+        Check the type of the passed api_key and return the correct header value\n+        for the `API Key authentication <https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html>`\n+        :arg api_key, either a tuple or a base64 encoded string\n+        \"\"\"\n+        if isinstance(api_key, (tuple, list)):\n+            s = \"{0}:{1}\".format(api_key[0], api_key[1]).encode(\"utf-8\")\n+            return \"ApiKey \" + binascii.b2a_base64(s).rstrip(b\"\\r\\n\").decode(\"utf-8\")\n+        return \"ApiKey \" + api_key"
            },
            {
                "sha": "d88fe70bb451360354342a9778377856c787cb38",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_requests.py",
                "status": "added",
                "additions": 195,
                "deletions": 0,
                "changes": 195,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_requests.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_requests.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_requests.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,195 @@\n+import time\n+import warnings\n+\n+try:\n+    import requests\n+\n+    REQUESTS_AVAILABLE = True\n+except ImportError:\n+    REQUESTS_AVAILABLE = False\n+\n+from .base import Connection\n+from ..exceptions import (\n+    ConnectionError,\n+    ImproperlyConfigured,\n+    ConnectionTimeout,\n+    SSLError,\n+)\n+from ..compat import urlencode, string_types\n+\n+\n+class RequestsHttpConnection(Connection):\n+    \"\"\"\n+    Connection using the `requests` library.\n+\n+    :arg http_auth: optional http auth information as either ':' separated\n+        string or a tuple. Any value will be passed into requests as `auth`.\n+    :arg use_ssl: use ssl for the connection if `True`\n+    :arg verify_certs: whether to verify SSL certificates\n+    :arg ssl_show_warn: show warning when verify certs is disabled\n+    :arg ca_certs: optional path to CA bundle. By default standard requests'\n+        bundle will be used.\n+    :arg client_cert: path to the file containing the private key and the\n+        certificate, or cert only if using client_key\n+    :arg client_key: path to the file containing the private key if using\n+        separate cert and key files (client_cert will contain only the cert)\n+    :arg headers: any custom http headers to be add to requests\n+    :arg http_compress: Use gzip compression\n+    :arg cloud_id: The Cloud ID from ElasticCloud. Convenient way to connect to cloud instances.\n+        Other host connection params will be ignored.\n+    :arg api_key: optional API Key authentication as either base64 encoded string or a tuple.\n+    :arg opaque_id: Send this value in the 'X-Opaque-Id' HTTP header\n+        For tracing all requests made by this transport.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        host=\"localhost\",\n+        port=None,\n+        http_auth=None,\n+        use_ssl=False,\n+        verify_certs=True,\n+        ssl_show_warn=True,\n+        ca_certs=None,\n+        client_cert=None,\n+        client_key=None,\n+        headers=None,\n+        http_compress=None,\n+        cloud_id=None,\n+        api_key=None,\n+        opaque_id=None,\n+        **kwargs\n+    ):\n+        if not REQUESTS_AVAILABLE:\n+            raise ImproperlyConfigured(\n+                \"Please install requests to use RequestsHttpConnection.\"\n+            )\n+\n+        # Initialize Session so .headers works before calling super().__init__().\n+        self.session = requests.Session()\n+        for key in list(self.session.headers):\n+            self.session.headers.pop(key)\n+\n+        super(RequestsHttpConnection, self).__init__(\n+            host=host,\n+            port=port,\n+            use_ssl=use_ssl,\n+            headers=headers,\n+            http_compress=http_compress,\n+            cloud_id=cloud_id,\n+            api_key=api_key,\n+            opaque_id=opaque_id,\n+            **kwargs\n+        )\n+\n+        if not self.http_compress:\n+            # Need to set this to 'None' otherwise Requests adds its own.\n+            self.session.headers[\"accept-encoding\"] = None\n+\n+        if http_auth is not None:\n+            if isinstance(http_auth, (tuple, list)):\n+                http_auth = tuple(http_auth)\n+            elif isinstance(http_auth, string_types):\n+                http_auth = tuple(http_auth.split(\":\", 1))\n+            self.session.auth = http_auth\n+\n+        self.base_url = \"%s%s\" % (self.host, self.url_prefix,)\n+        self.session.verify = verify_certs\n+        if not client_key:\n+            self.session.cert = client_cert\n+        elif client_cert:\n+            # cert is a tuple of (certfile, keyfile)\n+            self.session.cert = (client_cert, client_key)\n+        if ca_certs:\n+            if not verify_certs:\n+                raise ImproperlyConfigured(\n+                    \"You cannot pass CA certificates when verify SSL is off.\"\n+                )\n+            self.session.verify = ca_certs\n+\n+        if not ssl_show_warn:\n+            requests.packages.urllib3.disable_warnings()\n+\n+        if self.use_ssl and not verify_certs and ssl_show_warn:\n+            warnings.warn(\n+                \"Connecting to %s using SSL with verify_certs=False is insecure.\"\n+                % self.host\n+            )\n+\n+    def perform_request(\n+        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None\n+    ):\n+        url = self.base_url + url\n+        headers = headers or {}\n+        if params:\n+            url = \"%s?%s\" % (url, urlencode(params or {}))\n+\n+        orig_body = body\n+        if self.http_compress and body:\n+            body = self._gzip_compress(body)\n+            headers[\"content-encoding\"] = \"gzip\"\n+\n+        start = time.time()\n+        request = requests.Request(method=method, headers=headers, url=url, data=body)\n+        prepared_request = self.session.prepare_request(request)\n+        settings = self.session.merge_environment_settings(\n+            prepared_request.url, {}, None, None, None\n+        )\n+        send_kwargs = {\"timeout\": timeout or self.timeout}\n+        send_kwargs.update(settings)\n+        try:\n+            response = self.session.send(prepared_request, **send_kwargs)\n+            duration = time.time() - start\n+            raw_data = response.text\n+        except Exception as e:\n+            self.log_request_fail(\n+                method,\n+                url,\n+                prepared_request.path_url,\n+                body,\n+                time.time() - start,\n+                exception=e,\n+            )\n+            if isinstance(e, requests.exceptions.SSLError):\n+                raise SSLError(\"N/A\", str(e), e)\n+            if isinstance(e, requests.Timeout):\n+                raise ConnectionTimeout(\"TIMEOUT\", str(e), e)\n+            raise ConnectionError(\"N/A\", str(e), e)\n+\n+        # raise errors based on http status codes, let the client handle those if needed\n+        if (\n+            not (200 <= response.status_code < 300)\n+            and response.status_code not in ignore\n+        ):\n+            self.log_request_fail(\n+                method,\n+                url,\n+                response.request.path_url,\n+                orig_body,\n+                duration,\n+                response.status_code,\n+                raw_data,\n+            )\n+            self._raise_error(response.status_code, raw_data)\n+\n+        self.log_request_success(\n+            method,\n+            url,\n+            response.request.path_url,\n+            orig_body,\n+            response.status_code,\n+            raw_data,\n+            duration,\n+        )\n+\n+        return response.status_code, response.headers, raw_data\n+\n+    @property\n+    def headers(self):\n+        return self.session.headers\n+\n+    def close(self):\n+        \"\"\"\n+        Explicitly closes connections\n+        \"\"\"\n+        self.session.close()"
            },
            {
                "sha": "7ba206a78cb1897fb0706f2aceae3d91ae2a89a9",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_urllib3.py",
                "status": "added",
                "additions": 260,
                "deletions": 0,
                "changes": 260,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_urllib3.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_urllib3.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_urllib3.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,260 @@\n+import time\n+import ssl\n+import urllib3\n+from urllib3.exceptions import ReadTimeoutError, SSLError as UrllibSSLError\n+from urllib3.util.retry import Retry\n+import warnings\n+\n+from .base import Connection\n+from ..exceptions import (\n+    ConnectionError,\n+    ImproperlyConfigured,\n+    ConnectionTimeout,\n+    SSLError,\n+)\n+from ..compat import urlencode\n+\n+# sentinel value for `verify_certs` and `ssl_show_warn`.\n+# This is used to detect if a user is passing in a value\n+# for SSL kwargs if also using an SSLContext.\n+VERIFY_CERTS_DEFAULT = object()\n+SSL_SHOW_WARN_DEFAULT = object()\n+\n+CA_CERTS = None\n+\n+try:\n+    import certifi\n+\n+    CA_CERTS = certifi.where()\n+except ImportError:\n+    pass\n+\n+\n+def create_ssl_context(**kwargs):\n+    \"\"\"\n+    A helper function around creating an SSL context\n+\n+    https://docs.python.org/3/library/ssl.html#context-creation\n+\n+    Accepts kwargs in the same manner as `create_default_context`.\n+    \"\"\"\n+    ctx = ssl.create_default_context(**kwargs)\n+    return ctx\n+\n+\n+class Urllib3HttpConnection(Connection):\n+    \"\"\"\n+    Default connection class using the `urllib3` library and the http protocol.\n+\n+    :arg host: hostname of the node (default: localhost)\n+    :arg port: port to use (integer, default: 9200)\n+    :arg url_prefix: optional url prefix for elasticsearch7\n+    :arg timeout: default timeout in seconds (float, default: 10)\n+    :arg http_auth: optional http auth information as either ':' separated\n+        string or a tuple\n+    :arg use_ssl: use ssl for the connection if `True`\n+    :arg verify_certs: whether to verify SSL certificates\n+    :arg ssl_show_warn: show warning when verify certs is disabled\n+    :arg ca_certs: optional path to CA bundle.\n+        See https://urllib3.readthedocs.io/en/latest/security.html#using-certifi-with-urllib3\n+        for instructions how to get default set\n+    :arg client_cert: path to the file containing the private key and the\n+        certificate, or cert only if using client_key\n+    :arg client_key: path to the file containing the private key if using\n+        separate cert and key files (client_cert will contain only the cert)\n+    :arg ssl_version: version of the SSL protocol to use. Choices are:\n+        SSLv23 (default) SSLv2 SSLv3 TLSv1 (see ``PROTOCOL_*`` constants in the\n+        ``ssl`` module for exact options for your environment).\n+    :arg ssl_assert_hostname: use hostname verification if not `False`\n+    :arg ssl_assert_fingerprint: verify the supplied certificate fingerprint if not `None`\n+    :arg maxsize: the number of connections which will be kept open to this\n+        host. See https://urllib3.readthedocs.io/en/1.4/pools.html#api for more\n+        information.\n+    :arg headers: any custom http headers to be add to requests\n+    :arg http_compress: Use gzip compression\n+    :arg cloud_id: The Cloud ID from ElasticCloud. Convenient way to connect to cloud instances.\n+        Other host connection params will be ignored.\n+    :arg api_key: optional API Key authentication as either base64 encoded string or a tuple.\n+    :arg opaque_id: Send this value in the 'X-Opaque-Id' HTTP header\n+        For tracing all requests made by this transport.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        host=\"localhost\",\n+        port=None,\n+        http_auth=None,\n+        use_ssl=False,\n+        verify_certs=VERIFY_CERTS_DEFAULT,\n+        ssl_show_warn=SSL_SHOW_WARN_DEFAULT,\n+        ca_certs=None,\n+        client_cert=None,\n+        client_key=None,\n+        ssl_version=None,\n+        ssl_assert_hostname=None,\n+        ssl_assert_fingerprint=None,\n+        maxsize=10,\n+        headers=None,\n+        ssl_context=None,\n+        http_compress=None,\n+        cloud_id=None,\n+        api_key=None,\n+        opaque_id=None,\n+        **kwargs\n+    ):\n+        # Initialize headers before calling super().__init__().\n+        self.headers = urllib3.make_headers(keep_alive=True)\n+\n+        super(Urllib3HttpConnection, self).__init__(\n+            host=host,\n+            port=port,\n+            use_ssl=use_ssl,\n+            headers=headers,\n+            http_compress=http_compress,\n+            cloud_id=cloud_id,\n+            api_key=api_key,\n+            opaque_id=opaque_id,\n+            **kwargs\n+        )\n+        if http_auth is not None:\n+            if isinstance(http_auth, (tuple, list)):\n+                http_auth = \":\".join(http_auth)\n+            self.headers.update(urllib3.make_headers(basic_auth=http_auth))\n+\n+        pool_class = urllib3.HTTPConnectionPool\n+        kw = {}\n+\n+        # if providing an SSL context, raise error if any other SSL related flag is used\n+        if ssl_context and (\n+            (verify_certs is not VERIFY_CERTS_DEFAULT)\n+            or (ssl_show_warn is not SSL_SHOW_WARN_DEFAULT)\n+            or ca_certs\n+            or client_cert\n+            or client_key\n+            or ssl_version\n+        ):\n+            warnings.warn(\n+                \"When using `ssl_context`, all other SSL related kwargs are ignored\"\n+            )\n+\n+        # if ssl_context provided use SSL by default\n+        if ssl_context and self.use_ssl:\n+            pool_class = urllib3.HTTPSConnectionPool\n+            kw.update(\n+                {\n+                    \"assert_fingerprint\": ssl_assert_fingerprint,\n+                    \"ssl_context\": ssl_context,\n+                }\n+            )\n+\n+        elif self.use_ssl:\n+            pool_class = urllib3.HTTPSConnectionPool\n+            kw.update(\n+                {\n+                    \"ssl_version\": ssl_version,\n+                    \"assert_hostname\": ssl_assert_hostname,\n+                    \"assert_fingerprint\": ssl_assert_fingerprint,\n+                }\n+            )\n+\n+            # Convert all sentinel values to their actual default\n+            # values if not using an SSLContext.\n+            if verify_certs is VERIFY_CERTS_DEFAULT:\n+                verify_certs = True\n+            if ssl_show_warn is SSL_SHOW_WARN_DEFAULT:\n+                ssl_show_warn = True\n+\n+            ca_certs = CA_CERTS if ca_certs is None else ca_certs\n+            if verify_certs:\n+                if not ca_certs:\n+                    raise ImproperlyConfigured(\n+                        \"Root certificates are missing for certificate \"\n+                        \"validation. Either pass them in using the ca_certs parameter or \"\n+                        \"install certifi to use it automatically.\"\n+                    )\n+\n+                kw.update(\n+                    {\n+                        \"cert_reqs\": \"CERT_REQUIRED\",\n+                        \"ca_certs\": ca_certs,\n+                        \"cert_file\": client_cert,\n+                        \"key_file\": client_key,\n+                    }\n+                )\n+            else:\n+                kw[\"cert_reqs\"] = \"CERT_NONE\"\n+                if ssl_show_warn:\n+                    warnings.warn(\n+                        \"Connecting to %s using SSL with verify_certs=False is insecure.\"\n+                        % self.host\n+                    )\n+                if not ssl_show_warn:\n+                    urllib3.disable_warnings()\n+\n+        self.pool = pool_class(\n+            self.hostname, port=self.port, timeout=self.timeout, maxsize=maxsize, **kw\n+        )\n+\n+    def perform_request(\n+        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None\n+    ):\n+        url = self.url_prefix + url\n+        if params:\n+            url = \"%s?%s\" % (url, urlencode(params))\n+        full_url = self.host + url\n+\n+        start = time.time()\n+        orig_body = body\n+        try:\n+            kw = {}\n+            if timeout:\n+                kw[\"timeout\"] = timeout\n+\n+            # in python2 we need to make sure the url and method are not\n+            # unicode. Otherwise the body will be decoded into unicode too and\n+            # that will fail (#133, #201).\n+            if not isinstance(url, str):\n+                url = url.encode(\"utf-8\")\n+            if not isinstance(method, str):\n+                method = method.encode(\"utf-8\")\n+\n+            request_headers = self.headers.copy()\n+            request_headers.update(headers or ())\n+\n+            if self.http_compress and body:\n+                body = self._gzip_compress(body)\n+                request_headers[\"content-encoding\"] = \"gzip\"\n+\n+            response = self.pool.urlopen(\n+                method, url, body, retries=Retry(False), headers=request_headers, **kw\n+            )\n+            duration = time.time() - start\n+            raw_data = response.data.decode(\"utf-8\")\n+        except Exception as e:\n+            self.log_request_fail(\n+                method, full_url, url, orig_body, time.time() - start, exception=e\n+            )\n+            if isinstance(e, UrllibSSLError):\n+                raise SSLError(\"N/A\", str(e), e)\n+            if isinstance(e, ReadTimeoutError):\n+                raise ConnectionTimeout(\"TIMEOUT\", str(e), e)\n+            raise ConnectionError(\"N/A\", str(e), e)\n+\n+        # raise errors based on http status codes, let the client handle those if needed\n+        if not (200 <= response.status < 300) and response.status not in ignore:\n+            self.log_request_fail(\n+                method, full_url, url, orig_body, duration, response.status, raw_data\n+            )\n+            self._raise_error(response.status, raw_data)\n+\n+        self.log_request_success(\n+            method, full_url, url, orig_body, response.status, raw_data, duration\n+        )\n+\n+        return response.status, response.getheaders(), raw_data\n+\n+    def close(self):\n+        \"\"\"\n+        Explicitly closes connection\n+        \"\"\"\n+        self.pool.close()"
            },
            {
                "sha": "dd5431e1517103f1b5ca7a7865736c614ebfdb8f",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/pooling.py",
                "status": "added",
                "additions": 33,
                "deletions": 0,
                "changes": 33,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/pooling.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/pooling.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/pooling.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,33 @@\n+try:\n+    import queue\n+except ImportError:\n+    import Queue as queue\n+from .base import Connection\n+\n+\n+class PoolingConnection(Connection):\n+    \"\"\"\n+    Base connection class for connections that use libraries without thread\n+    safety and no capacity for connection pooling. To use this just implement a\n+    ``_make_connection`` method that constructs a new connection and returns\n+    it.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        self._free_connections = queue.Queue()\n+        super(PoolingConnection, self).__init__(*args, **kwargs)\n+\n+    def _get_connection(self):\n+        try:\n+            return self._free_connections.get_nowait()\n+        except queue.Empty:\n+            return self._make_connection()\n+\n+    def _release_connection(self, con):\n+        self._free_connections.put(con)\n+\n+    def close(self):\n+        \"\"\"\n+        Explicitly close connection\n+        \"\"\"\n+        pass"
            },
            {
                "sha": "d6851786b320d2bc8ff5c393f686a8860d87b1a2",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection_pool.py",
                "status": "added",
                "additions": 282,
                "deletions": 0,
                "changes": 282,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection_pool.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection_pool.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection_pool.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,282 @@\n+import time\n+import random\n+import logging\n+import threading\n+\n+try:\n+    from Queue import PriorityQueue, Empty\n+except ImportError:\n+    from queue import PriorityQueue, Empty\n+\n+from .exceptions import ImproperlyConfigured\n+\n+logger = logging.getLogger(\"elasticsearch7\")\n+\n+\n+class ConnectionSelector(object):\n+    \"\"\"\n+    Simple class used to select a connection from a list of currently live\n+    connection instances. In init time it is passed a dictionary containing all\n+    the connections' options which it can then use during the selection\n+    process. When the `select` method is called it is given a list of\n+    *currently* live connections to choose from.\n+\n+    The options dictionary is the one that has been passed to\n+    :class:`~elasticsearch7.Transport` as `hosts` param and the same that is\n+    used to construct the Connection object itself. When the Connection was\n+    created from information retrieved from the cluster via the sniffing\n+    process it will be the dictionary returned by the `host_info_callback`.\n+\n+    Example of where this would be useful is a zone-aware selector that would\n+    only select connections from it's own zones and only fall back to other\n+    connections where there would be none in it's zones.\n+    \"\"\"\n+\n+    def __init__(self, opts):\n+        \"\"\"\n+        :arg opts: dictionary of connection instances and their options\n+        \"\"\"\n+        self.connection_opts = opts\n+\n+    def select(self, connections):\n+        \"\"\"\n+        Select a connection from the given list.\n+\n+        :arg connections: list of live connections to choose from\n+        \"\"\"\n+        pass\n+\n+\n+class RandomSelector(ConnectionSelector):\n+    \"\"\"\n+    Select a connection at random\n+    \"\"\"\n+\n+    def select(self, connections):\n+        return random.choice(connections)\n+\n+\n+class RoundRobinSelector(ConnectionSelector):\n+    \"\"\"\n+    Selector using round-robin.\n+    \"\"\"\n+\n+    def __init__(self, opts):\n+        super(RoundRobinSelector, self).__init__(opts)\n+        self.data = threading.local()\n+\n+    def select(self, connections):\n+        self.data.rr = getattr(self.data, \"rr\", -1) + 1\n+        self.data.rr %= len(connections)\n+        return connections[self.data.rr]\n+\n+\n+class ConnectionPool(object):\n+    \"\"\"\n+    Container holding the :class:`~elasticsearch7.Connection` instances,\n+    managing the selection process (via a\n+    :class:`~elasticsearch7.ConnectionSelector`) and dead connections.\n+\n+    It's only interactions are with the :class:`~elasticsearch7.Transport` class\n+    that drives all the actions within `ConnectionPool`.\n+\n+    Initially connections are stored on the class as a list and, along with the\n+    connection options, get passed to the `ConnectionSelector` instance for\n+    future reference.\n+\n+    Upon each request the `Transport` will ask for a `Connection` via the\n+    `get_connection` method. If the connection fails (it's `perform_request`\n+    raises a `ConnectionError`) it will be marked as dead (via `mark_dead`) and\n+    put on a timeout (if it fails N times in a row the timeout is exponentially\n+    longer - the formula is `default_timeout * 2 ** (fail_count - 1)`). When\n+    the timeout is over the connection will be resurrected and returned to the\n+    live pool. A connection that has been previously marked as dead and\n+    succeeds will be marked as live (its fail count will be deleted).\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        connections,\n+        dead_timeout=60,\n+        timeout_cutoff=5,\n+        selector_class=RoundRobinSelector,\n+        randomize_hosts=True,\n+        **kwargs\n+    ):\n+        \"\"\"\n+        :arg connections: list of tuples containing the\n+            :class:`~elasticsearch7.Connection` instance and it's options\n+        :arg dead_timeout: number of seconds a connection should be retired for\n+            after a failure, increases on consecutive failures\n+        :arg timeout_cutoff: number of consecutive failures after which the\n+            timeout doesn't increase\n+        :arg selector_class: :class:`~elasticsearch7.ConnectionSelector`\n+            subclass to use if more than one connection is live\n+        :arg randomize_hosts: shuffle the list of connections upon arrival to\n+            avoid dog piling effect across processes\n+        \"\"\"\n+        if not connections:\n+            raise ImproperlyConfigured(\n+                \"No defined connections, you need to \" \"specify at least one host.\"\n+            )\n+        self.connection_opts = connections\n+        self.connections = [c for (c, opts) in connections]\n+        # remember original connection list for resurrect(force=True)\n+        self.orig_connections = tuple(self.connections)\n+        # PriorityQueue for thread safety and ease of timeout management\n+        self.dead = PriorityQueue(len(self.connections))\n+        self.dead_count = {}\n+\n+        if randomize_hosts:\n+            # randomize the connection list to avoid all clients hitting same node\n+            # after startup/restart\n+            random.shuffle(self.connections)\n+\n+        # default timeout after which to try resurrecting a connection\n+        self.dead_timeout = dead_timeout\n+        self.timeout_cutoff = timeout_cutoff\n+\n+        self.selector = selector_class(dict(connections))\n+\n+    def mark_dead(self, connection, now=None):\n+        \"\"\"\n+        Mark the connection as dead (failed). Remove it from the live pool and\n+        put it on a timeout.\n+\n+        :arg connection: the failed instance\n+        \"\"\"\n+        # allow inject for testing purposes\n+        now = now if now else time.time()\n+        try:\n+            self.connections.remove(connection)\n+        except ValueError:\n+            logger.info(\n+                \"Attempted to remove %r, but it does not exist in the connection pool.\",\n+                connection,\n+            )\n+            # connection not alive or another thread marked it already, ignore\n+            return\n+        else:\n+            dead_count = self.dead_count.get(connection, 0) + 1\n+            self.dead_count[connection] = dead_count\n+            timeout = self.dead_timeout * 2 ** min(dead_count - 1, self.timeout_cutoff)\n+            self.dead.put((now + timeout, connection))\n+            logger.warning(\n+                \"Connection %r has failed for %i times in a row, putting on %i second timeout.\",\n+                connection,\n+                dead_count,\n+                timeout,\n+            )\n+\n+    def mark_live(self, connection):\n+        \"\"\"\n+        Mark connection as healthy after a resurrection. Resets the fail\n+        counter for the connection.\n+\n+        :arg connection: the connection to redeem\n+        \"\"\"\n+        try:\n+            del self.dead_count[connection]\n+        except KeyError:\n+            # race condition, safe to ignore\n+            pass\n+\n+    def resurrect(self, force=False):\n+        \"\"\"\n+        Attempt to resurrect a connection from the dead pool. It will try to\n+        locate one (not all) eligible (it's timeout is over) connection to\n+        return to the live pool. Any resurrected connection is also returned.\n+\n+        :arg force: resurrect a connection even if there is none eligible (used\n+            when we have no live connections). If force is specified resurrect\n+            always returns a connection.\n+\n+        \"\"\"\n+        # no dead connections\n+        if self.dead.empty():\n+            # we are forced to return a connection, take one from the original\n+            # list. This is to avoid a race condition where get_connection can\n+            # see no live connections but when it calls resurrect self.dead is\n+            # also empty. We assume that other threat has resurrected all\n+            # available connections so we can safely return one at random.\n+            if force:\n+                return random.choice(self.orig_connections)\n+            return\n+\n+        try:\n+            # retrieve a connection to check\n+            timeout, connection = self.dead.get(block=False)\n+        except Empty:\n+            # other thread has been faster and the queue is now empty. If we\n+            # are forced, return a connection at random again.\n+            if force:\n+                return random.choice(self.orig_connections)\n+            return\n+\n+        if not force and timeout > time.time():\n+            # return it back if not eligible and not forced\n+            self.dead.put((timeout, connection))\n+            return\n+\n+        # either we were forced or the connection is elligible to be retried\n+        self.connections.append(connection)\n+        logger.info(\"Resurrecting connection %r (force=%s).\", connection, force)\n+        return connection\n+\n+    def get_connection(self):\n+        \"\"\"\n+        Return a connection from the pool using the `ConnectionSelector`\n+        instance.\n+\n+        It tries to resurrect eligible connections, forces a resurrection when\n+        no connections are availible and passes the list of live connections to\n+        the selector instance to choose from.\n+\n+        Returns a connection instance and it's current fail count.\n+        \"\"\"\n+        self.resurrect()\n+        connections = self.connections[:]\n+\n+        # no live nodes, resurrect one by force and return it\n+        if not connections:\n+            return self.resurrect(True)\n+\n+        # only call selector if we have a selection\n+        if len(connections) > 1:\n+            return self.selector.select(connections)\n+\n+        # only one connection, no need for a selector\n+        return connections[0]\n+\n+    def close(self):\n+        \"\"\"\n+        Explicitly closes connections\n+        \"\"\"\n+        for conn in self.orig_connections:\n+            conn.close()\n+\n+\n+class DummyConnectionPool(ConnectionPool):\n+    def __init__(self, connections, **kwargs):\n+        if len(connections) != 1:\n+            raise ImproperlyConfigured(\n+                \"DummyConnectionPool needs exactly one \" \"connection defined.\"\n+            )\n+        # we need connection opts for sniffing logic\n+        self.connection_opts = connections\n+        self.connection = connections[0][0]\n+        self.connections = (self.connection,)\n+\n+    def get_connection(self):\n+        return self.connection\n+\n+    def close(self):\n+        \"\"\"\n+        Explicitly closes connections\n+        \"\"\"\n+        self.connection.close()\n+\n+    def _noop(self, *args, **kwargs):\n+        pass\n+\n+    mark_dead = mark_live = resurrect = _noop"
            },
            {
                "sha": "7068f2cb1ed1f1654ad25be156b4d70c52ed4ba2",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/exceptions.py",
                "status": "added",
                "additions": 146,
                "deletions": 0,
                "changes": 146,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/exceptions.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/exceptions.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/exceptions.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,146 @@\n+__all__ = [\n+    \"ImproperlyConfigured\",\n+    \"ElasticsearchException\",\n+    \"SerializationError\",\n+    \"TransportError\",\n+    \"NotFoundError\",\n+    \"ConflictError\",\n+    \"RequestError\",\n+    \"ConnectionError\",\n+    \"SSLError\",\n+    \"ConnectionTimeout\",\n+    \"AuthenticationException\",\n+    \"AuthorizationException\",\n+]\n+\n+\n+class ImproperlyConfigured(Exception):\n+    \"\"\"\n+    Exception raised when the config passed to the client is inconsistent or invalid.\n+    \"\"\"\n+\n+\n+class ElasticsearchException(Exception):\n+    \"\"\"\n+    Base class for all exceptions raised by this package's operations (doesn't\n+    apply to :class:`~elasticsearch7.ImproperlyConfigured`).\n+    \"\"\"\n+\n+\n+class SerializationError(ElasticsearchException):\n+    \"\"\"\n+    Data passed in failed to serialize properly in the ``Serializer`` being\n+    used.\n+    \"\"\"\n+\n+\n+class TransportError(ElasticsearchException):\n+    \"\"\"\n+    Exception raised when ES returns a non-OK (>=400) HTTP status code. Or when\n+    an actual connection error happens; in that case the ``status_code`` will\n+    be set to ``'N/A'``.\n+    \"\"\"\n+\n+    @property\n+    def status_code(self):\n+        \"\"\"\n+        The HTTP status code of the response that precipitated the error or\n+        ``'N/A'`` if not applicable.\n+        \"\"\"\n+        return self.args[0]\n+\n+    @property\n+    def error(self):\n+        \"\"\" A string error message. \"\"\"\n+        return self.args[1]\n+\n+    @property\n+    def info(self):\n+        \"\"\"\n+        Dict of returned error info from ES, where available, underlying\n+        exception when not.\n+        \"\"\"\n+        return self.args[2]\n+\n+    def __str__(self):\n+        cause = \"\"\n+        try:\n+            if self.info and \"error\" in self.info:\n+                if isinstance(self.info[\"error\"], dict):\n+                    root_cause = self.info[\"error\"][\"root_cause\"][0]\n+                    cause = \", \".join(\n+                        filter(\n+                            None,\n+                            [\n+                                repr(root_cause[\"reason\"]),\n+                                root_cause.get(\"resource.id\"),\n+                                root_cause.get(\"resource.type\"),\n+                            ],\n+                        )\n+                    )\n+\n+                else:\n+                    cause = repr(self.info[\"error\"])\n+        except LookupError:\n+            pass\n+        msg = \", \".join(filter(None, [str(self.status_code), repr(self.error), cause]))\n+        return \"%s(%s)\" % (self.__class__.__name__, msg)\n+\n+\n+class ConnectionError(TransportError):\n+    \"\"\"\n+    Error raised when there was an exception while talking to ES. Original\n+    exception from the underlying :class:`~elasticsearch7.Connection`\n+    implementation is available as ``.info``.\n+    \"\"\"\n+\n+    def __str__(self):\n+        return \"ConnectionError(%s) caused by: %s(%s)\" % (\n+            self.error,\n+            self.info.__class__.__name__,\n+            self.info,\n+        )\n+\n+\n+class SSLError(ConnectionError):\n+    \"\"\" Error raised when encountering SSL errors. \"\"\"\n+\n+\n+class ConnectionTimeout(ConnectionError):\n+    \"\"\" A network timeout. Doesn't cause a node retry by default. \"\"\"\n+\n+    def __str__(self):\n+        return \"ConnectionTimeout caused by - %s(%s)\" % (\n+            self.info.__class__.__name__,\n+            self.info,\n+        )\n+\n+\n+class NotFoundError(TransportError):\n+    \"\"\" Exception representing a 404 status code. \"\"\"\n+\n+\n+class ConflictError(TransportError):\n+    \"\"\" Exception representing a 409 status code. \"\"\"\n+\n+\n+class RequestError(TransportError):\n+    \"\"\" Exception representing a 400 status code. \"\"\"\n+\n+\n+class AuthenticationException(TransportError):\n+    \"\"\" Exception representing a 401 status code. \"\"\"\n+\n+\n+class AuthorizationException(TransportError):\n+    \"\"\" Exception representing a 403 status code. \"\"\"\n+\n+\n+# more generic mappings from status_code to python exceptions\n+HTTP_EXCEPTIONS = {\n+    400: RequestError,\n+    401: AuthenticationException,\n+    403: AuthorizationException,\n+    404: NotFoundError,\n+    409: ConflictError,\n+}"
            },
            {
                "sha": "28a11c303e776ad8df4051c7e990e567ce8cc7dc",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/__init__.py",
                "status": "added",
                "additions": 17,
                "deletions": 0,
                "changes": 17,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/__init__.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,17 @@\n+from .errors import BulkIndexError, ScanError\n+from .actions import expand_action, streaming_bulk, bulk, parallel_bulk\n+from .actions import scan, reindex\n+from .actions import _chunk_actions, _process_bulk_chunk\n+\n+__all__ = [\n+    \"BulkIndexError\",\n+    \"ScanError\",\n+    \"expand_action\",\n+    \"streaming_bulk\",\n+    \"bulk\",\n+    \"parallel_bulk\",\n+    \"scan\",\n+    \"reindex\",\n+    \"_chunk_actions\",\n+    \"_process_bulk_chunk\",\n+]"
            },
            {
                "sha": "b664671593836a08bb6675eb4806cfaf29342d9a",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/actions.py",
                "status": "added",
                "additions": 543,
                "deletions": 0,
                "changes": 543,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/actions.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/actions.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/actions.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,543 @@\n+from operator import methodcaller\n+import time\n+\n+from ..exceptions import TransportError\n+from ..compat import map, string_types, Queue\n+\n+from .errors import ScanError, BulkIndexError\n+\n+import logging\n+\n+\n+logger = logging.getLogger(\"elasticsearch7.helpers\")\n+\n+\n+def expand_action(data):\n+    \"\"\"\n+    From one document or action definition passed in by the user extract the\n+    action/data lines needed for elasticsearch7's\n+    :meth:`~elasticsearch7.Elasticsearch.bulk` api.\n+    \"\"\"\n+    # when given a string, assume user wants to index raw json\n+    if isinstance(data, string_types):\n+        return '{\"index\":{}}', data\n+\n+    # make sure we don't alter the action\n+    data = data.copy()\n+    op_type = data.pop(\"_op_type\", \"index\")\n+    action = {op_type: {}}\n+    for key in (\n+        \"_id\",\n+        \"_index\",\n+        \"_parent\",\n+        \"_percolate\",\n+        \"_retry_on_conflict\",\n+        \"_routing\",\n+        \"_timestamp\",\n+        \"_type\",\n+        \"_version\",\n+        \"_version_type\",\n+        \"parent\",\n+        \"pipeline\",\n+        \"retry_on_conflict\",\n+        \"routing\",\n+        \"version\",\n+        \"version_type\",\n+    ):\n+        if key in data:\n+            if key in [\n+                \"_parent\",\n+                \"_retry_on_conflict\",\n+                \"_routing\",\n+                \"_version\",\n+                \"_version_type\",\n+            ]:\n+                action[op_type][key[1:]] = data.pop(key)\n+            else:\n+                action[op_type][key] = data.pop(key)\n+\n+    # no data payload for delete\n+    if op_type == \"delete\":\n+        return action, None\n+\n+    return action, data.get(\"_source\", data)\n+\n+\n+def _chunk_actions(actions, chunk_size, max_chunk_bytes, serializer):\n+    \"\"\"\n+    Split actions into chunks by number or size, serialize them into strings in\n+    the process.\n+    \"\"\"\n+    bulk_actions, bulk_data = [], []\n+    size, action_count = 0, 0\n+    for action, data in actions:\n+        raw_data, raw_action = data, action\n+        action = serializer.dumps(action)\n+        # +1 to account for the trailing new line character\n+        cur_size = len(action.encode(\"utf-8\")) + 1\n+\n+        if data is not None:\n+            data = serializer.dumps(data)\n+            cur_size += len(data.encode(\"utf-8\")) + 1\n+\n+        # full chunk, send it and start a new one\n+        if bulk_actions and (\n+            size + cur_size > max_chunk_bytes or action_count == chunk_size\n+        ):\n+            yield bulk_data, bulk_actions\n+            bulk_actions, bulk_data = [], []\n+            size, action_count = 0, 0\n+\n+        bulk_actions.append(action)\n+        if data is not None:\n+            bulk_actions.append(data)\n+            bulk_data.append((raw_action, raw_data))\n+        else:\n+            bulk_data.append((raw_action,))\n+\n+        size += cur_size\n+        action_count += 1\n+\n+    if bulk_actions:\n+        yield bulk_data, bulk_actions\n+\n+\n+def _process_bulk_chunk(\n+    client,\n+    bulk_actions,\n+    bulk_data,\n+    raise_on_exception=True,\n+    raise_on_error=True,\n+    *args,\n+    **kwargs\n+):\n+    \"\"\"\n+    Send a bulk request to elasticsearch7 and process the output.\n+    \"\"\"\n+    # if raise on error is set, we need to collect errors per chunk before raising them\n+    errors = []\n+\n+    try:\n+        # send the actual request\n+        resp = client.bulk(\"\\n\".join(bulk_actions) + \"\\n\", *args, **kwargs)\n+    except TransportError as e:\n+        # default behavior - just propagate exception\n+        if raise_on_exception:\n+            raise e\n+\n+        # if we are not propagating, mark all actions in current chunk as failed\n+        err_message = str(e)\n+        exc_errors = []\n+\n+        for data in bulk_data:\n+            # collect all the information about failed actions\n+            op_type, action = data[0].copy().popitem()\n+            info = {\"error\": err_message, \"status\": e.status_code, \"exception\": e}\n+            if op_type != \"delete\":\n+                info[\"data\"] = data[1]\n+            info.update(action)\n+            exc_errors.append({op_type: info})\n+\n+        # emulate standard behavior for failed actions\n+        if raise_on_error:\n+            raise BulkIndexError(\n+                \"%i document(s) failed to index.\" % len(exc_errors), exc_errors\n+            )\n+        else:\n+            for err in exc_errors:\n+                yield False, err\n+            return\n+\n+    # go through request-response pairs and detect failures\n+    for data, (op_type, item) in zip(\n+        bulk_data, map(methodcaller(\"popitem\"), resp[\"items\"])\n+    ):\n+        ok = 200 <= item.get(\"status\", 500) < 300\n+        if not ok and raise_on_error:\n+            # include original document source\n+            if len(data) > 1:\n+                item[\"data\"] = data[1]\n+            errors.append({op_type: item})\n+\n+        if ok or not errors:\n+            # if we are not just recording all errors to be able to raise\n+            # them all at once, yield items individually\n+            yield ok, {op_type: item}\n+\n+    if errors:\n+        raise BulkIndexError(\"%i document(s) failed to index.\" % len(errors), errors)\n+\n+\n+def streaming_bulk(\n+    client,\n+    actions,\n+    chunk_size=500,\n+    max_chunk_bytes=100 * 1024 * 1024,\n+    raise_on_error=True,\n+    expand_action_callback=expand_action,\n+    raise_on_exception=True,\n+    max_retries=0,\n+    initial_backoff=2,\n+    max_backoff=600,\n+    yield_ok=True,\n+    *args,\n+    **kwargs\n+):\n+\n+    \"\"\"\n+    Streaming bulk consumes actions from the iterable passed in and yields\n+    results per action. For non-streaming usecases use\n+    :func:`~elasticsearch7.helpers.bulk` which is a wrapper around streaming\n+    bulk that returns summary information about the bulk operation once the\n+    entire input is consumed and sent.\n+\n+    If you specify ``max_retries`` it will also retry any documents that were\n+    rejected with a ``429`` status code. To do this it will wait (**by calling\n+    time.sleep which will block**) for ``initial_backoff`` seconds and then,\n+    every subsequent rejection for the same chunk, for double the time every\n+    time up to ``max_backoff`` seconds.\n+\n+    :arg client: instance of :class:`~elasticsearch7.Elasticsearch` to use\n+    :arg actions: iterable containing the actions to be executed\n+    :arg chunk_size: number of docs in one chunk sent to es (default: 500)\n+    :arg max_chunk_bytes: the maximum size of the request in bytes (default: 100MB)\n+    :arg raise_on_error: raise ``BulkIndexError`` containing errors (as `.errors`)\n+        from the execution of the last chunk when some occur. By default we raise.\n+    :arg raise_on_exception: if ``False`` then don't propagate exceptions from\n+        call to ``bulk`` and just report the items that failed as failed.\n+    :arg expand_action_callback: callback executed on each action passed in,\n+        should return a tuple containing the action line and the data line\n+        (`None` if data line should be omitted).\n+    :arg max_retries: maximum number of times a document will be retried when\n+        ``429`` is received, set to 0 (default) for no retries on ``429``\n+    :arg initial_backoff: number of seconds we should wait before the first\n+        retry. Any subsequent retries will be powers of ``initial_backoff *\n+        2**retry_number``\n+    :arg max_backoff: maximum number of seconds a retry will wait\n+    :arg yield_ok: if set to False will skip successful documents in the output\n+    \"\"\"\n+    actions = map(expand_action_callback, actions)\n+\n+    for bulk_data, bulk_actions in _chunk_actions(\n+        actions, chunk_size, max_chunk_bytes, client.transport.serializer\n+    ):\n+\n+        for attempt in range(max_retries + 1):\n+            to_retry, to_retry_data = [], []\n+            if attempt:\n+                time.sleep(min(max_backoff, initial_backoff * 2 ** (attempt - 1)))\n+\n+            try:\n+                for data, (ok, info) in zip(\n+                    bulk_data,\n+                    _process_bulk_chunk(\n+                        client,\n+                        bulk_actions,\n+                        bulk_data,\n+                        raise_on_exception,\n+                        raise_on_error,\n+                        *args,\n+                        **kwargs\n+                    ),\n+                ):\n+\n+                    if not ok:\n+                        action, info = info.popitem()\n+                        # retry if retries enabled, we get 429, and we are not\n+                        # in the last attempt\n+                        if (\n+                            max_retries\n+                            and info[\"status\"] == 429\n+                            and (attempt + 1) <= max_retries\n+                        ):\n+                            # _process_bulk_chunk expects strings so we need to\n+                            # re-serialize the data\n+                            to_retry.extend(\n+                                map(client.transport.serializer.dumps, data)\n+                            )\n+                            to_retry_data.append(data)\n+                        else:\n+                            yield ok, {action: info}\n+                    elif yield_ok:\n+                        yield ok, info\n+\n+            except TransportError as e:\n+                # suppress 429 errors since we will retry them\n+                if attempt == max_retries or e.status_code != 429:\n+                    raise\n+            else:\n+                if not to_retry:\n+                    break\n+                # retry only subset of documents that didn't succeed\n+                bulk_actions, bulk_data = to_retry, to_retry_data\n+\n+\n+def bulk(client, actions, stats_only=False, *args, **kwargs):\n+    \"\"\"\n+    Helper for the :meth:`~elasticsearch7.Elasticsearch.bulk` api that provides\n+    a more human friendly interface - it consumes an iterator of actions and\n+    sends them to elasticsearch7 in chunks. It returns a tuple with summary\n+    information - number of successfully executed actions and either list of\n+    errors or number of errors if ``stats_only`` is set to ``True``. Note that\n+    by default we raise a ``BulkIndexError`` when we encounter an error so\n+    options like ``stats_only`` only apply when ``raise_on_error`` is set to\n+    ``False``.\n+\n+    When errors are being collected original document data is included in the\n+    error dictionary which can lead to an extra high memory usage. If you need\n+    to process a lot of data and want to ignore/collect errors please consider\n+    using the :func:`~elasticsearch7.helpers.streaming_bulk` helper which will\n+    just return the errors and not store them in memory.\n+\n+\n+    :arg client: instance of :class:`~elasticsearch7.Elasticsearch` to use\n+    :arg actions: iterator containing the actions\n+    :arg stats_only: if `True` only report number of successful/failed\n+        operations instead of just number of successful and a list of error responses\n+\n+    Any additional keyword arguments will be passed to\n+    :func:`~elasticsearch7.helpers.streaming_bulk` which is used to execute\n+    the operation, see :func:`~elasticsearch7.helpers.streaming_bulk` for more\n+    accepted parameters.\n+    \"\"\"\n+    success, failed = 0, 0\n+\n+    # list of errors to be collected is not stats_only\n+    errors = []\n+\n+    # make streaming_bulk yield successful results so we can count them\n+    kwargs[\"yield_ok\"] = True\n+    for ok, item in streaming_bulk(client, actions, *args, **kwargs):\n+        # go through request-response pairs and detect failures\n+        if not ok:\n+            if not stats_only:\n+                errors.append(item)\n+            failed += 1\n+        else:\n+            success += 1\n+\n+    return success, failed if stats_only else errors\n+\n+\n+def parallel_bulk(\n+    client,\n+    actions,\n+    thread_count=4,\n+    chunk_size=500,\n+    max_chunk_bytes=100 * 1024 * 1024,\n+    queue_size=4,\n+    expand_action_callback=expand_action,\n+    *args,\n+    **kwargs\n+):\n+    \"\"\"\n+    Parallel version of the bulk helper run in multiple threads at once.\n+\n+    :arg client: instance of :class:`~elasticsearch7.Elasticsearch` to use\n+    :arg actions: iterator containing the actions\n+    :arg thread_count: size of the threadpool to use for the bulk requests\n+    :arg chunk_size: number of docs in one chunk sent to es (default: 500)\n+    :arg max_chunk_bytes: the maximum size of the request in bytes (default: 100MB)\n+    :arg raise_on_error: raise ``BulkIndexError`` containing errors (as `.errors`)\n+        from the execution of the last chunk when some occur. By default we raise.\n+    :arg raise_on_exception: if ``False`` then don't propagate exceptions from\n+        call to ``bulk`` and just report the items that failed as failed.\n+    :arg expand_action_callback: callback executed on each action passed in,\n+        should return a tuple containing the action line and the data line\n+        (`None` if data line should be omitted).\n+    :arg queue_size: size of the task queue between the main thread (producing\n+        chunks to send) and the processing threads.\n+    \"\"\"\n+    # Avoid importing multiprocessing unless parallel_bulk is used\n+    # to avoid exceptions on restricted environments like App Engine\n+    from multiprocessing.pool import ThreadPool\n+\n+    actions = map(expand_action_callback, actions)\n+\n+    class BlockingPool(ThreadPool):\n+        def _setup_queues(self):\n+            super(BlockingPool, self)._setup_queues()\n+            # The queue must be at least the size of the number of threads to\n+            # prevent hanging when inserting sentinel values during teardown.\n+            self._inqueue = Queue(max(queue_size, thread_count))\n+            self._quick_put = self._inqueue.put\n+\n+    pool = BlockingPool(thread_count)\n+\n+    try:\n+        for result in pool.imap(\n+            lambda bulk_chunk: list(\n+                _process_bulk_chunk(\n+                    client, bulk_chunk[1], bulk_chunk[0], *args, **kwargs\n+                )\n+            ),\n+            _chunk_actions(\n+                actions, chunk_size, max_chunk_bytes, client.transport.serializer\n+            ),\n+        ):\n+            for item in result:\n+                yield item\n+\n+    finally:\n+        pool.close()\n+        pool.join()\n+\n+\n+def scan(\n+    client,\n+    query=None,\n+    scroll=\"5m\",\n+    raise_on_error=True,\n+    preserve_order=False,\n+    size=1000,\n+    request_timeout=None,\n+    clear_scroll=True,\n+    scroll_kwargs=None,\n+    **kwargs\n+):\n+    \"\"\"\n+    Simple abstraction on top of the\n+    :meth:`~elasticsearch7.Elasticsearch.scroll` api - a simple iterator that\n+    yields all hits as returned by underlining scroll requests.\n+\n+    By default scan does not return results in any pre-determined order. To\n+    have a standard order in the returned documents (either by score or\n+    explicit sort definition) when scrolling, use ``preserve_order=True``. This\n+    may be an expensive operation and will negate the performance benefits of\n+    using ``scan``.\n+\n+    :arg client: instance of :class:`~elasticsearch7.Elasticsearch` to use\n+    :arg query: body for the :meth:`~elasticsearch7.Elasticsearch.search` api\n+    :arg scroll: Specify how long a consistent view of the index should be\n+        maintained for scrolled search\n+    :arg raise_on_error: raises an exception (``ScanError``) if an error is\n+        encountered (some shards fail to execute). By default we raise.\n+    :arg preserve_order: don't set the ``search_type`` to ``scan`` - this will\n+        cause the scroll to paginate with preserving the order. Note that this\n+        can be an extremely expensive operation and can easily lead to\n+        unpredictable results, use with caution.\n+    :arg size: size (per shard) of the batch send at each iteration.\n+    :arg request_timeout: explicit timeout for each call to ``scan``\n+    :arg clear_scroll: explicitly calls delete on the scroll id via the clear\n+        scroll API at the end of the method on completion or error, defaults\n+        to true.\n+    :arg scroll_kwargs: additional kwargs to be passed to\n+        :meth:`~elasticsearch7.Elasticsearch.scroll`\n+\n+    Any additional keyword arguments will be passed to the initial\n+    :meth:`~elasticsearch7.Elasticsearch.search` call::\n+\n+        scan(es,\n+            query={\"query\": {\"match\": {\"title\": \"python\"}}},\n+            index=\"orders-*\",\n+            doc_type=\"books\"\n+        )\n+\n+    \"\"\"\n+    scroll_kwargs = scroll_kwargs or {}\n+\n+    if not preserve_order:\n+        query = query.copy() if query else {}\n+        query[\"sort\"] = \"_doc\"\n+\n+    # initial search\n+    resp = client.search(\n+        body=query, scroll=scroll, size=size, request_timeout=request_timeout, **kwargs\n+    )\n+    scroll_id = resp.get(\"_scroll_id\")\n+\n+    try:\n+        while scroll_id and resp[\"hits\"][\"hits\"]:\n+            for hit in resp[\"hits\"][\"hits\"]:\n+                yield hit\n+\n+            # check if we have any errors\n+            if (resp[\"_shards\"][\"successful\"] + resp[\"_shards\"][\"skipped\"]) < resp[\n+                \"_shards\"\n+            ][\"total\"]:\n+                logger.warning(\n+                    \"Scroll request has only succeeded on %d (+%d skipped) shards out of %d.\",\n+                    resp[\"_shards\"][\"successful\"],\n+                    resp[\"_shards\"][\"skipped\"],\n+                    resp[\"_shards\"][\"total\"],\n+                )\n+                if raise_on_error:\n+                    raise ScanError(\n+                        scroll_id,\n+                        \"Scroll request has only succeeded on %d (+%d skiped) shards out of %d.\"\n+                        % (\n+                            resp[\"_shards\"][\"successful\"],\n+                            resp[\"_shards\"][\"skipped\"],\n+                            resp[\"_shards\"][\"total\"],\n+                        ),\n+                    )\n+            resp = client.scroll(\n+                body={\"scroll_id\": scroll_id, \"scroll\": scroll}, **scroll_kwargs\n+            )\n+            scroll_id = resp.get(\"_scroll_id\")\n+\n+    finally:\n+        if scroll_id and clear_scroll:\n+            client.clear_scroll(body={\"scroll_id\": [scroll_id]}, ignore=(404,))\n+\n+\n+def reindex(\n+    client,\n+    source_index,\n+    target_index,\n+    query=None,\n+    target_client=None,\n+    chunk_size=500,\n+    scroll=\"5m\",\n+    scan_kwargs={},\n+    bulk_kwargs={},\n+):\n+\n+    \"\"\"\n+    Reindex all documents from one index that satisfy a given query\n+    to another, potentially (if `target_client` is specified) on a different cluster.\n+    If you don't specify the query you will reindex all the documents.\n+\n+    Since ``2.3`` a :meth:`~elasticsearch7.Elasticsearch.reindex` api is\n+    available as part of elasticsearch7 itself. It is recommended to use the api\n+    instead of this helper wherever possible. The helper is here mostly for\n+    backwards compatibility and for situations where more flexibility is\n+    needed.\n+\n+    .. note::\n+\n+        This helper doesn't transfer mappings, just the data.\n+\n+    :arg client: instance of :class:`~elasticsearch7.Elasticsearch` to use (for\n+        read if `target_client` is specified as well)\n+    :arg source_index: index (or list of indices) to read documents from\n+    :arg target_index: name of the index in the target cluster to populate\n+    :arg query: body for the :meth:`~elasticsearch7.Elasticsearch.search` api\n+    :arg target_client: optional, is specified will be used for writing (thus\n+        enabling reindex between clusters)\n+    :arg chunk_size: number of docs in one chunk sent to es (default: 500)\n+    :arg scroll: Specify how long a consistent view of the index should be\n+        maintained for scrolled search\n+    :arg scan_kwargs: additional kwargs to be passed to\n+        :func:`~elasticsearch7.helpers.scan`\n+    :arg bulk_kwargs: additional kwargs to be passed to\n+        :func:`~elasticsearch7.helpers.bulk`\n+    \"\"\"\n+    target_client = client if target_client is None else target_client\n+    docs = scan(client, query=query, index=source_index, scroll=scroll, **scan_kwargs)\n+\n+    def _change_doc_index(hits, index):\n+        for h in hits:\n+            h[\"_index\"] = index\n+            if \"fields\" in h:\n+                h.update(h.pop(\"fields\"))\n+            yield h\n+\n+    kwargs = {\"stats_only\": True}\n+    kwargs.update(bulk_kwargs)\n+    return bulk(\n+        target_client,\n+        _change_doc_index(docs, target_index),\n+        chunk_size=chunk_size,\n+        **kwargs\n+    )"
            },
            {
                "sha": "6261822e51707e111761126b83e2b904d4db8235",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/errors.py",
                "status": "added",
                "additions": 14,
                "deletions": 0,
                "changes": 14,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/errors.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/errors.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/errors.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,14 @@\n+from ..exceptions import ElasticsearchException\n+\n+\n+class BulkIndexError(ElasticsearchException):\n+    @property\n+    def errors(self):\n+        \"\"\" List of errors from execution of the last chunk. \"\"\"\n+        return self.args[1]\n+\n+\n+class ScanError(ElasticsearchException):\n+    def __init__(self, scroll_id, *args, **kwargs):\n+        super(ScanError, self).__init__(*args, **kwargs)\n+        self.scroll_id = scroll_id"
            },
            {
                "sha": "938f27576bff5bd9c38e09670fb4c89c490f55b0",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/test.py",
                "status": "added",
                "additions": 63,
                "deletions": 0,
                "changes": 63,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/test.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/test.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/test.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,63 @@\n+import time\n+import os\n+from unittest import TestCase, SkipTest\n+\n+from elasticsearch7 import Elasticsearch\n+from elasticsearch7.exceptions import ConnectionError\n+\n+\n+def get_test_client(nowait=False, **kwargs):\n+    # construct kwargs from the environment\n+    kw = {\"timeout\": 30, \"ca_certs\": \".ci/certs/ca.pem\"}\n+\n+    if \"PYTHON_CONNECTION_CLASS\" in os.environ:\n+        from elasticsearch7 import connection\n+\n+        kw[\"connection_class\"] = getattr(\n+            connection, os.environ[\"PYTHON_CONNECTION_CLASS\"]\n+        )\n+\n+    kw.update(kwargs)\n+    client = Elasticsearch([os.environ.get(\"ELASTICSEARCH_HOST\", {})], **kw)\n+\n+    # wait for yellow status\n+    for _ in range(1 if nowait else 100):\n+        try:\n+            client.cluster.health(wait_for_status=\"yellow\")\n+            return client\n+        except ConnectionError:\n+            time.sleep(0.1)\n+    else:\n+        # timeout\n+        raise SkipTest(\"Elasticsearch failed to start.\")\n+\n+\n+def _get_version(version_string):\n+    if \".\" not in version_string:\n+        return ()\n+    version = version_string.strip().split(\".\")\n+    return tuple(int(v) if v.isdigit() else 999 for v in version)\n+\n+\n+class ElasticsearchTestCase(TestCase):\n+    @staticmethod\n+    def _get_client():\n+        return get_test_client()\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        super(ElasticsearchTestCase, cls).setUpClass()\n+        cls.client = cls._get_client()\n+\n+    def tearDown(self):\n+        super(ElasticsearchTestCase, self).tearDown()\n+        self.client.indices.delete(index=\"*\", ignore=404)\n+        self.client.indices.delete_template(name=\"*\", ignore=404)\n+        self.client.indices.delete_alias(index=\"_all\", name=\"_all\", ignore=404)\n+\n+    @property\n+    def es_version(self):\n+        if not hasattr(self, \"_es_version\"):\n+            version_string = self.client.info()[\"version\"][\"number\"]\n+            self._es_version = _get_version(version_string)\n+        return self._es_version"
            },
            {
                "sha": "dd7d0dc87ac835d0f9400829013cc2a7400e2e92",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/serializer.py",
                "status": "added",
                "additions": 86,
                "deletions": 0,
                "changes": 86,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/serializer.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/serializer.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/serializer.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,86 @@\n+try:\n+    import simplejson as json\n+except ImportError:\n+    import json\n+import uuid\n+from datetime import date, datetime\n+from decimal import Decimal\n+\n+from .exceptions import SerializationError, ImproperlyConfigured\n+from .compat import string_types\n+\n+\n+class TextSerializer(object):\n+    mimetype = \"text/plain\"\n+\n+    def loads(self, s):\n+        return s\n+\n+    def dumps(self, data):\n+        if isinstance(data, string_types):\n+            return data\n+\n+        raise SerializationError(\"Cannot serialize %r into text.\" % data)\n+\n+\n+class JSONSerializer(object):\n+    mimetype = \"application/json\"\n+\n+    def default(self, data):\n+        if isinstance(data, (date, datetime)):\n+            return data.isoformat()\n+        elif isinstance(data, Decimal):\n+            return float(data)\n+        elif isinstance(data, uuid.UUID):\n+            return str(data)\n+        raise TypeError(\"Unable to serialize %r (type: %s)\" % (data, type(data)))\n+\n+    def loads(self, s):\n+        try:\n+            return json.loads(s)\n+        except (ValueError, TypeError) as e:\n+            raise SerializationError(s, e)\n+\n+    def dumps(self, data):\n+        # don't serialize strings\n+        if isinstance(data, string_types):\n+            return data\n+\n+        try:\n+            return json.dumps(\n+                data, default=self.default, ensure_ascii=False, separators=(\",\", \":\")\n+            )\n+        except (ValueError, TypeError) as e:\n+            raise SerializationError(data, e)\n+\n+\n+DEFAULT_SERIALIZERS = {\n+    JSONSerializer.mimetype: JSONSerializer(),\n+    TextSerializer.mimetype: TextSerializer(),\n+}\n+\n+\n+class Deserializer(object):\n+    def __init__(self, serializers, default_mimetype=\"application/json\"):\n+        try:\n+            self.default = serializers[default_mimetype]\n+        except KeyError:\n+            raise ImproperlyConfigured(\n+                \"Cannot find default serializer (%s)\" % default_mimetype\n+            )\n+        self.serializers = serializers\n+\n+    def loads(self, s, mimetype=None):\n+        if not mimetype:\n+            deserializer = self.default\n+        else:\n+            # split out charset\n+            mimetype, _, _ = mimetype.partition(\";\")\n+            try:\n+                deserializer = self.serializers[mimetype]\n+            except KeyError:\n+                raise SerializationError(\n+                    \"Unknown mimetype, unable to deserialize: %s\" % mimetype\n+                )\n+\n+        return deserializer.loads(s)"
            },
            {
                "sha": "2c3a89cf97003c4d5e1f81459820d062397b24e8",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/transport.py",
                "status": "added",
                "additions": 403,
                "deletions": 0,
                "changes": 403,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/transport.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/transport.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/transport.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,403 @@\n+import time\n+from itertools import chain\n+\n+from .connection import Urllib3HttpConnection\n+from .connection_pool import ConnectionPool, DummyConnectionPool\n+from .serializer import JSONSerializer, Deserializer, DEFAULT_SERIALIZERS\n+from .exceptions import (\n+    ConnectionError,\n+    TransportError,\n+    SerializationError,\n+    ConnectionTimeout,\n+)\n+\n+\n+def get_host_info(node_info, host):\n+    \"\"\"\n+    Simple callback that takes the node info from `/_cluster/nodes` and a\n+    parsed connection information and return the connection information. If\n+    `None` is returned this node will be skipped.\n+\n+    Useful for filtering nodes (by proximity for example) or if additional\n+    information needs to be provided for the :class:`~elasticsearch7.Connection`\n+    class. By default master only nodes are filtered out since they shouldn't\n+    typically be used for API operations.\n+\n+    :arg node_info: node information from `/_cluster/nodes`\n+    :arg host: connection information (host, port) extracted from the node info\n+    \"\"\"\n+    # ignore master only nodes\n+    if node_info.get(\"roles\", []) == [\"master\"]:\n+        return None\n+    return host\n+\n+\n+class Transport(object):\n+    \"\"\"\n+    Encapsulation of transport-related to logic. Handles instantiation of the\n+    individual connections as well as creating a connection pool to hold them.\n+\n+    Main interface is the `perform_request` method.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        hosts,\n+        connection_class=Urllib3HttpConnection,\n+        connection_pool_class=ConnectionPool,\n+        host_info_callback=get_host_info,\n+        sniff_on_start=False,\n+        sniffer_timeout=None,\n+        sniff_timeout=0.1,\n+        sniff_on_connection_fail=False,\n+        serializer=JSONSerializer(),\n+        serializers=None,\n+        default_mimetype=\"application/json\",\n+        max_retries=3,\n+        retry_on_status=(502, 503, 504),\n+        retry_on_timeout=False,\n+        send_get_body_as=\"GET\",\n+        **kwargs\n+    ):\n+        \"\"\"\n+        :arg hosts: list of dictionaries, each containing keyword arguments to\n+            create a `connection_class` instance\n+        :arg connection_class: subclass of :class:`~elasticsearch7.Connection` to use\n+        :arg connection_pool_class: subclass of :class:`~elasticsearch7.ConnectionPool` to use\n+        :arg host_info_callback: callback responsible for taking the node information from\n+            `/_cluster/nodes`, along with already extracted information, and\n+            producing a list of arguments (same as `hosts` parameter)\n+        :arg sniff_on_start: flag indicating whether to obtain a list of nodes\n+            from the cluster at startup time\n+        :arg sniffer_timeout: number of seconds between automatic sniffs\n+        :arg sniff_on_connection_fail: flag controlling if connection failure triggers a sniff\n+        :arg sniff_timeout: timeout used for the sniff request - it should be a\n+            fast api call and we are talking potentially to more nodes so we want\n+            to fail quickly. Not used during initial sniffing (if\n+            ``sniff_on_start`` is on) when the connection still isn't\n+            initialized.\n+        :arg serializer: serializer instance\n+        :arg serializers: optional dict of serializer instances that will be\n+            used for deserializing data coming from the server. (key is the mimetype)\n+        :arg default_mimetype: when no mimetype is specified by the server\n+            response assume this mimetype, defaults to `'application/json'`\n+        :arg max_retries: maximum number of retries before an exception is propagated\n+        :arg retry_on_status: set of HTTP status codes on which we should retry\n+            on a different node. defaults to ``(502, 503, 504)``\n+        :arg retry_on_timeout: should timeout trigger a retry on different\n+            node? (default `False`)\n+        :arg send_get_body_as: for GET requests with body this option allows\n+            you to specify an alternate way of execution for environments that\n+            don't support passing bodies with GET requests. If you set this to\n+            'POST' a POST method will be used instead, if to 'source' then the body\n+            will be serialized and passed as a query parameter `source`.\n+\n+        Any extra keyword arguments will be passed to the `connection_class`\n+        when creating and instance unless overridden by that connection's\n+        options provided as part of the hosts parameter.\n+        \"\"\"\n+\n+        # serialization config\n+        _serializers = DEFAULT_SERIALIZERS.copy()\n+        # if a serializer has been specified, use it for deserialization as well\n+        _serializers[serializer.mimetype] = serializer\n+        # if custom serializers map has been supplied, override the defaults with it\n+        if serializers:\n+            _serializers.update(serializers)\n+        # create a deserializer with our config\n+        self.deserializer = Deserializer(_serializers, default_mimetype)\n+\n+        self.max_retries = max_retries\n+        self.retry_on_timeout = retry_on_timeout\n+        self.retry_on_status = retry_on_status\n+        self.send_get_body_as = send_get_body_as\n+\n+        # data serializer\n+        self.serializer = serializer\n+\n+        # store all strategies...\n+        self.connection_pool_class = connection_pool_class\n+        self.connection_class = connection_class\n+\n+        # ...save kwargs to be passed to the connections\n+        self.kwargs = kwargs\n+        self.hosts = hosts\n+\n+        # ...and instantiate them\n+        self.set_connections(hosts)\n+        # retain the original connection instances for sniffing\n+        self.seed_connections = self.connection_pool.connections[:]\n+\n+        # Don't enable sniffing on Cloud instances.\n+        if kwargs.get(\"cloud_id\", False):\n+            sniff_on_start = False\n+            sniff_on_connection_fail = False\n+\n+        # sniffing data\n+        self.sniffer_timeout = sniffer_timeout\n+        self.sniff_on_connection_fail = sniff_on_connection_fail\n+        self.last_sniff = time.time()\n+        self.sniff_timeout = sniff_timeout\n+\n+        # callback to construct host dict from data in /_cluster/nodes\n+        self.host_info_callback = host_info_callback\n+\n+        if sniff_on_start:\n+            self.sniff_hosts(True)\n+\n+    def add_connection(self, host):\n+        \"\"\"\n+        Create a new :class:`~elasticsearch7.Connection` instance and add it to the pool.\n+\n+        :arg host: kwargs that will be used to create the instance\n+        \"\"\"\n+        self.hosts.append(host)\n+        self.set_connections(self.hosts)\n+\n+    def set_connections(self, hosts):\n+        \"\"\"\n+        Instantiate all the connections and create new connection pool to hold them.\n+        Tries to identify unchanged hosts and re-use existing\n+        :class:`~elasticsearch7.Connection` instances.\n+\n+        :arg hosts: same as `__init__`\n+        \"\"\"\n+        # construct the connections\n+        def _create_connection(host):\n+            # if this is not the initial setup look at the existing connection\n+            # options and identify connections that haven't changed and can be\n+            # kept around.\n+            if hasattr(self, \"connection_pool\"):\n+                for (connection, old_host) in self.connection_pool.connection_opts:\n+                    if old_host == host:\n+                        return connection\n+\n+            # previously unseen params, create new connection\n+            kwargs = self.kwargs.copy()\n+            kwargs.update(host)\n+            return self.connection_class(**kwargs)\n+\n+        connections = map(_create_connection, hosts)\n+\n+        connections = list(zip(connections, hosts))\n+        if len(connections) == 1:\n+            self.connection_pool = DummyConnectionPool(connections)\n+        else:\n+            # pass the hosts dicts to the connection pool to optionally extract parameters from\n+            self.connection_pool = self.connection_pool_class(\n+                connections, **self.kwargs\n+            )\n+\n+    def get_connection(self):\n+        \"\"\"\n+        Retrieve a :class:`~elasticsearch7.Connection` instance from the\n+        :class:`~elasticsearch7.ConnectionPool` instance.\n+        \"\"\"\n+        if self.sniffer_timeout:\n+            if time.time() >= self.last_sniff + self.sniffer_timeout:\n+                self.sniff_hosts()\n+        return self.connection_pool.get_connection()\n+\n+    def _get_sniff_data(self, initial=False):\n+        \"\"\"\n+        Perform the request to get sniffing information. Returns a list of\n+        dictionaries (one per node) containing all the information from the\n+        cluster.\n+\n+        It also sets the last_sniff attribute in case of a successful attempt.\n+\n+        In rare cases it might be possible to override this method in your\n+        custom Transport class to serve data from alternative source like\n+        configuration management.\n+        \"\"\"\n+        previous_sniff = self.last_sniff\n+\n+        try:\n+            # reset last_sniff timestamp\n+            self.last_sniff = time.time()\n+            # go through all current connections as well as the\n+            # seed_connections for good measure\n+            for c in chain(self.connection_pool.connections, self.seed_connections):\n+                try:\n+                    # use small timeout for the sniffing request, should be a fast api call\n+                    _, headers, node_info = c.perform_request(\n+                        \"GET\",\n+                        \"/_nodes/_all/http\",\n+                        timeout=self.sniff_timeout if not initial else None,\n+                    )\n+                    node_info = self.deserializer.loads(\n+                        node_info, headers.get(\"content-type\")\n+                    )\n+                    break\n+                except (ConnectionError, SerializationError):\n+                    pass\n+            else:\n+                raise TransportError(\"N/A\", \"Unable to sniff hosts.\")\n+        except Exception:\n+            # keep the previous value on error\n+            self.last_sniff = previous_sniff\n+            raise\n+\n+        return list(node_info[\"nodes\"].values())\n+\n+    def _get_host_info(self, host_info):\n+        host = {}\n+        address = host_info.get(\"http\", {}).get(\"publish_address\")\n+\n+        # malformed or no address given\n+        if not address or \":\" not in address:\n+            return None\n+\n+        if \"/\" in address:\n+            # Support 7.x host/ip:port behavior where http.publish_host has been set.\n+            fqdn, ipaddress = address.split(\"/\", 1)\n+            host[\"host\"] = fqdn\n+            _, host[\"port\"] = ipaddress.rsplit(\":\", 1)\n+            host[\"port\"] = int(host[\"port\"])\n+\n+        else:\n+            host[\"host\"], host[\"port\"] = address.rsplit(\":\", 1)\n+            host[\"port\"] = int(host[\"port\"])\n+\n+        return self.host_info_callback(host_info, host)\n+\n+    def sniff_hosts(self, initial=False):\n+        \"\"\"\n+        Obtain a list of nodes from the cluster and create a new connection\n+        pool using the information retrieved.\n+\n+        To extract the node connection parameters use the ``nodes_to_host_callback``.\n+\n+        :arg initial: flag indicating if this is during startup\n+            (``sniff_on_start``), ignore the ``sniff_timeout`` if ``True``\n+        \"\"\"\n+        node_info = self._get_sniff_data(initial)\n+\n+        hosts = list(filter(None, (self._get_host_info(n) for n in node_info)))\n+\n+        # we weren't able to get any nodes or host_info_callback blocked all -\n+        # raise error.\n+        if not hosts:\n+            raise TransportError(\n+                \"N/A\", \"Unable to sniff hosts - no viable hosts found.\"\n+            )\n+\n+        self.set_connections(hosts)\n+\n+    def mark_dead(self, connection):\n+        \"\"\"\n+        Mark a connection as dead (failed) in the connection pool. If sniffing\n+        on failure is enabled this will initiate the sniffing process.\n+\n+        :arg connection: instance of :class:`~elasticsearch7.Connection` that failed\n+        \"\"\"\n+        # mark as dead even when sniffing to avoid hitting this host during the sniff process\n+        self.connection_pool.mark_dead(connection)\n+        if self.sniff_on_connection_fail:\n+            self.sniff_hosts()\n+\n+    def perform_request(self, method, url, headers=None, params=None, body=None):\n+        \"\"\"\n+        Perform the actual request. Retrieve a connection from the connection\n+        pool, pass all the information to it's perform_request method and\n+        return the data.\n+\n+        If an exception was raised, mark the connection as failed and retry (up\n+        to `max_retries` times).\n+\n+        If the operation was successful and the connection used was previously\n+        marked as dead, mark it as live, resetting it's failure count.\n+\n+        :arg method: HTTP method to use\n+        :arg url: absolute url (without host) to target\n+        :arg headers: dictionary of headers, will be handed over to the\n+            underlying :class:`~elasticsearch7.Connection` class\n+        :arg params: dictionary of query parameters, will be handed over to the\n+            underlying :class:`~elasticsearch7.Connection` class for serialization\n+        :arg body: body of the request, will be serialized using serializer and\n+            passed to the connection\n+        \"\"\"\n+        if body is not None:\n+            body = self.serializer.dumps(body)\n+\n+            # some clients or environments don't support sending GET with body\n+            if method in (\"HEAD\", \"GET\") and self.send_get_body_as != \"GET\":\n+                # send it as post instead\n+                if self.send_get_body_as == \"POST\":\n+                    method = \"POST\"\n+\n+                # or as source parameter\n+                elif self.send_get_body_as == \"source\":\n+                    if params is None:\n+                        params = {}\n+                    params[\"source\"] = body\n+                    body = None\n+\n+        if body is not None:\n+            try:\n+                body = body.encode(\"utf-8\", \"surrogatepass\")\n+            except (UnicodeDecodeError, AttributeError):\n+                # bytes/str - no need to re-encode\n+                pass\n+\n+        ignore = ()\n+        timeout = None\n+        if params:\n+            timeout = params.pop(\"request_timeout\", None)\n+            ignore = params.pop(\"ignore\", ())\n+            if isinstance(ignore, int):\n+                ignore = (ignore,)\n+\n+        for attempt in range(self.max_retries + 1):\n+            connection = self.get_connection()\n+\n+            try:\n+                status, headers_response, data = connection.perform_request(\n+                    method,\n+                    url,\n+                    params,\n+                    body,\n+                    headers=headers,\n+                    ignore=ignore,\n+                    timeout=timeout,\n+                )\n+\n+            except TransportError as e:\n+                if method == \"HEAD\" and e.status_code == 404:\n+                    return False\n+\n+                retry = False\n+                if isinstance(e, ConnectionTimeout):\n+                    retry = self.retry_on_timeout\n+                elif isinstance(e, ConnectionError):\n+                    retry = True\n+                elif e.status_code in self.retry_on_status:\n+                    retry = True\n+\n+                if retry:\n+                    # only mark as dead if we are retrying\n+                    self.mark_dead(connection)\n+                    # raise exception on last retry\n+                    if attempt == self.max_retries:\n+                        raise\n+                else:\n+                    raise\n+\n+            else:\n+                # connection didn't fail, confirm it's live status\n+                self.connection_pool.mark_live(connection)\n+\n+                if method == \"HEAD\":\n+                    return 200 <= status < 300\n+\n+                if data:\n+                    data = self.deserializer.loads(\n+                        data, headers_response.get(\"content-type\")\n+                    )\n+                return data\n+\n+    def close(self):\n+        \"\"\"\n+        Explicitly closes connections\n+        \"\"\"\n+        self.connection_pool.close()"
            },
            {
                "sha": "6a40f5332707f1a1197f898fd97b172438acab9d",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/setup.py",
                "status": "added",
                "additions": 66,
                "deletions": 0,
                "changes": 66,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/setup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/58d037c0c9e03651b3216ac0a0e525aa84bfa9fd/github-analytics/libs/elasticsearch7-7.6.0/setup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/setup.py?ref=58d037c0c9e03651b3216ac0a0e525aa84bfa9fd",
                "patch": "@@ -0,0 +1,66 @@\n+# -*- coding: utf-8 -*-\n+from os.path import join, dirname\n+from setuptools import setup, find_packages\n+import sys\n+\n+VERSION = (7, 6, 0)\n+__version__ = VERSION\n+__versionstr__ = \"7.6.0\"\n+\n+with open(join(dirname(__file__), \"README\")) as f:\n+    long_description = f.read().strip()\n+\n+install_requires = [\"urllib3>=1.21.1\"]\n+tests_require = [\n+    \"requests>=2.0.0, <3.0.0\",\n+    \"nose\",\n+    \"coverage\",\n+    \"mock\",\n+    \"pyyaml\",\n+    \"nosexcover\",\n+]\n+\n+docs_require = [\"sphinx<1.7\", \"sphinx_rtd_theme\"]\n+generate_require = [\"black\", \"jinja2\"]\n+\n+setup(\n+    name=\"elasticsearch7\",\n+    description=\"Python client for Elasticsearch\",\n+    license=\"Apache-2.0\",\n+    url=\"https://github.com/elastic/elasticsearch-py\",\n+    download_url=\"https://github.com/shdkpr2008/elasticsearch-py/archive/7.6.0.tar.gz\",\n+    long_description=long_description,\n+    long_description_content_type=\"text/x-rst\",\n+    version=__versionstr__,\n+    author=\"Honza Kr\u00e1l, Nick Lang\",\n+    author_email=\"honza.kral@gmail.com, nick@nicklang.com\",\n+    maintainer=\"Seth Michael Larson\",\n+    maintainer_email=\"seth.larson@elastic.co\",\n+    packages=find_packages(where=\".\", exclude=(\"test_elasticsearch*\",)),\n+    classifiers=[\n+        \"Development Status :: 5 - Production/Stable\",\n+        \"License :: OSI Approved :: Apache Software License\",\n+        \"Intended Audience :: Developers\",\n+        \"Operating System :: OS Independent\",\n+        \"Programming Language :: Python\",\n+        \"Programming Language :: Python :: 2\",\n+        \"Programming Language :: Python :: 2.7\",\n+        \"Programming Language :: Python :: 3\",\n+        \"Programming Language :: Python :: 3.4\",\n+        \"Programming Language :: Python :: 3.5\",\n+        \"Programming Language :: Python :: 3.6\",\n+        \"Programming Language :: Python :: 3.7\",\n+        \"Programming Language :: Python :: 3.8\",\n+        \"Programming Language :: Python :: Implementation :: CPython\",\n+        \"Programming Language :: Python :: Implementation :: PyPy\",\n+    ],\n+    python_requires=\">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, <4\",\n+    install_requires=install_requires,\n+    test_suite=\"test_elasticsearch7.run_tests.run_all\",\n+    tests_require=tests_require,\n+    extras_require={\n+        \"develop\": tests_require + docs_require + generate_require,\n+        \"docs\": docs_require,\n+        \"requests\": [\"requests>=2.4.0, <3.0.0\"],\n+    },\n+)"
            }
        ]
    },
    {
        "sha": "e500cf3ea83383cf4a9c6e8522a0710a042b2785",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmU1MDBjZjNlYTgzMzgzY2Y0YTljNmU4NTIyYTA3MTBhMDQyYjI3ODU=",
        "commit": {
            "author": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-18T19:06:25Z"
            },
            "committer": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-18T19:06:25Z"
            },
            "message": "Merge remote-tracking branch 'origin/master' into shreyas",
            "tree": {
                "sha": "ac434b4c56ec63c6b1c49114fc71b4f951387d08",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/ac434b4c56ec63c6b1c49114fc71b4f951387d08"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/e500cf3ea83383cf4a9c6e8522a0710a042b2785",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/e500cf3ea83383cf4a9c6e8522a0710a042b2785",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/e500cf3ea83383cf4a9c6e8522a0710a042b2785",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/e500cf3ea83383cf4a9c6e8522a0710a042b2785/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/734cadd4cd38fd334af09ba24fe48194d04e6d60"
            },
            {
                "sha": "bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14"
            }
        ],
        "stats": {
            "total": 7966,
            "additions": 7960,
            "deletions": 6
        },
        "files": [
            {
                "sha": "dbf6b196b731844d58da03d2e3074adb6fc42c9a",
                "filename": "django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60"
            },
            {
                "sha": "b2d7d6592a36fb06c1b9651c07f1732412140302",
                "filename": "django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60"
            },
            {
                "sha": "6dff987ab1fe0796006b6985024176363b001cee",
                "filename": "django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60"
            },
            {
                "sha": "ac5e7b3240fa9a57f5725d27bb0c66c0a0414594",
                "filename": "django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60"
            },
            {
                "sha": "57f000ea0b2ec023842ae3100069b40c43f07a59",
                "filename": "django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60"
            },
            {
                "sha": "c1822c41af31cf69213a8254754370766dbea5cf",
                "filename": "django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e500cf3ea83383cf4a9c6e8522a0710a042b2785/django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e500cf3ea83383cf4a9c6e8522a0710a042b2785/django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc?ref=e500cf3ea83383cf4a9c6e8522a0710a042b2785"
            },
            {
                "sha": "ef448b20dac8ff827a0b3cb100f906e75060ff4c",
                "filename": "django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60"
            },
            {
                "sha": "1a0871e09e88291d2cce6e6a409a2decdcc3550b",
                "filename": "django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60"
            },
            {
                "sha": "abf7528accc4cd0182652f8d597491fd778aca7a",
                "filename": "django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60"
            },
            {
                "sha": "45f5ff925cdf0aa1dac25e82e4b0a797c54b4c98",
                "filename": "django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60"
            },
            {
                "sha": "cf9dfc09c6708f5109e1dc64473da13cacb06ad8",
                "filename": "github-crawler/Templates_commands/single_commit.txt",
                "status": "added",
                "additions": 404,
                "deletions": 0,
                "changes": 404,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e500cf3ea83383cf4a9c6e8522a0710a042b2785/github-crawler/Templates_commands/single_commit.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e500cf3ea83383cf4a9c6e8522a0710a042b2785/github-crawler/Templates_commands/single_commit.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/single_commit.txt?ref=e500cf3ea83383cf4a9c6e8522a0710a042b2785",
                "patch": "@@ -0,0 +1,404 @@\n+PUT commit\n+{\n+\t\"mappings\": {\n+\t\t\"properties\": {\n+\t\t\t\"sha\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"node_id\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"commit\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"author\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"name\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"email\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"date\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\t\"committer\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"name\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"email\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"date\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\t\"message\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"tree\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"sha\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"comment_count\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"verification\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"verified\": {\n+\t\t\t\t\t\t\t\t\"type\": \"boolean\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"reason\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"signature\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"payload\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\t\"url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"html_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"comments_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"author\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"login\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"id\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"node_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"avatar_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gravatar_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"html_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"followers_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"following_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gists_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"starred_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"subscriptions_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"organizations_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"repos_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"received_events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"type\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"site_admin\": {\n+\t\t\t\t\t\t\"type\": \"boolean\"\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\t\"committer\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"login\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"id\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"node_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"avatar_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gravatar_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"html_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"followers_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"following_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gists_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"starred_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"subscriptions_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"organizations_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"repos_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"received_events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"type\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"site_admin\": {\n+\t\t\t\t\t\t\"type\": \"boolean\"\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\t\"parents\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"sha\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"html_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\t\t\"stat\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"total\": {\n+\t\t\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"additions\": {\n+\t\t\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"deletions\": {\n+\t\t\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\t\"files\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"sha\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+          \"filename\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+                \"status\":{\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+                \"additions\":{\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+                \"deletions\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+                \"changes\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+                \"blob_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+                \"raw_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+                \"contents_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+POST commit/_doc/1\n+{\n+        \"sha\": \"8d46f35175b0deac76cfc940d0f37c2d0266feb9\",\n+        \"node_id\": \"MDY6Q29tbWl0MjM5MzczNzkxOjhkNDZmMzUxNzViMGRlYWM3NmNmYzk0MGQwZjM3YzJkMDI2NmZlYjk=\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"vishwakulkarni\",\n+                \"email\": \"vishwa.kulkarni@gmail.com\",\n+                \"date\": \"2020-04-17T23:43:05Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"vishwakulkarni\",\n+                \"email\": \"vishwa.kulkarni@gmail.com\",\n+                \"date\": \"2020-04-17T23:43:52Z\"\n+            },\n+            \"message\": \"sending each commit to commit elastic search\",\n+            \"tree\": {\n+                \"sha\": \"d63d8d00bddbeeeb34c68395dee3e2a8b3b9fe2e\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/d63d8d00bddbeeeb34c68395dee3e2a8b3b9fe2e\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/8d46f35175b0deac76cfc940d0f37c2d0266feb9\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/8d46f35175b0deac76cfc940d0f37c2d0266feb9\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/commit/8d46f35175b0deac76cfc940d0f37c2d0266feb9\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/8d46f35175b0deac76cfc940d0f37c2d0266feb9/comments\",\n+        \"author\": {\n+            \"login\": \"vishwakulkarni\",\n+            \"id\": 5782419,\n+            \"node_id\": \"MDQ6VXNlcjU3ODI0MTk=\",\n+            \"avatar_url\": \"https://avatars2.githubusercontent.com/u/5782419?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/vishwakulkarni\",\n+            \"html_url\": \"https://github.com/vishwakulkarni\",\n+            \"followers_url\": \"https://api.github.com/users/vishwakulkarni/followers\",\n+            \"following_url\": \"https://api.github.com/users/vishwakulkarni/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/vishwakulkarni/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/vishwakulkarni/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/vishwakulkarni/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/vishwakulkarni/repos\",\n+            \"events_url\": \"https://api.github.com/users/vishwakulkarni/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/vishwakulkarni/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"vishwakulkarni\",\n+            \"id\": 5782419,\n+            \"node_id\": \"MDQ6VXNlcjU3ODI0MTk=\",\n+            \"avatar_url\": \"https://avatars2.githubusercontent.com/u/5782419?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/vishwakulkarni\",\n+            \"html_url\": \"https://github.com/vishwakulkarni\",\n+            \"followers_url\": \"https://api.github.com/users/vishwakulkarni/followers\",\n+            \"following_url\": \"https://api.github.com/users/vishwakulkarni/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/vishwakulkarni/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/vishwakulkarni/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/vishwakulkarni/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/vishwakulkarni/repos\",\n+            \"events_url\": \"https://api.github.com/users/vishwakulkarni/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/vishwakulkarni/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"7e0ce0a9e453f38906631c18bffd2259520ccc4a\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/7e0ce0a9e453f38906631c18bffd2259520ccc4a\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/commit/7e0ce0a9e453f38906631c18bffd2259520ccc4a\"\n+            }\n+        ],\n+        \"stats\": {\n+            \"total\": 37,\n+            \"additions\": 30,\n+            \"deletions\": 7\n+        },\n+        \"files\": [\n+            {\n+                \"sha\": \"ea825674a4ea96dfc360b8604cf348be17c77862\",\n+                \"filename\": \"github-crawler/lib/__pycache__/config.cpython-37.pyc\",\n+                \"status\": \"modified\",\n+                \"additions\": 0,\n+                \"deletions\": 0,\n+                \"changes\": 0,\n+                \"blob_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/blob/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/__pycache__/config.cpython-37.pyc\",\n+                \"raw_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/raw/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/__pycache__/config.cpython-37.pyc\",\n+                \"contents_url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/config.cpython-37.pyc?ref=8d46f35175b0deac76cfc940d0f37c2d0266feb9\"\n+            },\n+            {\n+                \"sha\": \"3d0afa90c8139736e73d85fdfc7a62ecae9e9bb9\",\n+                \"filename\": \"github-crawler/lib/__pycache__/helper.cpython-37.pyc\",\n+                \"status\": \"modified\",\n+                \"additions\": 0,\n+                \"deletions\": 0,\n+                \"changes\": 0,\n+                \"blob_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/blob/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/__pycache__/helper.cpython-37.pyc\",\n+                \"raw_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/raw/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/__pycache__/helper.cpython-37.pyc\",\n+                \"contents_url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/helper.cpython-37.pyc?ref=8d46f35175b0deac76cfc940d0f37c2d0266feb9\"\n+            },\n+            {\n+                \"sha\": \"c53f6f839bbf8e1821f46baef596036946a02353\",\n+                \"filename\": \"github-crawler/lib/config.py\",\n+                \"status\": \"modified\",\n+                \"additions\": 2,\n+                \"deletions\": 2,\n+                \"changes\": 4,\n+                \"blob_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/blob/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/config.py\",\n+                \"raw_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/raw/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/config.py\",\n+                \"contents_url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/config.py?ref=8d46f35175b0deac76cfc940d0f37c2d0266feb9\",\n+                \"patch\": \"@@ -1,5 +1,5 @@\\n GITEA_APP_URL = 'YOUR_GITEA_API'\\n-GITEA_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\\n+GITEA_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\\n GITHUB_USERNAME = 'vishwakulkarni'\\n-GITHUB_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\\n+GITHUB_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\\n SQL_ALCHEMY_STRING = ''\"\n+            },\n+            {\n+                \"sha\": \"0d6d1134efc800f1e44f14e710929e09e24b9514\",\n+                \"filename\": \"github-crawler/lib/helper.py\",\n+                \"status\": \"modified\",\n+                \"additions\": 28,\n+                \"deletions\": 5,\n+                \"changes\": 33,\n+                \"blob_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/blob/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/helper.py\",\n+                \"raw_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/raw/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/helper.py\",\n+                \"contents_url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/helper.py?ref=8d46f35175b0deac76cfc940d0f37c2d0266feb9\",\n+                \"patch\": \"@@ -12,9 +12,9 @@\\n from elasticsearch import Elasticsearch\\n \\n GITEA_APP_URL = 'YOUR_GITEA_API'\\n-GITEA_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\\n+GITEA_TOKEN = 'aff8945f859b696bf05932045f0be7e1b8379ddb'\\n GITHUB_USERNAME = 'vishwakulkarni'\\n-GITHUB_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\\n+GITHUB_TOKEN = 'aff8945f859b696bf05932045f0be7e1b8379ddb'\\n SQL_ALCHEMY_STRING = ''\\n \\n \\n@@ -78,14 +78,37 @@ def get_org_users(self,owner,api):\\n         \\n         return members_list\\n \\n+    #save commits of repos as json\\n+    def commits_of_repo_github(self,repo, owner, api):\\n+        commits = []\\n+        next = True\\n+        i = 1\\n+        while next == True:\\n+            url = api + '/repos/{}/{}/commits?page={}&per_page=100'.format(owner, repo, i)\\n+            commit_pg = self.gh_session.get(url = url)\\n+            commit_tp = json.loads(commit_pg.content)\\n+            for commit in commit_tp:\\n+                commits.append(commit) \\n+                print(commit)\\n+            if 'Link' in commit_pg.headers:\\n+                if 'rel=\\\"next\\\"' not in commit_pg.headers['Link']:\\n+                    next = False\\n+            i = i + 1\\n+        return commits\\n+\\n+\\n \\n \\n #testing comment after use\\n h = Helper()\\n github_api = \\\"https://api.github.com\\\"\\n h.set_org_name(\\\"CUBigDataClass\\\")\\n #print(h.get_org_information(\\\"vishwakulkarni\\\",github_api))\\n-#k=h.get_repositories('vishwakulkarni',github_api)\\n+k=h.get_repositories('vishwakulkarni',github_api)\\n+commits = h.commits_of_repo_github('kode-kallas','cubigdataclass',github_api)\\n+for commit in commits:\\n+    h.send_to_elasticInstance(commit,'commit',commit['sha'])\\n #print(len(k))\\n-#for mem in k:\\n-#    print(mem['name'])\\n+for mem in k:\\n+    print(mem['name'])\\n+    #commits = h.commits_of_repo_github(mem['name'],'vishwakulkarni',github_api)\\n\\\\ No newline at end of file\"\n+            }\n+        ]\n+    }"
            },
            {
                "sha": "ee3f72b649851d53e136e9b44dbe6e9d42127767",
                "filename": "github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e500cf3ea83383cf4a9c6e8522a0710a042b2785/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e500cf3ea83383cf4a9c6e8522a0710a042b2785/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/config.cpython-37.pyc?ref=e500cf3ea83383cf4a9c6e8522a0710a042b2785"
            },
            {
                "sha": "284d76f5429a504d3eeb16e9fd2316969817f711",
                "filename": "github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e500cf3ea83383cf4a9c6e8522a0710a042b2785/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e500cf3ea83383cf4a9c6e8522a0710a042b2785/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/helper.cpython-37.pyc?ref=e500cf3ea83383cf4a9c6e8522a0710a042b2785"
            },
            {
                "sha": "c1bdbf4fc514200e2add8c9fa94e061374bc329f",
                "filename": "github-crawler/lib/commits.json",
                "status": "added",
                "additions": 7544,
                "deletions": 0,
                "changes": 7544,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e500cf3ea83383cf4a9c6e8522a0710a042b2785/github-crawler/lib/commits.json",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e500cf3ea83383cf4a9c6e8522a0710a042b2785/github-crawler/lib/commits.json",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/commits.json?ref=e500cf3ea83383cf4a9c6e8522a0710a042b2785"
            },
            {
                "sha": "46de99cb50435a0169426ee53f861f57a4db2b50",
                "filename": "github-crawler/lib/helper.py",
                "status": "modified",
                "additions": 12,
                "deletions": 6,
                "changes": 18,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e500cf3ea83383cf4a9c6e8522a0710a042b2785/github-crawler/lib/helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e500cf3ea83383cf4a9c6e8522a0710a042b2785/github-crawler/lib/helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/helper.py?ref=e500cf3ea83383cf4a9c6e8522a0710a042b2785",
                "patch": "@@ -12,9 +12,9 @@\n from elasticsearch import Elasticsearch\n \n GITEA_APP_URL = 'YOUR_GITEA_API'\n-GITEA_TOKEN = 'aff8945f859b696bf05932045f0be7e1b8379ddb'\n+GITEA_TOKEN = '48fb007c866e8fb377a4d61a60af3dc881707df4'\n GITHUB_USERNAME = 'vishwakulkarni'\n-GITHUB_TOKEN = 'aff8945f859b696bf05932045f0be7e1b8379ddb'\n+GITHUB_TOKEN = '48fb007c866e8fb377a4d61a60af3dc881707df4'\n SQL_ALCHEMY_STRING = ''\n \n \n@@ -88,8 +88,11 @@ def commits_of_repo_github(self,repo, owner, api):\n             commit_pg = self.gh_session.get(url = url)\n             commit_tp = json.loads(commit_pg.content)\n             for commit in commit_tp:\n-                commits.append(commit) \n-                print(commit)\n+                url = commit['url']\n+                single_commit = self.gh_session.get(url = url)\n+                single_commit = json.loads(single_commit.content)\n+                commits.append(single_commit)\n+                #print(commit)\n             if 'Link' in commit_pg.headers:\n                 if 'rel=\"next\"' not in commit_pg.headers['Link']:\n                     next = False\n@@ -100,15 +103,18 @@ def commits_of_repo_github(self,repo, owner, api):\n \n \n #testing comment after use\n-h = Helper()\n+'''h = Helper()\n github_api = \"https://api.github.com\"\n h.set_org_name(\"CUBigDataClass\")\n #print(h.get_org_information(\"vishwakulkarni\",github_api))\n k=h.get_repositories('vishwakulkarni',github_api)\n commits = h.commits_of_repo_github('kode-kallas','cubigdataclass',github_api)\n+with open(\"commits.json\", \"w\") as outfile: \n+    outfile.write(json.dumps(commits,indent=4)) \n+#print(commits[0]['stat'])\n for commit in commits:\n     h.send_to_elasticInstance(commit,'commit',commit['sha'])\n #print(len(k))\n for mem in k:\n     print(mem['name'])\n-    #commits = h.commits_of_repo_github(mem['name'],'vishwakulkarni',github_api)\n\\ No newline at end of file\n+    #commits = h.commits_of_repo_github(mem['name'],'vishwakulkarni',github_api)'''\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "734cadd4cd38fd334af09ba24fe48194d04e6d60",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjczNGNhZGQ0Y2QzOGZkMzM0YWYwOWJhMjRmZTQ4MTk0ZDA0ZTZkNjA=",
        "commit": {
            "author": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-18T19:04:50Z"
            },
            "committer": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-18T19:04:50Z"
            },
            "message": "Analytics on collaborators, issues, languages and commits. Used multithreading for faster processing.",
            "tree": {
                "sha": "ee537b70d639aec00a4dd6696e9f30602246ba71",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/ee537b70d639aec00a4dd6696e9f30602246ba71"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/734cadd4cd38fd334af09ba24fe48194d04e6d60",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/734cadd4cd38fd334af09ba24fe48194d04e6d60",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/734cadd4cd38fd334af09ba24fe48194d04e6d60",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/734cadd4cd38fd334af09ba24fe48194d04e6d60/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "34510f3de8d1271fa3c008cbd4f2680083393c3c",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/34510f3de8d1271fa3c008cbd4f2680083393c3c",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/34510f3de8d1271fa3c008cbd4f2680083393c3c"
            }
        ],
        "stats": {
            "total": 14111,
            "additions": 14078,
            "deletions": 33
        },
        "files": [
            {
                "sha": "a1ab0c3daca63c456c2f25214b2dcd67b4c6254b",
                "filename": "github-analytics/CassandraHelper/CassandraRepoData.py",
                "status": "modified",
                "additions": 33,
                "deletions": 29,
                "changes": 62,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/CassandraHelper/CassandraRepoData.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/CassandraHelper/CassandraRepoData.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/CassandraRepoData.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -1,34 +1,38 @@\n from CassandraHelper import Utils as utils\n+from CassandraHelper import config\n+from multiprocessing import Pool\n \n class CassandraRepoData():\n     def __init__(self, elasticOrgData):\n         dataList = elasticOrgData['hits']['hits']\n-        self.data = list()\n-        for i in range(len(dataList)):\n-            repoDataItem = dataList[i]['_source']\n-            repoData = {\n-                \"contributors\": utils.get(utils.getContributorsList, repoDataItem['contributors_url']),\n-                \"name\": repoDataItem['name'],\n-                \"commits_url\": repoDataItem['commits_url'],\n-                \"created_at\": repoDataItem['created_at'],\n-                \"issues_url\": repoDataItem['issues_url'],\n-                \"id\": repoDataItem['id'],\n-                \"watchers_count\": repoDataItem['watchers_count'],\n-                \"description\": repoDataItem['description'],\n-                \"forks_count\": repoDataItem['forks_count'],\n-                \"forks_url\": repoDataItem['forks_url'],\n-                \"full_name\": repoDataItem['full_name'],\n-                \"html_url\": repoDataItem['html_url'],\n-                \"languages_url\": repoDataItem['languages_url'],\n-                \"owner\":{\n-                    \"login\": repoDataItem['owner']['login'],\n-                    \"avatar_url\": repoDataItem['owner']['avatar_url'],\n-                    \"html_url\": repoDataItem['owner']['html_url'],\n-                    \"id\": repoDataItem['owner']['id'],\n-                    \"organizations_url\": repoDataItem['owner']['organizations_url']\n-                },\n-                \"open_issues_count\": repoDataItem['open_issues_count'],\n-                \"tags_url\": repoDataItem['tags_url'],\n-                \"updated_at\": repoDataItem['updated_at']\n-            }\n-            self.data.append(repoData)\n\\ No newline at end of file\n+        threadPool = Pool(config.THREAD_COUNT)\n+        self.data = threadPool.map(self.processDataList, dataList)\n+\n+    def processDataList(self, repoDataItem):\n+        repoDataItem = repoDataItem['_source']\n+        repoData = {\n+            \"contributors\": utils.get(utils.getContributorsList, repoDataItem['contributors_url']),\n+            \"name\": repoDataItem['name'],\n+            \"commits\": utils.get(utils.getCommitsList, repoDataItem['commits_url'][:-6]),\n+            \"created_at\": repoDataItem['created_at'],\n+            \"issues\": utils.get(utils.getIssuesList, repoDataItem['issues_url'][:-9]),\n+            \"id\": repoDataItem['id'],\n+            \"watchers_count\": repoDataItem['watchers_count'],\n+            \"description\": repoDataItem['description'],\n+            \"forks_count\": repoDataItem['forks_count'],\n+            \"forks_url\": repoDataItem['forks_url'],\n+            \"full_name\": repoDataItem['full_name'],\n+            \"html_url\": repoDataItem['html_url'],\n+            \"languages\": utils.get(utils.getLanguages, repoDataItem['languages_url'], False),\n+            \"owner\": {\n+                \"name\": repoDataItem['owner']['login'],\n+                \"avatar_url\": repoDataItem['owner']['avatar_url'],\n+                \"html_url\": repoDataItem['owner']['html_url'],\n+                \"id\": repoDataItem['owner']['id'],\n+                \"organizations_url\": repoDataItem['owner']['organizations_url']\n+            },\n+            \"open_issues_count\": repoDataItem['open_issues_count'],\n+            \"tags_url\": repoDataItem['tags_url'],\n+            \"updated_at\": repoDataItem['updated_at']\n+        }\n+        return repoData\n\\ No newline at end of file"
            },
            {
                "sha": "7667996b0eeedf8911c2984ef82241821fc8958c",
                "filename": "github-analytics/CassandraHelper/Utils.py",
                "status": "modified",
                "additions": 36,
                "deletions": 4,
                "changes": 40,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/CassandraHelper/Utils.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/CassandraHelper/Utils.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/Utils.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -4,22 +4,54 @@\n import json\n \n # Http get request with callback\n-def get(function, url):\n+def get(function, url, isOutputlist=True):\n     try:\n         r = requests.get(url, auth=HTTPBasicAuth(config.GITHUB_USER, config.GITHUB_TOKEN))\n         return function(r.json())\n     except:\n+        if(not isOutputlist):\n+            return {}\n         return []\n \n def getContributorsList(data):\n     contributorsList = list()\n     for i in range(len(data)):\n-        # print(\"99999999999999999\", data, \"\\n\\n\\n\")\n         contributor = dict()\n-        contributor['login'] = data[i]['login']\n+        contributor['name'] = data[i]['login']\n         contributor['id'] = data[i]['id']\n         contributor['avatar_url'] = data[i]['avatar_url']\n         contributor['html_url'] = data[i]['html_url']\n         contributor['contributions'] = data[i]['contributions']\n         contributorsList.append(contributor)\n-    return contributorsList\n\\ No newline at end of file\n+    return contributorsList\n+\n+def getLanguages(data):\n+    return data\n+\n+def getCommitsList(data):\n+    commitsList = list()\n+    for i in range(len(data)):\n+        commit = dict()\n+        commit['sha'] = data[i]['sha']\n+        commit['message'] = data[i]['commit']['message']\n+        commit['date'] = data[i]['commit']['committer']['date']\n+        commit['commiter-name'] = data[i]['committer']['login']\n+        commit['commiter-id'] = data[i]['committer']['id']\n+        commitsList.append(commit)\n+    return commitsList\n+\n+def getIssuesList(data):\n+    issuesList = list()\n+    for i in range(len(data)):\n+        issue = dict()\n+        issue['id'] = data[i]['id']\n+        issue['number'] = data[i]['number']\n+        issue['title'] = data[i]['title']\n+        issue['body'] = data[i]['body']\n+        issue['state'] = data[i]['state']\n+        issue['created_at'] = data[i]['created_at']\n+        issue['updated_at'] = data[i]['updated_at']\n+        issue['closed_at'] = data[i]['closed_at']\n+        issue['labels'] = data[i]['labels']\n+        issuesList.append(issue)\n+    return issuesList\n\\ No newline at end of file"
            },
            {
                "sha": "f53fa24e39ce5b4abc2f3182231f90efbfec62a2",
                "filename": "github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoup.py",
                "status": "added",
                "additions": 2032,
                "deletions": 0,
                "changes": 2032,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoup.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,2032 @@\n+\"\"\"Beautiful Soup\n+Elixir and Tonic\n+\"The Screen-Scraper's Friend\"\n+http://www.crummy.com/software/BeautifulSoup/\n+\n+Beautiful Soup parses a (possibly invalid) XML or HTML document into a\n+tree representation. It provides methods and Pythonic idioms that make\n+it easy to navigate, search, and modify the tree.\n+\n+A well-formed XML/HTML document yields a well-formed data\n+structure. An ill-formed XML/HTML document yields a correspondingly\n+ill-formed data structure. If your document is only locally\n+well-formed, you can use this library to find and process the\n+well-formed part of it.\n+\n+Beautiful Soup works with Python 2.2 and up. It has no external\n+dependencies, but you'll have more success at converting data to UTF-8\n+if you also install these three packages:\n+\n+* chardet, for auto-detecting character encodings\n+  http://chardet.feedparser.org/\n+* cjkcodecs and iconv_codec, which add more encodings to the ones supported\n+  by stock Python.\n+  http://cjkpython.i18n.org/\n+\n+Beautiful Soup defines classes for two main parsing strategies:\n+\n+ * BeautifulStoneSoup, for parsing XML, SGML, or your domain-specific\n+   language that kind of looks like XML.\n+\n+ * BeautifulSoup, for parsing run-of-the-mill HTML code, be it valid\n+   or invalid. This class has web browser-like heuristics for\n+   obtaining a sensible parse tree in the face of common HTML errors.\n+\n+Beautiful Soup also defines a class (UnicodeDammit) for autodetecting\n+the encoding of an HTML or XML document, and converting it to\n+Unicode. Much of this code is taken from Mark Pilgrim's Universal Feed Parser.\n+\n+For more than you ever wanted to know about Beautiful Soup, see the\n+documentation:\n+http://www.crummy.com/software/BeautifulSoup/documentation.html\n+\n+Here, have some legalese:\n+\n+Copyright (c) 2004-2019, Leonard Richardson\n+\n+All rights reserved.\n+\n+Redistribution and use in source and binary forms, with or without\n+modification, are permitted provided that the following conditions are\n+met:\n+\n+  * Redistributions of source code must retain the above copyright\n+    notice, this list of conditions and the following disclaimer.\n+\n+  * Redistributions in binary form must reproduce the above\n+    copyright notice, this list of conditions and the following\n+    disclaimer in the documentation and/or other materials provided\n+    with the distribution.\n+\n+  * Neither the name of the the Beautiful Soup Consortium and All\n+    Night Kosher Bakery nor the names of its contributors may be\n+    used to endorse or promote products derived from this software\n+    without specific prior written permission.\n+\n+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n+\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n+CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n+EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n+PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n+PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n+LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n+NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE, DAMMIT.\n+\n+\"\"\"\n+from __future__ import generators\n+\n+__author__ = \"Leonard Richardson (leonardr@segfault.org)\"\n+__version__ = \"3.2.2\"\n+__copyright__ = \"Copyright (c) 2004-2019 Leonard Richardson\"\n+__license__ = \"New-style BSD\"\n+\n+from sgmllib import SGMLParser, SGMLParseError\n+import codecs\n+import markupbase\n+import types\n+import re\n+import sgmllib\n+try:\n+  from htmlentitydefs import name2codepoint\n+except ImportError:\n+  name2codepoint = {}\n+try:\n+    set\n+except NameError:\n+    from sets import Set as set\n+\n+# The very first thing we do is give a useful error if someone is\n+# running this code under Python 3.\n+\"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"<>\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n+\n+# Everyone else gets a deprecation warning.\n+import warnings\n+warnings.warn(\"\"\"You are using a very old release of Beautiful Soup, last updated in 2011. If you installed the 'beautifulsoup' package through pip, you should know the 'beautifulsoup' package name is about to be reclaimed by a more recent version of Beautiful Soup which is incompatible with this version.\n+\n+This will happen at some point after January 1, 2021.\n+\n+If you just started this project, this is easy to fix. Install the 'beautifulsoup4' package instead of 'beautifulsoup' and start using Beautiful Soup 4.\n+\n+If this is an existing project that depends on Beautiful Soup 3, the project maintainer (potentially you) needs to start the process of migrating to Beautiful Soup 4. This should be a relatively easy part of the Python 3 migration.\n+\"\"\")\n+\n+#These hacks make Beautiful Soup able to parse XML with namespaces\n+sgmllib.tagfind = re.compile('[a-zA-Z][-_.:a-zA-Z0-9]*')\n+markupbase._declname_match = re.compile(r'[a-zA-Z][-_.:a-zA-Z0-9]*\\s*').match\n+\n+DEFAULT_OUTPUT_ENCODING = \"utf-8\"\n+\n+def _match_css_class(str):\n+    \"\"\"Build a RE to match the given CSS class.\"\"\"\n+    return re.compile(r\"(^|.*\\s)%s($|\\s)\" % str)\n+\n+# First, the classes that represent markup elements.\n+\n+class PageElement(object):\n+    \"\"\"Contains the navigational information for some part of the page\n+    (either a tag or a piece of text)\"\"\"\n+\n+    def _invert(h):\n+        \"Cheap function to invert a hash.\"\n+        i = {}\n+        for k,v in h.items():\n+            i[v] = k\n+        return i\n+\n+    XML_ENTITIES_TO_SPECIAL_CHARS = { \"apos\" : \"'\",\n+                                      \"quot\" : '\"',\n+                                      \"amp\" : \"&\",\n+                                      \"lt\" : \"<\",\n+                                      \"gt\" : \">\" }\n+\n+    XML_SPECIAL_CHARS_TO_ENTITIES = _invert(XML_ENTITIES_TO_SPECIAL_CHARS)\n+\n+    def setup(self, parent=None, previous=None):\n+        \"\"\"Sets up the initial relations between this element and\n+        other elements.\"\"\"\n+        self.parent = parent\n+        self.previous = previous\n+        self.next = None\n+        self.previousSibling = None\n+        self.nextSibling = None\n+        if self.parent and self.parent.contents:\n+            self.previousSibling = self.parent.contents[-1]\n+            self.previousSibling.nextSibling = self\n+\n+    def replaceWith(self, replaceWith):\n+        oldParent = self.parent\n+        myIndex = self.parent.index(self)\n+        if hasattr(replaceWith, \"parent\")\\\n+                  and replaceWith.parent is self.parent:\n+            # We're replacing this element with one of its siblings.\n+            index = replaceWith.parent.index(replaceWith)\n+            if index and index < myIndex:\n+                # Furthermore, it comes before this element. That\n+                # means that when we extract it, the index of this\n+                # element will change.\n+                myIndex = myIndex - 1\n+        self.extract()\n+        oldParent.insert(myIndex, replaceWith)\n+\n+    def replaceWithChildren(self):\n+        myParent = self.parent\n+        myIndex = self.parent.index(self)\n+        self.extract()\n+        reversedChildren = list(self.contents)\n+        reversedChildren.reverse()\n+        for child in reversedChildren:\n+            myParent.insert(myIndex, child)\n+\n+    def extract(self):\n+        \"\"\"Destructively rips this element out of the tree.\"\"\"\n+        if self.parent:\n+            try:\n+                del self.parent.contents[self.parent.index(self)]\n+            except ValueError:\n+                pass\n+\n+        #Find the two elements that would be next to each other if\n+        #this element (and any children) hadn't been parsed. Connect\n+        #the two.\n+        lastChild = self._lastRecursiveChild()\n+        nextElement = lastChild.next\n+\n+        if self.previous:\n+            self.previous.next = nextElement\n+        if nextElement:\n+            nextElement.previous = self.previous\n+        self.previous = None\n+        lastChild.next = None\n+\n+        self.parent = None\n+        if self.previousSibling:\n+            self.previousSibling.nextSibling = self.nextSibling\n+        if self.nextSibling:\n+            self.nextSibling.previousSibling = self.previousSibling\n+        self.previousSibling = self.nextSibling = None\n+        return self\n+\n+    def _lastRecursiveChild(self):\n+        \"Finds the last element beneath this object to be parsed.\"\n+        lastChild = self\n+        while hasattr(lastChild, 'contents') and lastChild.contents:\n+            lastChild = lastChild.contents[-1]\n+        return lastChild\n+\n+    def insert(self, position, newChild):\n+        if isinstance(newChild, basestring) \\\n+            and not isinstance(newChild, NavigableString):\n+            newChild = NavigableString(newChild)\n+\n+        position =  min(position, len(self.contents))\n+        if hasattr(newChild, 'parent') and newChild.parent is not None:\n+            # We're 'inserting' an element that's already one\n+            # of this object's children.\n+            if newChild.parent is self:\n+                index = self.index(newChild)\n+                if index > position:\n+                    # Furthermore we're moving it further down the\n+                    # list of this object's children. That means that\n+                    # when we extract this element, our target index\n+                    # will jump down one.\n+                    position = position - 1\n+            newChild.extract()\n+\n+        newChild.parent = self\n+        previousChild = None\n+        if position == 0:\n+            newChild.previousSibling = None\n+            newChild.previous = self\n+        else:\n+            previousChild = self.contents[position-1]\n+            newChild.previousSibling = previousChild\n+            newChild.previousSibling.nextSibling = newChild\n+            newChild.previous = previousChild._lastRecursiveChild()\n+        if newChild.previous:\n+            newChild.previous.next = newChild\n+\n+        newChildsLastElement = newChild._lastRecursiveChild()\n+\n+        if position >= len(self.contents):\n+            newChild.nextSibling = None\n+\n+            parent = self\n+            parentsNextSibling = None\n+            while not parentsNextSibling:\n+                parentsNextSibling = parent.nextSibling\n+                parent = parent.parent\n+                if not parent: # This is the last element in the document.\n+                    break\n+            if parentsNextSibling:\n+                newChildsLastElement.next = parentsNextSibling\n+            else:\n+                newChildsLastElement.next = None\n+        else:\n+            nextChild = self.contents[position]\n+            newChild.nextSibling = nextChild\n+            if newChild.nextSibling:\n+                newChild.nextSibling.previousSibling = newChild\n+            newChildsLastElement.next = nextChild\n+\n+        if newChildsLastElement.next:\n+            newChildsLastElement.next.previous = newChildsLastElement\n+        self.contents.insert(position, newChild)\n+\n+    def append(self, tag):\n+        \"\"\"Appends the given tag to the contents of this tag.\"\"\"\n+        self.insert(len(self.contents), tag)\n+\n+    def findNext(self, name=None, attrs={}, text=None, **kwargs):\n+        \"\"\"Returns the first item that matches the given criteria and\n+        appears after this Tag in the document.\"\"\"\n+        return self._findOne(self.findAllNext, name, attrs, text, **kwargs)\n+\n+    def findAllNext(self, name=None, attrs={}, text=None, limit=None,\n+                    **kwargs):\n+        \"\"\"Returns all items that match the given criteria and appear\n+        after this Tag in the document.\"\"\"\n+        return self._findAll(name, attrs, text, limit, self.nextGenerator,\n+                             **kwargs)\n+\n+    def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n+        \"\"\"Returns the closest sibling to this Tag that matches the\n+        given criteria and appears after this Tag in the document.\"\"\"\n+        return self._findOne(self.findNextSiblings, name, attrs, text,\n+                             **kwargs)\n+\n+    def findNextSiblings(self, name=None, attrs={}, text=None, limit=None,\n+                         **kwargs):\n+        \"\"\"Returns the siblings of this Tag that match the given\n+        criteria and appear after this Tag in the document.\"\"\"\n+        return self._findAll(name, attrs, text, limit,\n+                             self.nextSiblingGenerator, **kwargs)\n+    fetchNextSiblings = findNextSiblings # Compatibility with pre-3.x\n+\n+    def findPrevious(self, name=None, attrs={}, text=None, **kwargs):\n+        \"\"\"Returns the first item that matches the given criteria and\n+        appears before this Tag in the document.\"\"\"\n+        return self._findOne(self.findAllPrevious, name, attrs, text, **kwargs)\n+\n+    def findAllPrevious(self, name=None, attrs={}, text=None, limit=None,\n+                        **kwargs):\n+        \"\"\"Returns all items that match the given criteria and appear\n+        before this Tag in the document.\"\"\"\n+        return self._findAll(name, attrs, text, limit, self.previousGenerator,\n+                           **kwargs)\n+    fetchPrevious = findAllPrevious # Compatibility with pre-3.x\n+\n+    def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):\n+        \"\"\"Returns the closest sibling to this Tag that matches the\n+        given criteria and appears before this Tag in the document.\"\"\"\n+        return self._findOne(self.findPreviousSiblings, name, attrs, text,\n+                             **kwargs)\n+\n+    def findPreviousSiblings(self, name=None, attrs={}, text=None,\n+                             limit=None, **kwargs):\n+        \"\"\"Returns the siblings of this Tag that match the given\n+        criteria and appear before this Tag in the document.\"\"\"\n+        return self._findAll(name, attrs, text, limit,\n+                             self.previousSiblingGenerator, **kwargs)\n+    fetchPreviousSiblings = findPreviousSiblings # Compatibility with pre-3.x\n+\n+    def findParent(self, name=None, attrs={}, **kwargs):\n+        \"\"\"Returns the closest parent of this Tag that matches the given\n+        criteria.\"\"\"\n+        # NOTE: We can't use _findOne because findParents takes a different\n+        # set of arguments.\n+        r = None\n+        l = self.findParents(name, attrs, 1)\n+        if l:\n+            r = l[0]\n+        return r\n+\n+    def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n+        \"\"\"Returns the parents of this Tag that match the given\n+        criteria.\"\"\"\n+\n+        return self._findAll(name, attrs, None, limit, self.parentGenerator,\n+                             **kwargs)\n+    fetchParents = findParents # Compatibility with pre-3.x\n+\n+    #These methods do the real heavy lifting.\n+\n+    def _findOne(self, method, name, attrs, text, **kwargs):\n+        r = None\n+        l = method(name, attrs, text, 1, **kwargs)\n+        if l:\n+            r = l[0]\n+        return r\n+\n+    def _findAll(self, name, attrs, text, limit, generator, **kwargs):\n+        \"Iterates over a generator looking for things that match.\"\n+\n+        if isinstance(name, SoupStrainer):\n+            strainer = name\n+        # (Possibly) special case some findAll*(...) searches\n+        elif text is None and not limit and not attrs and not kwargs:\n+            # findAll*(True)\n+            if name is True:\n+                return [element for element in generator()\n+                        if isinstance(element, Tag)]\n+            # findAll*('tag-name')\n+            elif isinstance(name, basestring):\n+                return [element for element in generator()\n+                        if isinstance(element, Tag) and\n+                        element.name == name]\n+            else:\n+                strainer = SoupStrainer(name, attrs, text, **kwargs)\n+        # Build a SoupStrainer\n+        else:\n+            strainer = SoupStrainer(name, attrs, text, **kwargs)\n+        results = ResultSet(strainer)\n+        g = generator()\n+        while True:\n+            try:\n+                i = g.next()\n+            except StopIteration:\n+                break\n+            if i:\n+                found = strainer.search(i)\n+                if found:\n+                    results.append(found)\n+                    if limit and len(results) >= limit:\n+                        break\n+        return results\n+\n+    #These Generators can be used to navigate starting from both\n+    #NavigableStrings and Tags.\n+    def nextGenerator(self):\n+        i = self\n+        while i is not None:\n+            i = i.next\n+            yield i\n+\n+    def nextSiblingGenerator(self):\n+        i = self\n+        while i is not None:\n+            i = i.nextSibling\n+            yield i\n+\n+    def previousGenerator(self):\n+        i = self\n+        while i is not None:\n+            i = i.previous\n+            yield i\n+\n+    def previousSiblingGenerator(self):\n+        i = self\n+        while i is not None:\n+            i = i.previousSibling\n+            yield i\n+\n+    def parentGenerator(self):\n+        i = self\n+        while i is not None:\n+            i = i.parent\n+            yield i\n+\n+    # Utility methods\n+    def substituteEncoding(self, str, encoding=None):\n+        encoding = encoding or \"utf-8\"\n+        return str.replace(\"%SOUP-ENCODING%\", encoding)\n+\n+    def toEncoding(self, s, encoding=None):\n+        \"\"\"Encodes an object to a string in some encoding, or to Unicode.\n+        .\"\"\"\n+        if isinstance(s, unicode):\n+            if encoding:\n+                s = s.encode(encoding)\n+        elif isinstance(s, str):\n+            if encoding:\n+                s = s.encode(encoding)\n+            else:\n+                s = unicode(s)\n+        else:\n+            if encoding:\n+                s  = self.toEncoding(str(s), encoding)\n+            else:\n+                s = unicode(s)\n+        return s\n+\n+    BARE_AMPERSAND_OR_BRACKET = re.compile(\"([<>]|\"\n+                                           + \"&(?!#\\d+;|#x[0-9a-fA-F]+;|\\w+;)\"\n+                                           + \")\")\n+\n+    def _sub_entity(self, x):\n+        \"\"\"Used with a regular expression to substitute the\n+        appropriate XML entity for an XML special character.\"\"\"\n+        return \"&\" + self.XML_SPECIAL_CHARS_TO_ENTITIES[x.group(0)[0]] + \";\"\n+\n+\n+class NavigableString(unicode, PageElement):\n+\n+    def __new__(cls, value):\n+        \"\"\"Create a new NavigableString.\n+\n+        When unpickling a NavigableString, this method is called with\n+        the string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be\n+        passed in to the superclass's __new__ or the superclass won't know\n+        how to handle non-ASCII characters.\n+        \"\"\"\n+        if isinstance(value, unicode):\n+            return unicode.__new__(cls, value)\n+        return unicode.__new__(cls, value, DEFAULT_OUTPUT_ENCODING)\n+\n+    def __getnewargs__(self):\n+        return (NavigableString.__str__(self),)\n+\n+    def __getattr__(self, attr):\n+        \"\"\"text.string gives you text. This is for backwards\n+        compatibility for Navigable*String, but for CData* it lets you\n+        get the string without the CData wrapper.\"\"\"\n+        if attr == 'string':\n+            return self\n+        else:\n+            raise AttributeError, \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, attr)\n+\n+    def __unicode__(self):\n+        return str(self).decode(DEFAULT_OUTPUT_ENCODING)\n+\n+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        # Substitute outgoing XML entities.\n+        data = self.BARE_AMPERSAND_OR_BRACKET.sub(self._sub_entity, self)\n+        if encoding:\n+            return data.encode(encoding)\n+        else:\n+            return data\n+\n+class CData(NavigableString):\n+\n+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        return \"<![CDATA[%s]]>\" % NavigableString.__str__(self, encoding)\n+\n+class ProcessingInstruction(NavigableString):\n+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        output = self\n+        if \"%SOUP-ENCODING%\" in output:\n+            output = self.substituteEncoding(output, encoding)\n+        return \"<?%s?>\" % self.toEncoding(output, encoding)\n+\n+class Comment(NavigableString):\n+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        return \"<!--%s-->\" % NavigableString.__str__(self, encoding)\n+\n+class Declaration(NavigableString):\n+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        return \"<!%s>\" % NavigableString.__str__(self, encoding)\n+\n+class Tag(PageElement):\n+\n+    \"\"\"Represents a found HTML tag with its attributes and contents.\"\"\"\n+\n+    def _convertEntities(self, match):\n+        \"\"\"Used in a call to re.sub to replace HTML, XML, and numeric\n+        entities with the appropriate Unicode characters. If HTML\n+        entities are being converted, any unrecognized entities are\n+        escaped.\"\"\"\n+        x = match.group(1)\n+        if self.convertHTMLEntities and x in name2codepoint:\n+            return unichr(name2codepoint[x])\n+        elif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:\n+            if self.convertXMLEntities:\n+                return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]\n+            else:\n+                return u'&%s;' % x\n+        elif len(x) > 0 and x[0] == '#':\n+            # Handle numeric entities\n+            if len(x) > 1 and x[1] == 'x':\n+                return unichr(int(x[2:], 16))\n+            else:\n+                return unichr(int(x[1:]))\n+\n+        elif self.escapeUnrecognizedEntities:\n+            return u'&amp;%s;' % x\n+        else:\n+            return u'&%s;' % x\n+\n+    def __init__(self, parser, name, attrs=None, parent=None,\n+                 previous=None):\n+        \"Basic constructor.\"\n+\n+        # We don't actually store the parser object: that lets extracted\n+        # chunks be garbage-collected\n+        self.parserClass = parser.__class__\n+        self.isSelfClosing = parser.isSelfClosingTag(name)\n+        self.name = name\n+        if attrs is None:\n+            attrs = []\n+        elif isinstance(attrs, dict):\n+            attrs = attrs.items()\n+        self.attrs = attrs\n+        self.contents = []\n+        self.setup(parent, previous)\n+        self.hidden = False\n+        self.containsSubstitutions = False\n+        self.convertHTMLEntities = parser.convertHTMLEntities\n+        self.convertXMLEntities = parser.convertXMLEntities\n+        self.escapeUnrecognizedEntities = parser.escapeUnrecognizedEntities\n+\n+        # Convert any HTML, XML, or numeric entities in the attribute values.\n+        convert = lambda(k, val): (k,\n+                                   re.sub(\"&(#\\d+|#x[0-9a-fA-F]+|\\w+);\",\n+                                          self._convertEntities,\n+                                          val))\n+        self.attrs = map(convert, self.attrs)\n+\n+    def getString(self):\n+        if (len(self.contents) == 1\n+            and isinstance(self.contents[0], NavigableString)):\n+            return self.contents[0]\n+\n+    def setString(self, string):\n+        \"\"\"Replace the contents of the tag with a string\"\"\"\n+        self.clear()\n+        self.append(string)\n+\n+    string = property(getString, setString)\n+\n+    def getText(self, separator=u\"\"):\n+        if not len(self.contents):\n+            return u\"\"\n+        stopNode = self._lastRecursiveChild().next\n+        strings = []\n+        current = self.contents[0]\n+        while current is not stopNode:\n+            if isinstance(current, NavigableString):\n+                strings.append(current.strip())\n+            current = current.next\n+        return separator.join(strings)\n+\n+    text = property(getText)\n+\n+    def get(self, key, default=None):\n+        \"\"\"Returns the value of the 'key' attribute for the tag, or\n+        the value given for 'default' if it doesn't have that\n+        attribute.\"\"\"\n+        return self._getAttrMap().get(key, default)\n+\n+    def clear(self):\n+        \"\"\"Extract all children.\"\"\"\n+        for child in self.contents[:]:\n+            child.extract()\n+\n+    def index(self, element):\n+        for i, child in enumerate(self.contents):\n+            if child is element:\n+                return i\n+        raise ValueError(\"Tag.index: element not in tag\")\n+\n+    def has_key(self, key):\n+        return self._getAttrMap().has_key(key)\n+\n+    def __getitem__(self, key):\n+        \"\"\"tag[key] returns the value of the 'key' attribute for the tag,\n+        and throws an exception if it's not there.\"\"\"\n+        return self._getAttrMap()[key]\n+\n+    def __iter__(self):\n+        \"Iterating over a tag iterates over its contents.\"\n+        return iter(self.contents)\n+\n+    def __len__(self):\n+        \"The length of a tag is the length of its list of contents.\"\n+        return len(self.contents)\n+\n+    def __contains__(self, x):\n+        return x in self.contents\n+\n+    def __nonzero__(self):\n+        \"A tag is non-None even if it has no contents.\"\n+        return True\n+\n+    def __setitem__(self, key, value):\n+        \"\"\"Setting tag[key] sets the value of the 'key' attribute for the\n+        tag.\"\"\"\n+        self._getAttrMap()\n+        self.attrMap[key] = value\n+        found = False\n+        for i in range(0, len(self.attrs)):\n+            if self.attrs[i][0] == key:\n+                self.attrs[i] = (key, value)\n+                found = True\n+        if not found:\n+            self.attrs.append((key, value))\n+        self._getAttrMap()[key] = value\n+\n+    def __delitem__(self, key):\n+        \"Deleting tag[key] deletes all 'key' attributes for the tag.\"\n+        for item in self.attrs:\n+            if item[0] == key:\n+                self.attrs.remove(item)\n+                #We don't break because bad HTML can define the same\n+                #attribute multiple times.\n+            self._getAttrMap()\n+            if self.attrMap.has_key(key):\n+                del self.attrMap[key]\n+\n+    def __call__(self, *args, **kwargs):\n+        \"\"\"Calling a tag like a function is the same as calling its\n+        findAll() method. Eg. tag('a') returns a list of all the A tags\n+        found within this tag.\"\"\"\n+        return apply(self.findAll, args, kwargs)\n+\n+    def __getattr__(self, tag):\n+        #print \"Getattr %s.%s\" % (self.__class__, tag)\n+        if len(tag) > 3 and tag.rfind('Tag') == len(tag)-3:\n+            return self.find(tag[:-3])\n+        elif tag.find('__') != 0:\n+            return self.find(tag)\n+        raise AttributeError, \"'%s' object has no attribute '%s'\" % (self.__class__, tag)\n+\n+    def __eq__(self, other):\n+        \"\"\"Returns true iff this tag has the same name, the same attributes,\n+        and the same contents (recursively) as the given tag.\n+\n+        NOTE: right now this will return false if two tags have the\n+        same attributes in a different order. Should this be fixed?\"\"\"\n+        if other is self:\n+            return True\n+        if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n+            return False\n+        for i in range(0, len(self.contents)):\n+            if self.contents[i] != other.contents[i]:\n+                return False\n+        return True\n+\n+    def __ne__(self, other):\n+        \"\"\"Returns true iff this tag is not identical to the other tag,\n+        as defined in __eq__.\"\"\"\n+        return not self == other\n+\n+    def __repr__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        \"\"\"Renders this tag as a string.\"\"\"\n+        return self.__str__(encoding)\n+\n+    def __unicode__(self):\n+        return self.__str__(None)\n+\n+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING,\n+                prettyPrint=False, indentLevel=0):\n+        \"\"\"Returns a string or Unicode representation of this tag and\n+        its contents. To get Unicode, pass None for encoding.\n+\n+        NOTE: since Python's HTML parser consumes whitespace, this\n+        method is not certain to reproduce the whitespace present in\n+        the original string.\"\"\"\n+\n+        encodedName = self.toEncoding(self.name, encoding)\n+\n+        attrs = []\n+        if self.attrs:\n+            for key, val in self.attrs:\n+                fmt = '%s=\"%s\"'\n+                if isinstance(val, basestring):\n+                    if self.containsSubstitutions and '%SOUP-ENCODING%' in val:\n+                        val = self.substituteEncoding(val, encoding)\n+\n+                    # The attribute value either:\n+                    #\n+                    # * Contains no embedded double quotes or single quotes.\n+                    #   No problem: we enclose it in double quotes.\n+                    # * Contains embedded single quotes. No problem:\n+                    #   double quotes work here too.\n+                    # * Contains embedded double quotes. No problem:\n+                    #   we enclose it in single quotes.\n+                    # * Embeds both single _and_ double quotes. This\n+                    #   can't happen naturally, but it can happen if\n+                    #   you modify an attribute value after parsing\n+                    #   the document. Now we have a bit of a\n+                    #   problem. We solve it by enclosing the\n+                    #   attribute in single quotes, and escaping any\n+                    #   embedded single quotes to XML entities.\n+                    if '\"' in val:\n+                        fmt = \"%s='%s'\"\n+                        if \"'\" in val:\n+                            # TODO: replace with apos when\n+                            # appropriate.\n+                            val = val.replace(\"'\", \"&squot;\")\n+\n+                    # Now we're okay w/r/t quotes. But the attribute\n+                    # value might also contain angle brackets, or\n+                    # ampersands that aren't part of entities. We need\n+                    # to escape those to XML entities too.\n+                    val = self.BARE_AMPERSAND_OR_BRACKET.sub(self._sub_entity, val)\n+\n+                attrs.append(fmt % (self.toEncoding(key, encoding),\n+                                    self.toEncoding(val, encoding)))\n+        close = ''\n+        closeTag = ''\n+        if self.isSelfClosing:\n+            close = ' /'\n+        else:\n+            closeTag = '</%s>' % encodedName\n+\n+        indentTag, indentContents = 0, 0\n+        if prettyPrint:\n+            indentTag = indentLevel\n+            space = (' ' * (indentTag-1))\n+            indentContents = indentTag + 1\n+        contents = self.renderContents(encoding, prettyPrint, indentContents)\n+        if self.hidden:\n+            s = contents\n+        else:\n+            s = []\n+            attributeString = ''\n+            if attrs:\n+                attributeString = ' ' + ' '.join(attrs)\n+            if prettyPrint:\n+                s.append(space)\n+            s.append('<%s%s%s>' % (encodedName, attributeString, close))\n+            if prettyPrint:\n+                s.append(\"\\n\")\n+            s.append(contents)\n+            if prettyPrint and contents and contents[-1] != \"\\n\":\n+                s.append(\"\\n\")\n+            if prettyPrint and closeTag:\n+                s.append(space)\n+            s.append(closeTag)\n+            if prettyPrint and closeTag and self.nextSibling:\n+                s.append(\"\\n\")\n+            s = ''.join(s)\n+        return s\n+\n+    def decompose(self):\n+        \"\"\"Recursively destroys the contents of this tree.\"\"\"\n+        self.extract()\n+        if len(self.contents) == 0:\n+            return\n+        current = self.contents[0]\n+        while current is not None:\n+            next = current.next\n+            if isinstance(current, Tag):\n+                del current.contents[:]\n+            current.parent = None\n+            current.previous = None\n+            current.previousSibling = None\n+            current.next = None\n+            current.nextSibling = None\n+            current = next\n+\n+    def prettify(self, encoding=DEFAULT_OUTPUT_ENCODING):\n+        return self.__str__(encoding, True)\n+\n+    def renderContents(self, encoding=DEFAULT_OUTPUT_ENCODING,\n+                       prettyPrint=False, indentLevel=0):\n+        \"\"\"Renders the contents of this tag as a string in the given\n+        encoding. If encoding is None, returns a Unicode string..\"\"\"\n+        s=[]\n+        for c in self:\n+            text = None\n+            if isinstance(c, NavigableString):\n+                text = c.__str__(encoding)\n+            elif isinstance(c, Tag):\n+                s.append(c.__str__(encoding, prettyPrint, indentLevel))\n+            if text and prettyPrint:\n+                text = text.strip()\n+            if text:\n+                if prettyPrint:\n+                    s.append(\" \" * (indentLevel-1))\n+                s.append(text)\n+                if prettyPrint:\n+                    s.append(\"\\n\")\n+        return ''.join(s)\n+\n+    #Soup methods\n+\n+    def find(self, name=None, attrs={}, recursive=True, text=None,\n+             **kwargs):\n+        \"\"\"Return only the first child of this Tag matching the given\n+        criteria.\"\"\"\n+        r = None\n+        l = self.findAll(name, attrs, recursive, text, 1, **kwargs)\n+        if l:\n+            r = l[0]\n+        return r\n+    findChild = find\n+\n+    def findAll(self, name=None, attrs={}, recursive=True, text=None,\n+                limit=None, **kwargs):\n+        \"\"\"Extracts a list of Tag objects that match the given\n+        criteria.  You can specify the name of the Tag and any\n+        attributes you want the Tag to have.\n+\n+        The value of a key-value pair in the 'attrs' map can be a\n+        string, a list of strings, a regular expression object, or a\n+        callable that takes a string and returns whether or not the\n+        string matches for some custom definition of 'matches'. The\n+        same is true of the tag name.\"\"\"\n+        generator = self.recursiveChildGenerator\n+        if not recursive:\n+            generator = self.childGenerator\n+        return self._findAll(name, attrs, text, limit, generator, **kwargs)\n+    findChildren = findAll\n+\n+    # Pre-3.x compatibility methods\n+    first = find\n+    fetch = findAll\n+\n+    def fetchText(self, text=None, recursive=True, limit=None):\n+        return self.findAll(text=text, recursive=recursive, limit=limit)\n+\n+    def firstText(self, text=None, recursive=True):\n+        return self.find(text=text, recursive=recursive)\n+\n+    #Private methods\n+\n+    def _getAttrMap(self):\n+        \"\"\"Initializes a map representation of this tag's attributes,\n+        if not already initialized.\"\"\"\n+        if not getattr(self, 'attrMap'):\n+            self.attrMap = {}\n+            for (key, value) in self.attrs:\n+                self.attrMap[key] = value\n+        return self.attrMap\n+\n+    #Generator methods\n+    def childGenerator(self):\n+        # Just use the iterator from the contents\n+        return iter(self.contents)\n+\n+    def recursiveChildGenerator(self):\n+        if not len(self.contents):\n+            raise StopIteration\n+        stopNode = self._lastRecursiveChild().next\n+        current = self.contents[0]\n+        while current is not stopNode:\n+            yield current\n+            current = current.next\n+\n+\n+# Next, a couple classes to represent queries and their results.\n+class SoupStrainer:\n+    \"\"\"Encapsulates a number of ways of matching a markup element (tag or\n+    text).\"\"\"\n+\n+    def __init__(self, name=None, attrs={}, text=None, **kwargs):\n+        self.name = name\n+        if isinstance(attrs, basestring):\n+            kwargs['class'] = _match_css_class(attrs)\n+            attrs = None\n+        if kwargs:\n+            if attrs:\n+                attrs = attrs.copy()\n+                attrs.update(kwargs)\n+            else:\n+                attrs = kwargs\n+        self.attrs = attrs\n+        self.text = text\n+\n+    def __str__(self):\n+        if self.text:\n+            return self.text\n+        else:\n+            return \"%s|%s\" % (self.name, self.attrs)\n+\n+    def searchTag(self, markupName=None, markupAttrs={}):\n+        found = None\n+        markup = None\n+        if isinstance(markupName, Tag):\n+            markup = markupName\n+            markupAttrs = markup\n+        callFunctionWithTagData = callable(self.name) \\\n+                                and not isinstance(markupName, Tag)\n+\n+        if (not self.name) \\\n+               or callFunctionWithTagData \\\n+               or (markup and self._matches(markup, self.name)) \\\n+               or (not markup and self._matches(markupName, self.name)):\n+            if callFunctionWithTagData:\n+                match = self.name(markupName, markupAttrs)\n+            else:\n+                match = True\n+                markupAttrMap = None\n+                for attr, matchAgainst in self.attrs.items():\n+                    if not markupAttrMap:\n+                         if hasattr(markupAttrs, 'get'):\n+                            markupAttrMap = markupAttrs\n+                         else:\n+                            markupAttrMap = {}\n+                            for k,v in markupAttrs:\n+                                markupAttrMap[k] = v\n+                    attrValue = markupAttrMap.get(attr)\n+                    if not self._matches(attrValue, matchAgainst):\n+                        match = False\n+                        break\n+            if match:\n+                if markup:\n+                    found = markup\n+                else:\n+                    found = markupName\n+        return found\n+\n+    def search(self, markup):\n+        #print 'looking for %s in %s' % (self, markup)\n+        found = None\n+        # If given a list of items, scan it for a text element that\n+        # matches.\n+        if hasattr(markup, \"__iter__\") \\\n+                and not isinstance(markup, Tag):\n+            for element in markup:\n+                if isinstance(element, NavigableString) \\\n+                       and self.search(element):\n+                    found = element\n+                    break\n+        # If it's a Tag, make sure its name or attributes match.\n+        # Don't bother with Tags if we're searching for text.\n+        elif isinstance(markup, Tag):\n+            if not self.text:\n+                found = self.searchTag(markup)\n+        # If it's text, make sure the text matches.\n+        elif isinstance(markup, NavigableString) or \\\n+                 isinstance(markup, basestring):\n+            if self._matches(markup, self.text):\n+                found = markup\n+        else:\n+            raise Exception, \"I don't know how to match against a %s\" \\\n+                  % markup.__class__\n+        return found\n+\n+    def _matches(self, markup, matchAgainst):\n+        #print \"Matching %s against %s\" % (markup, matchAgainst)\n+        result = False\n+        if matchAgainst is True:\n+            result = markup is not None\n+        elif callable(matchAgainst):\n+            result = matchAgainst(markup)\n+        else:\n+            #Custom match methods take the tag as an argument, but all\n+            #other ways of matching match the tag name as a string.\n+            if isinstance(markup, Tag):\n+                markup = markup.name\n+            if markup and not isinstance(markup, basestring):\n+                markup = unicode(markup)\n+            #Now we know that chunk is either a string, or None.\n+            if hasattr(matchAgainst, 'match'):\n+                # It's a regexp object.\n+                result = markup and matchAgainst.search(markup)\n+            elif hasattr(matchAgainst, '__iter__'): # list-like\n+                result = markup in matchAgainst\n+            elif hasattr(matchAgainst, 'items'):\n+                result = markup.has_key(matchAgainst)\n+            elif matchAgainst and isinstance(markup, basestring):\n+                if isinstance(markup, unicode):\n+                    matchAgainst = unicode(matchAgainst)\n+                else:\n+                    matchAgainst = str(matchAgainst)\n+\n+            if not result:\n+                result = matchAgainst == markup\n+        return result\n+\n+class ResultSet(list):\n+    \"\"\"A ResultSet is just a list that keeps track of the SoupStrainer\n+    that created it.\"\"\"\n+    def __init__(self, source):\n+        list.__init__([])\n+        self.source = source\n+\n+# Now, some helper functions.\n+\n+def buildTagMap(default, *args):\n+    \"\"\"Turns a list of maps, lists, or scalars into a single map.\n+    Used to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\n+    NESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n+    built = {}\n+    for portion in args:\n+        if hasattr(portion, 'items'):\n+            #It's a map. Merge it.\n+            for k,v in portion.items():\n+                built[k] = v\n+        elif hasattr(portion, '__iter__'): # is a list\n+            #It's a list. Map each item to the default.\n+            for k in portion:\n+                built[k] = default\n+        else:\n+            #It's a scalar. Map it to the default.\n+            built[portion] = default\n+    return built\n+\n+# Now, the parser classes.\n+\n+class BeautifulStoneSoup(Tag, SGMLParser):\n+\n+    \"\"\"This class contains the basic parser and search code. It defines\n+    a parser that knows nothing about tag behavior except for the\n+    following:\n+\n+      You can't close a tag without closing all the tags it encloses.\n+      That is, \"<foo><bar></foo>\" actually means\n+      \"<foo><bar></bar></foo>\".\n+\n+    [Another possible explanation is \"<foo><bar /></foo>\", but since\n+    this class defines no SELF_CLOSING_TAGS, it will never use that\n+    explanation.]\n+\n+    This class is useful for parsing XML or made-up markup languages,\n+    or when BeautifulSoup makes an assumption counter to what you were\n+    expecting.\"\"\"\n+\n+    SELF_CLOSING_TAGS = {}\n+    NESTABLE_TAGS = {}\n+    RESET_NESTING_TAGS = {}\n+    QUOTE_TAGS = {}\n+    PRESERVE_WHITESPACE_TAGS = []\n+\n+    MARKUP_MASSAGE = [(re.compile('(<[^<>]*)/>'),\n+                       lambda x: x.group(1) + ' />'),\n+                      (re.compile('<!\\s+([^<>]*)>'),\n+                       lambda x: '<!' + x.group(1) + '>')\n+                      ]\n+\n+    ROOT_TAG_NAME = u'[document]'\n+\n+    HTML_ENTITIES = \"html\"\n+    XML_ENTITIES = \"xml\"\n+    XHTML_ENTITIES = \"xhtml\"\n+    # TODO: This only exists for backwards-compatibility\n+    ALL_ENTITIES = XHTML_ENTITIES\n+\n+    # Used when determining whether a text node is all whitespace and\n+    # can be replaced with a single space. A text node that contains\n+    # fancy Unicode spaces (usually non-breaking) should be left\n+    # alone.\n+    STRIP_ASCII_SPACES = { 9: None, 10: None, 12: None, 13: None, 32: None, }\n+\n+    def __init__(self, markup=\"\", parseOnlyThese=None, fromEncoding=None,\n+                 markupMassage=True, smartQuotesTo=XML_ENTITIES,\n+                 convertEntities=None, selfClosingTags=None, isHTML=False):\n+        \"\"\"The Soup object is initialized as the 'root tag', and the\n+        provided markup (which can be a string or a file-like object)\n+        is fed into the underlying parser.\n+\n+        sgmllib will process most bad HTML, and the BeautifulSoup\n+        class has some tricks for dealing with some HTML that kills\n+        sgmllib, but Beautiful Soup can nonetheless choke or lose data\n+        if your data uses self-closing tags or declarations\n+        incorrectly.\n+\n+        By default, Beautiful Soup uses regexes to sanitize input,\n+        avoiding the vast majority of these problems. If the problems\n+        don't apply to you, pass in False for markupMassage, and\n+        you'll get better performance.\n+\n+        The default parser massage techniques fix the two most common\n+        instances of invalid HTML that choke sgmllib:\n+\n+         <br/> (No space between name of closing tag and tag close)\n+         <! --Comment--> (Extraneous whitespace in declaration)\n+\n+        You can pass in a custom list of (RE object, replace method)\n+        tuples to get Beautiful Soup to scrub your input the way you\n+        want.\"\"\"\n+\n+        self.parseOnlyThese = parseOnlyThese\n+        self.fromEncoding = fromEncoding\n+        self.smartQuotesTo = smartQuotesTo\n+        self.convertEntities = convertEntities\n+        # Set the rules for how we'll deal with the entities we\n+        # encounter\n+        if self.convertEntities:\n+            # It doesn't make sense to convert encoded characters to\n+            # entities even while you're converting entities to Unicode.\n+            # Just convert it all to Unicode.\n+            self.smartQuotesTo = None\n+            if convertEntities == self.HTML_ENTITIES:\n+                self.convertXMLEntities = False\n+                self.convertHTMLEntities = True\n+                self.escapeUnrecognizedEntities = True\n+            elif convertEntities == self.XHTML_ENTITIES:\n+                self.convertXMLEntities = True\n+                self.convertHTMLEntities = True\n+                self.escapeUnrecognizedEntities = False\n+            elif convertEntities == self.XML_ENTITIES:\n+                self.convertXMLEntities = True\n+                self.convertHTMLEntities = False\n+                self.escapeUnrecognizedEntities = False\n+        else:\n+            self.convertXMLEntities = False\n+            self.convertHTMLEntities = False\n+            self.escapeUnrecognizedEntities = False\n+\n+        self.instanceSelfClosingTags = buildTagMap(None, selfClosingTags)\n+        SGMLParser.__init__(self)\n+\n+        if hasattr(markup, 'read'):        # It's a file-type object.\n+            markup = markup.read()\n+        self.markup = markup\n+        self.markupMassage = markupMassage\n+        try:\n+            self._feed(isHTML=isHTML)\n+        except StopParsing:\n+            pass\n+        self.markup = None                 # The markup can now be GCed\n+\n+    def convert_charref(self, name):\n+        \"\"\"This method fixes a bug in Python's SGMLParser.\"\"\"\n+        try:\n+            n = int(name)\n+        except ValueError:\n+            return\n+        if not 0 <= n <= 127 : # ASCII ends at 127, not 255\n+            return\n+        return self.convert_codepoint(n)\n+\n+    def _feed(self, inDocumentEncoding=None, isHTML=False):\n+        # Convert the document to Unicode.\n+        markup = self.markup\n+        if isinstance(markup, unicode):\n+            if not hasattr(self, 'originalEncoding'):\n+                self.originalEncoding = None\n+        else:\n+            dammit = UnicodeDammit\\\n+                     (markup, [self.fromEncoding, inDocumentEncoding],\n+                      smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)\n+            markup = dammit.unicode\n+            self.originalEncoding = dammit.originalEncoding\n+            self.declaredHTMLEncoding = dammit.declaredHTMLEncoding\n+        if markup:\n+            if self.markupMassage:\n+                if not hasattr(self.markupMassage, \"__iter__\"):\n+                    self.markupMassage = self.MARKUP_MASSAGE\n+                for fix, m in self.markupMassage:\n+                    markup = fix.sub(m, markup)\n+                # TODO: We get rid of markupMassage so that the\n+                # soup object can be deepcopied later on. Some\n+                # Python installations can't copy regexes. If anyone\n+                # was relying on the existence of markupMassage, this\n+                # might cause problems.\n+                del(self.markupMassage)\n+        self.reset()\n+\n+        SGMLParser.feed(self, markup)\n+        # Close out any unfinished strings and close all the open tags.\n+        self.endData()\n+        while self.currentTag.name != self.ROOT_TAG_NAME:\n+            self.popTag()\n+\n+    def __getattr__(self, methodName):\n+        \"\"\"This method routes method call requests to either the SGMLParser\n+        superclass or the Tag superclass, depending on the method name.\"\"\"\n+        #print \"__getattr__ called on %s.%s\" % (self.__class__, methodName)\n+\n+        if methodName.startswith('start_') or methodName.startswith('end_') \\\n+               or methodName.startswith('do_'):\n+            return SGMLParser.__getattr__(self, methodName)\n+        elif not methodName.startswith('__'):\n+            return Tag.__getattr__(self, methodName)\n+        else:\n+            raise AttributeError\n+\n+    def isSelfClosingTag(self, name):\n+        \"\"\"Returns true iff the given string is the name of a\n+        self-closing tag according to this parser.\"\"\"\n+        return self.SELF_CLOSING_TAGS.has_key(name) \\\n+               or self.instanceSelfClosingTags.has_key(name)\n+\n+    def reset(self):\n+        Tag.__init__(self, self, self.ROOT_TAG_NAME)\n+        self.hidden = 1\n+        SGMLParser.reset(self)\n+        self.currentData = []\n+        self.currentTag = None\n+        self.tagStack = []\n+        self.quoteStack = []\n+        self.pushTag(self)\n+\n+    def popTag(self):\n+        tag = self.tagStack.pop()\n+\n+        #print \"Pop\", tag.name\n+        if self.tagStack:\n+            self.currentTag = self.tagStack[-1]\n+        return self.currentTag\n+\n+    def pushTag(self, tag):\n+        #print \"Push\", tag.name\n+        if self.currentTag:\n+            self.currentTag.contents.append(tag)\n+        self.tagStack.append(tag)\n+        self.currentTag = self.tagStack[-1]\n+\n+    def endData(self, containerClass=NavigableString):\n+        if self.currentData:\n+            currentData = u''.join(self.currentData)\n+            if (currentData.translate(self.STRIP_ASCII_SPACES) == '' and\n+                not set([tag.name for tag in self.tagStack]).intersection(\n+                    self.PRESERVE_WHITESPACE_TAGS)):\n+                if '\\n' in currentData:\n+                    currentData = '\\n'\n+                else:\n+                    currentData = ' '\n+            self.currentData = []\n+            if self.parseOnlyThese and len(self.tagStack) <= 1 and \\\n+                   (not self.parseOnlyThese.text or \\\n+                    not self.parseOnlyThese.search(currentData)):\n+                return\n+            o = containerClass(currentData)\n+            o.setup(self.currentTag, self.previous)\n+            if self.previous:\n+                self.previous.next = o\n+            self.previous = o\n+            self.currentTag.contents.append(o)\n+\n+\n+    def _popToTag(self, name, inclusivePop=True):\n+        \"\"\"Pops the tag stack up to and including the most recent\n+        instance of the given tag. If inclusivePop is false, pops the tag\n+        stack up to but *not* including the most recent instqance of\n+        the given tag.\"\"\"\n+        #print \"Popping to %s\" % name\n+        if name == self.ROOT_TAG_NAME:\n+            return\n+\n+        numPops = 0\n+        mostRecentTag = None\n+        for i in range(len(self.tagStack)-1, 0, -1):\n+            if name == self.tagStack[i].name:\n+                numPops = len(self.tagStack)-i\n+                break\n+        if not inclusivePop:\n+            numPops = numPops - 1\n+\n+        for i in range(0, numPops):\n+            mostRecentTag = self.popTag()\n+        return mostRecentTag\n+\n+    def _smartPop(self, name):\n+\n+        \"\"\"We need to pop up to the previous tag of this type, unless\n+        one of this tag's nesting reset triggers comes between this\n+        tag and the previous tag of this type, OR unless this tag is a\n+        generic nesting trigger and another generic nesting trigger\n+        comes between this tag and the previous tag of this type.\n+\n+        Examples:\n+         <p>Foo<b>Bar *<p>* should pop to 'p', not 'b'.\n+         <p>Foo<table>Bar *<p>* should pop to 'table', not 'p'.\n+         <p>Foo<table><tr>Bar *<p>* should pop to 'tr', not 'p'.\n+\n+         <li><ul><li> *<li>* should pop to 'ul', not the first 'li'.\n+         <tr><table><tr> *<tr>* should pop to 'table', not the first 'tr'\n+         <td><tr><td> *<td>* should pop to 'tr', not the first 'td'\n+        \"\"\"\n+\n+        nestingResetTriggers = self.NESTABLE_TAGS.get(name)\n+        isNestable = nestingResetTriggers != None\n+        isResetNesting = self.RESET_NESTING_TAGS.has_key(name)\n+        popTo = None\n+        inclusive = True\n+        for i in range(len(self.tagStack)-1, 0, -1):\n+            p = self.tagStack[i]\n+            if (not p or p.name == name) and not isNestable:\n+                #Non-nestable tags get popped to the top or to their\n+                #last occurance.\n+                popTo = name\n+                break\n+            if (nestingResetTriggers is not None\n+                and p.name in nestingResetTriggers) \\\n+                or (nestingResetTriggers is None and isResetNesting\n+                    and self.RESET_NESTING_TAGS.has_key(p.name)):\n+\n+                #If we encounter one of the nesting reset triggers\n+                #peculiar to this tag, or we encounter another tag\n+                #that causes nesting to reset, pop up to but not\n+                #including that tag.\n+                popTo = p.name\n+                inclusive = False\n+                break\n+            p = p.parent\n+        if popTo:\n+            self._popToTag(popTo, inclusive)\n+\n+    def unknown_starttag(self, name, attrs, selfClosing=0):\n+        #print \"Start tag %s: %s\" % (name, attrs)\n+        if self.quoteStack:\n+            #This is not a real tag.\n+            #print \"<%s> is not real!\" % name\n+            attrs = ''.join([' %s=\"%s\"' % (x, y) for x, y in attrs])\n+            self.handle_data('<%s%s>' % (name, attrs))\n+            return\n+        self.endData()\n+\n+        if not self.isSelfClosingTag(name) and not selfClosing:\n+            self._smartPop(name)\n+\n+        if self.parseOnlyThese and len(self.tagStack) <= 1 \\\n+               and (self.parseOnlyThese.text or not self.parseOnlyThese.searchTag(name, attrs)):\n+            return\n+\n+        tag = Tag(self, name, attrs, self.currentTag, self.previous)\n+        if self.previous:\n+            self.previous.next = tag\n+        self.previous = tag\n+        self.pushTag(tag)\n+        if selfClosing or self.isSelfClosingTag(name):\n+            self.popTag()\n+        if name in self.QUOTE_TAGS:\n+            #print \"Beginning quote (%s)\" % name\n+            self.quoteStack.append(name)\n+            self.literal = 1\n+        return tag\n+\n+    def unknown_endtag(self, name):\n+        #print \"End tag %s\" % name\n+        if self.quoteStack and self.quoteStack[-1] != name:\n+            #This is not a real end tag.\n+            #print \"</%s> is not real!\" % name\n+            self.handle_data('</%s>' % name)\n+            return\n+        self.endData()\n+        self._popToTag(name)\n+        if self.quoteStack and self.quoteStack[-1] == name:\n+            self.quoteStack.pop()\n+            self.literal = (len(self.quoteStack) > 0)\n+\n+    def handle_data(self, data):\n+        self.currentData.append(data)\n+\n+    def _toStringSubclass(self, text, subclass):\n+        \"\"\"Adds a certain piece of text to the tree as a NavigableString\n+        subclass.\"\"\"\n+        self.endData()\n+        self.handle_data(text)\n+        self.endData(subclass)\n+\n+    def handle_pi(self, text):\n+        \"\"\"Handle a processing instruction as a ProcessingInstruction\n+        object, possibly one with a %SOUP-ENCODING% slot into which an\n+        encoding will be plugged later.\"\"\"\n+        if text[:3] == \"xml\":\n+            text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\n+        self._toStringSubclass(text, ProcessingInstruction)\n+\n+    def handle_comment(self, text):\n+        \"Handle comments as Comment objects.\"\n+        self._toStringSubclass(text, Comment)\n+\n+    def handle_charref(self, ref):\n+        \"Handle character references as data.\"\n+        if self.convertEntities:\n+            data = unichr(int(ref))\n+        else:\n+            data = '&#%s;' % ref\n+        self.handle_data(data)\n+\n+    def handle_entityref(self, ref):\n+        \"\"\"Handle entity references as data, possibly converting known\n+        HTML and/or XML entity references to the corresponding Unicode\n+        characters.\"\"\"\n+        data = None\n+        if self.convertHTMLEntities:\n+            try:\n+                data = unichr(name2codepoint[ref])\n+            except KeyError:\n+                pass\n+\n+        if not data and self.convertXMLEntities:\n+                data = self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n+\n+        if not data and self.convertHTMLEntities and \\\n+            not self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n+                # TODO: We've got a problem here. We're told this is\n+                # an entity reference, but it's not an XML entity\n+                # reference or an HTML entity reference. Nonetheless,\n+                # the logical thing to do is to pass it through as an\n+                # unrecognized entity reference.\n+                #\n+                # Except: when the input is \"&carol;\" this function\n+                # will be called with input \"carol\". When the input is\n+                # \"AT&T\", this function will be called with input\n+                # \"T\". We have no way of knowing whether a semicolon\n+                # was present originally, so we don't know whether\n+                # this is an unknown entity or just a misplaced\n+                # ampersand.\n+                #\n+                # The more common case is a misplaced ampersand, so I\n+                # escape the ampersand and omit the trailing semicolon.\n+                data = \"&amp;%s\" % ref\n+        if not data:\n+            # This case is different from the one above, because we\n+            # haven't already gone through a supposedly comprehensive\n+            # mapping of entities to Unicode characters. We might not\n+            # have gone through any mapping at all. So the chances are\n+            # very high that this is a real entity, and not a\n+            # misplaced ampersand.\n+            data = \"&%s;\" % ref\n+        self.handle_data(data)\n+\n+    def handle_decl(self, data):\n+        \"Handle DOCTYPEs and the like as Declaration objects.\"\n+        self._toStringSubclass(data, Declaration)\n+\n+    def parse_declaration(self, i):\n+        \"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\n+        declaration as a CData object.\"\"\"\n+        j = None\n+        if self.rawdata[i:i+9] == '<![CDATA[':\n+             k = self.rawdata.find(']]>', i)\n+             if k == -1:\n+                 k = len(self.rawdata)\n+             data = self.rawdata[i+9:k]\n+             j = k+3\n+             self._toStringSubclass(data, CData)\n+        else:\n+            try:\n+                j = SGMLParser.parse_declaration(self, i)\n+            except SGMLParseError:\n+                toHandle = self.rawdata[i:]\n+                self.handle_data(toHandle)\n+                j = i + len(toHandle)\n+        return j\n+\n+class BeautifulSoup(BeautifulStoneSoup):\n+\n+    \"\"\"This parser knows the following facts about HTML:\n+\n+    * Some tags have no closing tag and should be interpreted as being\n+      closed as soon as they are encountered.\n+\n+    * The text inside some tags (ie. 'script') may contain tags which\n+      are not really part of the document and which should be parsed\n+      as text, not tags. If you want to parse the text as tags, you can\n+      always fetch it and parse it explicitly.\n+\n+    * Tag nesting rules:\n+\n+      Most tags can't be nested at all. For instance, the occurance of\n+      a <p> tag should implicitly close the previous <p> tag.\n+\n+       <p>Para1<p>Para2\n+        should be transformed into:\n+       <p>Para1</p><p>Para2\n+\n+      Some tags can be nested arbitrarily. For instance, the occurance\n+      of a <blockquote> tag should _not_ implicitly close the previous\n+      <blockquote> tag.\n+\n+       Alice said: <blockquote>Bob said: <blockquote>Blah\n+        should NOT be transformed into:\n+       Alice said: <blockquote>Bob said: </blockquote><blockquote>Blah\n+\n+      Some tags can be nested, but the nesting is reset by the\n+      interposition of other tags. For instance, a <tr> tag should\n+      implicitly close the previous <tr> tag within the same <table>,\n+      but not close a <tr> tag in another table.\n+\n+       <table><tr>Blah<tr>Blah\n+        should be transformed into:\n+       <table><tr>Blah</tr><tr>Blah\n+        but,\n+       <tr>Blah<table><tr>Blah\n+        should NOT be transformed into\n+       <tr>Blah<table></tr><tr>Blah\n+\n+    Differing assumptions about tag nesting rules are a major source\n+    of problems with the BeautifulSoup class. If BeautifulSoup is not\n+    treating as nestable a tag your page author treats as nestable,\n+    try ICantBelieveItsBeautifulSoup, MinimalSoup, or\n+    BeautifulStoneSoup before writing your own subclass.\"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        if not kwargs.has_key('smartQuotesTo'):\n+            kwargs['smartQuotesTo'] = self.HTML_ENTITIES\n+        kwargs['isHTML'] = True\n+        BeautifulStoneSoup.__init__(self, *args, **kwargs)\n+\n+    SELF_CLOSING_TAGS = buildTagMap(None,\n+                                    ('br' , 'hr', 'input', 'img', 'meta',\n+                                    'spacer', 'link', 'frame', 'base', 'col'))\n+\n+    PRESERVE_WHITESPACE_TAGS = set(['pre', 'textarea'])\n+\n+    QUOTE_TAGS = {'script' : None, 'textarea' : None}\n+\n+    #According to the HTML standard, each of these inline tags can\n+    #contain another tag of the same type. Furthermore, it's common\n+    #to actually use these tags this way.\n+    NESTABLE_INLINE_TAGS = ('span', 'font', 'q', 'object', 'bdo', 'sub', 'sup',\n+                            'center')\n+\n+    #According to the HTML standard, these block tags can contain\n+    #another tag of the same type. Furthermore, it's common\n+    #to actually use these tags this way.\n+    NESTABLE_BLOCK_TAGS = ('blockquote', 'div', 'fieldset', 'ins', 'del')\n+\n+    #Lists can contain other lists, but there are restrictions.\n+    NESTABLE_LIST_TAGS = { 'ol' : [],\n+                           'ul' : [],\n+                           'li' : ['ul', 'ol'],\n+                           'dl' : [],\n+                           'dd' : ['dl'],\n+                           'dt' : ['dl'] }\n+\n+    #Tables can contain other tables, but there are restrictions.\n+    NESTABLE_TABLE_TAGS = {'table' : [],\n+                           'tr' : ['table', 'tbody', 'tfoot', 'thead'],\n+                           'td' : ['tr'],\n+                           'th' : ['tr'],\n+                           'thead' : ['table'],\n+                           'tbody' : ['table'],\n+                           'tfoot' : ['table'],\n+                           }\n+\n+    NON_NESTABLE_BLOCK_TAGS = ('address', 'form', 'p', 'pre')\n+\n+    #If one of these tags is encountered, all tags up to the next tag of\n+    #this type are popped.\n+    RESET_NESTING_TAGS = buildTagMap(None, NESTABLE_BLOCK_TAGS, 'noscript',\n+                                     NON_NESTABLE_BLOCK_TAGS,\n+                                     NESTABLE_LIST_TAGS,\n+                                     NESTABLE_TABLE_TAGS)\n+\n+    NESTABLE_TAGS = buildTagMap([], NESTABLE_INLINE_TAGS, NESTABLE_BLOCK_TAGS,\n+                                NESTABLE_LIST_TAGS, NESTABLE_TABLE_TAGS)\n+\n+    # Used to detect the charset in a META tag; see start_meta\n+    CHARSET_RE = re.compile(\"((^|;)\\s*charset=)([^;]*)\", re.M)\n+\n+    def start_meta(self, attrs):\n+        \"\"\"Beautiful Soup can detect a charset included in a META tag,\n+        try to convert the document to that charset, and re-parse the\n+        document from the beginning.\"\"\"\n+        httpEquiv = None\n+        contentType = None\n+        contentTypeIndex = None\n+        tagNeedsEncodingSubstitution = False\n+\n+        for i in range(0, len(attrs)):\n+            key, value = attrs[i]\n+            key = key.lower()\n+            if key == 'http-equiv':\n+                httpEquiv = value\n+            elif key == 'content':\n+                contentType = value\n+                contentTypeIndex = i\n+\n+        if httpEquiv and contentType: # It's an interesting meta tag.\n+            match = self.CHARSET_RE.search(contentType)\n+            if match:\n+                if (self.declaredHTMLEncoding is not None or\n+                    self.originalEncoding == self.fromEncoding):\n+                    # An HTML encoding was sniffed while converting\n+                    # the document to Unicode, or an HTML encoding was\n+                    # sniffed during a previous pass through the\n+                    # document, or an encoding was specified\n+                    # explicitly and it worked. Rewrite the meta tag.\n+                    def rewrite(match):\n+                        return match.group(1) + \"%SOUP-ENCODING%\"\n+                    newAttr = self.CHARSET_RE.sub(rewrite, contentType)\n+                    attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\n+                                               newAttr)\n+                    tagNeedsEncodingSubstitution = True\n+                else:\n+                    # This is our first pass through the document.\n+                    # Go through it again with the encoding information.\n+                    newCharset = match.group(3)\n+                    if newCharset and newCharset != self.originalEncoding:\n+                        self.declaredHTMLEncoding = newCharset\n+                        self._feed(self.declaredHTMLEncoding)\n+                        raise StopParsing\n+                    pass\n+        tag = self.unknown_starttag(\"meta\", attrs)\n+        if tag and tagNeedsEncodingSubstitution:\n+            tag.containsSubstitutions = True\n+\n+class StopParsing(Exception):\n+    pass\n+\n+class ICantBelieveItsBeautifulSoup(BeautifulSoup):\n+\n+    \"\"\"The BeautifulSoup class is oriented towards skipping over\n+    common HTML errors like unclosed tags. However, sometimes it makes\n+    errors of its own. For instance, consider this fragment:\n+\n+     <b>Foo<b>Bar</b></b>\n+\n+    This is perfectly valid (if bizarre) HTML. However, the\n+    BeautifulSoup class will implicitly close the first b tag when it\n+    encounters the second 'b'. It will think the author wrote\n+    \"<b>Foo<b>Bar\", and didn't close the first 'b' tag, because\n+    there's no real-world reason to bold something that's already\n+    bold. When it encounters '</b></b>' it will close two more 'b'\n+    tags, for a grand total of three tags closed instead of two. This\n+    can throw off the rest of your document structure. The same is\n+    true of a number of other tags, listed below.\n+\n+    It's much more common for someone to forget to close a 'b' tag\n+    than to actually use nested 'b' tags, and the BeautifulSoup class\n+    handles the common case. This class handles the not-co-common\n+    case: where you can't believe someone wrote what they did, but\n+    it's valid HTML and BeautifulSoup screwed up by assuming it\n+    wouldn't be.\"\"\"\n+\n+    I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS = \\\n+     ('em', 'big', 'i', 'small', 'tt', 'abbr', 'acronym', 'strong',\n+      'cite', 'code', 'dfn', 'kbd', 'samp', 'strong', 'var', 'b',\n+      'big')\n+\n+    I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS = ('noscript',)\n+\n+    NESTABLE_TAGS = buildTagMap([], BeautifulSoup.NESTABLE_TAGS,\n+                                I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS,\n+                                I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS)\n+\n+class MinimalSoup(BeautifulSoup):\n+    \"\"\"The MinimalSoup class is for parsing HTML that contains\n+    pathologically bad markup. It makes no assumptions about tag\n+    nesting, but it does know which tags are self-closing, that\n+    <script> tags contain Javascript and should not be parsed, that\n+    META tags may contain encoding information, and so on.\n+\n+    This also makes it better for subclassing than BeautifulStoneSoup\n+    or BeautifulSoup.\"\"\"\n+\n+    RESET_NESTING_TAGS = buildTagMap('noscript')\n+    NESTABLE_TAGS = {}\n+\n+class BeautifulSOAP(BeautifulStoneSoup):\n+    \"\"\"This class will push a tag with only a single string child into\n+    the tag's parent as an attribute. The attribute's name is the tag\n+    name, and the value is the string child. An example should give\n+    the flavor of the change:\n+\n+    <foo><bar>baz</bar></foo>\n+     =>\n+    <foo bar=\"baz\"><bar>baz</bar></foo>\n+\n+    You can then access fooTag['bar'] instead of fooTag.barTag.string.\n+\n+    This is, of course, useful for scraping structures that tend to\n+    use subelements instead of attributes, such as SOAP messages. Note\n+    that it modifies its input, so don't print the modified version\n+    out.\n+\n+    I'm not sure how many people really want to use this class; let me\n+    know if you do. Mainly I like the name.\"\"\"\n+\n+    def popTag(self):\n+        if len(self.tagStack) > 1:\n+            tag = self.tagStack[-1]\n+            parent = self.tagStack[-2]\n+            parent._getAttrMap()\n+            if (isinstance(tag, Tag) and len(tag.contents) == 1 and\n+                isinstance(tag.contents[0], NavigableString) and\n+                not parent.attrMap.has_key(tag.name)):\n+                parent[tag.name] = tag.contents[0]\n+        BeautifulStoneSoup.popTag(self)\n+\n+#Enterprise class names! It has come to our attention that some people\n+#think the names of the Beautiful Soup parser classes are too silly\n+#and \"unprofessional\" for use in enterprise screen-scraping. We feel\n+#your pain! For such-minded folk, the Beautiful Soup Consortium And\n+#All-Night Kosher Bakery recommends renaming this file to\n+#\"RobustParser.py\" (or, in cases of extreme enterprisiness,\n+#\"RobustParserBeanInterface.class\") and using the following\n+#enterprise-friendly class aliases:\n+class RobustXMLParser(BeautifulStoneSoup):\n+    pass\n+class RobustHTMLParser(BeautifulSoup):\n+    pass\n+class RobustWackAssHTMLParser(ICantBelieveItsBeautifulSoup):\n+    pass\n+class RobustInsanelyWackAssHTMLParser(MinimalSoup):\n+    pass\n+class SimplifyingSOAPParser(BeautifulSOAP):\n+    pass\n+\n+######################################################\n+#\n+# Bonus library: Unicode, Dammit\n+#\n+# This class forces XML data into a standard format (usually to UTF-8\n+# or Unicode).  It is heavily based on code from Mark Pilgrim's\n+# Universal Feed Parser. It does not rewrite the XML or HTML to\n+# reflect a new encoding: that happens in BeautifulStoneSoup.handle_pi\n+# (XML) and BeautifulSoup.start_meta (HTML).\n+\n+# Autodetects character encodings.\n+# Download from http://chardet.feedparser.org/\n+try:\n+    import chardet\n+#    import chardet.constants\n+#    chardet.constants._debug = 1\n+except ImportError:\n+    chardet = None\n+\n+# cjkcodecs and iconv_codec make Python know about more character encodings.\n+# Both are available from http://cjkpython.i18n.org/\n+# They're built in if you use Python 2.4.\n+try:\n+    import cjkcodecs.aliases\n+except ImportError:\n+    pass\n+try:\n+    import iconv_codec\n+except ImportError:\n+    pass\n+\n+class UnicodeDammit:\n+    \"\"\"A class for detecting the encoding of a *ML document and\n+    converting it to a Unicode string. If the source encoding is\n+    windows-1252, can replace MS smart quotes with their HTML or XML\n+    equivalents.\"\"\"\n+\n+    # This dictionary maps commonly seen values for \"charset\" in HTML\n+    # meta tags to the corresponding Python codec names. It only covers\n+    # values that aren't in Python's aliases and can't be determined\n+    # by the heuristics in find_codec.\n+    CHARSET_ALIASES = { \"macintosh\" : \"mac-roman\",\n+                        \"x-sjis\" : \"shift-jis\" }\n+\n+    def __init__(self, markup, overrideEncodings=[],\n+                 smartQuotesTo='xml', isHTML=False):\n+        self.declaredHTMLEncoding = None\n+        self.markup, documentEncoding, sniffedEncoding = \\\n+                     self._detectEncoding(markup, isHTML)\n+        self.smartQuotesTo = smartQuotesTo\n+        self.triedEncodings = []\n+        if markup == '' or isinstance(markup, unicode):\n+            self.originalEncoding = None\n+            self.unicode = unicode(markup)\n+            return\n+\n+        u = None\n+        for proposedEncoding in overrideEncodings:\n+            u = self._convertFrom(proposedEncoding)\n+            if u: break\n+        if not u:\n+            for proposedEncoding in (documentEncoding, sniffedEncoding):\n+                u = self._convertFrom(proposedEncoding)\n+                if u: break\n+\n+        # If no luck and we have auto-detection library, try that:\n+        if not u and chardet and not isinstance(self.markup, unicode):\n+            u = self._convertFrom(chardet.detect(self.markup)['encoding'])\n+\n+        # As a last resort, try utf-8 and windows-1252:\n+        if not u:\n+            for proposed_encoding in (\"utf-8\", \"windows-1252\"):\n+                u = self._convertFrom(proposed_encoding)\n+                if u: break\n+\n+        self.unicode = u\n+        if not u: self.originalEncoding = None\n+\n+    def _subMSChar(self, orig):\n+        \"\"\"Changes a MS smart quote character to an XML or HTML\n+        entity.\"\"\"\n+        sub = self.MS_CHARS.get(orig)\n+        if isinstance(sub, tuple):\n+            if self.smartQuotesTo == 'xml':\n+                sub = '&#x%s;' % sub[1]\n+            else:\n+                sub = '&%s;' % sub[0]\n+        return sub\n+\n+    def _convertFrom(self, proposed):\n+        proposed = self.find_codec(proposed)\n+        if not proposed or proposed in self.triedEncodings:\n+            return None\n+        self.triedEncodings.append(proposed)\n+        markup = self.markup\n+\n+        # Convert smart quotes to HTML if coming from an encoding\n+        # that might have them.\n+        if self.smartQuotesTo and proposed.lower() in(\"windows-1252\",\n+                                                      \"iso-8859-1\",\n+                                                      \"iso-8859-2\"):\n+            markup = re.compile(\"([\\x80-\\x9f])\").sub \\\n+                     (lambda(x): self._subMSChar(x.group(1)),\n+                      markup)\n+\n+        try:\n+            # print \"Trying to convert document to %s\" % proposed\n+            u = self._toUnicode(markup, proposed)\n+            self.markup = u\n+            self.originalEncoding = proposed\n+        except Exception, e:\n+            # print \"That didn't work!\"\n+            # print e\n+            return None\n+        #print \"Correct encoding: %s\" % proposed\n+        return self.markup\n+\n+    def _toUnicode(self, data, encoding):\n+        '''Given a string and its encoding, decodes the string into Unicode.\n+        %encoding is a string recognized by encodings.aliases'''\n+\n+        # strip Byte Order Mark (if present)\n+        if (len(data) >= 4) and (data[:2] == '\\xfe\\xff') \\\n+               and (data[2:4] != '\\x00\\x00'):\n+            encoding = 'utf-16be'\n+            data = data[2:]\n+        elif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') \\\n+                 and (data[2:4] != '\\x00\\x00'):\n+            encoding = 'utf-16le'\n+            data = data[2:]\n+        elif data[:3] == '\\xef\\xbb\\xbf':\n+            encoding = 'utf-8'\n+            data = data[3:]\n+        elif data[:4] == '\\x00\\x00\\xfe\\xff':\n+            encoding = 'utf-32be'\n+            data = data[4:]\n+        elif data[:4] == '\\xff\\xfe\\x00\\x00':\n+            encoding = 'utf-32le'\n+            data = data[4:]\n+        newdata = unicode(data, encoding)\n+        return newdata\n+\n+    def _detectEncoding(self, xml_data, isHTML=False):\n+        \"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n+        xml_encoding = sniffed_xml_encoding = None\n+        try:\n+            if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n+                # EBCDIC\n+                xml_data = self._ebcdic_to_ascii(xml_data)\n+            elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n+                # UTF-16BE\n+                sniffed_xml_encoding = 'utf-16be'\n+                xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n+            elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n+                     and (xml_data[2:4] != '\\x00\\x00'):\n+                # UTF-16BE with BOM\n+                sniffed_xml_encoding = 'utf-16be'\n+                xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n+            elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n+                # UTF-16LE\n+                sniffed_xml_encoding = 'utf-16le'\n+                xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n+            elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n+                     (xml_data[2:4] != '\\x00\\x00'):\n+                # UTF-16LE with BOM\n+                sniffed_xml_encoding = 'utf-16le'\n+                xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n+            elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n+                # UTF-32BE\n+                sniffed_xml_encoding = 'utf-32be'\n+                xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n+            elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n+                # UTF-32LE\n+                sniffed_xml_encoding = 'utf-32le'\n+                xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n+            elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n+                # UTF-32BE with BOM\n+                sniffed_xml_encoding = 'utf-32be'\n+                xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n+            elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n+                # UTF-32LE with BOM\n+                sniffed_xml_encoding = 'utf-32le'\n+                xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n+            elif xml_data[:3] == '\\xef\\xbb\\xbf':\n+                # UTF-8 with BOM\n+                sniffed_xml_encoding = 'utf-8'\n+                xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n+            else:\n+                sniffed_xml_encoding = 'ascii'\n+                pass\n+        except:\n+            xml_encoding_match = None\n+        xml_encoding_match = re.compile(\n+            '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\n+        if not xml_encoding_match and isHTML:\n+            regexp = re.compile('<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]', re.I)\n+            xml_encoding_match = regexp.search(xml_data)\n+        if xml_encoding_match is not None:\n+            xml_encoding = xml_encoding_match.groups()[0].lower()\n+            if isHTML:\n+                self.declaredHTMLEncoding = xml_encoding\n+            if sniffed_xml_encoding and \\\n+               (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n+                                 'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n+                                 'utf-16', 'utf-32', 'utf_16', 'utf_32',\n+                                 'utf16', 'u16')):\n+                xml_encoding = sniffed_xml_encoding\n+        return xml_data, xml_encoding, sniffed_xml_encoding\n+\n+\n+    def find_codec(self, charset):\n+        return self._codec(self.CHARSET_ALIASES.get(charset, charset)) \\\n+               or (charset and self._codec(charset.replace(\"-\", \"\"))) \\\n+               or (charset and self._codec(charset.replace(\"-\", \"_\"))) \\\n+               or charset\n+\n+    def _codec(self, charset):\n+        if not charset: return charset\n+        codec = None\n+        try:\n+            codecs.lookup(charset)\n+            codec = charset\n+        except (LookupError, ValueError):\n+            pass\n+        return codec\n+\n+    EBCDIC_TO_ASCII_MAP = None\n+    def _ebcdic_to_ascii(self, s):\n+        c = self.__class__\n+        if not c.EBCDIC_TO_ASCII_MAP:\n+            emap = (0,1,2,3,156,9,134,127,151,141,142,11,12,13,14,15,\n+                    16,17,18,19,157,133,8,135,24,25,146,143,28,29,30,31,\n+                    128,129,130,131,132,10,23,27,136,137,138,139,140,5,6,7,\n+                    144,145,22,147,148,149,150,4,152,153,154,155,20,21,158,26,\n+                    32,160,161,162,163,164,165,166,167,168,91,46,60,40,43,33,\n+                    38,169,170,171,172,173,174,175,176,177,93,36,42,41,59,94,\n+                    45,47,178,179,180,181,182,183,184,185,124,44,37,95,62,63,\n+                    186,187,188,189,190,191,192,193,194,96,58,35,64,39,61,34,\n+                    195,97,98,99,100,101,102,103,104,105,196,197,198,199,200,\n+                    201,202,106,107,108,109,110,111,112,113,114,203,204,205,\n+                    206,207,208,209,126,115,116,117,118,119,120,121,122,210,\n+                    211,212,213,214,215,216,217,218,219,220,221,222,223,224,\n+                    225,226,227,228,229,230,231,123,65,66,67,68,69,70,71,72,\n+                    73,232,233,234,235,236,237,125,74,75,76,77,78,79,80,81,\n+                    82,238,239,240,241,242,243,92,159,83,84,85,86,87,88,89,\n+                    90,244,245,246,247,248,249,48,49,50,51,52,53,54,55,56,57,\n+                    250,251,252,253,254,255)\n+            import string\n+            c.EBCDIC_TO_ASCII_MAP = string.maketrans( \\\n+            ''.join(map(chr, range(256))), ''.join(map(chr, emap)))\n+        return s.translate(c.EBCDIC_TO_ASCII_MAP)\n+\n+    MS_CHARS = { '\\x80' : ('euro', '20AC'),\n+                 '\\x81' : ' ',\n+                 '\\x82' : ('sbquo', '201A'),\n+                 '\\x83' : ('fnof', '192'),\n+                 '\\x84' : ('bdquo', '201E'),\n+                 '\\x85' : ('hellip', '2026'),\n+                 '\\x86' : ('dagger', '2020'),\n+                 '\\x87' : ('Dagger', '2021'),\n+                 '\\x88' : ('circ', '2C6'),\n+                 '\\x89' : ('permil', '2030'),\n+                 '\\x8A' : ('Scaron', '160'),\n+                 '\\x8B' : ('lsaquo', '2039'),\n+                 '\\x8C' : ('OElig', '152'),\n+                 '\\x8D' : '?',\n+                 '\\x8E' : ('#x17D', '17D'),\n+                 '\\x8F' : '?',\n+                 '\\x90' : '?',\n+                 '\\x91' : ('lsquo', '2018'),\n+                 '\\x92' : ('rsquo', '2019'),\n+                 '\\x93' : ('ldquo', '201C'),\n+                 '\\x94' : ('rdquo', '201D'),\n+                 '\\x95' : ('bull', '2022'),\n+                 '\\x96' : ('ndash', '2013'),\n+                 '\\x97' : ('mdash', '2014'),\n+                 '\\x98' : ('tilde', '2DC'),\n+                 '\\x99' : ('trade', '2122'),\n+                 '\\x9a' : ('scaron', '161'),\n+                 '\\x9b' : ('rsaquo', '203A'),\n+                 '\\x9c' : ('oelig', '153'),\n+                 '\\x9d' : '?',\n+                 '\\x9e' : ('#x17E', '17E'),\n+                 '\\x9f' : ('Yuml', ''),}\n+\n+#######################################################################\n+\n+\n+#By default, act as an HTML pretty-printer.\n+if __name__ == '__main__':\n+    import sys\n+    soup = BeautifulSoup(sys.stdin)\n+    print soup.prettify()"
            },
            {
                "sha": "d3b35368514cbdb649b314329ee1b1973ca1e750",
                "filename": "github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoupTests.py",
                "status": "added",
                "additions": 906,
                "deletions": 0,
                "changes": 906,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoupTests.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoupTests.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/BeautifulSoup-3.2.2/BeautifulSoupTests.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,906 @@\n+# -*- coding: utf-8 -*-\n+\"\"\"Unit tests for Beautiful Soup.\n+\n+These tests make sure the Beautiful Soup works as it should. If you\n+find a bug in Beautiful Soup, the best way to express it is as a test\n+case like this that fails.\"\"\"\n+\n+# The very first thing we do is give a useful error if someone is\n+# running this code under Python 3.\n+\"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"<>\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n+\n+import unittest\n+from BeautifulSoup import *\n+\n+class SoupTest(unittest.TestCase):\n+\n+    def assertSoupEquals(self, toParse, rep=None, c=BeautifulSoup):\n+        \"\"\"Parse the given text and make sure its string rep is the other\n+        given text.\"\"\"\n+        if rep == None:\n+            rep = toParse\n+        self.assertEqual(str(c(toParse)), rep)\n+\n+\n+class FollowThatTag(SoupTest):\n+\n+    \"Tests the various ways of fetching tags from a soup.\"\n+\n+    def setUp(self):\n+        ml = \"\"\"\n+        <a id=\"x\">1</a>\n+        <A id=\"a\">2</a>\n+        <b id=\"b\">3</a>\n+        <b href=\"foo\" id=\"x\">4</a>\n+        <ac width=100>4</ac>\"\"\"\n+        self.soup = BeautifulStoneSoup(ml)\n+\n+    def testFindAllByName(self):\n+        matching = self.soup('a')\n+        self.assertEqual(len(matching), 2)\n+        self.assertEqual(matching[0].name, 'a')\n+        self.assertEqual(matching, self.soup.findAll('a'))\n+        self.assertEqual(matching, self.soup.findAll(SoupStrainer('a')))\n+\n+    def testFindAllByAttribute(self):\n+        matching = self.soup.findAll(id='x')\n+        self.assertEqual(len(matching), 2)\n+        self.assertEqual(matching[0].name, 'a')\n+        self.assertEqual(matching[1].name, 'b')\n+\n+        matching2 = self.soup.findAll(attrs={'id' : 'x'})\n+        self.assertEqual(matching, matching2)\n+\n+        strainer = SoupStrainer(attrs={'id' : 'x'})\n+        self.assertEqual(matching, self.soup.findAll(strainer))\n+\n+        self.assertEqual(len(self.soup.findAll(id=None)), 1)\n+\n+        self.assertEqual(len(self.soup.findAll(width=100)), 1)\n+        self.assertEqual(len(self.soup.findAll(junk=None)), 5)\n+        self.assertEqual(len(self.soup.findAll(junk=[1, None])), 5)\n+\n+        self.assertEqual(len(self.soup.findAll(junk=re.compile('.*'))), 0)\n+        self.assertEqual(len(self.soup.findAll(junk=True)), 0)\n+\n+        self.assertEqual(len(self.soup.findAll(junk=True)), 0)\n+        self.assertEqual(len(self.soup.findAll(href=True)), 1)\n+\n+    def testFindallByClass(self):\n+        soup = BeautifulSoup('<b class=\"foo\">Foo</b><a class=\"1 23 4\">Bar</a>')\n+        self.assertEqual(soup.find(attrs='foo').string, \"Foo\")\n+        self.assertEqual(soup.find('a', '1').string, \"Bar\")\n+        self.assertEqual(soup.find('a', '23').string, \"Bar\")\n+        self.assertEqual(soup.find('a', '4').string, \"Bar\")\n+\n+        self.assertEqual(soup.find('a', '2'), None)\n+\n+    def testFindAllByList(self):\n+        matching = self.soup(['a', 'ac'])\n+        self.assertEqual(len(matching), 3)\n+\n+    def testFindAllByHash(self):\n+        matching = self.soup({'a' : True, 'b' : True})\n+        self.assertEqual(len(matching), 4)\n+\n+    def testFindAllText(self):\n+        soup = BeautifulSoup(\"<html>\\xbb</html>\")\n+        self.assertEqual(soup.findAll(text=re.compile('.*')),\n+                         [u'\\xbb'])\n+\n+    def testFindAllByRE(self):\n+        import re\n+        r = re.compile('a.*')\n+        self.assertEqual(len(self.soup(r)), 3)\n+\n+    def testFindAllByMethod(self):\n+        def matchTagWhereIDMatchesName(tag):\n+            return tag.name == tag.get('id')\n+\n+        matching = self.soup.findAll(matchTagWhereIDMatchesName)\n+        self.assertEqual(len(matching), 2)\n+        self.assertEqual(matching[0].name, 'a')\n+\n+    def testFindByIndex(self):\n+        \"\"\"For when you have the tag and you want to know where it is.\"\"\"\n+        tag = self.soup.find('a', id=\"a\")\n+        self.assertEqual(self.soup.index(tag), 3)\n+\n+        # It works for NavigableStrings as well.\n+        s = tag.string\n+        self.assertEqual(tag.index(s), 0)\n+\n+        # If the tag isn't present, a ValueError is raised.\n+        soup2 = BeautifulSoup(\"<b></b>\")\n+        tag2 = soup2.find('b')\n+        self.assertRaises(ValueError, self.soup.index, tag2)\n+\n+    def testConflictingFindArguments(self):\n+        \"\"\"The 'text' argument takes precedence.\"\"\"\n+        soup = BeautifulSoup('Foo<b>Bar</b>Baz')\n+        self.assertEqual(soup.find('b', text='Baz'), 'Baz')\n+        self.assertEqual(soup.findAll('b', text='Baz'), ['Baz'])\n+\n+        self.assertEqual(soup.find(True, text='Baz'), 'Baz')\n+        self.assertEqual(soup.findAll(True, text='Baz'), ['Baz'])\n+\n+    def testParents(self):\n+        soup = BeautifulSoup('<ul id=\"foo\"></ul><ul id=\"foo\"><ul><ul id=\"foo\" a=\"b\"><b>Blah')\n+        b = soup.b\n+        self.assertEquals(len(b.findParents('ul', {'id' : 'foo'})), 2)\n+        self.assertEquals(b.findParent('ul')['a'], 'b')\n+\n+    PROXIMITY_TEST = BeautifulSoup('<b id=\"1\"><b id=\"2\"><b id=\"3\"><b id=\"4\">')\n+\n+    def testNext(self):\n+        soup = self.PROXIMITY_TEST\n+        b = soup.find('b', {'id' : 2})\n+        self.assertEquals(b.findNext('b')['id'], '3')\n+        self.assertEquals(b.findNext('b')['id'], '3')\n+        self.assertEquals(len(b.findAllNext('b')), 2)\n+        self.assertEquals(len(b.findAllNext('b', {'id' : 4})), 1)\n+\n+    def testPrevious(self):\n+        soup = self.PROXIMITY_TEST\n+        b = soup.find('b', {'id' : 3})\n+        self.assertEquals(b.findPrevious('b')['id'], '2')\n+        self.assertEquals(b.findPrevious('b')['id'], '2')\n+        self.assertEquals(len(b.findAllPrevious('b')), 2)\n+        self.assertEquals(len(b.findAllPrevious('b', {'id' : 2})), 1)\n+\n+\n+    SIBLING_TEST = BeautifulSoup('<blockquote id=\"1\"><blockquote id=\"1.1\"></blockquote></blockquote><blockquote id=\"2\"><blockquote id=\"2.1\"></blockquote></blockquote><blockquote id=\"3\"><blockquote id=\"3.1\"></blockquote></blockquote><blockquote id=\"4\">')\n+\n+    def testNextSibling(self):\n+        soup = self.SIBLING_TEST\n+        tag = 'blockquote'\n+        b = soup.find(tag, {'id' : 2})\n+        self.assertEquals(b.findNext(tag)['id'], '2.1')\n+        self.assertEquals(b.findNextSibling(tag)['id'], '3')\n+        self.assertEquals(b.findNextSibling(tag)['id'], '3')\n+        self.assertEquals(len(b.findNextSiblings(tag)), 2)\n+        self.assertEquals(len(b.findNextSiblings(tag, {'id' : 4})), 1)\n+\n+    def testPreviousSibling(self):\n+        soup = self.SIBLING_TEST\n+        tag = 'blockquote'\n+        b = soup.find(tag, {'id' : 3})\n+        self.assertEquals(b.findPrevious(tag)['id'], '2.1')\n+        self.assertEquals(b.findPreviousSibling(tag)['id'], '2')\n+        self.assertEquals(b.findPreviousSibling(tag)['id'], '2')\n+        self.assertEquals(len(b.findPreviousSiblings(tag)), 2)\n+        self.assertEquals(len(b.findPreviousSiblings(tag, id=1)), 1)\n+\n+    def testTextNavigation(self):\n+        soup = BeautifulSoup('Foo<b>Bar</b><i id=\"1\"><b>Baz<br />Blee<hr id=\"1\"/></b></i>Blargh')\n+        baz = soup.find(text='Baz')\n+        self.assertEquals(baz.findParent(\"i\")['id'], '1')\n+        self.assertEquals(baz.findNext(text='Blee'), 'Blee')\n+        self.assertEquals(baz.findNextSibling(text='Blee'), 'Blee')\n+        self.assertEquals(baz.findNextSibling(text='Blargh'), None)\n+        self.assertEquals(baz.findNextSibling('hr')['id'], '1')\n+\n+class SiblingRivalry(SoupTest):\n+    \"Tests the nextSibling and previousSibling navigation.\"\n+\n+    def testSiblings(self):\n+        soup = BeautifulSoup(\"<ul><li>1<p>A</p>B<li>2<li>3</ul>\")\n+        secondLI = soup.find('li').nextSibling\n+        self.assert_(secondLI.name == 'li' and secondLI.string == '2')\n+        self.assertEquals(soup.find(text='1').nextSibling.name, 'p')\n+        self.assertEquals(soup.find('p').nextSibling, 'B')\n+        self.assertEquals(soup.find('p').nextSibling.previousSibling.nextSibling, 'B')\n+\n+class TagsAreObjectsToo(SoupTest):\n+    \"Tests the various built-in functions of Tag objects.\"\n+\n+    def testLen(self):\n+        soup = BeautifulSoup(\"<top>1<b>2</b>3</top>\")\n+        self.assertEquals(len(soup.top), 3)\n+\n+class StringEmUp(SoupTest):\n+    \"Tests the use of 'string' as an alias for a tag's only content.\"\n+\n+    def testString(self):\n+        s = BeautifulSoup(\"<b>foo</b>\")\n+        self.assertEquals(s.b.string, 'foo')\n+\n+    def testLackOfString(self):\n+        s = BeautifulSoup(\"<b>f<i>e</i>o</b>\")\n+        self.assert_(not s.b.string)\n+\n+    def testStringAssign(self):\n+        s = BeautifulSoup(\"<b></b>\")\n+        b = s.b\n+        b.string = \"foo\"\n+        string = b.string\n+        self.assertEquals(string, \"foo\")\n+        self.assert_(isinstance(string, NavigableString))\n+\n+class AllText(SoupTest):\n+    \"Tests the use of 'text' to get all of string content from the tag.\"\n+\n+    def testText(self):\n+        soup = BeautifulSoup(\"<ul><li>spam</li><li>eggs</li><li>cheese</li>\")\n+        self.assertEquals(soup.ul.text, \"spameggscheese\")\n+        self.assertEquals(soup.ul.getText('/'), \"spam/eggs/cheese\")\n+\n+class ThatsMyLimit(SoupTest):\n+    \"Tests the limit argument.\"\n+\n+    def testBasicLimits(self):\n+        s = BeautifulSoup('<br id=\"1\" /><br id=\"1\" /><br id=\"1\" /><br id=\"1\" />')\n+        self.assertEquals(len(s.findAll('br')), 4)\n+        self.assertEquals(len(s.findAll('br', limit=2)), 2)\n+        self.assertEquals(len(s('br', limit=2)), 2)\n+\n+class OnlyTheLonely(SoupTest):\n+    \"Tests the parseOnly argument to the constructor.\"\n+    def setUp(self):\n+        x = []\n+        for i in range(1,6):\n+            x.append('<a id=\"%s\">' % i)\n+            for j in range(100,103):\n+                x.append('<b id=\"%s.%s\">Content %s.%s</b>' % (i,j, i,j))\n+            x.append('</a>')\n+        self.x = ''.join(x)\n+\n+    def testOnly(self):\n+        strainer = SoupStrainer(\"b\")\n+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)\n+        self.assertEquals(len(soup), 15)\n+\n+        strainer = SoupStrainer(id=re.compile(\"100.*\"))\n+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)\n+        self.assertEquals(len(soup), 5)\n+\n+        strainer = SoupStrainer(text=re.compile(\"10[01].*\"))\n+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)\n+        self.assertEquals(len(soup), 10)\n+\n+        strainer = SoupStrainer(text=lambda(x):x[8]=='3')\n+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)\n+        self.assertEquals(len(soup), 3)\n+\n+class PickleMeThis(SoupTest):\n+    \"Testing features like pickle and deepcopy.\"\n+\n+    def setUp(self):\n+        self.page = \"\"\"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\n+\"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n+<html>\n+<head>\n+<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n+<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n+<link rev=\"made\" href=\"mailto:leonardr@segfault.org\">\n+<meta name=\"Description\" content=\"Beautiful Soup: an HTML parser optimized for screen-scraping.\">\n+<meta name=\"generator\" content=\"Markov Approximation 1.4 (module: leonardr)\">\n+<meta name=\"author\" content=\"Leonard Richardson\">\n+</head>\n+<body>\n+<a href=\"foo\">foo</a>\n+<a href=\"foo\"><b>bar</b></a>\n+</body>\n+</html>\"\"\"\n+\n+        self.soup = BeautifulSoup(self.page)\n+\n+    def testPickle(self):\n+        import pickle\n+        dumped = pickle.dumps(self.soup, 2)\n+        loaded = pickle.loads(dumped)\n+        self.assertEqual(loaded.__class__, BeautifulSoup)\n+        self.assertEqual(str(loaded), str(self.soup))\n+\n+    def testDeepcopy(self):\n+        from copy import deepcopy\n+        copied = deepcopy(self.soup)\n+        self.assertEqual(str(copied), str(self.soup))\n+\n+    def testUnicodePickle(self):\n+        import cPickle as pickle\n+        html = \"<b>\" + chr(0xc3) + \"</b>\"\n+        soup = BeautifulSoup(html)\n+        dumped = pickle.dumps(soup, pickle.HIGHEST_PROTOCOL)\n+        loaded = pickle.loads(dumped)\n+        self.assertEqual(str(loaded), str(soup))\n+\n+\n+class WriteOnlyCode(SoupTest):\n+    \"Testing the modification of the tree.\"\n+\n+    def testModifyAttributes(self):\n+        soup = BeautifulSoup('<a id=\"1\"></a>')\n+        soup.a['id'] = 2\n+        self.assertEqual(soup.renderContents(), '<a id=\"2\"></a>')\n+        del(soup.a['id'])\n+        self.assertEqual(soup.renderContents(), '<a></a>')\n+        soup.a['id2'] = 'foo'\n+        self.assertEqual(soup.renderContents(), '<a id2=\"foo\"></a>')\n+\n+    def testNewTagCreation(self):\n+        \"Makes sure tags don't step on each others' toes.\"\n+        soup = BeautifulSoup()\n+        a = Tag(soup, 'a')\n+        ol = Tag(soup, 'ol')\n+        a['href'] = 'http://foo.com/'\n+        self.assertRaises(KeyError, lambda : ol['href'])\n+\n+    def testNewTagWithAttributes(self):\n+        \"\"\"Makes sure new tags can be created complete with attributes.\"\"\"\n+        soup = BeautifulSoup()\n+        a = Tag(soup, 'a', [('href', 'foo')])\n+        b = Tag(soup, 'b', {'class':'bar'})\n+        soup.insert(0,a)\n+        soup.insert(1,b)\n+        self.assertEqual(soup.a['href'], 'foo')\n+        self.assertEqual(soup.b['class'], 'bar')\n+\n+    def testTagReplacement(self):\n+        # Make sure you can replace an element with itself.\n+        text = \"<a><b></b><c>Foo<d></d></c></a><a><e></e></a>\"\n+        soup = BeautifulSoup(text)\n+        c = soup.c\n+        soup.c.replaceWith(c)\n+        self.assertEquals(str(soup), text)\n+\n+        # A very simple case\n+        soup = BeautifulSoup(\"<b>Argh!</b>\")\n+        soup.find(text=\"Argh!\").replaceWith(\"Hooray!\")\n+        newText = soup.find(text=\"Hooray!\")\n+        b = soup.b\n+        self.assertEqual(newText.previous, b)\n+        self.assertEqual(newText.parent, b)\n+        self.assertEqual(newText.previous.next, newText)\n+        self.assertEqual(newText.next, None)\n+\n+        # A more complex case\n+        soup = BeautifulSoup(\"<a><b>Argh!</b><c></c><d></d></a>\")\n+        soup.b.insert(1, \"Hooray!\")\n+        newText = soup.find(text=\"Hooray!\")\n+        self.assertEqual(newText.previous, \"Argh!\")\n+        self.assertEqual(newText.previous.next, newText)\n+\n+        self.assertEqual(newText.previousSibling, \"Argh!\")\n+        self.assertEqual(newText.previousSibling.nextSibling, newText)\n+\n+        self.assertEqual(newText.nextSibling, None)\n+        self.assertEqual(newText.next, soup.c)\n+\n+        text = \"<html>There's <b>no</b> business like <b>show</b> business</html>\"\n+        soup = BeautifulSoup(text)\n+        no, show = soup.findAll('b')\n+        show.replaceWith(no)\n+        self.assertEquals(str(soup), \"<html>There's  business like <b>no</b> business</html>\")\n+\n+        # Even more complex\n+        soup = BeautifulSoup(\"<a><b>Find</b><c>lady!</c><d></d></a>\")\n+        tag = Tag(soup, 'magictag')\n+        tag.insert(0, \"the\")\n+        soup.a.insert(1, tag)\n+\n+        b = soup.b\n+        c = soup.c\n+        theText = tag.find(text=True)\n+        findText = b.find(text=\"Find\")\n+\n+        self.assertEqual(findText.next, tag)\n+        self.assertEqual(tag.previous, findText)\n+        self.assertEqual(b.nextSibling, tag)\n+        self.assertEqual(tag.previousSibling, b)\n+        self.assertEqual(tag.nextSibling, c)\n+        self.assertEqual(c.previousSibling, tag)\n+\n+        self.assertEqual(theText.next, c)\n+        self.assertEqual(c.previous, theText)\n+\n+        # Aand... incredibly complex.\n+        soup = BeautifulSoup(\"\"\"<a>We<b>reserve<c>the</c><d>right</d></b></a><e>to<f>refuse</f><g>service</g></e>\"\"\")\n+        f = soup.f\n+        a = soup.a\n+        c = soup.c\n+        e = soup.e\n+        weText = a.find(text=\"We\")\n+        soup.b.replaceWith(soup.f)\n+        self.assertEqual(str(soup), \"<a>We<f>refuse</f></a><e>to<g>service</g></e>\")\n+\n+        self.assertEqual(f.previous, weText)\n+        self.assertEqual(weText.next, f)\n+        self.assertEqual(f.previousSibling, weText)\n+        self.assertEqual(f.nextSibling, None)\n+        self.assertEqual(weText.nextSibling, f)\n+\n+    def testReplaceWithChildren(self):\n+        soup = BeautifulStoneSoup(\n+            \"<top><replace><child1/><child2/></replace></top>\",\n+            selfClosingTags=[\"child1\", \"child2\"])\n+        soup.replaceTag.replaceWithChildren()\n+        self.assertEqual(soup.top.contents[0].name, \"child1\")\n+        self.assertEqual(soup.top.contents[1].name, \"child2\")\n+\n+    def testAppend(self):\n+       doc = \"<p>Don't leave me <b>here</b>.</p> <p>Don't leave me.</p>\"\n+       soup = BeautifulSoup(doc)\n+       second_para = soup('p')[1]\n+       bold = soup.find('b')\n+       soup('p')[1].append(soup.find('b'))\n+       self.assertEqual(bold.parent, second_para)\n+       self.assertEqual(str(soup),\n+                        \"<p>Don't leave me .</p> \"\n+                        \"<p>Don't leave me.<b>here</b></p>\")\n+\n+    def testTagExtraction(self):\n+        # A very simple case\n+        text = '<html><div id=\"nav\">Nav crap</div>Real content here.</html>'\n+        soup = BeautifulSoup(text)\n+        extracted = soup.find(\"div\", id=\"nav\").extract()\n+        self.assertEqual(str(soup), \"<html>Real content here.</html>\")\n+        self.assertEqual(str(extracted), '<div id=\"nav\">Nav crap</div>')\n+\n+        # A simple case, a more complex test.\n+        text = \"<doc><a>1<b>2</b></a><a>i<b>ii</b></a><a>A<b>B</b></a></doc>\"\n+        soup = BeautifulStoneSoup(text)\n+        doc = soup.doc\n+        numbers, roman, letters = soup(\"a\")\n+\n+        self.assertEqual(roman.parent, doc)\n+        oldPrevious = roman.previous\n+        endOfThisTag = roman.nextSibling.previous\n+        self.assertEqual(oldPrevious, \"2\")\n+        self.assertEqual(roman.next, \"i\")\n+        self.assertEqual(endOfThisTag, \"ii\")\n+        self.assertEqual(roman.previousSibling, numbers)\n+        self.assertEqual(roman.nextSibling, letters)\n+\n+        roman.extract()\n+        self.assertEqual(roman.parent, None)\n+        self.assertEqual(roman.previous, None)\n+        self.assertEqual(roman.next, \"i\")\n+        self.assertEqual(letters.previous, '2')\n+        self.assertEqual(roman.previousSibling, None)\n+        self.assertEqual(roman.nextSibling, None)\n+        self.assertEqual(endOfThisTag.next, None)\n+        self.assertEqual(roman.b.contents[0].next, None)\n+        self.assertEqual(numbers.nextSibling, letters)\n+        self.assertEqual(letters.previousSibling, numbers)\n+        self.assertEqual(len(doc.contents), 2)\n+        self.assertEqual(doc.contents[0], numbers)\n+        self.assertEqual(doc.contents[1], letters)\n+\n+        # A more complex case.\n+        text = \"<a>1<b>2<c>Hollywood, baby!</c></b></a>3\"\n+        soup = BeautifulStoneSoup(text)\n+        one = soup.find(text=\"1\")\n+        three = soup.find(text=\"3\")\n+        toExtract = soup.b\n+        soup.b.extract()\n+        self.assertEqual(one.next, three)\n+        self.assertEqual(three.previous, one)\n+        self.assertEqual(one.parent.nextSibling, three)\n+        self.assertEqual(three.previousSibling, soup.a)\n+        \n+    def testClear(self):\n+        soup = BeautifulSoup(\"<ul><li></li><li></li></ul>\")\n+        soup.ul.clear()\n+        self.assertEqual(len(soup.ul.contents), 0)\n+\n+class TheManWithoutAttributes(SoupTest):\n+    \"Test attribute access\"\n+\n+    def testHasKey(self):\n+        text = \"<foo attr='bar'>\"\n+        self.assertEquals(BeautifulSoup(text).foo.has_key('attr'), True)\n+\n+class QuoteMeOnThat(SoupTest):\n+    \"Test quoting\"\n+    def testQuotedAttributeValues(self):\n+        self.assertSoupEquals(\"<foo attr='bar'></foo>\",\n+                              '<foo attr=\"bar\"></foo>')\n+\n+        text = \"\"\"<foo attr='bar \"brawls\" happen'>a</foo>\"\"\"\n+        soup = BeautifulSoup(text)\n+        self.assertEquals(soup.renderContents(), text)\n+\n+        soup.foo['attr'] = 'Brawls happen at \"Bob\\'s Bar\"'\n+        newText = \"\"\"<foo attr='Brawls happen at \"Bob&squot;s Bar\"'>a</foo>\"\"\"\n+        self.assertSoupEquals(soup.renderContents(), newText)\n+\n+        self.assertSoupEquals('<this is=\"really messed up & stuff\">',\n+                              '<this is=\"really messed up &amp; stuff\"></this>')\n+\n+        # This is not what the original author had in mind, but it's\n+        # a legitimate interpretation of what they wrote.\n+        self.assertSoupEquals(\"\"\"<a href=\"foo</a>, </a><a href=\"bar\">baz</a>\"\"\",\n+        '<a href=\"foo&lt;/a&gt;, &lt;/a&gt;&lt;a href=\"></a>, <a href=\"bar\">baz</a>')\n+\n+        # SGMLParser generates bogus parse events when attribute values\n+        # contain embedded brackets, but at least Beautiful Soup fixes\n+        # it up a little.\n+        self.assertSoupEquals('<a b=\"<a>\">', '<a b=\"&lt;a&gt;\"></a><a>\"&gt;</a>')\n+        self.assertSoupEquals('<a href=\"http://foo.com/<a> and blah and blah',\n+                              \"\"\"<a href='\"http://foo.com/'></a><a> and blah and blah</a>\"\"\")\n+\n+\n+\n+class YoureSoLiteral(SoupTest):\n+    \"Test literal mode.\"\n+    def testLiteralMode(self):\n+        text = \"<script>if (i<imgs.length)</script><b>Foo</b>\"\n+        soup = BeautifulSoup(text)\n+        self.assertEqual(soup.script.contents[0], \"if (i<imgs.length)\")\n+        self.assertEqual(soup.b.contents[0], \"Foo\")\n+\n+    def testTextArea(self):\n+        text = \"<textarea><b>This is an example of an HTML tag</b><&<&</textarea>\"\n+        soup = BeautifulSoup(text)\n+        self.assertEqual(soup.textarea.contents[0],\n+                         \"<b>This is an example of an HTML tag</b><&<&\")\n+\n+class OperatorOverload(SoupTest):\n+    \"Our operators do it all! Call now!\"\n+\n+    def testTagNameAsFind(self):\n+        \"Tests that referencing a tag name as a member delegates to find().\"\n+        soup = BeautifulSoup('<b id=\"1\">foo<i>bar</i></b><b>Red herring</b>')\n+        self.assertEqual(soup.b.i, soup.find('b').find('i'))\n+        self.assertEqual(soup.b.i.string, 'bar')\n+        self.assertEqual(soup.b['id'], '1')\n+        self.assertEqual(soup.b.contents[0], 'foo')\n+        self.assert_(not soup.a)\n+\n+        #Test the .fooTag variant of .foo.\n+        self.assertEqual(soup.bTag.iTag.string, 'bar')\n+        self.assertEqual(soup.b.iTag.string, 'bar')\n+        self.assertEqual(soup.find('b').find('i'), soup.bTag.iTag)\n+\n+class NestableEgg(SoupTest):\n+    \"\"\"Here we test tag nesting. TEST THE NEST, DUDE! X-TREME!\"\"\"\n+\n+    def testParaInsideBlockquote(self):\n+        soup = BeautifulSoup('<blockquote><p><b>Foo</blockquote><p>Bar')\n+        self.assertEqual(soup.blockquote.p.b.string, 'Foo')\n+        self.assertEqual(soup.blockquote.b.string, 'Foo')\n+        self.assertEqual(soup.find('p', recursive=False).string, 'Bar')\n+\n+    def testNestedTables(self):\n+        text = \"\"\"<table id=\"1\"><tr><td>Here's another table:\n+        <table id=\"2\"><tr><td>Juicy text</td></tr></table></td></tr></table>\"\"\"\n+        soup = BeautifulSoup(text)\n+        self.assertEquals(soup.table.table.td.string, 'Juicy text')\n+        self.assertEquals(len(soup.findAll('table')), 2)\n+        self.assertEquals(len(soup.table.findAll('table')), 1)\n+        self.assertEquals(soup.find('table', {'id' : 2}).parent.parent.parent.name,\n+                          'table')\n+\n+        text = \"<table><tr><td><div><table>Foo</table></div></td></tr></table>\"\n+        soup = BeautifulSoup(text)\n+        self.assertEquals(soup.table.tr.td.div.table.contents[0], \"Foo\")\n+\n+        text = \"\"\"<table><thead><tr>Foo</tr></thead><tbody><tr>Bar</tr></tbody>\n+        <tfoot><tr>Baz</tr></tfoot></table>\"\"\"\n+        soup = BeautifulSoup(text)\n+        self.assertEquals(soup.table.thead.tr.contents[0], \"Foo\")\n+\n+    def testBadNestedTables(self):\n+        soup = BeautifulSoup(\"<table><tr><table><tr id='nested'>\")\n+        self.assertEquals(soup.table.tr.table.tr['id'], 'nested')\n+\n+class CleanupOnAisleFour(SoupTest):\n+    \"\"\"Here we test cleanup of text that breaks SGMLParser or is just\n+    obnoxious.\"\"\"\n+\n+    def testSelfClosingtag(self):\n+        self.assertEqual(str(BeautifulSoup(\"Foo<br/>Bar\").find('br')),\n+                         '<br />')\n+\n+        self.assertSoupEquals('<p>test1<br/>test2</p>',\n+                              '<p>test1<br />test2</p>')\n+\n+        text = '<p>test1<selfclosing>test2'\n+        soup = BeautifulStoneSoup(text)\n+        self.assertEqual(str(soup),\n+                         '<p>test1<selfclosing>test2</selfclosing></p>')\n+\n+        soup = BeautifulStoneSoup(text, selfClosingTags='selfclosing')\n+        self.assertEqual(str(soup),\n+                         '<p>test1<selfclosing />test2</p>')\n+\n+    def testSelfClosingTagOrNot(self):\n+        text = \"<item><link>http://foo.com/</link></item>\"\n+        self.assertEqual(BeautifulStoneSoup(text).renderContents(), text)\n+        self.assertEqual(BeautifulSoup(text).renderContents(),\n+                         '<item><link />http://foo.com/</item>')\n+\n+    def testCData(self):\n+        xml = \"<root>foo<![CDATA[foobar]]>bar</root>\"\n+        self.assertSoupEquals(xml, xml)\n+        r = re.compile(\"foo.*bar\")\n+        soup = BeautifulSoup(xml)\n+        self.assertEquals(soup.find(text=r).string, \"foobar\")\n+        self.assertEquals(soup.find(text=r).__class__, CData)\n+\n+    def testComments(self):\n+        xml = \"foo<!--foobar-->baz\"\n+        self.assertSoupEquals(xml)\n+        r = re.compile(\"foo.*bar\")\n+        soup = BeautifulSoup(xml)\n+        self.assertEquals(soup.find(text=r).string, \"foobar\")\n+        self.assertEquals(soup.find(text=\"foobar\").__class__, Comment)\n+\n+    def testDeclaration(self):\n+        xml = \"foo<!DOCTYPE foobar>baz\"\n+        self.assertSoupEquals(xml)\n+        r = re.compile(\".*foo.*bar\")\n+        soup = BeautifulSoup(xml)\n+        text = \"DOCTYPE foobar\"\n+        self.assertEquals(soup.find(text=r).string, text)\n+        self.assertEquals(soup.find(text=text).__class__, Declaration)\n+\n+        namespaced_doctype = ('<!DOCTYPE xsl:stylesheet SYSTEM \"htmlent.dtd\">'\n+                              '<html>foo</html>')\n+        soup = BeautifulSoup(namespaced_doctype)\n+        self.assertEquals(soup.contents[0],\n+                          'DOCTYPE xsl:stylesheet SYSTEM \"htmlent.dtd\"')\n+        self.assertEquals(soup.html.contents[0], 'foo')\n+\n+    def testEntityConversions(self):\n+        text = \"&lt;&lt;sacr&eacute;&#32;bleu!&gt;&gt;\"\n+        soup = BeautifulStoneSoup(text)\n+        self.assertSoupEquals(text)\n+\n+        xmlEnt = BeautifulStoneSoup.XML_ENTITIES\n+        htmlEnt = BeautifulStoneSoup.HTML_ENTITIES\n+        xhtmlEnt = BeautifulStoneSoup.XHTML_ENTITIES\n+\n+        soup = BeautifulStoneSoup(text, convertEntities=xmlEnt)\n+        self.assertEquals(str(soup), \"&lt;&lt;sacr&eacute; bleu!&gt;&gt;\")\n+\n+        soup = BeautifulStoneSoup(text, convertEntities=htmlEnt)\n+        self.assertEquals(unicode(soup), u\"&lt;&lt;sacr\\xe9 bleu!&gt;&gt;\")\n+\n+        # Make sure the \"XML\", \"HTML\", and \"XHTML\" settings work.\n+        text = \"&lt;&trade;&apos;\"\n+        soup = BeautifulStoneSoup(text, convertEntities=xmlEnt)\n+        self.assertEquals(unicode(soup), u\"&lt;&trade;'\")\n+\n+        soup = BeautifulStoneSoup(text, convertEntities=htmlEnt)\n+        self.assertEquals(unicode(soup), u\"&lt;\\u2122&apos;\")\n+\n+        soup = BeautifulStoneSoup(text, convertEntities=xhtmlEnt)\n+        self.assertEquals(unicode(soup), u\"&lt;\\u2122'\")\n+\n+        invalidEntity = \"foo&#bar;baz\"\n+        soup = BeautifulStoneSoup\\\n+               (invalidEntity,\n+                convertEntities=htmlEnt)\n+        self.assertEquals(str(soup), \"foo&amp;#bar;baz\")\n+\n+        nonexistentEntity = \"foo&bar;baz\"\n+        soup = BeautifulStoneSoup\\\n+               (nonexistentEntity,\n+                convertEntities=\"xml\")\n+        self.assertEquals(str(soup), nonexistentEntity)\n+\n+\n+    def testNonBreakingSpaces(self):\n+        soup = BeautifulSoup(\"<a>&nbsp;&nbsp;</a>\",\n+                             convertEntities=BeautifulStoneSoup.HTML_ENTITIES)\n+        self.assertEquals(unicode(soup), u\"<a>\\xa0\\xa0</a>\")\n+\n+    def testWhitespaceInDeclaration(self):\n+        self.assertSoupEquals('<! DOCTYPE>', '<!DOCTYPE>')\n+\n+    def testJunkInDeclaration(self):\n+        self.assertSoupEquals('<! Foo = -8>a', '&lt;!Foo = -8&gt;a')\n+\n+    def testIncompleteDeclaration(self):\n+        self.assertSoupEquals('a<!b <p>c', 'a&lt;!b &lt;p&gt;c')\n+\n+    def testEntityReplacement(self):\n+        self.assertSoupEquals('<b>hello&nbsp;there</b>')\n+\n+    def testEntitiesInAttributeValues(self):\n+        self.assertSoupEquals('<x t=\"x&#241;\">', '<x t=\"x\\xc3\\xb1\"></x>')\n+        self.assertSoupEquals('<x t=\"x&#xf1;\">', '<x t=\"x\\xc3\\xb1\"></x>')\n+\n+        soup = BeautifulSoup('<x t=\"&gt;&trade;\">',\n+                             convertEntities=BeautifulStoneSoup.HTML_ENTITIES)\n+        self.assertEquals(unicode(soup), u'<x t=\"&gt;\\u2122\"></x>')\n+\n+        uri = \"http://crummy.com?sacr&eacute;&amp;bleu\"\n+        link = '<a href=\"%s\"></a>' % uri\n+        soup = BeautifulSoup(link)\n+        self.assertEquals(unicode(soup), link)\n+        #self.assertEquals(unicode(soup.a['href']), uri)\n+\n+        soup = BeautifulSoup(link, convertEntities=BeautifulSoup.HTML_ENTITIES)\n+        self.assertEquals(unicode(soup),\n+                          link.replace(\"&eacute;\", u\"\\xe9\"))\n+\n+        uri = \"http://crummy.com?sacr&eacute;&bleu\"\n+        link = '<a href=\"%s\"></a>' % uri\n+        soup = BeautifulSoup(link, convertEntities=BeautifulSoup.HTML_ENTITIES)\n+        self.assertEquals(unicode(soup.a['href']),\n+                          uri.replace(\"&eacute;\", u\"\\xe9\"))\n+\n+    def testNakedAmpersands(self):\n+        html = {'convertEntities':BeautifulStoneSoup.HTML_ENTITIES}\n+        soup = BeautifulStoneSoup(\"AT&T \", **html)\n+        self.assertEquals(str(soup), 'AT&amp;T ')\n+\n+        nakedAmpersandInASentence = \"AT&T was Ma Bell\"\n+        soup = BeautifulStoneSoup(nakedAmpersandInASentence,**html)\n+        self.assertEquals(str(soup), \\\n+               nakedAmpersandInASentence.replace('&','&amp;'))\n+\n+        invalidURL = '<a href=\"http://example.org?a=1&b=2;3\">foo</a>'\n+        validURL = invalidURL.replace('&','&amp;')\n+        soup = BeautifulStoneSoup(invalidURL)\n+        self.assertEquals(str(soup), validURL)\n+\n+        soup = BeautifulStoneSoup(validURL)\n+        self.assertEquals(str(soup), validURL)\n+\n+\n+class EncodeRed(SoupTest):\n+    \"\"\"Tests encoding conversion, Unicode conversion, and Microsoft\n+    smart quote fixes.\"\"\"\n+\n+    def testUnicodeDammitStandalone(self):\n+        markup = \"<foo>\\x92</foo>\"\n+        dammit = UnicodeDammit(markup)\n+        self.assertEquals(dammit.unicode, \"<foo>&#x2019;</foo>\")\n+\n+        hebrew = \"\\xed\\xe5\\xec\\xf9\"\n+        dammit = UnicodeDammit(hebrew, [\"iso-8859-8\"])\n+        self.assertEquals(dammit.unicode, u'\\u05dd\\u05d5\\u05dc\\u05e9')\n+        self.assertEquals(dammit.originalEncoding, 'iso-8859-8')\n+\n+    def testGarbageInGarbageOut(self):\n+        ascii = \"<foo>a</foo>\"\n+        asciiSoup = BeautifulStoneSoup(ascii)\n+        self.assertEquals(ascii, str(asciiSoup))\n+\n+        unicodeData = u\"<foo>\\u00FC</foo>\"\n+        utf8 = unicodeData.encode(\"utf-8\")\n+        self.assertEquals(utf8, '<foo>\\xc3\\xbc</foo>')\n+\n+        unicodeSoup = BeautifulStoneSoup(unicodeData)\n+        self.assertEquals(unicodeData, unicode(unicodeSoup))\n+        self.assertEquals(unicode(unicodeSoup.foo.string), u'\\u00FC')\n+\n+        utf8Soup = BeautifulStoneSoup(utf8, fromEncoding='utf-8')\n+        self.assertEquals(utf8, str(utf8Soup))\n+        self.assertEquals(utf8Soup.originalEncoding, \"utf-8\")\n+\n+        utf8Soup = BeautifulStoneSoup(unicodeData)\n+        self.assertEquals(utf8, str(utf8Soup))\n+        self.assertEquals(utf8Soup.originalEncoding, None)\n+\n+\n+    def testHandleInvalidCodec(self):\n+        for bad_encoding in ['.utf8', '...', 'utF---16.!']:\n+            soup = BeautifulSoup(\"R\u00e4ksm\u00f6rg\u00e5s\", fromEncoding=bad_encoding)\n+            self.assertEquals(soup.originalEncoding, 'utf-8')\n+\n+    def testUnicodeSearch(self):\n+        html = u'<html><body><h1>R\u00e4ksm\u00f6rg\u00e5s</h1></body></html>'\n+        soup = BeautifulSoup(html)\n+        self.assertEqual(soup.find(text=u'R\u00e4ksm\u00f6rg\u00e5s'),u'R\u00e4ksm\u00f6rg\u00e5s')\n+\n+    def testRewrittenXMLHeader(self):\n+        euc_jp = '<?xml version=\"1.0 encoding=\"euc-jp\"?>\\n<foo>\\n\\xa4\\xb3\\xa4\\xec\\xa4\\xcfEUC-JP\\xa4\\xc7\\xa5\\xb3\\xa1\\xbc\\xa5\\xc7\\xa5\\xa3\\xa5\\xf3\\xa5\\xb0\\xa4\\xb5\\xa4\\xec\\xa4\\xbf\\xc6\\xfc\\xcb\\xdc\\xb8\\xec\\xa4\\xce\\xa5\\xd5\\xa5\\xa1\\xa5\\xa4\\xa5\\xeb\\xa4\\xc7\\xa4\\xb9\\xa1\\xa3\\n</foo>\\n'\n+        utf8 = \"<?xml version='1.0' encoding='utf-8'?>\\n<foo>\\n\\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xafEUC-JP\\xe3\\x81\\xa7\\xe3\\x82\\xb3\\xe3\\x83\\xbc\\xe3\\x83\\x87\\xe3\\x82\\xa3\\xe3\\x83\\xb3\\xe3\\x82\\xb0\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x81\\x9f\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe8\\xaa\\x9e\\xe3\\x81\\xae\\xe3\\x83\\x95\\xe3\\x82\\xa1\\xe3\\x82\\xa4\\xe3\\x83\\xab\\xe3\\x81\\xa7\\xe3\\x81\\x99\\xe3\\x80\\x82\\n</foo>\\n\"\n+        soup = BeautifulStoneSoup(euc_jp)\n+        if soup.originalEncoding != \"euc-jp\":\n+            raise Exception(\"Test failed when parsing euc-jp document. \"\n+                            \"If you're running Python >=2.4, or you have \"\n+                            \"cjkcodecs installed, this is a real problem. \"\n+                            \"Otherwise, ignore it.\")\n+\n+        self.assertEquals(soup.originalEncoding, \"euc-jp\")\n+        self.assertEquals(str(soup), utf8)\n+\n+        old_text = \"<?xml encoding='windows-1252'><foo>\\x92</foo>\"\n+        new_text = \"<?xml version='1.0' encoding='utf-8'?><foo>&rsquo;</foo>\"\n+        self.assertSoupEquals(old_text, new_text)\n+\n+    def testRewrittenMetaTag(self):\n+        no_shift_jis_html = '''<html><head>\\n<meta http-equiv=\"Content-language\" content=\"ja\" /></head><body><pre>\\n\\x82\\xb1\\x82\\xea\\x82\\xcdShift-JIS\\x82\\xc5\\x83R\\x81[\\x83f\\x83B\\x83\\x93\\x83O\\x82\\xb3\\x82\\xea\\x82\\xbd\\x93\\xfa\\x96{\\x8c\\xea\\x82\\xcc\\x83t\\x83@\\x83C\\x83\\x8b\\x82\\xc5\\x82\\xb7\\x81B\\n</pre></body></html>'''\n+        soup = BeautifulSoup(no_shift_jis_html)\n+\n+        # Beautiful Soup used to try to rewrite the meta tag even if the\n+        # meta tag got filtered out by the strainer. This test makes\n+        # sure that doesn't happen.\n+        strainer = SoupStrainer('pre')\n+        soup = BeautifulSoup(no_shift_jis_html, parseOnlyThese=strainer)\n+        self.assertEquals(soup.contents[0].name, 'pre')\n+\n+        meta_tag = ('<meta content=\"text/html; charset=x-sjis\" '\n+                    'http-equiv=\"Content-type\" />')\n+        shift_jis_html = (\n+            '<html><head>\\n%s\\n'\n+            '<meta http-equiv=\"Content-language\" content=\"ja\" />'\n+            '</head><body><pre>\\n'\n+            '\\x82\\xb1\\x82\\xea\\x82\\xcdShift-JIS\\x82\\xc5\\x83R\\x81[\\x83f'\n+            '\\x83B\\x83\\x93\\x83O\\x82\\xb3\\x82\\xea\\x82\\xbd\\x93\\xfa\\x96{\\x8c'\n+            '\\xea\\x82\\xcc\\x83t\\x83@\\x83C\\x83\\x8b\\x82\\xc5\\x82\\xb7\\x81B\\n'\n+            '</pre></body></html>') % meta_tag\n+        soup = BeautifulSoup(shift_jis_html)\n+        if soup.originalEncoding != \"shift-jis\":\n+            raise Exception(\"Test failed when parsing shift-jis document \"\n+                            \"with meta tag '%s'.\"\n+                            \"If you're running Python >=2.4, or you have \"\n+                            \"cjkcodecs installed, this is a real problem. \"\n+                            \"Otherwise, ignore it.\" % meta_tag)\n+        self.assertEquals(soup.originalEncoding, \"shift-jis\")\n+\n+        content_type_tag = soup.meta['content']\n+        self.assertEquals(content_type_tag[content_type_tag.find('charset='):],\n+                          'charset=%SOUP-ENCODING%')\n+        content_type = str(soup.meta)\n+        index = content_type.find('charset=')\n+        self.assertEqual(content_type[index:index+len('charset=utf8')+1],\n+                         'charset=utf-8')\n+        content_type = soup.meta.__str__('shift-jis')\n+        index = content_type.find('charset=')\n+        self.assertEqual(content_type[index:index+len('charset=shift-jis')],\n+                         'charset=shift-jis')\n+\n+        self.assertEquals(str(soup), (\n+                '<html><head>\\n'\n+                '<meta content=\"text/html; charset=utf-8\" '\n+                'http-equiv=\"Content-type\" />\\n'\n+                '<meta http-equiv=\"Content-language\" content=\"ja\" />'\n+                '</head><body><pre>\\n'\n+                '\\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xafShift-JIS\\xe3\\x81\\xa7\\xe3'\n+                '\\x82\\xb3\\xe3\\x83\\xbc\\xe3\\x83\\x87\\xe3\\x82\\xa3\\xe3\\x83\\xb3\\xe3'\n+                '\\x82\\xb0\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x81\\x9f\\xe6\\x97\\xa5\\xe6'\n+                '\\x9c\\xac\\xe8\\xaa\\x9e\\xe3\\x81\\xae\\xe3\\x83\\x95\\xe3\\x82\\xa1\\xe3'\n+                '\\x82\\xa4\\xe3\\x83\\xab\\xe3\\x81\\xa7\\xe3\\x81\\x99\\xe3\\x80\\x82\\n'\n+                '</pre></body></html>'))\n+        self.assertEquals(soup.renderContents(\"shift-jis\"),\n+                          shift_jis_html.replace('x-sjis', 'shift-jis'))\n+\n+        isolatin =\"\"\"<html><meta http-equiv=\"Content-type\" content=\"text/html; charset=ISO-Latin-1\" />Sacr\\xe9 bleu!</html>\"\"\"\n+        soup = BeautifulSoup(isolatin)\n+        self.assertSoupEquals(soup.__str__(\"utf-8\"),\n+                              isolatin.replace(\"ISO-Latin-1\", \"utf-8\").replace(\"\\xe9\", \"\\xc3\\xa9\"))\n+\n+    def testHebrew(self):\n+        iso_8859_8= '<HEAD>\\n<TITLE>Hebrew (ISO 8859-8) in Visual Directionality</TITLE>\\n\\n\\n\\n</HEAD>\\n<BODY>\\n<H1>Hebrew (ISO 8859-8) in Visual Directionality</H1>\\n\\xed\\xe5\\xec\\xf9\\n</BODY>\\n'\n+        utf8 = '<head>\\n<title>Hebrew (ISO 8859-8) in Visual Directionality</title>\\n</head>\\n<body>\\n<h1>Hebrew (ISO 8859-8) in Visual Directionality</h1>\\n\\xd7\\x9d\\xd7\\x95\\xd7\\x9c\\xd7\\xa9\\n</body>\\n'\n+        soup = BeautifulStoneSoup(iso_8859_8, fromEncoding=\"iso-8859-8\")\n+        self.assertEquals(str(soup), utf8)\n+\n+    def testSmartQuotesNotSoSmartAnymore(self):\n+        self.assertSoupEquals(\"\\x91Foo\\x92 <!--blah-->\",\n+                              '&lsquo;Foo&rsquo; <!--blah-->')\n+\n+    def testDontConvertSmartQuotesWhenAlsoConvertingEntities(self):\n+        smartQuotes = \"Il a dit, \\x8BSacr&eacute; bl&#101;u!\\x9b\"\n+        soup = BeautifulSoup(smartQuotes)\n+        self.assertEquals(str(soup),\n+                          'Il a dit, &lsaquo;Sacr&eacute; bl&#101;u!&rsaquo;')\n+        soup = BeautifulSoup(smartQuotes, convertEntities=\"html\")\n+        self.assertEquals(str(soup),\n+                          'Il a dit, \\xe2\\x80\\xb9Sacr\\xc3\\xa9 bleu!\\xe2\\x80\\xba')\n+\n+    def testDontSeeSmartQuotesWhereThereAreNone(self):\n+        utf_8 = \"\\343\\202\\261\\343\\203\\274\\343\\202\\277\\343\\202\\244 Watch\"\n+        self.assertSoupEquals(utf_8)\n+\n+\n+class Whitewash(SoupTest):\n+    \"\"\"Test whitespace preservation.\"\"\"\n+\n+    def testPreservedWhitespace(self):\n+        self.assertSoupEquals(\"<pre>   </pre>\")\n+        self.assertSoupEquals(\"<pre> woo  </pre>\")\n+\n+    def testCollapsedWhitespace(self):\n+        self.assertSoupEquals(\"<p>   </p>\", \"<p> </p>\")\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()"
            },
            {
                "sha": "6198f516e48a50a85826af93d0d1a85be38b5679",
                "filename": "github-analytics/libs/BeautifulSoup-3.2.2/README.md",
                "status": "added",
                "additions": 18,
                "deletions": 0,
                "changes": 18,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/BeautifulSoup-3.2.2/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/BeautifulSoup-3.2.2/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/BeautifulSoup-3.2.2/README.md?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,18 @@\n+Beautiful Soup is a library that makes it easy to scrape information\n+from web pages. It sits atop an HTML or XML parser, providing Pythonic\n+idioms for iterating, searching, and modifying the parse tree.\n+\n+# Discontinuation notice\n+\n+You should use the 'beautifulsoup4' package instead of this package.\n+\n+Development on the 3.x series of Beautiful Soup ended in 2011, and the\n+series will be discontinued on January 1, 2021, one year after the\n+Python 2 sunsetting date. At some point after that, the\n+'beautifulsoup' pip package will be updated to a recent version of\n+Beautiful Soup. This will free up the 'beautifulsoup' package name to\n+be used by a more recent release.\n+\n+If you're relying on version 3 of Beautiful Soup, you really ought to\n+port your code to Python 3. A relatively small part of this work will\n+be migrating your Beautiful Soup code to Beautiful Soup 4."
            },
            {
                "sha": "4413a904de5a5fa8d93a225de178e79ae9930227",
                "filename": "github-analytics/libs/BeautifulSoup-3.2.2/setup.py",
                "status": "added",
                "additions": 36,
                "deletions": 0,
                "changes": 36,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/BeautifulSoup-3.2.2/setup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/BeautifulSoup-3.2.2/setup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/BeautifulSoup-3.2.2/setup.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,36 @@\n+# The very first thing we do is give a useful error if someone is\n+# running this code under Python 3.\n+\"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"<>\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n+\n+from setuptools import (\n+    setup,\n+    find_packages,\n+)\n+from BeautifulSoup import __version__\n+\n+with open(\"README.md\", \"r\") as fh:\n+    long_description = fh.read()\n+\n+setup(\n+    name=\"BeautifulSoup\",\n+    version = __version__,\n+    author=\"Leonard Richardson\",\n+    author_email='leonardr@segfault.org',\n+    url=\"http://www.crummy.com/software/BeautifulSoup/\",\n+    download_url = \"http://www.crummy.com/software/BeautifulSoup/download/\",\n+    description=\"Screen-scraping library\",\n+    long_description=long_description,\n+    long_description_content_type=\"text/markdown\",\n+    license=\"MIT\",\n+    py_modules=['BeautifulSoup', 'BeautifulSoupTests'],\n+    classifiers=[\"Development Status :: 5 - Production/Stable\",\n+                 \"Intended Audience :: Developers\",\n+                 \"License :: OSI Approved :: Python Software Foundation License\",\n+                 \"Programming Language :: Python\",\n+                 \"Programming Language :: Python :: 2.7\",\n+                 \"Topic :: Text Processing :: Markup :: HTML\",\n+                 \"Topic :: Text Processing :: Markup :: XML\",\n+                 \"Topic :: Text Processing :: Markup :: SGML\",\n+                 \"Topic :: Software Development :: Libraries :: Python Modules\",\n+             ],\n+)"
            },
            {
                "sha": "afd39cff3bc2d9d449d2674f27a2a0d3e450c8d9",
                "filename": "github-analytics/libs/Flask-Cassandra-0.14/PKG-INFO",
                "status": "added",
                "additions": 25,
                "deletions": 0,
                "changes": 25,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/Flask-Cassandra-0.14/PKG-INFO",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/Flask-Cassandra-0.14/PKG-INFO",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/Flask-Cassandra-0.14/PKG-INFO?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,25 @@\n+Metadata-Version: 1.1\n+Name: Flask-Cassandra\n+Version: 0.14\n+Summary: Provides a connection to a Cassandra cluster in a Flask app\n+Home-page: http://terbiumlabs.com/flask-cassandra/\n+Author: Michael Moore\n+Author-email: michael@terbiumlabs.com\n+License: BSD\n+Description: \n+        Flask-Cassandra\n+        -------------\n+        \n+        Flask-Cassandra provides an application-level connection\n+        to an Apache Cassandra database. This connection can be\n+        used to interact with a Cassandra cluster.\n+        \n+        \n+Platform: any\n+Classifier: Environment :: Web Environment\n+Classifier: Intended Audience :: Developers\n+Classifier: License :: OSI Approved :: BSD License\n+Classifier: Operating System :: OS Independent\n+Classifier: Programming Language :: Python\n+Classifier: Topic :: Internet :: WWW/HTTP :: Dynamic Content\n+Classifier: Topic :: Software Development :: Libraries :: Python Modules"
            },
            {
                "sha": "b6e4a39fcab36d79b5e1403d80830c0ccc000e11",
                "filename": "github-analytics/libs/Flask-Cassandra-0.14/flask_cassandra.py",
                "status": "added",
                "additions": 77,
                "deletions": 0,
                "changes": 77,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/Flask-Cassandra-0.14/flask_cassandra.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/Flask-Cassandra-0.14/flask_cassandra.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/Flask-Cassandra-0.14/flask_cassandra.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,77 @@\n+# -*- coding: utf-8 -*-\n+'''\n+    flask-cassandra\n+    ---------------\n+    Flask-Cassandra provides an application-level connection\n+    to an Apache Cassandra database. This connection can be\n+    used to interact with a Cassandra cluster.\n+\n+    :copyright: (c) 2015 by Terbium Labs.\n+    :license: BSD, see LICENSE for more details.\n+'''\n+\n+__version_info__ = ('0', '1', '4')\n+__version__ = '.'.join(__version_info__)\n+__author__ = 'Michael Moore'\n+__license__ = 'BSD'\n+__copyright__ = '(c) 2015 by TerbiumLabs'\n+\n+from cassandra.cluster import Cluster\n+import logging\n+\n+from flask import current_app\n+\n+log = logging.getLogger(__name__)\n+\n+try:\n+    from flask import _app_ctx_stack as stack\n+except ImportError:\n+    from flask import _request_ctx_stack as stack\n+\n+\n+try:\n+    unicode\n+except NameError:  # Python3\n+    unicode = str\n+\n+\n+class CassandraCluster(object):\n+\n+    def __init__(self, app=None):\n+        self.app = app\n+        self.cluster = None\n+        if app is not None:\n+            self.init_app(app)\n+\n+    def init_app(self, app):\n+        app.config.setdefault('CASSANDRA_CLUSTER', ':memory:')\n+        if hasattr(app, 'teardown_appcontext'):\n+            app.teardown_appcontext(self.teardown)\n+        else:\n+            app.teardown_request(self.teardown)\n+\n+    def connect(self):\n+        log.debug(\"Connecting to CASSANDRA NODES {}\".format(current_app.config['CASSANDRA_NODES']))\n+        if self.cluster is None:\n+            if isinstance(current_app.config['CASSANDRA_NODES'], (list, tuple)):\n+                self.cluster = Cluster(current_app.config['CASSANDRA_NODES'])\n+            elif isinstance(current_app.config['CASSANDRA_NODES'], (str, unicode)):\n+                self.cluster = Cluster([current_app.config['CASSANDRA_NODES']])\n+            else:\n+                raise TypeError(\"CASSANDRA_NODES must be defined as a list, tuple, string, or unicode object.\")\n+\n+        online_cluster = self.cluster.connect()\n+        return online_cluster\n+\n+    def teardown(self, exception):\n+        ctx = stack.top\n+        if hasattr(ctx, 'cassandra_cluster'):\n+            ctx.cassandra_cluster.shutdown()\n+\n+    @property\n+    def connection(self):\n+        ctx = stack.top\n+        if ctx is not None:\n+            if not hasattr(ctx, 'cassandra_cluster'):\n+                ctx.cassandra_cluster = self.connect()\n+            return ctx.cassandra_cluster"
            },
            {
                "sha": "861a9f554263efb088d8636c4f17a30696e495ad",
                "filename": "github-analytics/libs/Flask-Cassandra-0.14/setup.cfg",
                "status": "added",
                "additions": 5,
                "deletions": 0,
                "changes": 5,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/Flask-Cassandra-0.14/setup.cfg",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/Flask-Cassandra-0.14/setup.cfg",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/Flask-Cassandra-0.14/setup.cfg?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,5 @@\n+[egg_info]\n+tag_build = \n+tag_date = 0\n+tag_svn_revision = 0\n+"
            },
            {
                "sha": "9538d34fc73f63d9f53c4a34c231a99ca41579d4",
                "filename": "github-analytics/libs/Flask-Cassandra-0.14/setup.py",
                "status": "added",
                "additions": 39,
                "deletions": 0,
                "changes": 39,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/Flask-Cassandra-0.14/setup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/Flask-Cassandra-0.14/setup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/Flask-Cassandra-0.14/setup.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,39 @@\n+\"\"\"\n+Flask-Cassandra\n+-------------\n+\n+Flask-Cassandra provides an application-level connection\n+to an Apache Cassandra database. This connection can be\n+used to interact with a Cassandra cluster.\n+\n+\"\"\"\n+from setuptools import setup\n+\n+\n+setup(\n+    name='Flask-Cassandra',\n+    version='0.14',\n+    url='http://terbiumlabs.com/flask-cassandra/',\n+    license='BSD',\n+    author='Michael Moore',\n+    author_email='michael@terbiumlabs.com',\n+    description='Provides a connection to a Cassandra cluster in a Flask app',\n+    long_description=__doc__,\n+    py_modules=['flask_cassandra'],\n+    zip_safe=False,\n+    include_package_data=True,\n+    platforms='any',\n+    install_requires=[\n+        'Flask',\n+        'cassandra-driver'\n+    ],\n+    classifiers=[\n+        'Environment :: Web Environment',\n+        'Intended Audience :: Developers',\n+        'License :: OSI Approved :: BSD License',\n+        'Operating System :: OS Independent',\n+        'Programming Language :: Python',\n+        'Topic :: Internet :: WWW/HTTP :: Dynamic Content',\n+        'Topic :: Software Development :: Libraries :: Python Modules'\n+    ]\n+)"
            },
            {
                "sha": "68c771a099958211169377d766a7389422f5573d",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/LICENSE",
                "status": "added",
                "additions": 176,
                "deletions": 0,
                "changes": 176,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/LICENSE",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/LICENSE",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/LICENSE?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,176 @@\n+\n+                                 Apache License\n+                           Version 2.0, January 2004\n+                        http://www.apache.org/licenses/\n+\n+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n+\n+   1. Definitions.\n+\n+      \"License\" shall mean the terms and conditions for use, reproduction,\n+      and distribution as defined by Sections 1 through 9 of this document.\n+\n+      \"Licensor\" shall mean the copyright owner or entity authorized by\n+      the copyright owner that is granting the License.\n+\n+      \"Legal Entity\" shall mean the union of the acting entity and all\n+      other entities that control, are controlled by, or are under common\n+      control with that entity. For the purposes of this definition,\n+      \"control\" means (i) the power, direct or indirect, to cause the\n+      direction or management of such entity, whether by contract or\n+      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n+      outstanding shares, or (iii) beneficial ownership of such entity.\n+\n+      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n+      exercising permissions granted by this License.\n+\n+      \"Source\" form shall mean the preferred form for making modifications,\n+      including but not limited to software source code, documentation\n+      source, and configuration files.\n+\n+      \"Object\" form shall mean any form resulting from mechanical\n+      transformation or translation of a Source form, including but\n+      not limited to compiled object code, generated documentation,\n+      and conversions to other media types.\n+\n+      \"Work\" shall mean the work of authorship, whether in Source or\n+      Object form, made available under the License, as indicated by a\n+      copyright notice that is included in or attached to the work\n+      (an example is provided in the Appendix below).\n+\n+      \"Derivative Works\" shall mean any work, whether in Source or Object\n+      form, that is based on (or derived from) the Work and for which the\n+      editorial revisions, annotations, elaborations, or other modifications\n+      represent, as a whole, an original work of authorship. For the purposes\n+      of this License, Derivative Works shall not include works that remain\n+      separable from, or merely link (or bind by name) to the interfaces of,\n+      the Work and Derivative Works thereof.\n+\n+      \"Contribution\" shall mean any work of authorship, including\n+      the original version of the Work and any modifications or additions\n+      to that Work or Derivative Works thereof, that is intentionally\n+      submitted to Licensor for inclusion in the Work by the copyright owner\n+      or by an individual or Legal Entity authorized to submit on behalf of\n+      the copyright owner. For the purposes of this definition, \"submitted\"\n+      means any form of electronic, verbal, or written communication sent\n+      to the Licensor or its representatives, including but not limited to\n+      communication on electronic mailing lists, source code control systems,\n+      and issue tracking systems that are managed by, or on behalf of, the\n+      Licensor for the purpose of discussing and improving the Work, but\n+      excluding communication that is conspicuously marked or otherwise\n+      designated in writing by the copyright owner as \"Not a Contribution.\"\n+\n+      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n+      on behalf of whom a Contribution has been received by Licensor and\n+      subsequently incorporated within the Work.\n+\n+   2. Grant of Copyright License. Subject to the terms and conditions of\n+      this License, each Contributor hereby grants to You a perpetual,\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n+      copyright license to reproduce, prepare Derivative Works of,\n+      publicly display, publicly perform, sublicense, and distribute the\n+      Work and such Derivative Works in Source or Object form.\n+\n+   3. Grant of Patent License. Subject to the terms and conditions of\n+      this License, each Contributor hereby grants to You a perpetual,\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n+      (except as stated in this section) patent license to make, have made,\n+      use, offer to sell, sell, import, and otherwise transfer the Work,\n+      where such license applies only to those patent claims licensable\n+      by such Contributor that are necessarily infringed by their\n+      Contribution(s) alone or by combination of their Contribution(s)\n+      with the Work to which such Contribution(s) was submitted. If You\n+      institute patent litigation against any entity (including a\n+      cross-claim or counterclaim in a lawsuit) alleging that the Work\n+      or a Contribution incorporated within the Work constitutes direct\n+      or contributory patent infringement, then any patent licenses\n+      granted to You under this License for that Work shall terminate\n+      as of the date such litigation is filed.\n+\n+   4. Redistribution. You may reproduce and distribute copies of the\n+      Work or Derivative Works thereof in any medium, with or without\n+      modifications, and in Source or Object form, provided that You\n+      meet the following conditions:\n+\n+      (a) You must give any other recipients of the Work or\n+          Derivative Works a copy of this License; and\n+\n+      (b) You must cause any modified files to carry prominent notices\n+          stating that You changed the files; and\n+\n+      (c) You must retain, in the Source form of any Derivative Works\n+          that You distribute, all copyright, patent, trademark, and\n+          attribution notices from the Source form of the Work,\n+          excluding those notices that do not pertain to any part of\n+          the Derivative Works; and\n+\n+      (d) If the Work includes a \"NOTICE\" text file as part of its\n+          distribution, then any Derivative Works that You distribute must\n+          include a readable copy of the attribution notices contained\n+          within such NOTICE file, excluding those notices that do not\n+          pertain to any part of the Derivative Works, in at least one\n+          of the following places: within a NOTICE text file distributed\n+          as part of the Derivative Works; within the Source form or\n+          documentation, if provided along with the Derivative Works; or,\n+          within a display generated by the Derivative Works, if and\n+          wherever such third-party notices normally appear. The contents\n+          of the NOTICE file are for informational purposes only and\n+          do not modify the License. You may add Your own attribution\n+          notices within Derivative Works that You distribute, alongside\n+          or as an addendum to the NOTICE text from the Work, provided\n+          that such additional attribution notices cannot be construed\n+          as modifying the License.\n+\n+      You may add Your own copyright statement to Your modifications and\n+      may provide additional or different license terms and conditions\n+      for use, reproduction, or distribution of Your modifications, or\n+      for any such Derivative Works as a whole, provided Your use,\n+      reproduction, and distribution of the Work otherwise complies with\n+      the conditions stated in this License.\n+\n+   5. Submission of Contributions. Unless You explicitly state otherwise,\n+      any Contribution intentionally submitted for inclusion in the Work\n+      by You to the Licensor shall be under the terms and conditions of\n+      this License, without any additional terms or conditions.\n+      Notwithstanding the above, nothing herein shall supersede or modify\n+      the terms of any separate license agreement you may have executed\n+      with Licensor regarding such Contributions.\n+\n+   6. Trademarks. This License does not grant permission to use the trade\n+      names, trademarks, service marks, or product names of the Licensor,\n+      except as required for reasonable and customary use in describing the\n+      origin of the Work and reproducing the content of the NOTICE file.\n+\n+   7. Disclaimer of Warranty. Unless required by applicable law or\n+      agreed to in writing, Licensor provides the Work (and each\n+      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+      implied, including, without limitation, any warranties or conditions\n+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n+      PARTICULAR PURPOSE. You are solely responsible for determining the\n+      appropriateness of using or redistributing the Work and assume any\n+      risks associated with Your exercise of permissions under this License.\n+\n+   8. Limitation of Liability. In no event and under no legal theory,\n+      whether in tort (including negligence), contract, or otherwise,\n+      unless required by applicable law (such as deliberate and grossly\n+      negligent acts) or agreed to in writing, shall any Contributor be\n+      liable to You for damages, including any direct, indirect, special,\n+      incidental, or consequential damages of any character arising as a\n+      result of this License or out of the use or inability to use the\n+      Work (including but not limited to damages for loss of goodwill,\n+      work stoppage, computer failure or malfunction, or any and all\n+      other commercial damages or losses), even if such Contributor\n+      has been advised of the possibility of such damages.\n+\n+   9. Accepting Warranty or Additional Liability. While redistributing\n+      the Work or Derivative Works thereof, You may choose to offer,\n+      and charge a fee for, acceptance of support, warranty, indemnity,\n+      or other liability obligations and/or rights consistent with this\n+      License. However, in accepting such obligations, You may act only\n+      on Your own behalf and on Your sole responsibility, not on behalf\n+      of any other Contributor, and only if You agree to indemnify,\n+      defend, and hold each Contributor harmless for any liability\n+      incurred by, or claims asserted against, such Contributor by reason\n+      of your accepting any such warranty or additional liability.\n+"
            },
            {
                "sha": "b5bad501e41ffb70d9cacbaf0b860b6ef0ede6e7",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/README",
                "status": "added",
                "additions": 161,
                "deletions": 0,
                "changes": 161,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/README",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/README",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/README?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,161 @@\n+Python Elasticsearch Client\n+===========================\n+\n+Official low-level client for Elasticsearch. Its goal is to provide common\n+ground for all Elasticsearch-related code in Python; because of this it tries\n+to be opinion-free and very extendable.\n+\n+For a more high level client library with more limited scope, have a look at\n+`elasticsearch-dsl`_ - a more pythonic library sitting on top of\n+``elasticsearch-py``.\n+\n+It provides a more convenient and idiomatic way to write and manipulate\n+`queries`_. It stays close to the Elasticsearch JSON DSL, mirroring its\n+terminology and structure while exposing the whole range of the DSL from Python\n+either directly using defined classes or a queryset-like expressions.\n+\n+It also provides an optional `persistence layer`_ for working with documents as\n+Python objects in an ORM-like fashion: defining mappings, retrieving and saving\n+documents, wrapping the document data in user-defined classes.\n+\n+.. _elasticsearch-dsl: https://elasticsearch-dsl.readthedocs.io/\n+.. _queries: https://elasticsearch-dsl.readthedocs.io/en/latest/search_dsl.html\n+.. _persistence layer: https://elasticsearch-dsl.readthedocs.io/en/latest/persistence.html#doctype\n+\n+Compatibility\n+-------------\n+\n+The library is compatible with all Elasticsearch versions since ``0.90.x`` but you\n+**have to use a matching major version**:\n+\n+For **Elasticsearch 7.0** and later, use the major version 7 (``7.x.y``) of the\n+library.\n+\n+For **Elasticsearch 6.0** and later, use the major version 6 (``6.x.y``) of the\n+library.\n+\n+For **Elasticsearch 5.0** and later, use the major version 5 (``5.x.y``) of the\n+library.\n+\n+For **Elasticsearch 2.0** and later, use the major version 2 (``2.x.y``) of the\n+library, and so on.\n+\n+The recommended way to set your requirements in your `setup.py` or\n+`requirements.txt` is::\n+\n+    # Elasticsearch 7.x\n+    elasticsearch>=7.0.0,<8.0.0\n+\n+    # Elasticsearch 6.x\n+    elasticsearch>=6.0.0,<7.0.0\n+\n+    # Elasticsearch 5.x\n+    elasticsearch>=5.0.0,<6.0.0\n+\n+    # Elasticsearch 2.x\n+    elasticsearch>=2.0.0,<3.0.0\n+\n+If you have a need to have multiple versions installed at the same time older\n+versions are also released as ``elasticsearch2`` and ``elasticsearch5``.\n+\n+Installation\n+------------\n+\n+Install the ``elasticsearch`` package with `pip\n+<https://pypi.python.org/pypi/elasticsearch>`_::\n+\n+    pip install elasticsearch\n+\n+\n+Example use\n+-----------\n+\n+Simple use-case::\n+\n+    >>> from datetime import datetime\n+    >>> from elasticsearch import Elasticsearch\n+\n+    # by default we connect to localhost:9200\n+    >>> es = Elasticsearch()\n+\n+    # create an index in elasticsearch, ignore status code 400 (index already exists)\n+    >>> es.indices.create(index='my-index', ignore=400)\n+    {'acknowledged': True, 'shards_acknowledged': True, 'index': 'my-index'}\n+\n+    # datetimes will be serialized\n+    >>> es.index(index=\"my-index\", id=42, body={\"any\": \"data\", \"timestamp\": datetime.now()})\n+    {'_index': 'my-index',\n+     '_type': '_doc',\n+     '_id': '42',\n+     '_version': 1,\n+     'result': 'created',\n+     '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n+     '_seq_no': 0,\n+     '_primary_term': 1}\n+\n+    # but not deserialized\n+    >>> es.get(index=\"my-index\", id=42)['_source']\n+    {'any': 'data', 'timestamp': '2019-05-17T17:28:10.329598'}\n+\n+`Full documentation`_.\n+\n+.. _Full documentation: https://elasticsearch-py.readthedocs.io/\n+\n+Elastic Cloud (and SSL) use-case::\n+\n+    >>> from elasticsearch import Elasticsearch\n+    >>> es = Elasticsearch(cloud_id=\"<some_long_cloud_id>\", http_auth=('elastic','yourpassword'))\n+    >>> es.info()\n+\n+Using SSL Context with a self-signed cert use-case::\n+\n+    >>> from elasticsearch import Elasticsearch\n+    >>> from ssl import create_default_context\n+\n+    >>> context = create_default_context(cafile=\"path/to/cafile.pem\")\n+    >>> es = Elasticsearch(\"https://elasticsearch.url:port\", ssl_context=context, http_auth=('elastic','yourpassword'))\n+    >>> es.info()\n+\n+\n+\n+Features\n+--------\n+\n+The client's features include:\n+\n+ * translating basic Python data types to and from json (datetimes are not\n+   decoded for performance reasons)\n+ * configurable automatic discovery of cluster nodes\n+ * persistent connections\n+ * load balancing (with pluggable selection strategy) across all available nodes\n+ * failed connection penalization (time based - failed connections won't be\n+   retried until a timeout is reached)\n+ * support for ssl and http authentication\n+ * thread safety\n+ * pluggable architecture\n+\n+\n+License\n+-------\n+\n+Copyright 2019 Elasticsearch\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+Build status\n+------------\n+.. image:: https://readthedocs.org/projects/elasticsearch-py/badge/?version=latest&style=flat\n+   :target: https://elasticsearch-py.readthedocs.io/en/master/\n+\n+.. image:: https://clients-ci.elastic.co/job/elastic+elasticsearch-py+master/badge/icon\n+   :target: https://clients-ci.elastic.co/job/elastic+elasticsearch-py+master/"
            },
            {
                "sha": "a4f42e5aebdc6e6e72f4a78c68b336e2b836049d",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/__init__.py",
                "status": "added",
                "additions": 29,
                "deletions": 0,
                "changes": 29,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/__init__.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,29 @@\n+# flake8: noqa\n+from __future__ import absolute_import\n+\n+VERSION = (7, 6, 0)\n+__version__ = VERSION\n+__versionstr__ = \".\".join(map(str, VERSION))\n+\n+import logging\n+\n+try:  # Python 2.7+\n+    from logging import NullHandler\n+except ImportError:\n+\n+    class NullHandler(logging.Handler):\n+        def emit(self, record):\n+            pass\n+\n+\n+import sys\n+\n+logger = logging.getLogger(\"elasticsearch7\")\n+logger.addHandler(logging.NullHandler())\n+\n+from .client import Elasticsearch\n+from .transport import Transport\n+from .connection_pool import ConnectionPool, ConnectionSelector, RoundRobinSelector\n+from .serializer import JSONSerializer\n+from .connection import Connection, RequestsHttpConnection, Urllib3HttpConnection\n+from .exceptions import *"
            },
            {
                "sha": "387440aa393a134c496ca0fb93e66b083af7dc9e",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/__init__.py",
                "status": "added",
                "additions": 2034,
                "deletions": 0,
                "changes": 2034,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/__init__.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,2034 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+import logging\n+\n+from ..transport import Transport\n+from ..exceptions import TransportError\n+from ..compat import string_types, urlparse, unquote\n+from .indices import IndicesClient\n+from .ingest import IngestClient\n+from .cluster import ClusterClient\n+from .cat import CatClient\n+from .nodes import NodesClient\n+from .remote import RemoteClient\n+from .snapshot import SnapshotClient\n+from .tasks import TasksClient\n+from .xpack import XPackClient\n+from .utils import query_params, _make_path, SKIP_IN_PATH, _bulk_body\n+\n+# xpack APIs\n+from .ccr import CcrClient\n+from .data_frame import Data_FrameClient\n+from .deprecation import DeprecationClient\n+from .graph import GraphClient\n+from .ilm import IlmClient\n+from .license import LicenseClient\n+from .migration import MigrationClient\n+from .ml import MlClient\n+from .monitoring import MonitoringClient\n+from .rollup import RollupClient\n+from .security import SecurityClient\n+from .sql import SqlClient\n+from .ssl import SslClient\n+from .watcher import WatcherClient\n+from .enrich import EnrichClient\n+from .slm import SlmClient\n+from .transform import TransformClient\n+\n+\n+logger = logging.getLogger(\"elasticsearch7\")\n+\n+\n+def _normalize_hosts(hosts):\n+    \"\"\"\n+    Helper function to transform hosts argument to\n+    :class:`~elasticsearch7.Elasticsearch` to a list of dicts.\n+    \"\"\"\n+    # if hosts are empty, just defer to defaults down the line\n+    if hosts is None:\n+        return [{}]\n+\n+    # passed in just one string\n+    if isinstance(hosts, string_types):\n+        hosts = [hosts]\n+\n+    out = []\n+    # normalize hosts to dicts\n+    for host in hosts:\n+        if isinstance(host, string_types):\n+            if \"://\" not in host:\n+                host = \"//%s\" % host\n+\n+            parsed_url = urlparse(host)\n+            h = {\"host\": parsed_url.hostname}\n+\n+            if parsed_url.port:\n+                h[\"port\"] = parsed_url.port\n+\n+            if parsed_url.scheme == \"https\":\n+                h[\"port\"] = parsed_url.port or 443\n+                h[\"use_ssl\"] = True\n+\n+            if parsed_url.username or parsed_url.password:\n+                h[\"http_auth\"] = \"%s:%s\" % (\n+                    unquote(parsed_url.username),\n+                    unquote(parsed_url.password),\n+                )\n+\n+            if parsed_url.path and parsed_url.path != \"/\":\n+                h[\"url_prefix\"] = parsed_url.path\n+\n+            out.append(h)\n+        else:\n+            out.append(host)\n+    return out\n+\n+\n+class Elasticsearch(object):\n+    \"\"\"\n+    Elasticsearch low-level client. Provides a straightforward mapping from\n+    Python to ES REST endpoints.\n+\n+    The instance has attributes ``cat``, ``cluster``, ``indices``, ``ingest``,\n+    ``nodes``, ``snapshot`` and ``tasks`` that provide access to instances of\n+    :class:`~elasticsearch7.client.CatClient`,\n+    :class:`~elasticsearchelasticsearch7.client.ClusterClient`,\n+    :class:`~elasticsearch7.client.IndicesClient`,\n+    :class:`~elasticsearch7.client.IngestClient`,\n+    :class:`~elasticsearch7.client.NodesClient`,\n+    :class:`~elasticsearch7.client.SnapshotClient` and\n+    :class:`~elasticsearch7.client.TasksClient` respectively. This is the\n+    preferred (and only supported) way to get access to those classes and their\n+    methods.\n+\n+    You can specify your own connection class which should be used by providing\n+    the ``connection_class`` parameter::\n+\n+        # create connection to localhost using the ThriftConnection\n+        es = Elasticsearch(connection_class=ThriftConnection)\n+\n+    If you want to turn on :ref:`sniffing` you have several options (described\n+    in :class:`~elasticsearch7.Transport`)::\n+\n+        # create connection that will automatically inspect the cluster to get\n+        # the list of active nodes. Start with nodes running on 'esnode1' and\n+        # 'esnode2'\n+        es = Elasticsearch(\n+            ['esnode1', 'esnode2'],\n+            # sniff before doing anything\n+            sniff_on_start=True,\n+            # refresh nodes after a node fails to respond\n+            sniff_on_connection_fail=True,\n+            # and also every 60 seconds\n+            sniffer_timeout=60\n+        )\n+\n+    Different hosts can have different parameters, use a dictionary per node to\n+    specify those::\n+\n+        # connect to localhost directly and another node using SSL on port 443\n+        # and an url_prefix. Note that ``port`` needs to be an int.\n+        es = Elasticsearch([\n+            {'host': 'localhost'},\n+            {'host': 'othernode', 'port': 443, 'url_prefix': 'es', 'use_ssl': True},\n+        ])\n+\n+    If using SSL, there are several parameters that control how we deal with\n+    certificates (see :class:`~elasticsearch7.Urllib3HttpConnection` for\n+    detailed description of the options)::\n+\n+        es = Elasticsearch(\n+            ['localhost:443', 'other_host:443'],\n+            # turn on SSL\n+            use_ssl=True,\n+            # make sure we verify SSL certificates\n+            verify_certs=True,\n+            # provide a path to CA certs on disk\n+            ca_certs='/path/to/CA_certs'\n+        )\n+\n+    If using SSL, but don't verify the certs, a warning message is showed\n+    optionally (see :class:`~elasticsearch7.Urllib3HttpConnection` for\n+    detailed description of the options)::\n+\n+        es = Elasticsearch(\n+            ['localhost:443', 'other_host:443'],\n+            # turn on SSL\n+            use_ssl=True,\n+            # no verify SSL certificates\n+            verify_certs=False,\n+            # don't show warnings about ssl certs verification\n+            ssl_show_warn=False\n+        )\n+\n+    SSL client authentication is supported\n+    (see :class:`~elasticsearch7.Urllib3HttpConnection` for\n+    detailed description of the options)::\n+\n+        es = Elasticsearch(\n+            ['localhost:443', 'other_host:443'],\n+            # turn on SSL\n+            use_ssl=True,\n+            # make sure we verify SSL certificates\n+            verify_certs=True,\n+            # provide a path to CA certs on disk\n+            ca_certs='/path/to/CA_certs',\n+            # PEM formatted SSL client certificate\n+            client_cert='/path/to/clientcert.pem',\n+            # PEM formatted SSL client key\n+            client_key='/path/to/clientkey.pem'\n+        )\n+\n+    Alternatively you can use RFC-1738 formatted URLs, as long as they are not\n+    in conflict with other options::\n+\n+        es = Elasticsearch(\n+            [\n+                'http://user:secret@localhost:9200/',\n+                'https://user:secret@other_host:443/production'\n+            ],\n+            verify_certs=True\n+        )\n+\n+    By default, `JSONSerializer\n+    <https://github.com/elastic/elasticsearch/blob/master/elasticsearch/serializer.py#L24>`_\n+    is used to encode all outgoing requests.\n+    However, you can implement your own custom serializer::\n+\n+        from elasticsearch7.serializer import JSONSerializer\n+\n+        class SetEncoder(JSONSerializer):\n+            def default(self, obj):\n+                if isinstance(obj, set):\n+                    return list(obj)\n+                if isinstance(obj, Something):\n+                    return 'CustomSomethingRepresentation'\n+                return JSONSerializer.default(self, obj)\n+\n+        es = Elasticsearch(serializer=SetEncoder())\n+\n+    \"\"\"\n+\n+    def __init__(self, hosts=None, transport_class=Transport, **kwargs):\n+        \"\"\"\n+        :arg hosts: list of nodes, or a single node, we should connect to.\n+            Node should be a dictionary ({\"host\": \"localhost\", \"port\": 9200}),\n+            the entire dictionary will be passed to the :class:`~elasticsearch7.Connection`\n+            class as kwargs, or a string in the format of ``host[:port]`` which will be\n+            translated to a dictionary automatically.  If no value is given the\n+            :class:`~elasticsearch7.Connection` class defaults will be used.\n+\n+        :arg transport_class: :class:`~elasticsearch7.Transport` subclass to use.\n+\n+        :arg kwargs: any additional arguments will be passed on to the\n+            :class:`~elasticsearch7.Transport` class and, subsequently, to the\n+            :class:`~elasticsearch7.Connection` instances.\n+        \"\"\"\n+        self.transport = transport_class(_normalize_hosts(hosts), **kwargs)\n+\n+        # namespaced clients for compatibility with API names\n+        self.indices = IndicesClient(self)\n+        self.ingest = IngestClient(self)\n+        self.cluster = ClusterClient(self)\n+        self.cat = CatClient(self)\n+        self.nodes = NodesClient(self)\n+        self.remote = RemoteClient(self)\n+        self.snapshot = SnapshotClient(self)\n+        self.tasks = TasksClient(self)\n+\n+        self.xpack = XPackClient(self)\n+        self.ccr = CcrClient(self)\n+        self.data_frame = Data_FrameClient(self)\n+        self.deprecation = DeprecationClient(self)\n+        self.graph = GraphClient(self)\n+        self.ilm = IlmClient(self)\n+        self.indices = IndicesClient(self)\n+        self.license = LicenseClient(self)\n+        self.migration = MigrationClient(self)\n+        self.ml = MlClient(self)\n+        self.monitoring = MonitoringClient(self)\n+        self.rollup = RollupClient(self)\n+        self.security = SecurityClient(self)\n+        self.sql = SqlClient(self)\n+        self.ssl = SslClient(self)\n+        self.watcher = WatcherClient(self)\n+        self.enrich = EnrichClient(self)\n+        self.slm = SlmClient(self)\n+        self.transform = TransformClient(self)\n+\n+    def __repr__(self):\n+        try:\n+            # get a list of all connections\n+            cons = self.transport.hosts\n+            # truncate to 5 if there are too many\n+            if len(cons) > 5:\n+                cons = cons[:5] + [\"...\"]\n+            return \"<{cls}({cons})>\".format(cls=self.__class__.__name__, cons=cons)\n+        except Exception:\n+            # probably operating on custom transport and connection_pool, ignore\n+            return super(Elasticsearch, self).__repr__()\n+\n+    # AUTO-GENERATED-API-DEFINITIONS #\n+    @query_params()\n+    def ping(self, params=None, headers=None):\n+        \"\"\"\n+        Returns whether the cluster is running.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html>`_\n+\n+        \"\"\"\n+        try:\n+            return self.transport.perform_request(\n+                \"HEAD\", \"/\", params=params, headers=headers\n+            )\n+        except TransportError:\n+            return False\n+\n+    @query_params()\n+    def info(self, params=None, headers=None):\n+        \"\"\"\n+        Returns basic information about the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/\", params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"pipeline\",\n+        \"refresh\",\n+        \"routing\",\n+        \"timeout\",\n+        \"version\",\n+        \"version_type\",\n+        \"wait_for_active_shards\",\n+    )\n+    def create(self, index, id, body, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Creates a new document in the index.  Returns a 409 response when a document\n+        with a same ID already exists in the index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-index_.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: Document ID\n+        :arg body: The document\n+        :arg doc_type: The type of the document\n+        :arg pipeline: The pipeline id to preprocess incoming documents\n+            with\n+        :arg refresh: If `true` then refresh the affected shards to make\n+            this operation visible to search, if `wait_for` then wait for a refresh\n+            to make this operation visible to search, if `false` (the default) then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        :arg routing: Specific routing value\n+        :arg timeout: Explicit operation timeout\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the index operation. Defaults\n+            to 1, meaning the primary shard only. Set to `all` for all shard copies,\n+            otherwise set to any non-negative value less than or equal to the total\n+            number of copies for the shard (number of replicas + 1)\n+        \"\"\"\n+        for param in (index, id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, doc_type, id, \"_create\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"if_primary_term\",\n+        \"if_seq_no\",\n+        \"op_type\",\n+        \"pipeline\",\n+        \"refresh\",\n+        \"routing\",\n+        \"timeout\",\n+        \"version\",\n+        \"version_type\",\n+        \"wait_for_active_shards\",\n+    )\n+    def index(self, index, body, doc_type=None, id=None, params=None, headers=None):\n+        \"\"\"\n+        Creates or updates a document in an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-index_.html>`_\n+\n+        :arg index: The name of the index\n+        :arg body: The document\n+        :arg doc_type: The type of the document\n+        :arg id: Document ID\n+        :arg if_primary_term: only perform the index operation if the\n+            last operation that has changed the document has the specified primary\n+            term\n+        :arg if_seq_no: only perform the index operation if the last\n+            operation that has changed the document has the specified sequence\n+            number\n+        :arg op_type: Explicit operation type. Defaults to `index` for\n+            requests with an explicit document ID, and to `create`for requests\n+            without an explicit document ID  Valid choices: index, create\n+        :arg pipeline: The pipeline id to preprocess incoming documents\n+            with\n+        :arg refresh: If `true` then refresh the affected shards to make\n+            this operation visible to search, if `wait_for` then wait for a refresh\n+            to make this operation visible to search, if `false` (the default) then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        :arg routing: Specific routing value\n+        :arg timeout: Explicit operation timeout\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the index operation. Defaults\n+            to 1, meaning the primary shard only. Set to `all` for all shard copies,\n+            otherwise set to any non-negative value less than or equal to the total\n+            number of copies for the shard (number of replicas + 1)\n+        \"\"\"\n+        for param in (index, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type is None:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"POST\" if id in SKIP_IN_PATH else \"PUT\",\n+            _make_path(index, doc_type, id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"pipeline\",\n+        \"refresh\",\n+        \"routing\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def bulk(self, body, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to perform multiple index/update/delete operations in a single request.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-bulk.html>`_\n+\n+        :arg body: The operation definition and data (action-data\n+            pairs), separated by newlines\n+        :arg index: Default index for items which don't provide one\n+        :arg doc_type: Default document type for items which don't\n+            provide one\n+        :arg _source: True or false to return the _source field or not,\n+            or default list of fields to return, can be overridden on each sub-\n+            request\n+        :arg _source_excludes: Default list of fields to exclude from\n+            the returned _source field, can be overridden on each sub-request\n+        :arg _source_includes: Default list of fields to extract and\n+            return from the _source field, can be overridden on each sub-request\n+        :arg doc_type: Default document type for items which don't\n+            provide one\n+        :arg pipeline: The pipeline id to preprocess incoming documents\n+            with\n+        :arg refresh: If `true` then refresh the affected shards to make\n+            this operation visible to search, if `wait_for` then wait for a refresh\n+            to make this operation visible to search, if `false` (the default) then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        :arg routing: Specific routing value\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the bulk operation. Defaults\n+            to 1, meaning the primary shard only. Set to `all` for all shard copies,\n+            otherwise set to any non-negative value less than or equal to the total\n+            number of copies for the shard (number of replicas + 1)\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        body = _bulk_body(self.transport.serializer, body)\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_bulk\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def clear_scroll(self, body=None, scroll_id=None, params=None, headers=None):\n+        \"\"\"\n+        Explicitly clears the search context for a scroll.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-request-body.html#_clear_scroll_api>`_\n+\n+        :arg body: A comma-separated list of scroll IDs to clear if none\n+            was specified via the scroll_id parameter\n+        :arg scroll_id: A comma-separated list of scroll IDs to clear\n+        \"\"\"\n+        if scroll_id in SKIP_IN_PATH and body in SKIP_IN_PATH:\n+            raise ValueError(\"You need to supply scroll_id or body.\")\n+        elif scroll_id and not body:\n+            body = {\"scroll_id\": [scroll_id]}\n+        elif scroll_id:\n+            params[\"scroll_id\"] = scroll_id\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", \"/_search/scroll\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"analyze_wildcard\",\n+        \"analyzer\",\n+        \"default_operator\",\n+        \"df\",\n+        \"expand_wildcards\",\n+        \"ignore_throttled\",\n+        \"ignore_unavailable\",\n+        \"lenient\",\n+        \"min_score\",\n+        \"preference\",\n+        \"q\",\n+        \"routing\",\n+        \"terminate_after\",\n+    )\n+    def count(self, body=None, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns number of documents matching a query.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-count.html>`_\n+\n+        :arg body: A query to restrict the results specified with the\n+            Query DSL (optional)\n+        :arg index: A comma-separated list of indices to restrict the\n+            results\n+        :arg doc_type: A comma-separated list of types to restrict the\n+            results\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg analyze_wildcard: Specify whether wildcard and prefix\n+            queries should be analyzed (default: false)\n+        :arg analyzer: The analyzer to use for the query string\n+        :arg default_operator: The default operator for query string\n+            query (AND or OR)  Valid choices: AND, OR  Default: OR\n+        :arg df: The field to use as default where no field prefix is\n+            given in the query string\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_throttled: Whether specified concrete, expanded or\n+            aliased indices should be ignored when throttled\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg lenient: Specify whether format-based query failures (such\n+            as providing text to a numeric field) should be ignored\n+        :arg min_score: Include only documents with a specific `_score`\n+            value in the result\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg q: Query in the Lucene query string syntax\n+        :arg routing: A comma-separated list of specific routing values\n+        :arg terminate_after: The maximum count for each shard, upon\n+            reaching which the query execution will terminate early\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_count\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"if_primary_term\",\n+        \"if_seq_no\",\n+        \"refresh\",\n+        \"routing\",\n+        \"timeout\",\n+        \"version\",\n+        \"version_type\",\n+        \"wait_for_active_shards\",\n+    )\n+    def delete(self, index, id, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Removes a document from the index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-delete.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: The document ID\n+        :arg doc_type: The type of the document\n+        :arg if_primary_term: only perform the delete operation if the\n+            last operation that has changed the document has the specified primary\n+            term\n+        :arg if_seq_no: only perform the delete operation if the last\n+            operation that has changed the document has the specified sequence\n+            number\n+        :arg refresh: If `true` then refresh the affected shards to make\n+            this operation visible to search, if `wait_for` then wait for a refresh\n+            to make this operation visible to search, if `false` (the default) then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        :arg routing: Specific routing value\n+        :arg timeout: Explicit operation timeout\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the delete operation.\n+            Defaults to 1, meaning the primary shard only. Set to `all` for all\n+            shard copies, otherwise set to any non-negative value less than or equal\n+            to the total number of copies for the shard (number of replicas + 1)\n+        \"\"\"\n+        for param in (index, id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", _make_path(index, doc_type, id), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"allow_no_indices\",\n+        \"analyze_wildcard\",\n+        \"analyzer\",\n+        \"conflicts\",\n+        \"default_operator\",\n+        \"df\",\n+        \"expand_wildcards\",\n+        \"from_\",\n+        \"ignore_unavailable\",\n+        \"lenient\",\n+        \"max_docs\",\n+        \"preference\",\n+        \"q\",\n+        \"refresh\",\n+        \"request_cache\",\n+        \"requests_per_second\",\n+        \"routing\",\n+        \"scroll\",\n+        \"scroll_size\",\n+        \"search_timeout\",\n+        \"search_type\",\n+        \"size\",\n+        \"slices\",\n+        \"sort\",\n+        \"stats\",\n+        \"terminate_after\",\n+        \"timeout\",\n+        \"version\",\n+        \"wait_for_active_shards\",\n+        \"wait_for_completion\",\n+    )\n+    def delete_by_query(self, index, body, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Deletes documents matching the provided query.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-delete-by-query.html>`_\n+\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg body: The search definition using the Query DSL\n+        :arg doc_type: A comma-separated list of document types to\n+            search; leave empty to perform the operation on all types\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg analyze_wildcard: Specify whether wildcard and prefix\n+            queries should be analyzed (default: false)\n+        :arg analyzer: The analyzer to use for the query string\n+        :arg conflicts: What to do when the delete by query hits version\n+            conflicts?  Valid choices: abort, proceed  Default: abort\n+        :arg default_operator: The default operator for query string\n+            query (AND or OR)  Valid choices: AND, OR  Default: OR\n+        :arg df: The field to use as default where no field prefix is\n+            given in the query string\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg from_: Starting offset (default: 0)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg lenient: Specify whether format-based query failures (such\n+            as providing text to a numeric field) should be ignored\n+        :arg max_docs: Maximum number of documents to process (default:\n+            all documents)\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg q: Query in the Lucene query string syntax\n+        :arg refresh: Should the effected indexes be refreshed?\n+        :arg request_cache: Specify if request cache should be used for\n+            this request or not, defaults to index level setting\n+        :arg requests_per_second: The throttle for this request in sub-\n+            requests per second. -1 means no throttle.\n+        :arg routing: A comma-separated list of specific routing values\n+        :arg scroll: Specify how long a consistent view of the index\n+            should be maintained for scrolled search\n+        :arg scroll_size: Size on the scroll request powering the delete\n+            by query\n+        :arg search_timeout: Explicit timeout for each search request.\n+            Defaults to no timeout.\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, dfs_query_then_fetch\n+        :arg size: Deprecated, please use `max_docs` instead\n+        :arg slices: The number of slices this task should be divided\n+            into. Defaults to 1 meaning the task isn't sliced into subtasks.\n+            Default: 1\n+        :arg sort: A comma-separated list of <field>:<direction> pairs\n+        :arg stats: Specific 'tag' of the request for logging and\n+            statistical purposes\n+        :arg terminate_after: The maximum number of documents to collect\n+            for each shard, upon reaching which the query execution will terminate\n+            early.\n+        :arg timeout: Time each individual bulk request should wait for\n+            shards that are unavailable.  Default: 1m\n+        :arg version: Specify whether to return document version as part\n+            of a hit\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the delete by query\n+            operation. Defaults to 1, meaning the primary shard only. Set to `all`\n+            for all shard copies, otherwise set to any non-negative value less than\n+            or equal to the total number of copies for the shard (number of replicas\n+            + 1)\n+        :arg wait_for_completion: Should the request should block until\n+            the delete by query is complete.  Default: True\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        for param in (index, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_delete_by_query\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"requests_per_second\")\n+    def delete_by_query_rethrottle(self, task_id, params=None, headers=None):\n+        \"\"\"\n+        Changes the number of requests per second for a particular Delete By Query\n+        operation.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-delete-by-query.html>`_\n+\n+        :arg task_id: The task id to rethrottle\n+        :arg requests_per_second: The throttle to set on this request in\n+            floating sub-requests per second. -1 means set no throttle.\n+        \"\"\"\n+        if task_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'task_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_delete_by_query\", task_id, \"_rethrottle\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def delete_script(self, id, params=None, headers=None):\n+        \"\"\"\n+        Deletes a script.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting.html>`_\n+\n+        :arg id: Script ID\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", _make_path(\"_scripts\", id), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"preference\",\n+        \"realtime\",\n+        \"refresh\",\n+        \"routing\",\n+        \"stored_fields\",\n+        \"version\",\n+        \"version_type\",\n+    )\n+    def exists(self, index, id, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about whether a document exists in an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-get.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: The document ID\n+        :arg doc_type: The type of the document (use `_all` to fetch the\n+            first document matching the ID across all types)\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg realtime: Specify whether to perform the operation in\n+            realtime or search mode\n+        :arg refresh: Refresh the shard containing the document before\n+            performing the operation\n+        :arg routing: Specific routing value\n+        :arg stored_fields: A comma-separated list of stored fields to\n+            return in the response\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        \"\"\"\n+        for param in (index, id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"HEAD\", _make_path(index, doc_type, id), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"preference\",\n+        \"realtime\",\n+        \"refresh\",\n+        \"routing\",\n+        \"version\",\n+        \"version_type\",\n+    )\n+    def exists_source(self, index, id, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about whether a document source exists in an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-get.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: The document ID\n+        :arg doc_type: The type of the document; deprecated and optional\n+            starting with 7.0\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg realtime: Specify whether to perform the operation in\n+            realtime or search mode\n+        :arg refresh: Refresh the shard containing the document before\n+            performing the operation\n+        :arg routing: Specific routing value\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        \"\"\"\n+        for param in (index, id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"HEAD\",\n+            _make_path(index, doc_type, id, \"_source\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"analyze_wildcard\",\n+        \"analyzer\",\n+        \"default_operator\",\n+        \"df\",\n+        \"lenient\",\n+        \"preference\",\n+        \"q\",\n+        \"routing\",\n+        \"stored_fields\",\n+    )\n+    def explain(self, index, id, body=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about why a specific matches (or doesn't match) a query.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-explain.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: The document ID\n+        :arg body: The query definition using the Query DSL\n+        :arg doc_type: The type of the document\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg analyze_wildcard: Specify whether wildcards and prefix\n+            queries in the query string query should be analyzed (default: false)\n+        :arg analyzer: The analyzer for the query string query\n+        :arg default_operator: The default operator for query string\n+            query (AND or OR)  Valid choices: AND, OR  Default: OR\n+        :arg df: The default field for query string query (default:\n+            _all)\n+        :arg lenient: Specify whether format-based query failures (such\n+            as providing text to a numeric field) should be ignored\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg q: Query in the Lucene query string syntax\n+        :arg routing: Specific routing value\n+        :arg stored_fields: A comma-separated list of stored fields to\n+            return in the response\n+        \"\"\"\n+        for param in (index, id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, id, \"_explain\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"fields\",\n+        \"ignore_unavailable\",\n+        \"include_unmapped\",\n+    )\n+    def field_caps(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns the information about the capabilities of fields among multiple\n+        indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-field-caps.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg fields: A comma-separated list of field names\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg include_unmapped: Indicates whether unmapped fields should\n+            be included in the response.\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_field_caps\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"preference\",\n+        \"realtime\",\n+        \"refresh\",\n+        \"routing\",\n+        \"stored_fields\",\n+        \"version\",\n+        \"version_type\",\n+    )\n+    def get(self, index, id, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns a document.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-get.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: The document ID\n+        :arg doc_type: The type of the document (use `_all` to fetch the\n+            first document matching the ID across all types)\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg realtime: Specify whether to perform the operation in\n+            realtime or search mode\n+        :arg refresh: Refresh the shard containing the document before\n+            performing the operation\n+        :arg routing: Specific routing value\n+        :arg stored_fields: A comma-separated list of stored fields to\n+            return in the response\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        \"\"\"\n+        for param in (index, id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, doc_type, id), params=params, headers=headers\n+        )\n+\n+    @query_params(\"master_timeout\")\n+    def get_script(self, id, params=None, headers=None):\n+        \"\"\"\n+        Returns a script.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting.html>`_\n+\n+        :arg id: Script ID\n+        :arg master_timeout: Specify timeout for connection to master\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_scripts\", id), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"preference\",\n+        \"realtime\",\n+        \"refresh\",\n+        \"routing\",\n+        \"version\",\n+        \"version_type\",\n+    )\n+    def get_source(self, index, id, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns the source of a document.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-get.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: The document ID\n+        :arg doc_type: The type of the document; deprecated and optional\n+            starting with 7.0\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg realtime: Specify whether to perform the operation in\n+            realtime or search mode\n+        :arg refresh: Refresh the shard containing the document before\n+            performing the operation\n+        :arg routing: Specific routing value\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        \"\"\"\n+        for param in (index, id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(index, doc_type, id, \"_source\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"preference\",\n+        \"realtime\",\n+        \"refresh\",\n+        \"routing\",\n+        \"stored_fields\",\n+    )\n+    def mget(self, body, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to get multiple documents in one request.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-multi-get.html>`_\n+\n+        :arg body: Document identifiers; can be either `docs`\n+            (containing full document information) or `ids` (when index and type is\n+            provided in the URL.\n+        :arg index: The name of the index\n+        :arg doc_type: The type of the document\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg realtime: Specify whether to perform the operation in\n+            realtime or search mode\n+        :arg refresh: Refresh the shard containing the document before\n+            performing the operation\n+        :arg routing: Specific routing value\n+        :arg stored_fields: A comma-separated list of stored fields to\n+            return in the response\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_mget\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"ccs_minimize_roundtrips\",\n+        \"max_concurrent_searches\",\n+        \"max_concurrent_shard_requests\",\n+        \"pre_filter_shard_size\",\n+        \"rest_total_hits_as_int\",\n+        \"search_type\",\n+        \"typed_keys\",\n+    )\n+    def msearch(self, body, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to execute several search operations in one request.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-multi-search.html>`_\n+\n+        :arg body: The request definitions (metadata-search request\n+            definition pairs), separated by newlines\n+        :arg index: A comma-separated list of index names to use as\n+            default\n+        :arg doc_type: A comma-separated list of document types to use\n+            as default\n+        :arg ccs_minimize_roundtrips: Indicates whether network round-\n+            trips should be minimized as part of cross-cluster search requests\n+            execution  Default: true\n+        :arg max_concurrent_searches: Controls the maximum number of\n+            concurrent searches the multi search api will execute\n+        :arg max_concurrent_shard_requests: The number of concurrent\n+            shard requests each sub search executes concurrently per node. This\n+            value should be used to limit the impact of the search on the cluster in\n+            order to limit the number of concurrent shard requests  Default: 5\n+        :arg pre_filter_shard_size: A threshold that enforces a pre-\n+            filter roundtrip to prefilter search shards based on query rewriting if\n+            the\u00a0number of shards the search request expands to exceeds the\n+            threshold. This filter roundtrip can limit the number of shards\n+            significantly if for instance a shard can not match any documents based\n+            on it's rewrite method ie. if date filters are mandatory to match but\n+            the shard bounds and the query are disjoint.  Default: 128\n+        :arg rest_total_hits_as_int: Indicates whether hits.total should\n+            be rendered as an integer or an object in the rest search response\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, query_and_fetch, dfs_query_then_fetch,\n+            dfs_query_and_fetch\n+        :arg typed_keys: Specify whether aggregation and suggester names\n+            should be prefixed by their respective types in the response\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        body = _bulk_body(self.transport.serializer, body)\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_msearch\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"ccs_minimize_roundtrips\",\n+        \"max_concurrent_searches\",\n+        \"rest_total_hits_as_int\",\n+        \"search_type\",\n+        \"typed_keys\",\n+    )\n+    def msearch_template(\n+        self, body, index=None, doc_type=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Allows to execute several search template operations in one request.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/search-multi-search.html>`_\n+\n+        :arg body: The request definitions (metadata-search request\n+            definition pairs), separated by newlines\n+        :arg index: A comma-separated list of index names to use as\n+            default\n+        :arg doc_type: A comma-separated list of document types to use\n+            as default\n+        :arg ccs_minimize_roundtrips: Indicates whether network round-\n+            trips should be minimized as part of cross-cluster search requests\n+            execution  Default: true\n+        :arg max_concurrent_searches: Controls the maximum number of\n+            concurrent searches the multi search api will execute\n+        :arg rest_total_hits_as_int: Indicates whether hits.total should\n+            be rendered as an integer or an object in the rest search response\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, query_and_fetch, dfs_query_then_fetch,\n+            dfs_query_and_fetch\n+        :arg typed_keys: Specify whether aggregation and suggester names\n+            should be prefixed by their respective types in the response\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        body = _bulk_body(self.transport.serializer, body)\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_msearch\", \"template\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"field_statistics\",\n+        \"fields\",\n+        \"ids\",\n+        \"offsets\",\n+        \"payloads\",\n+        \"positions\",\n+        \"preference\",\n+        \"realtime\",\n+        \"routing\",\n+        \"term_statistics\",\n+        \"version\",\n+        \"version_type\",\n+    )\n+    def mtermvectors(\n+        self, body=None, index=None, doc_type=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Returns multiple termvectors in one request.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-multi-termvectors.html>`_\n+\n+        :arg body: Define ids, documents, parameters or a list of\n+            parameters per document here. You must at least provide a list of\n+            document ids. See documentation.\n+        :arg index: The index in which the document resides.\n+        :arg doc_type: The type of the document.\n+        :arg field_statistics: Specifies if document count, sum of\n+            document frequencies and sum of total term frequencies should be\n+            returned. Applies to all returned documents unless otherwise specified\n+            in body \"params\" or \"docs\".  Default: True\n+        :arg fields: A comma-separated list of fields to return. Applies\n+            to all returned documents unless otherwise specified in body \"params\" or\n+            \"docs\".\n+        :arg ids: A comma-separated list of documents ids. You must\n+            define ids as parameter or set \"ids\" or \"docs\" in the request body\n+        :arg offsets: Specifies if term offsets should be returned.\n+            Applies to all returned documents unless otherwise specified in body\n+            \"params\" or \"docs\".  Default: True\n+        :arg payloads: Specifies if term payloads should be returned.\n+            Applies to all returned documents unless otherwise specified in body\n+            \"params\" or \"docs\".  Default: True\n+        :arg positions: Specifies if term positions should be returned.\n+            Applies to all returned documents unless otherwise specified in body\n+            \"params\" or \"docs\".  Default: True\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random) .Applies to all returned documents\n+            unless otherwise specified in body \"params\" or \"docs\".\n+        :arg realtime: Specifies if requests are real-time as opposed to\n+            near-real-time (default: true).\n+        :arg routing: Specific routing value. Applies to all returned\n+            documents unless otherwise specified in body \"params\" or \"docs\".\n+        :arg term_statistics: Specifies if total term frequency and\n+            document frequency should be returned. Applies to all returned documents\n+            unless otherwise specified in body \"params\" or \"docs\".\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_mtermvectors\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def put_script(self, id, body, context=None, params=None, headers=None):\n+        \"\"\"\n+        Creates or updates a script.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting.html>`_\n+\n+        :arg id: Script ID\n+        :arg body: The document\n+        :arg context: Script context\n+        :arg context: Context name to compile script against\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        for param in (id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_scripts\", id, context),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\", \"search_type\"\n+    )\n+    def rank_eval(self, body, index=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to evaluate the quality of ranked search results over a set of typical\n+        search queries\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-rank-eval.html>`_\n+\n+        :arg body: The ranking evaluation search definition, including\n+            search requests, document ratings and ranking metric definition.\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, dfs_query_then_fetch\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_rank_eval\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"max_docs\",\n+        \"refresh\",\n+        \"requests_per_second\",\n+        \"scroll\",\n+        \"slices\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+        \"wait_for_completion\",\n+    )\n+    def reindex(self, body, params=None, headers=None):\n+        \"\"\"\n+        Allows to copy documents from one index to another, optionally filtering the\n+        source documents by a query, changing the destination index settings, or\n+        fetching the documents from a remote cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-reindex.html>`_\n+\n+        :arg body: The search definition using the Query DSL and the\n+            prototype for the index request.\n+        :arg max_docs: Maximum number of documents to process (default:\n+            all documents)\n+        :arg refresh: Should the affected indexes be refreshed?\n+        :arg requests_per_second: The throttle to set on this request in\n+            sub-requests per second. -1 means no throttle.\n+        :arg scroll: Control how long to keep the search context alive\n+            Default: 5m\n+        :arg slices: The number of slices this task should be divided\n+            into. Defaults to 1 meaning the task isn't sliced into subtasks.\n+            Default: 1\n+        :arg timeout: Time each individual bulk request should wait for\n+            shards that are unavailable.  Default: 1m\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the reindex operation.\n+            Defaults to 1, meaning the primary shard only. Set to `all` for all\n+            shard copies, otherwise set to any non-negative value less than or equal\n+            to the total number of copies for the shard (number of replicas + 1)\n+        :arg wait_for_completion: Should the request should block until\n+            the reindex is complete.  Default: True\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_reindex\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"requests_per_second\")\n+    def reindex_rethrottle(self, task_id, params=None, headers=None):\n+        \"\"\"\n+        Changes the number of requests per second for a particular Reindex operation.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-reindex.html>`_\n+\n+        :arg task_id: The task id to rethrottle\n+        :arg requests_per_second: The throttle to set on this request in\n+            floating sub-requests per second. -1 means set no throttle.\n+        \"\"\"\n+        if task_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'task_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_reindex\", task_id, \"_rethrottle\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def render_search_template(self, body=None, id=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to use the Mustache language to pre-render a search definition.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html#_validating_templates>`_\n+\n+        :arg body: The search definition template and its params\n+        :arg id: The id of the stored search template\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_render\", \"template\", id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def scripts_painless_execute(self, body=None, params=None, headers=None):\n+        \"\"\"\n+        Allows an arbitrary script to be executed and a result to be returned\n+        `<https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-execute-api.html>`_\n+\n+        :arg body: The script to execute\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_scripts/painless/_execute\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"rest_total_hits_as_int\", \"scroll\")\n+    def scroll(self, body=None, scroll_id=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to retrieve a large numbers of results from a single search request.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-request-body.html#request-body-search-scroll>`_\n+\n+        :arg body: The scroll ID if not passed by URL or query\n+            parameter.\n+        :arg scroll_id: The scroll ID\n+        :arg rest_total_hits_as_int: Indicates whether hits.total should\n+            be rendered as an integer or an object in the rest search response\n+        :arg scroll: Specify how long a consistent view of the index\n+            should be maintained for scrolled search\n+        :arg scroll_id: The scroll ID for scrolled search\n+        \"\"\"\n+        if scroll_id in SKIP_IN_PATH and body in SKIP_IN_PATH:\n+            raise ValueError(\"You need to supply scroll_id or body.\")\n+        elif scroll_id and not body:\n+            body = {\"scroll_id\": scroll_id}\n+        elif scroll_id:\n+            params[\"scroll_id\"] = scroll_id\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_search/scroll\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"allow_no_indices\",\n+        \"allow_partial_search_results\",\n+        \"analyze_wildcard\",\n+        \"analyzer\",\n+        \"batched_reduce_size\",\n+        \"ccs_minimize_roundtrips\",\n+        \"default_operator\",\n+        \"df\",\n+        \"docvalue_fields\",\n+        \"expand_wildcards\",\n+        \"explain\",\n+        \"from_\",\n+        \"ignore_throttled\",\n+        \"ignore_unavailable\",\n+        \"lenient\",\n+        \"max_concurrent_shard_requests\",\n+        \"pre_filter_shard_size\",\n+        \"preference\",\n+        \"q\",\n+        \"request_cache\",\n+        \"rest_total_hits_as_int\",\n+        \"routing\",\n+        \"scroll\",\n+        \"search_type\",\n+        \"seq_no_primary_term\",\n+        \"size\",\n+        \"sort\",\n+        \"stats\",\n+        \"stored_fields\",\n+        \"suggest_field\",\n+        \"suggest_mode\",\n+        \"suggest_size\",\n+        \"suggest_text\",\n+        \"terminate_after\",\n+        \"timeout\",\n+        \"track_scores\",\n+        \"track_total_hits\",\n+        \"typed_keys\",\n+        \"version\",\n+    )\n+    def search(self, body=None, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns results matching a query.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-search.html>`_\n+\n+        :arg body: The search definition using the Query DSL\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg doc_type: A comma-separated list of document types to\n+            search; leave empty to perform the operation on all types\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg allow_partial_search_results: Indicate if an error should\n+            be returned if there is a partial search failure or timeout  Default:\n+            True\n+        :arg analyze_wildcard: Specify whether wildcard and prefix\n+            queries should be analyzed (default: false)\n+        :arg analyzer: The analyzer to use for the query string\n+        :arg batched_reduce_size: The number of shard results that\n+            should be reduced at once on the coordinating node. This value should be\n+            used as a protection mechanism to reduce the memory overhead per search\n+            request if the potential number of shards in the request can be large.\n+            Default: 512\n+        :arg ccs_minimize_roundtrips: Indicates whether network round-\n+            trips should be minimized as part of cross-cluster search requests\n+            execution  Default: true\n+        :arg default_operator: The default operator for query string\n+            query (AND or OR)  Valid choices: AND, OR  Default: OR\n+        :arg df: The field to use as default where no field prefix is\n+            given in the query string\n+        :arg docvalue_fields: A comma-separated list of fields to return\n+            as the docvalue representation of a field for each hit\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg explain: Specify whether to return detailed information\n+            about score computation as part of a hit\n+        :arg from_: Starting offset (default: 0)\n+        :arg ignore_throttled: Whether specified concrete, expanded or\n+            aliased indices should be ignored when throttled\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg lenient: Specify whether format-based query failures (such\n+            as providing text to a numeric field) should be ignored\n+        :arg max_concurrent_shard_requests: The number of concurrent\n+            shard requests per node this search executes concurrently. This value\n+            should be used to limit the impact of the search on the cluster in order\n+            to limit the number of concurrent shard requests  Default: 5\n+        :arg pre_filter_shard_size: A threshold that enforces a pre-\n+            filter roundtrip to prefilter search shards based on query rewriting if\n+            the\u00a0number of shards the search request expands to exceeds the\n+            threshold. This filter roundtrip can limit the number of shards\n+            significantly if for instance a shard can not match any documents based\n+            on it's rewrite method ie. if date filters are mandatory to match but\n+            the shard bounds and the query are disjoint.  Default: 128\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg q: Query in the Lucene query string syntax\n+        :arg request_cache: Specify if request cache should be used for\n+            this request or not, defaults to index level setting\n+        :arg rest_total_hits_as_int: Indicates whether hits.total should\n+            be rendered as an integer or an object in the rest search response\n+        :arg routing: A comma-separated list of specific routing values\n+        :arg scroll: Specify how long a consistent view of the index\n+            should be maintained for scrolled search\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, dfs_query_then_fetch\n+        :arg seq_no_primary_term: Specify whether to return sequence\n+            number and primary term of the last modification of each hit\n+        :arg size: Number of hits to return (default: 10)\n+        :arg sort: A comma-separated list of <field>:<direction> pairs\n+        :arg stats: Specific 'tag' of the request for logging and\n+            statistical purposes\n+        :arg stored_fields: A comma-separated list of stored fields to\n+            return as part of a hit\n+        :arg suggest_field: Specify which field to use for suggestions\n+        :arg suggest_mode: Specify suggest mode  Valid choices: missing,\n+            popular, always  Default: missing\n+        :arg suggest_size: How many suggestions to return in response\n+        :arg suggest_text: The source text for which the suggestions\n+            should be returned\n+        :arg terminate_after: The maximum number of documents to collect\n+            for each shard, upon reaching which the query execution will terminate\n+            early.\n+        :arg timeout: Explicit operation timeout\n+        :arg track_scores: Whether to calculate and return scores even\n+            if they are not used for sorting\n+        :arg track_total_hits: Indicate if the number of documents that\n+            match the query should be tracked\n+        :arg typed_keys: Specify whether aggregation and suggester names\n+            should be prefixed by their respective types in the response\n+        :arg version: Specify whether to return document version as part\n+            of a hit\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_search\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"local\",\n+        \"preference\",\n+        \"routing\",\n+    )\n+    def search_shards(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about the indices and shards that a search request would be\n+        executed against.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-shards.html>`_\n+\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg routing: Specific routing value\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_search_shards\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"ccs_minimize_roundtrips\",\n+        \"expand_wildcards\",\n+        \"explain\",\n+        \"ignore_throttled\",\n+        \"ignore_unavailable\",\n+        \"preference\",\n+        \"profile\",\n+        \"rest_total_hits_as_int\",\n+        \"routing\",\n+        \"scroll\",\n+        \"search_type\",\n+        \"typed_keys\",\n+    )\n+    def search_template(\n+        self, body, index=None, doc_type=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Allows to use the Mustache language to pre-render a search definition.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html>`_\n+\n+        :arg body: The search definition template and its params\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg doc_type: A comma-separated list of document types to\n+            search; leave empty to perform the operation on all types\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg ccs_minimize_roundtrips: Indicates whether network round-\n+            trips should be minimized as part of cross-cluster search requests\n+            execution  Default: true\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg explain: Specify whether to return detailed information\n+            about score computation as part of a hit\n+        :arg ignore_throttled: Whether specified concrete, expanded or\n+            aliased indices should be ignored when throttled\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg profile: Specify whether to profile the query execution\n+        :arg rest_total_hits_as_int: Indicates whether hits.total should\n+            be rendered as an integer or an object in the rest search response\n+        :arg routing: A comma-separated list of specific routing values\n+        :arg scroll: Specify how long a consistent view of the index\n+            should be maintained for scrolled search\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, query_and_fetch, dfs_query_then_fetch,\n+            dfs_query_and_fetch\n+        :arg typed_keys: Specify whether aggregation and suggester names\n+            should be prefixed by their respective types in the response\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_search\", \"template\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"field_statistics\",\n+        \"fields\",\n+        \"offsets\",\n+        \"payloads\",\n+        \"positions\",\n+        \"preference\",\n+        \"realtime\",\n+        \"routing\",\n+        \"term_statistics\",\n+        \"version\",\n+        \"version_type\",\n+    )\n+    def termvectors(\n+        self, index, body=None, doc_type=None, id=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Returns information and statistics about terms in the fields of a particular\n+        document.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-termvectors.html>`_\n+\n+        :arg index: The index in which the document resides.\n+        :arg body: Define parameters and or supply a document to get\n+            termvectors for. See documentation.\n+        :arg doc_type: The type of the document.\n+        :arg id: The id of the document, when not specified a doc param\n+            should be supplied.\n+        :arg field_statistics: Specifies if document count, sum of\n+            document frequencies and sum of total term frequencies should be\n+            returned.  Default: True\n+        :arg fields: A comma-separated list of fields to return.\n+        :arg offsets: Specifies if term offsets should be returned.\n+            Default: True\n+        :arg payloads: Specifies if term payloads should be returned.\n+            Default: True\n+        :arg positions: Specifies if term positions should be returned.\n+            Default: True\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random).\n+        :arg realtime: Specifies if request is real-time as opposed to\n+            near-real-time (default: true).\n+        :arg routing: Specific routing value.\n+        :arg term_statistics: Specifies if total term frequency and\n+            document frequency should be returned.\n+        :arg version: Explicit version number for concurrency control\n+        :arg version_type: Specific version type  Valid choices:\n+            internal, external, external_gte, force\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, id, \"_termvectors\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"if_primary_term\",\n+        \"if_seq_no\",\n+        \"lang\",\n+        \"refresh\",\n+        \"retry_on_conflict\",\n+        \"routing\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def update(self, index, id, body, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Updates a document with a script or partial document.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-update.html>`_\n+\n+        :arg index: The name of the index\n+        :arg id: Document ID\n+        :arg body: The request definition requires either `script` or\n+            partial `doc`\n+        :arg doc_type: The type of the document\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg if_primary_term: only perform the update operation if the\n+            last operation that has changed the document has the specified primary\n+            term\n+        :arg if_seq_no: only perform the update operation if the last\n+            operation that has changed the document has the specified sequence\n+            number\n+        :arg lang: The script language (default: painless)\n+        :arg refresh: If `true` then refresh the affected shards to make\n+            this operation visible to search, if `wait_for` then wait for a refresh\n+            to make this operation visible to search, if `false` (the default) then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        :arg retry_on_conflict: Specify how many times should the\n+            operation be retried when a conflict occurs (default: 0)\n+        :arg routing: Specific routing value\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the update operation.\n+            Defaults to 1, meaning the primary shard only. Set to `all` for all\n+            shard copies, otherwise set to any non-negative value less than or equal\n+            to the total number of copies for the shard (number of replicas + 1)\n+        \"\"\"\n+        for param in (index, id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        if doc_type in SKIP_IN_PATH:\n+            doc_type = \"_doc\"\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, id, \"_update\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"_source\",\n+        \"_source_excludes\",\n+        \"_source_includes\",\n+        \"allow_no_indices\",\n+        \"analyze_wildcard\",\n+        \"analyzer\",\n+        \"conflicts\",\n+        \"default_operator\",\n+        \"df\",\n+        \"expand_wildcards\",\n+        \"from_\",\n+        \"ignore_unavailable\",\n+        \"lenient\",\n+        \"max_docs\",\n+        \"pipeline\",\n+        \"preference\",\n+        \"q\",\n+        \"refresh\",\n+        \"request_cache\",\n+        \"requests_per_second\",\n+        \"routing\",\n+        \"scroll\",\n+        \"scroll_size\",\n+        \"search_timeout\",\n+        \"search_type\",\n+        \"size\",\n+        \"slices\",\n+        \"sort\",\n+        \"stats\",\n+        \"terminate_after\",\n+        \"timeout\",\n+        \"version\",\n+        \"version_type\",\n+        \"wait_for_active_shards\",\n+        \"wait_for_completion\",\n+    )\n+    def update_by_query(\n+        self, index, body=None, doc_type=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Performs an update on every document in the index without changing the source,\n+        for example to pick up a mapping change.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-update-by-query.html>`_\n+\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg body: The search definition using the Query DSL\n+        :arg doc_type: A comma-separated list of document types to\n+            search; leave empty to perform the operation on all types\n+        :arg _source: True or false to return the _source field or not,\n+            or a list of fields to return\n+        :arg _source_excludes: A list of fields to exclude from the\n+            returned _source field\n+        :arg _source_includes: A list of fields to extract and return\n+            from the _source field\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg analyze_wildcard: Specify whether wildcard and prefix\n+            queries should be analyzed (default: false)\n+        :arg analyzer: The analyzer to use for the query string\n+        :arg conflicts: What to do when the update by query hits version\n+            conflicts?  Valid choices: abort, proceed  Default: abort\n+        :arg default_operator: The default operator for query string\n+            query (AND or OR)  Valid choices: AND, OR  Default: OR\n+        :arg df: The field to use as default where no field prefix is\n+            given in the query string\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg from_: Starting offset (default: 0)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg lenient: Specify whether format-based query failures (such\n+            as providing text to a numeric field) should be ignored\n+        :arg max_docs: Maximum number of documents to process (default:\n+            all documents)\n+        :arg pipeline: Ingest pipeline to set on index requests made by\n+            this action. (default: none)\n+        :arg preference: Specify the node or shard the operation should\n+            be performed on (default: random)\n+        :arg q: Query in the Lucene query string syntax\n+        :arg refresh: Should the affected indexes be refreshed?\n+        :arg request_cache: Specify if request cache should be used for\n+            this request or not, defaults to index level setting\n+        :arg requests_per_second: The throttle to set on this request in\n+            sub-requests per second. -1 means no throttle.\n+        :arg routing: A comma-separated list of specific routing values\n+        :arg scroll: Specify how long a consistent view of the index\n+            should be maintained for scrolled search\n+        :arg scroll_size: Size on the scroll request powering the update\n+            by query\n+        :arg search_timeout: Explicit timeout for each search request.\n+            Defaults to no timeout.\n+        :arg search_type: Search operation type  Valid choices:\n+            query_then_fetch, dfs_query_then_fetch\n+        :arg size: Deprecated, please use `max_docs` instead\n+        :arg slices: The number of slices this task should be divided\n+            into. Defaults to 1 meaning the task isn't sliced into subtasks.\n+            Default: 1\n+        :arg sort: A comma-separated list of <field>:<direction> pairs\n+        :arg stats: Specific 'tag' of the request for logging and\n+            statistical purposes\n+        :arg terminate_after: The maximum number of documents to collect\n+            for each shard, upon reaching which the query execution will terminate\n+            early.\n+        :arg timeout: Time each individual bulk request should wait for\n+            shards that are unavailable.  Default: 1m\n+        :arg version: Specify whether to return document version as part\n+            of a hit\n+        :arg version_type: Should the document increment the version\n+            number (internal) on hit or not (reindex)\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before proceeding with the update by query\n+            operation. Defaults to 1, meaning the primary shard only. Set to `all`\n+            for all shard copies, otherwise set to any non-negative value less than\n+            or equal to the total number of copies for the shard (number of replicas\n+            + 1)\n+        :arg wait_for_completion: Should the request should block until\n+            the update by query operation is complete.  Default: True\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_update_by_query\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"requests_per_second\")\n+    def update_by_query_rethrottle(self, task_id, params=None, headers=None):\n+        \"\"\"\n+        Changes the number of requests per second for a particular Update By Query\n+        operation.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update-by-query.html>`_\n+\n+        :arg task_id: The task id to rethrottle\n+        :arg requests_per_second: The throttle to set on this request in\n+            floating sub-requests per second. -1 means set no throttle.\n+        \"\"\"\n+        if task_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'task_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_update_by_query\", task_id, \"_rethrottle\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_script_context(self, params=None, headers=None):\n+        \"\"\"\n+        Returns all script contexts.\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_script_context\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_script_languages(self, params=None, headers=None):\n+        \"\"\"\n+        Returns available script types, languages and contexts\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_script_language\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "6c4ae7606733655e31ecb4af0aa90143058dce70",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cat.py",
                "status": "added",
                "additions": 556,
                "deletions": 0,
                "changes": 556,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cat.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cat.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cat.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,556 @@\n+from .utils import NamespacedClient, query_params, _make_path\n+\n+\n+class CatClient(NamespacedClient):\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"s\", \"v\")\n+    def aliases(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        Shows information about currently configured aliases to indices including\n+        filter and routing infos.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-alias.html>`_\n+\n+        :arg name: A comma-separated list of alias names to return\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"aliases\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\"bytes\", \"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"v\")\n+    def allocation(self, node_id=None, params=None, headers=None):\n+        \"\"\"\n+        Provides a snapshot of how many shards are allocated to each data node and how\n+        much disk space they are using.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-allocation.html>`_\n+\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_cat\", \"allocation\", node_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"s\", \"v\")\n+    def count(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Provides quick access to the document count of the entire cluster, or\n+        individual indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-count.html>`_\n+\n+        :arg index: A comma-separated list of index names to limit the\n+            returned information\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"count\", index), params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"s\", \"time\", \"ts\", \"v\")\n+    def health(self, params=None, headers=None):\n+        \"\"\"\n+        Returns a concise representation of the cluster health.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-health.html>`_\n+\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg ts: Set to false to disable timestamping  Default: True\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/health\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"help\", \"s\")\n+    def help(self, params=None, headers=None):\n+        \"\"\"\n+        Returns help for the Cat APIs.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat.html>`_\n+\n+        :arg help: Return help information\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat\", params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"bytes\",\n+        \"format\",\n+        \"h\",\n+        \"health\",\n+        \"help\",\n+        \"include_unloaded_segments\",\n+        \"local\",\n+        \"master_timeout\",\n+        \"pri\",\n+        \"s\",\n+        \"time\",\n+        \"v\",\n+    )\n+    def indices(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about indices: number of primaries and replicas, document\n+        counts, disk size, ...\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-indices.html>`_\n+\n+        :arg index: A comma-separated list of index names to limit the\n+            returned information\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg health: A health status (\"green\", \"yellow\", or \"red\" to\n+            filter only indices matching the specified health status  Valid choices:\n+            green, yellow, red\n+        :arg help: Return help information\n+        :arg include_unloaded_segments: If set to true segment stats\n+            will include stats for segments that are not currently loaded into\n+            memory\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg pri: Set to true to return stats only for primary shards\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"indices\", index), params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"v\")\n+    def master(self, params=None, headers=None):\n+        \"\"\"\n+        Returns information about the master node.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-master.html>`_\n+\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/master\", params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"bytes\",\n+        \"format\",\n+        \"full_id\",\n+        \"h\",\n+        \"help\",\n+        \"local\",\n+        \"master_timeout\",\n+        \"s\",\n+        \"time\",\n+        \"v\",\n+    )\n+    def nodes(self, params=None, headers=None):\n+        \"\"\"\n+        Returns basic statistics about performance of cluster nodes.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-nodes.html>`_\n+\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg full_id: Return the full node ID instead of the shortened\n+            version (default: false)\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Calculate the selected nodes using the local cluster\n+            state rather than the state from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/nodes\", params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"active_only\", \"bytes\", \"detailed\", \"format\", \"h\", \"help\", \"s\", \"time\", \"v\"\n+    )\n+    def recovery(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about index shard recoveries, both on-going completed.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-recovery.html>`_\n+\n+        :arg index: Comma-separated list or wildcard expression of index\n+            names to limit the returned information\n+        :arg active_only: If `true`, the response only includes ongoing\n+            shard recoveries\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg detailed: If `true`, the response includes detailed\n+            information about shard recoveries\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg index: Comma-separated list or wildcard expression of index\n+            names to limit the returned information\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"recovery\", index), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"bytes\", \"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"time\", \"v\"\n+    )\n+    def shards(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Provides a detailed view of shard allocation on nodes.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-shards.html>`_\n+\n+        :arg index: A comma-separated list of index names to limit the\n+            returned information\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"shards\", index), params=params, headers=headers\n+        )\n+\n+    @query_params(\"bytes\", \"format\", \"h\", \"help\", \"s\", \"v\")\n+    def segments(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Provides low-level information about the segments in the shards of an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-segments.html>`_\n+\n+        :arg index: A comma-separated list of index names to limit the\n+            returned information\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"segments\", index), params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"time\", \"v\")\n+    def pending_tasks(self, params=None, headers=None):\n+        \"\"\"\n+        Returns a concise representation of the cluster pending tasks.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-pending-tasks.html>`_\n+\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/pending_tasks\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"size\", \"v\")\n+    def thread_pool(self, thread_pool_patterns=None, params=None, headers=None):\n+        \"\"\"\n+        Returns cluster-wide thread pool statistics per node. By default the active,\n+        queue and rejected statistics are returned for all thread pools.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-thread-pool.html>`_\n+\n+        :arg thread_pool_patterns: A comma-separated list of regular-\n+            expressions to filter the thread pools in the output\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg size: The multiplier in which to display values  Valid\n+            choices: , k, m, g, t, p\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_cat\", \"thread_pool\", thread_pool_patterns),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"bytes\", \"format\", \"h\", \"help\", \"s\", \"v\")\n+    def fielddata(self, fields=None, params=None, headers=None):\n+        \"\"\"\n+        Shows how much heap memory is currently being used by fielddata on every data\n+        node in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-fielddata.html>`_\n+\n+        :arg fields: A comma-separated list of fields to return the\n+            fielddata size\n+        :arg bytes: The unit in which to display byte values  Valid\n+            choices: b, k, kb, m, mb, g, gb, t, tb, p, pb\n+        :arg fields: A comma-separated list of fields to return in the\n+            output\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_cat\", \"fielddata\", fields),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"v\")\n+    def plugins(self, params=None, headers=None):\n+        \"\"\"\n+        Returns information about installed plugins across nodes node.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-plugins.html>`_\n+\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/plugins\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"v\")\n+    def nodeattrs(self, params=None, headers=None):\n+        \"\"\"\n+        Returns information about custom node attributes.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-nodeattrs.html>`_\n+\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/nodeattrs\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"v\")\n+    def repositories(self, params=None, headers=None):\n+        \"\"\"\n+        Returns information about snapshot repositories registered in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-repositories.html>`_\n+\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/repositories\", params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"format\", \"h\", \"help\", \"ignore_unavailable\", \"master_timeout\", \"s\", \"time\", \"v\"\n+    )\n+    def snapshots(self, repository=None, params=None, headers=None):\n+        \"\"\"\n+        Returns all snapshots in a specific repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-snapshots.html>`_\n+\n+        :arg repository: Name of repository from which to fetch the\n+            snapshot information\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg ignore_unavailable: Set to true to ignore unavailable\n+            snapshots\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_cat\", \"snapshots\", repository),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"actions\",\n+        \"detailed\",\n+        \"format\",\n+        \"h\",\n+        \"help\",\n+        \"node_id\",\n+        \"parent_task\",\n+        \"s\",\n+        \"time\",\n+        \"v\",\n+    )\n+    def tasks(self, params=None, headers=None):\n+        \"\"\"\n+        Returns information about the tasks currently executing on one or more nodes in\n+        the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/tasks.html>`_\n+\n+        :arg actions: A comma-separated list of actions that should be\n+            returned. Leave empty to return all.\n+        :arg detailed: Return detailed task information (default: false)\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information; use `_local` to return information from\n+            the node you're connecting to, leave empty to get information from all\n+            nodes\n+        :arg parent_task: Return tasks with specified parent task id.\n+            Set to -1 to return all.\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg time: The unit in which to display time values  Valid\n+            choices: d (Days), h (Hours), m (Minutes), s (Seconds), ms\n+            (Milliseconds), micros (Microseconds), nanos (Nanoseconds)\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cat/tasks\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"format\", \"h\", \"help\", \"local\", \"master_timeout\", \"s\", \"v\")\n+    def templates(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about existing templates.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-templates.html>`_\n+\n+        :arg name: A pattern that returned template names must match\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        :arg h: Comma-separated list of column names to display\n+        :arg help: Return help information\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg s: Comma-separated list of column names or column aliases\n+            to sort by\n+        :arg v: Verbose mode. Display column headers\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_cat\", \"templates\", name), params=params, headers=headers\n+        )"
            },
            {
                "sha": "4f44c6e3bcec484fbe7a9f58f96a6150f6c4a5f5",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ccr.py",
                "status": "added",
                "additions": 236,
                "deletions": 0,
                "changes": 236,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ccr.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ccr.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ccr.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,236 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class CcrClient(NamespacedClient):\n+    @query_params()\n+    def delete_auto_follow_pattern(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-delete-auto-follow-pattern.html>`_\n+\n+        :arg name: The name of the auto follow pattern.\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ccr\", \"auto_follow\", name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"wait_for_active_shards\")\n+    def follow(self, index, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-put-follow.html>`_\n+\n+        :arg index: The name of the follower index\n+        :arg body: The name of the leader index and other optional ccr\n+            related parameters\n+        :arg wait_for_active_shards: Sets the number of shard copies\n+            that must be active before returning. Defaults to 0. Set to `all` for\n+            all shard copies, otherwise set to any non-negative value less than or\n+            equal to the total number of copies for the shard (number of replicas +\n+            1)  Default: 0\n+        \"\"\"\n+        for param in (index, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, \"_ccr\", \"follow\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def follow_info(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-follow-info.html>`_\n+\n+        :arg index: A comma-separated list of index patterns; use `_all`\n+            to perform the operation on all indices\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_ccr\", \"info\"), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def follow_stats(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-follow-stats.html>`_\n+\n+        :arg index: A comma-separated list of index patterns; use `_all`\n+            to perform the operation on all indices\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_ccr\", \"stats\"), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def forget_follower(self, index, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current>`_\n+\n+        :arg index: the name of the leader index for which specified\n+            follower retention leases should be removed\n+        :arg body: the name and UUID of the follower index, the name of\n+            the cluster containing the follower index, and the alias from the\n+            perspective of that cluster for the remote cluster containing the leader\n+            index\n+        \"\"\"\n+        for param in (index, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_ccr\", \"forget_follower\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def get_auto_follow_pattern(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-auto-follow-pattern.html>`_\n+\n+        :arg name: The name of the auto follow pattern.\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ccr\", \"auto_follow\", name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def pause_follow(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-post-pause-follow.html>`_\n+\n+        :arg index: The name of the follower index that should pause\n+            following its leader index.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_ccr\", \"pause_follow\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def put_auto_follow_pattern(self, name, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-put-auto-follow-pattern.html>`_\n+\n+        :arg name: The name of the auto follow pattern.\n+        :arg body: The specification of the auto follow pattern\n+        \"\"\"\n+        for param in (name, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ccr\", \"auto_follow\", name),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def resume_follow(self, index, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-post-resume-follow.html>`_\n+\n+        :arg index: The name of the follow index to resume following.\n+        :arg body: The name of the leader index and other optional ccr\n+            related parameters\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_ccr\", \"resume_follow\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def stats(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-stats.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_ccr/stats\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def unfollow(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current>`_\n+\n+        :arg index: The name of the follower index that should be turned\n+            into a regular index.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_ccr\", \"unfollow\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def pause_auto_follow_pattern(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-pause-auto-follow-pattern.html>`_\n+\n+        :arg name: The name of the auto follow pattern that should pause\n+            discovering new indices to follow.\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ccr\", \"auto_follow\", name, \"pause\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def resume_auto_follow_pattern(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-resume-auto-follow-pattern.html>`_\n+\n+        :arg name: The name of the auto follow pattern to resume\n+            discovering new indices to follow.\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ccr\", \"auto_follow\", name, \"resume\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "4cbcddc7139168c0a2e0541bfa7a86c674154e11",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cluster.py",
                "status": "added",
                "additions": 237,
                "deletions": 0,
                "changes": 237,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cluster.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cluster.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/cluster.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,237 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class ClusterClient(NamespacedClient):\n+    @query_params(\n+        \"expand_wildcards\",\n+        \"level\",\n+        \"local\",\n+        \"master_timeout\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+        \"wait_for_events\",\n+        \"wait_for_no_initializing_shards\",\n+        \"wait_for_no_relocating_shards\",\n+        \"wait_for_nodes\",\n+        \"wait_for_status\",\n+    )\n+    def health(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns basic information about the health of the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-health.html>`_\n+\n+        :arg index: Limit the information returned to a specific index\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: all\n+        :arg level: Specify the level of detail for returned information\n+            Valid choices: cluster, indices, shards  Default: cluster\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Wait until the specified number of\n+            shards is active\n+        :arg wait_for_events: Wait until all currently queued events\n+            with the given priority are processed  Valid choices: immediate, urgent,\n+            high, normal, low, languid\n+        :arg wait_for_no_initializing_shards: Whether to wait until\n+            there are no initializing shards in the cluster\n+        :arg wait_for_no_relocating_shards: Whether to wait until there\n+            are no relocating shards in the cluster\n+        :arg wait_for_nodes: Wait until the specified number of nodes is\n+            available\n+        :arg wait_for_status: Wait until cluster is in a specific state\n+            Valid choices: green, yellow, red\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_cluster\", \"health\", index),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"local\", \"master_timeout\")\n+    def pending_tasks(self, params=None, headers=None):\n+        \"\"\"\n+        Returns a list of any cluster-level changes (e.g. create index, update mapping,\n+        allocate or fail shard) which have not yet been executed.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-pending.html>`_\n+\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cluster/pending_tasks\", params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"flat_settings\",\n+        \"ignore_unavailable\",\n+        \"local\",\n+        \"master_timeout\",\n+        \"wait_for_metadata_version\",\n+        \"wait_for_timeout\",\n+    )\n+    def state(self, metric=None, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns a comprehensive information about the state of the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-state.html>`_\n+\n+        :arg metric: Limit the information returned to the specified\n+            metrics  Valid choices: _all, blocks, metadata, nodes, routing_table,\n+            routing_nodes, master_node, version\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg wait_for_metadata_version: Wait for the metadata version to\n+            be equal or greater than the specified metadata version\n+        :arg wait_for_timeout: The maximum time to wait for\n+            wait_for_metadata_version before timing out\n+        \"\"\"\n+        if index and metric in SKIP_IN_PATH:\n+            metric = \"_all\"\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_cluster\", \"state\", metric, index),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"flat_settings\", \"timeout\")\n+    def stats(self, node_id=None, params=None, headers=None):\n+        \"\"\"\n+        Returns high-level overview of cluster statistics.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-stats.html>`_\n+\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information; use `_local` to return information from\n+            the node you're connecting to, leave empty to get information from all\n+            nodes\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            \"/_cluster/stats\"\n+            if node_id in SKIP_IN_PATH\n+            else _make_path(\"_cluster\", \"stats\", \"nodes\", node_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"dry_run\", \"explain\", \"master_timeout\", \"metric\", \"retry_failed\", \"timeout\"\n+    )\n+    def reroute(self, body=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to manually change the allocation of individual shards in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-reroute.html>`_\n+\n+        :arg body: The definition of `commands` to perform (`move`,\n+            `cancel`, `allocate`)\n+        :arg dry_run: Simulate the operation only and return the\n+            resulting state\n+        :arg explain: Return an explanation of why the commands can or\n+            cannot be executed\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg metric: Limit the information returned to the specified\n+            metrics. Defaults to all but metadata  Valid choices: _all, blocks,\n+            metadata, nodes, routing_table, master_node, version\n+        :arg retry_failed: Retries allocation of shards that are blocked\n+            due to too many subsequent allocation failures\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_cluster/reroute\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"flat_settings\", \"include_defaults\", \"master_timeout\", \"timeout\")\n+    def get_settings(self, params=None, headers=None):\n+        \"\"\"\n+        Returns cluster settings.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-update-settings.html>`_\n+\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg include_defaults: Whether to return all default clusters\n+            setting.\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_cluster/settings\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"flat_settings\", \"master_timeout\", \"timeout\")\n+    def put_settings(self, body, params=None, headers=None):\n+        \"\"\"\n+        Updates the cluster settings.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-update-settings.html>`_\n+\n+        :arg body: The settings to be updated. Can be either `transient`\n+            or `persistent` (survives cluster restart).\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\", \"/_cluster/settings\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params()\n+    def remote_info(self, params=None, headers=None):\n+        \"\"\"\n+        Returns the information about configured remote clusters.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-remote-info.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_remote/info\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"include_disk_info\", \"include_yes_decisions\")\n+    def allocation_explain(self, body=None, params=None, headers=None):\n+        \"\"\"\n+        Provides explanations for shard allocations in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-allocation-explain.html>`_\n+\n+        :arg body: The index, shard, and primary flag to explain. Empty\n+            means 'explain the first unassigned shard'\n+        :arg include_disk_info: Return information about disk usage and\n+            shard sizes (default: false)\n+        :arg include_yes_decisions: Return 'YES' decisions in\n+            explanation (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_cluster/allocation/explain\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )"
            },
            {
                "sha": "e7ff49a565d55bd3b233b44aa5f00eeb9ee0fcf5",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/data_frame.py",
                "status": "added",
                "additions": 131,
                "deletions": 0,
                "changes": 131,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/data_frame.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/data_frame.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/data_frame.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,131 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class Data_FrameClient(NamespacedClient):\n+    @query_params()\n+    def delete_data_frame_transform(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/delete-data-frame-transform.html>`_\n+\n+        :arg transform_id: The id of the transform to delete\n+        \"\"\"\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_data_frame\", \"transforms\", transform_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"from_\", \"size\")\n+    def get_data_frame_transform(self, transform_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-data-frame-transform.html>`_\n+\n+        :arg transform_id: The id or comma delimited list of id expressions of\n+            the transforms to get, '_all' or '*' implies get all transforms\n+        :arg from_: skips a number of transform configs, defaults to 0\n+        :arg size: specifies a max number of transforms to get, defaults to 100\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_data_frame\", \"transforms\", transform_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_data_frame_transform_stats(\n+        self, transform_id=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-data-frame-transform-stats.html>`_\n+\n+        :arg transform_id: The id of the transform for which to get stats.\n+            '_all' or '*' implies all transforms\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_data_frame\", \"transforms\", transform_id, \"_stats\"),\n+            params=params,\n+        )\n+\n+    @query_params()\n+    def preview_data_frame_transform(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/preview-data-frame-transform.html>`_\n+\n+        :arg body: The definition for the data_frame transform to preview\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_data_frame/transforms/_preview\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def put_data_frame_transform(self, transform_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/put-data-frame-transform.html>`_\n+\n+        :arg transform_id: The id of the new transform.\n+        :arg body: The data frame transform definition\n+        \"\"\"\n+        for param in (transform_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_data_frame\", \"transforms\", transform_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"timeout\")\n+    def start_data_frame_transform(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/start-data-frame-transform.html>`_\n+\n+        :arg transform_id: The id of the transform to start\n+        :arg timeout: Controls the time to wait for the transform to start\n+        \"\"\"\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_data_frame\", \"transforms\", transform_id, \"_start\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"timeout\", \"wait_for_completion\")\n+    def stop_data_frame_transform(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/stop-data-frame-transform.html>`_\n+\n+        :arg transform_id: The id of the transform to stop\n+        :arg timeout: Controls the time to wait until the transform has stopped.\n+            Default to 30 seconds\n+        :arg wait_for_completion: Whether to wait for the transform to fully\n+            stop before returning or not. Default to false\n+        \"\"\"\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_data_frame\", \"transforms\", transform_id, \"_stop\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "8b0e9d70e39797179d77955b7ba9de13430fdd3e",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/deprecation.py",
                "status": "added",
                "additions": 17,
                "deletions": 0,
                "changes": 17,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/deprecation.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/deprecation.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/deprecation.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,17 @@\n+from .utils import NamespacedClient, query_params, _make_path\n+\n+\n+class DeprecationClient(NamespacedClient):\n+    @query_params()\n+    def info(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        `<http://www.elastic.co/guide/en/migration/current/migration-api-deprecation.html>`_\n+\n+        :arg index: Index pattern\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(index, \"_xpack\", \"migration\", \"deprecations\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "48f30902571a200762de8ab88759f1ecb7d00631",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/enrich.py",
                "status": "added",
                "additions": 80,
                "deletions": 0,
                "changes": 80,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/enrich.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/enrich.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/enrich.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,80 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class EnrichClient(NamespacedClient):\n+    @query_params()\n+    def delete_policy(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/enrich-delete-policy.html>`_\n+\n+        :arg name: The name of the enrich policy\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_enrich\", \"policy\", name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"wait_for_completion\")\n+    def execute_policy(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/enrich-execute-policy.html>`_\n+\n+        :arg name: The name of the enrich policy\n+        :arg wait_for_completion: Should the request should block until\n+            the execution is complete.  Default: True\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_enrich\", \"policy\", name, \"_execute\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_policy(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/enrich-get-policy.html>`_\n+\n+        :arg name: A comma-separated list of enrich policy names\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_enrich\", \"policy\", name), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def put_policy(self, name, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/enrich-put-policy.html>`_\n+\n+        :arg name: The name of the enrich policy\n+        :arg body: The enrich policy to register\n+        \"\"\"\n+        for param in (name, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_enrich\", \"policy\", name),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def stats(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/enrich-stats.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_enrich/_stats\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "526d1d273fcbebb38fdce6652d8d14dd290b3f33",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/graph.py",
                "status": "added",
                "additions": 27,
                "deletions": 0,
                "changes": 27,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/graph.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/graph.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/graph.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,27 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class GraphClient(NamespacedClient):\n+    @query_params(\"routing\", \"timeout\")\n+    def explore(self, index, body=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/graph-explore-api.html>`_\n+\n+        :arg index: A comma-separated list of index names to search; use\n+            `_all` or empty string to perform the operation on all indices\n+        :arg body: Graph Query DSL\n+        :arg doc_type: A comma-separated list of document types to\n+            search; leave empty to perform the operation on all types\n+        :arg routing: Specific routing value\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_graph\", \"explore\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )"
            },
            {
                "sha": "a7a147005ed6e4c84624c3fc491d8aaec9c210b3",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ilm.py",
                "status": "added",
                "additions": 147,
                "deletions": 0,
                "changes": 147,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ilm.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ilm.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ilm.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,147 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class IlmClient(NamespacedClient):\n+    @query_params()\n+    def delete_lifecycle(self, policy, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-delete-lifecycle.html>`_\n+\n+        :arg policy: The name of the index lifecycle policy\n+        \"\"\"\n+        if policy in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'policy'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ilm\", \"policy\", policy),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"only_errors\", \"only_managed\")\n+    def explain_lifecycle(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-explain-lifecycle.html>`_\n+\n+        :arg index: The name of the index to explain\n+        :arg only_errors: filters the indices included in the response\n+            to ones in an ILM error state, implies only_managed\n+        :arg only_managed: filters the indices included in the response\n+            to ones managed by ILM\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_ilm\", \"explain\"), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_lifecycle(self, policy=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-get-lifecycle.html>`_\n+\n+        :arg policy: The name of the index lifecycle policy\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_ilm\", \"policy\", policy), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_status(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-get-status.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_ilm/status\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def move_to_step(self, index, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-move-to-step.html>`_\n+\n+        :arg index: The name of the index whose lifecycle step is to\n+            change\n+        :arg body: The new lifecycle step to move to\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ilm\", \"move\", index),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def put_lifecycle(self, policy, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-put-lifecycle.html>`_\n+\n+        :arg policy: The name of the index lifecycle policy\n+        :arg body: The lifecycle policy definition to register\n+        \"\"\"\n+        if policy in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'policy'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ilm\", \"policy\", policy),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def remove_policy(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-remove-policy.html>`_\n+\n+        :arg index: The name of the index to remove policy on\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_ilm\", \"remove\"), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def retry(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-retry-policy.html>`_\n+\n+        :arg index: The name of the indices (comma-separated) whose\n+            failed lifecycle step is to be retry\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_ilm\", \"retry\"), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def start(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-start.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_ilm/start\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def stop(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-stop.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_ilm/stop\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "4e42e6da6614028057829e3d926a1b0f4adf4121",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/indices.py",
                "status": "added",
                "additions": 1276,
                "deletions": 0,
                "changes": 1276,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/indices.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/indices.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/indices.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,1276 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class IndicesClient(NamespacedClient):\n+    @query_params()\n+    def analyze(self, body=None, index=None, params=None, headers=None):\n+        \"\"\"\n+        Performs the analysis process on a text and return the tokens breakdown of the\n+        text.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-analyze.html>`_\n+\n+        :arg body: Define analyzer/tokenizer parameters and the text on\n+            which the analysis should be performed\n+        :arg index: The name of the index to scope the operation\n+        :arg index: The name of the index to scope the operation\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_analyze\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\")\n+    def refresh(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Performs the refresh operation in one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-refresh.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_refresh\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"force\",\n+        \"ignore_unavailable\",\n+        \"wait_if_ongoing\",\n+    )\n+    def flush(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Performs the flush operation on one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-flush.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string for all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg force: Whether a flush should be forced even if it is not\n+            necessarily needed ie. if no changes will be committed to the index.\n+            This is useful if transaction log IDs should be incremented even if no\n+            uncommitted changes are present. (This setting can be considered as\n+            internal)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg wait_if_ongoing: If set to true the flush operation will\n+            block until the flush can be executed if another flush operation is\n+            already executing. The default is true. If set to false the flush will\n+            be skipped iff if another flush operation is already running.\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_flush\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"include_type_name\", \"master_timeout\", \"timeout\", \"wait_for_active_shards\"\n+    )\n+    def create(self, index, body=None, params=None, headers=None):\n+        \"\"\"\n+        Creates an index with optional settings and mappings.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-create-index.html>`_\n+\n+        :arg index: The name of the index\n+        :arg body: The configuration for the index (`settings` and\n+            `mappings`)\n+        :arg include_type_name: Whether a type should be expected in the\n+            body of the mappings.\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Set the number of active shards to\n+            wait for before the operation returns.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\", _make_path(index), params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\", \"wait_for_active_shards\")\n+    def clone(self, index, target, body=None, params=None, headers=None):\n+        \"\"\"\n+        Clones an index\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-clone-index.html>`_\n+\n+        :arg index: The name of the source index to clone\n+        :arg target: The name of the target index to clone into\n+        :arg body: The configuration for the target index (`settings`\n+            and `aliases`)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Set the number of active shards to\n+            wait for on the cloned index before the operation returns.\n+        \"\"\"\n+        for param in (index, target):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, \"_clone\", target),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"flat_settings\",\n+        \"ignore_unavailable\",\n+        \"include_defaults\",\n+        \"include_type_name\",\n+        \"local\",\n+        \"master_timeout\",\n+    )\n+    def get(self, index, params=None, headers=None):\n+        \"\"\"\n+        Returns information about one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-get-index.html>`_\n+\n+        :arg index: A comma-separated list of index names\n+        :arg allow_no_indices: Ignore if a wildcard expression resolves\n+            to no concrete indices (default: false)\n+        :arg expand_wildcards: Whether wildcard expressions should get\n+            expanded to open or closed indices (default: open)  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg ignore_unavailable: Ignore unavailable indexes (default:\n+            false)\n+        :arg include_defaults: Whether to return all default setting for\n+            each of the indices.\n+        :arg include_type_name: Whether to add the type name to the\n+            response (default: false)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"master_timeout\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def open(self, index, params=None, headers=None):\n+        \"\"\"\n+        Opens an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-open-close.html>`_\n+\n+        :arg index: A comma separated list of indices to open\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: closed\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Sets the number of active shards to\n+            wait for before the operation returns.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_open\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"master_timeout\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def close(self, index, params=None, headers=None):\n+        \"\"\"\n+        Closes an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-open-close.html>`_\n+\n+        :arg index: A comma separated list of indices to close\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Sets the number of active shards to\n+            wait for before the operation returns.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_close\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"master_timeout\",\n+        \"timeout\",\n+    )\n+    def delete(self, index, params=None, headers=None):\n+        \"\"\"\n+        Deletes an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-delete-index.html>`_\n+\n+        :arg index: A comma-separated list of indices to delete; use\n+            `_all` or `*` string to delete all indices\n+        :arg allow_no_indices: Ignore if a wildcard expression resolves\n+            to no concrete indices (default: false)\n+        :arg expand_wildcards: Whether wildcard expressions should get\n+            expanded to open or closed indices (default: open)  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Ignore unavailable indexes (default:\n+            false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", _make_path(index), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"flat_settings\",\n+        \"ignore_unavailable\",\n+        \"include_defaults\",\n+        \"local\",\n+    )\n+    def exists(self, index, params=None, headers=None):\n+        \"\"\"\n+        Returns information about whether a particular index exists.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-exists.html>`_\n+\n+        :arg index: A comma-separated list of index names\n+        :arg allow_no_indices: Ignore if a wildcard expression resolves\n+            to no concrete indices (default: false)\n+        :arg expand_wildcards: Whether wildcard expressions should get\n+            expanded to open or closed indices (default: open)  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg ignore_unavailable: Ignore unavailable indexes (default:\n+            false)\n+        :arg include_defaults: Whether to return all default setting for\n+            each of the indices.\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"HEAD\", _make_path(index), params=params, headers=headers\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\", \"local\")\n+    def exists_type(self, index, doc_type, params=None, headers=None):\n+        \"\"\"\n+        Returns information about whether a particular document type exists.\n+        (DEPRECATED)\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-types-exists.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` to\n+            check the types across all indices\n+        :arg doc_type: A comma-separated list of document types to check\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        \"\"\"\n+        for param in (index, doc_type):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"HEAD\",\n+            _make_path(index, \"_mapping\", doc_type),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"include_type_name\",\n+        \"master_timeout\",\n+        \"timeout\",\n+    )\n+    def put_mapping(self, body, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Updates the index mappings.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-put-mapping.html>`_\n+\n+        :arg body: The mapping definition\n+        :arg index: A comma-separated list of index names the mapping\n+            should be added to (supports wildcards); use `_all` or omit to add the\n+            mapping on all indices.\n+        :arg doc_type: The name of the document type\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg include_type_name: Whether a type should be expected in the\n+            body of the mappings.\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        if doc_type not in SKIP_IN_PATH and index in SKIP_IN_PATH:\n+            index = \"_all\"\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, doc_type, \"_mapping\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"include_type_name\",\n+        \"local\",\n+        \"master_timeout\",\n+    )\n+    def get_mapping(self, index=None, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        Returns mappings for one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-get-mapping.html>`_\n+\n+        :arg index: A comma-separated list of index names\n+        :arg doc_type: A comma-separated list of document types\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg include_type_name: Whether to add the type name to the\n+            response (default: false)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(index, \"_mapping\", doc_type),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"include_defaults\",\n+        \"include_type_name\",\n+        \"local\",\n+    )\n+    def get_field_mapping(\n+        self, fields, index=None, doc_type=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Returns mapping for one or more fields.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-get-field-mapping.html>`_\n+\n+        :arg fields: A comma-separated list of fields\n+        :arg index: A comma-separated list of index names\n+        :arg doc_type: A comma-separated list of document types\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg include_defaults: Whether the default mapping values should\n+            be returned as well\n+        :arg include_type_name: Whether a type should be returned in the\n+            body of the mappings.\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        \"\"\"\n+        if fields in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'fields'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(index, \"_mapping\", doc_type, \"field\", fields),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def put_alias(self, index, name, body=None, params=None, headers=None):\n+        \"\"\"\n+        Creates or updates an alias.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-aliases.html>`_\n+\n+        :arg index: A comma-separated list of index names the alias\n+            should point to (supports wildcards); use `_all` to perform the\n+            operation on all indices.\n+        :arg name: The name of the alias to be created or updated\n+        :arg body: The settings for the alias, such as `routing` or\n+            `filter`\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit timestamp for the document\n+        \"\"\"\n+        for param in (index, name):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, \"_alias\", name),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\", \"local\")\n+    def exists_alias(self, name, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about whether a particular alias exists.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-aliases.html>`_\n+\n+        :arg name: A comma-separated list of alias names to return\n+        :arg index: A comma-separated list of index names to filter\n+            aliases\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: all\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"HEAD\", _make_path(index, \"_alias\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\", \"local\")\n+    def get_alias(self, index=None, name=None, params=None, headers=None):\n+        \"\"\"\n+        Returns an alias.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-aliases.html>`_\n+\n+        :arg index: A comma-separated list of index names to filter\n+            aliases\n+        :arg name: A comma-separated list of alias names to return\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: all\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_alias\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def update_aliases(self, body, params=None, headers=None):\n+        \"\"\"\n+        Updates index aliases.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-aliases.html>`_\n+\n+        :arg body: The definition of `actions` to perform\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Request timeout\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_aliases\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def delete_alias(self, index, name, params=None, headers=None):\n+        \"\"\"\n+        Deletes an alias.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-aliases.html>`_\n+\n+        :arg index: A comma-separated list of index names (supports\n+            wildcards); use `_all` for all indices\n+        :arg name: A comma-separated list of aliases to delete (supports\n+            wildcards); use `_all` to delete all aliases for the specified indices.\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit timestamp for the document\n+        \"\"\"\n+        for param in (index, name):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", _make_path(index, \"_alias\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"create\",\n+        \"flat_settings\",\n+        \"include_type_name\",\n+        \"master_timeout\",\n+        \"order\",\n+        \"timeout\",\n+    )\n+    def put_template(self, name, body, params=None, headers=None):\n+        \"\"\"\n+        Creates or updates an index template.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-templates.html>`_\n+\n+        :arg name: The name of the template\n+        :arg body: The template definition\n+        :arg create: Whether the index template should only be added if\n+            new or can also replace an existing one\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg include_type_name: Whether a type should be returned in the\n+            body of the mappings.\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg order: The order for this template when merging multiple\n+            matching ones (higher numbers are merged later, overriding the lower\n+            numbers)\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        for param in (name, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_template\", name),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"flat_settings\", \"local\", \"master_timeout\")\n+    def exists_template(self, name, params=None, headers=None):\n+        \"\"\"\n+        Returns information about whether a particular index template exists.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-templates.html>`_\n+\n+        :arg name: The comma separated names of the index templates\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"HEAD\", _make_path(\"_template\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\"flat_settings\", \"include_type_name\", \"local\", \"master_timeout\")\n+    def get_template(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        Returns an index template.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-templates.html>`_\n+\n+        :arg name: The comma separated names of the index templates\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg include_type_name: Whether a type should be returned in the\n+            body of the mappings.\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_template\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def delete_template(self, name, params=None, headers=None):\n+        \"\"\"\n+        Deletes an index template.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-templates.html>`_\n+\n+        :arg name: The name of the template\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", _make_path(\"_template\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"flat_settings\",\n+        \"ignore_unavailable\",\n+        \"include_defaults\",\n+        \"local\",\n+        \"master_timeout\",\n+    )\n+    def get_settings(self, index=None, name=None, params=None, headers=None):\n+        \"\"\"\n+        Returns settings for one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-get-settings.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg name: The name of the settings that should be included\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: ['open', 'closed']\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg include_defaults: Whether to return all default setting for\n+            each of the indices.\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_settings\", name), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"flat_settings\",\n+        \"ignore_unavailable\",\n+        \"master_timeout\",\n+        \"preserve_existing\",\n+        \"timeout\",\n+    )\n+    def put_settings(self, body, index=None, params=None, headers=None):\n+        \"\"\"\n+        Updates the index settings.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-update-settings.html>`_\n+\n+        :arg body: The index settings to be updated\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg preserve_existing: Whether to update existing settings. If\n+            set to `true` existing settings on an index remain unchanged, the\n+            default is `false`\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, \"_settings\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"completion_fields\",\n+        \"expand_wildcards\",\n+        \"fielddata_fields\",\n+        \"fields\",\n+        \"forbid_closed_indices\",\n+        \"groups\",\n+        \"include_segment_file_sizes\",\n+        \"include_unloaded_segments\",\n+        \"level\",\n+        \"types\",\n+    )\n+    def stats(self, index=None, metric=None, params=None, headers=None):\n+        \"\"\"\n+        Provides statistics on operations happening in an index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-stats.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg metric: Limit the information returned the specific\n+            metrics.  Valid choices: _all, completion, docs, fielddata, query_cache,\n+            flush, get, indexing, merge, request_cache, refresh, search, segments,\n+            store, warmer, suggest\n+        :arg completion_fields: A comma-separated list of fields for\n+            `fielddata` and `suggest` index metric (supports wildcards)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg fielddata_fields: A comma-separated list of fields for\n+            `fielddata` index metric (supports wildcards)\n+        :arg fields: A comma-separated list of fields for `fielddata`\n+            and `completion` index metric (supports wildcards)\n+        :arg forbid_closed_indices: If set to false stats will also\n+            collected from closed indices if explicitly specified or if\n+            expand_wildcards expands to closed indices  Default: True\n+        :arg groups: A comma-separated list of search groups for\n+            `search` index metric\n+        :arg include_segment_file_sizes: Whether to report the\n+            aggregated disk usage of each one of the Lucene index files (only\n+            applies if segment stats are requested)\n+        :arg include_unloaded_segments: If set to true segment stats\n+            will include stats for segments that are not currently loaded into\n+            memory\n+        :arg level: Return stats aggregated at cluster, index or shard\n+            level  Valid choices: cluster, indices, shards  Default: indices\n+        :arg types: A comma-separated list of document types for the\n+            `indexing` index metric\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_stats\", metric), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\", \"verbose\"\n+    )\n+    def segments(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Provides low-level information about segments in a Lucene index.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-segments.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg verbose: Includes detailed memory usage by Lucene.\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_segments\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"all_shards\",\n+        \"allow_no_indices\",\n+        \"analyze_wildcard\",\n+        \"analyzer\",\n+        \"default_operator\",\n+        \"df\",\n+        \"expand_wildcards\",\n+        \"explain\",\n+        \"ignore_unavailable\",\n+        \"lenient\",\n+        \"q\",\n+        \"rewrite\",\n+    )\n+    def validate_query(\n+        self, body=None, index=None, doc_type=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Allows a user to validate a potentially expensive query without executing it.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/search-validate.html>`_\n+\n+        :arg body: The query definition specified with the Query DSL\n+        :arg index: A comma-separated list of index names to restrict\n+            the operation; use `_all` or empty string to perform the operation on\n+            all indices\n+        :arg doc_type: A comma-separated list of document types to\n+            restrict the operation; leave empty to perform the operation on all\n+            types\n+        :arg all_shards: Execute validation on all shards instead of one\n+            random shard per index\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg analyze_wildcard: Specify whether wildcard and prefix\n+            queries should be analyzed (default: false)\n+        :arg analyzer: The analyzer to use for the query string\n+        :arg default_operator: The default operator for query string\n+            query (AND or OR)  Valid choices: AND, OR  Default: OR\n+        :arg df: The field to use as default where no field prefix is\n+            given in the query string\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg explain: Return detailed information about the error\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg lenient: Specify whether format-based query failures (such\n+            as providing text to a numeric field) should be ignored\n+        :arg q: Query in the Lucene query string syntax\n+        :arg rewrite: Provide a more detailed explanation showing the\n+            actual Lucene query that will be executed.\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_validate\", \"query\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"fielddata\",\n+        \"fields\",\n+        \"ignore_unavailable\",\n+        \"query\",\n+        \"request\",\n+    )\n+    def clear_cache(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Clears all or specific caches for one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-clearcache.html>`_\n+\n+        :arg index: A comma-separated list of index name to limit the\n+            operation\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg fielddata: Clear field data\n+        :arg fields: A comma-separated list of fields to clear when\n+            using the `fielddata` parameter (default: all)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg index: A comma-separated list of index name to limit the\n+            operation\n+        :arg query: Clear query caches\n+        :arg request: Clear request cache\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_cache\", \"clear\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\"active_only\", \"detailed\")\n+    def recovery(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about ongoing index shard recoveries.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-recovery.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg active_only: Display only those recoveries that are\n+            currently on-going\n+        :arg detailed: Whether to display detailed information about\n+            shard recovery\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_recovery\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"only_ancient_segments\",\n+        \"wait_for_completion\",\n+    )\n+    def upgrade(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        The _upgrade API is no longer useful and will be removed.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-upgrade.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg only_ancient_segments: If true, only ancient (an older\n+            Lucene major release) segments will be upgraded\n+        :arg wait_for_completion: Specify whether the request should\n+            block until the all segments are upgraded (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_upgrade\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\")\n+    def get_upgrade(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        The _upgrade API is no longer useful and will be removed.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-upgrade.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_upgrade\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\")\n+    def flush_synced(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Performs a synced flush operation on one or more indices. Synced flush is\n+        deprecated and will be removed in 8.0. Use flush instead\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-synced-flush-api.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string for all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, \"_flush\", \"synced\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\", \"status\"\n+    )\n+    def shard_stores(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Provides store information for shard copies of indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-shards-stores.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg status: A comma-separated list of statuses used to filter\n+            on shards to get store information for  Valid choices: green, yellow,\n+            red, all\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_shard_stores\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"flush\",\n+        \"ignore_unavailable\",\n+        \"max_num_segments\",\n+        \"only_expunge_deletes\",\n+    )\n+    def forcemerge(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        Performs the force merge operation on one or more indices.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-forcemerge.html>`_\n+\n+        :arg index: A comma-separated list of index names; use `_all` or\n+            empty string to perform the operation on all indices\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg flush: Specify whether the index should be flushed after\n+            performing the operation (default: true)\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg max_num_segments: The number of segments the index should\n+            be merged into (default: dynamic)\n+        :arg only_expunge_deletes: Specify whether the operation should\n+            only expunge deleted documents\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_forcemerge\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"copy_settings\", \"master_timeout\", \"timeout\", \"wait_for_active_shards\"\n+    )\n+    def shrink(self, index, target, body=None, params=None, headers=None):\n+        \"\"\"\n+        Allow to shrink an existing index into a new index with fewer primary shards.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-shrink-index.html>`_\n+\n+        :arg index: The name of the source index to shrink\n+        :arg target: The name of the target index to shrink into\n+        :arg body: The configuration for the target index (`settings`\n+            and `aliases`)\n+        :arg copy_settings: whether or not to copy settings from the\n+            source index (defaults to false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Set the number of active shards to\n+            wait for on the shrunken index before the operation returns.\n+        \"\"\"\n+        for param in (index, target):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, \"_shrink\", target),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"copy_settings\", \"master_timeout\", \"timeout\", \"wait_for_active_shards\"\n+    )\n+    def split(self, index, target, body=None, params=None, headers=None):\n+        \"\"\"\n+        Allows you to split an existing index into a new index with more primary\n+        shards.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-split-index.html>`_\n+\n+        :arg index: The name of the source index to split\n+        :arg target: The name of the target index to split into\n+        :arg body: The configuration for the target index (`settings`\n+            and `aliases`)\n+        :arg copy_settings: whether or not to copy settings from the\n+            source index (defaults to false)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Set the number of active shards to\n+            wait for on the shrunken index before the operation returns.\n+        \"\"\"\n+        for param in (index, target):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(index, \"_split\", target),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"dry_run\",\n+        \"include_type_name\",\n+        \"master_timeout\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def rollover(self, alias, body=None, new_index=None, params=None, headers=None):\n+        \"\"\"\n+        Updates an alias to point to a new index when the existing index is considered\n+        to be too large or too old.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-rollover-index.html>`_\n+\n+        :arg alias: The name of the alias to rollover\n+        :arg body: The conditions that needs to be met for executing\n+            rollover\n+        :arg new_index: The name of the rollover index\n+        :arg dry_run: If set to true the rollover action will only be\n+            validated but not actually performed even if a condition matches. The\n+            default is false\n+        :arg include_type_name: Whether a type should be included in the\n+            body of the mappings.\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Set the number of active shards to\n+            wait for on the newly created rollover index before the operation\n+            returns.\n+        \"\"\"\n+        if alias in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'alias'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(alias, \"_rollover\", new_index),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"master_timeout\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def freeze(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/frozen.html>`_\n+\n+        :arg index: The name of the index to freeze\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: closed\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Sets the number of active shards to\n+            wait for before the operation returns.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_freeze\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"allow_no_indices\",\n+        \"expand_wildcards\",\n+        \"ignore_unavailable\",\n+        \"master_timeout\",\n+        \"timeout\",\n+        \"wait_for_active_shards\",\n+    )\n+    def unfreeze(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/frozen.html>`_\n+\n+        :arg index: The name of the index to unfreeze\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: closed\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        :arg master_timeout: Specify timeout for connection to master\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_active_shards: Sets the number of active shards to\n+            wait for before the operation returns.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", _make_path(index, \"_unfreeze\"), params=params, headers=headers\n+        )\n+\n+    @query_params(\"allow_no_indices\", \"expand_wildcards\", \"ignore_unavailable\")\n+    def reload_search_analyzers(self, index, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-reload-analyzers.html>`_\n+\n+        :arg index: A comma-separated list of index names to reload\n+            analyzers for\n+        :arg allow_no_indices: Whether to ignore if a wildcard indices\n+            expression resolves into no concrete indices. (This includes `_all`\n+            string or when no indices have been specified)\n+        :arg expand_wildcards: Whether to expand wildcard expression to\n+            concrete indices that are open, closed or both.  Valid choices: open,\n+            closed, none, all  Default: open\n+        :arg ignore_unavailable: Whether specified concrete indices\n+            should be ignored when unavailable (missing or closed)\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(index, \"_reload_search_analyzers\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "50bd8831ab546ad66aa31e1f94e3b8ba8f57e52e",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ingest.py",
                "status": "added",
                "additions": 96,
                "deletions": 0,
                "changes": 96,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ingest.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ingest.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ingest.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,96 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class IngestClient(NamespacedClient):\n+    @query_params(\"master_timeout\")\n+    def get_pipeline(self, id=None, params=None, headers=None):\n+        \"\"\"\n+        Returns a pipeline.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/get-pipeline-api.html>`_\n+\n+        :arg id: Comma separated list of pipeline ids. Wildcards\n+            supported\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_ingest\", \"pipeline\", id), params=params, headers=headers\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def put_pipeline(self, id, body, params=None, headers=None):\n+        \"\"\"\n+        Creates or updates a pipeline.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/put-pipeline-api.html>`_\n+\n+        :arg id: Pipeline ID\n+        :arg body: The ingest definition\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        for param in (id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ingest\", \"pipeline\", id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def delete_pipeline(self, id, params=None, headers=None):\n+        \"\"\"\n+        Deletes a pipeline.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-pipeline-api.html>`_\n+\n+        :arg id: Pipeline ID\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ingest\", \"pipeline\", id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"verbose\")\n+    def simulate(self, body, id=None, params=None, headers=None):\n+        \"\"\"\n+        Allows to simulate a pipeline with example documents.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/simulate-pipeline-api.html>`_\n+\n+        :arg body: The simulate definition\n+        :arg id: Pipeline ID\n+        :arg verbose: Verbose mode. Display data output for each\n+            processor in executed pipeline\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ingest\", \"pipeline\", id, \"_simulate\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def processor_grok(self, params=None, headers=None):\n+        \"\"\"\n+        Returns a list of the built-in patterns.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/grok-processor.html#grok-processor-rest-get>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_ingest/processor/grok\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "dc2d52fe07db0e247a68de8c60981cd829f60297",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/license.py",
                "status": "added",
                "additions": 90,
                "deletions": 0,
                "changes": 90,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/license.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/license.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/license.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,90 @@\n+from .utils import NamespacedClient, query_params\n+\n+\n+class LicenseClient(NamespacedClient):\n+    @query_params()\n+    def delete(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/delete-license.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"DELETE\", \"/_license\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"accept_enterprise\", \"local\")\n+    def get(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/get-license.html>`_\n+\n+        :arg accept_enterprise: If the active license is an enterprise\n+            license, return type as 'enterprise' (default: false)\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_license\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_basic_status(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/get-basic-status.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_license/basic_status\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_trial_status(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/get-trial-status.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_license/trial_status\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"acknowledge\")\n+    def post(self, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/update-license.html>`_\n+\n+        :arg body: licenses to be installed\n+        :arg acknowledge: whether the user has acknowledged acknowledge\n+            messages (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"PUT\", \"/_license\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"acknowledge\")\n+    def post_start_basic(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/start-basic.html>`_\n+\n+        :arg acknowledge: whether the user has acknowledged acknowledge\n+            messages (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_license/start_basic\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"acknowledge\", \"doc_type\")\n+    def post_start_trial(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/start-trial.html>`_\n+\n+        :arg acknowledge: whether the user has acknowledged acknowledge\n+            messages (default: false)\n+        :arg doc_type: The type of trial license to generate (default:\n+            \"trial\")\n+        \"\"\"\n+        # type is a reserved word so it cannot be used, use doc_type instead\n+        if \"doc_type\" in params:\n+            params[\"type\"] = params.pop(\"doc_type\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_license/start_trial\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "1ab4e41c55ceec36f5e5eff4479df96d91e8e568",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/migration.py",
                "status": "added",
                "additions": 17,
                "deletions": 0,
                "changes": 17,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/migration.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/migration.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/migration.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,17 @@\n+from .utils import NamespacedClient, query_params, _make_path\n+\n+\n+class MigrationClient(NamespacedClient):\n+    @query_params()\n+    def deprecations(self, index=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/migration-api-deprecation.html>`_\n+\n+        :arg index: Index pattern\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(index, \"_migration\", \"deprecations\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "340c31ce4839698e93aced2be77746bb72a4cae8",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ml.py",
                "status": "added",
                "additions": 1353,
                "deletions": 0,
                "changes": 1353,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ml.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ml.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ml.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,1353 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH, _bulk_body\n+\n+\n+class MlClient(NamespacedClient):\n+    @query_params(\"allow_no_jobs\", \"force\", \"timeout\")\n+    def close_job(self, job_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-close-job.html>`_\n+\n+        :arg job_id: The name of the job to close\n+        :arg body: The URL params optionally sent in the body\n+        :arg allow_no_jobs: Whether to ignore if a wildcard expression\n+            matches no jobs. (This includes `_all` string or when no jobs have been\n+            specified)\n+        :arg force: True if the job should be forcefully closed\n+        :arg timeout: Controls the time to wait until a job has closed.\n+            Default to 30 minutes\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_close\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def delete_calendar(self, calendar_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar to delete\n+        \"\"\"\n+        if calendar_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'calendar_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def delete_calendar_event(self, calendar_id, event_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar to modify\n+        :arg event_id: The ID of the event to remove from the calendar\n+        \"\"\"\n+        for param in (calendar_id, event_id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id, \"events\", event_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def delete_calendar_job(self, calendar_id, job_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar to modify\n+        :arg job_id: The ID of the job to remove from the calendar\n+        \"\"\"\n+        for param in (calendar_id, job_id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id, \"jobs\", job_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"force\")\n+    def delete_datafeed(self, datafeed_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeed to delete\n+        :arg force: True if the datafeed should be forcefully deleted\n+        \"\"\"\n+        if datafeed_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'datafeed_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def delete_expired_data(self, params=None, headers=None):\n+        \"\"\"\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"DELETE\", \"/_ml/_delete_expired_data\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def delete_filter(self, filter_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg filter_id: The ID of the filter to delete\n+        \"\"\"\n+        if filter_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'filter_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"filters\", filter_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_forecasts\", \"timeout\")\n+    def delete_forecast(self, job_id, forecast_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-forecast.html>`_\n+\n+        :arg job_id: The ID of the job from which to delete forecasts\n+        :arg forecast_id: The ID of the forecast to delete, can be comma\n+            delimited list. Leaving blank implies `_all`\n+        :arg allow_no_forecasts: Whether to ignore if `_all` matches no\n+            forecasts\n+        :arg timeout: Controls the time to wait until the forecast(s)\n+            are deleted. Default to 30 seconds\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_forecast\", forecast_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"force\", \"wait_for_completion\")\n+    def delete_job(self, job_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-job.html>`_\n+\n+        :arg job_id: The ID of the job to delete\n+        :arg force: True if the job should be forcefully deleted\n+        :arg wait_for_completion: Should this request wait until the\n+            operation has completed before returning  Default: True\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def delete_model_snapshot(self, job_id, snapshot_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-snapshot.html>`_\n+\n+        :arg job_id: The ID of the job to fetch\n+        :arg snapshot_id: The ID of the snapshot to delete\n+        \"\"\"\n+        for param in (job_id, snapshot_id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\n+                \"_ml\", \"anomaly_detectors\", job_id, \"model_snapshots\", snapshot_id\n+            ),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"charset\",\n+        \"column_names\",\n+        \"delimiter\",\n+        \"explain\",\n+        \"format\",\n+        \"grok_pattern\",\n+        \"has_header_row\",\n+        \"line_merge_size_limit\",\n+        \"lines_to_sample\",\n+        \"quote\",\n+        \"should_trim_fields\",\n+        \"timeout\",\n+        \"timestamp_field\",\n+        \"timestamp_format\",\n+    )\n+    def find_file_structure(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-find-file-structure.html>`_\n+\n+        :arg body: The contents of the file to be analyzed\n+        :arg charset: Optional parameter to specify the character set of\n+            the file\n+        :arg column_names: Optional parameter containing a comma\n+            separated list of the column names for a delimited file\n+        :arg delimiter: Optional parameter to specify the delimiter\n+            character for a delimited file - must be a single character\n+        :arg explain: Whether to include a commentary on how the\n+            structure was derived\n+        :arg format: Optional parameter to specify the high level file\n+            format  Valid choices: ndjson, xml, delimited, semi_structured_text\n+        :arg grok_pattern: Optional parameter to specify the Grok\n+            pattern that should be used to extract fields from messages in a semi-\n+            structured text file\n+        :arg has_header_row: Optional parameter to specify whether a\n+            delimited file includes the column names in its first row\n+        :arg line_merge_size_limit: Maximum number of characters\n+            permitted in a single message when lines are merged to create messages.\n+            Default: 10000\n+        :arg lines_to_sample: How many lines of the file should be\n+            included in the analysis  Default: 1000\n+        :arg quote: Optional parameter to specify the quote character\n+            for a delimited file - must be a single character\n+        :arg should_trim_fields: Optional parameter to specify whether\n+            the values between delimiters in a delimited file should have whitespace\n+            trimmed from them\n+        :arg timeout: Timeout after which the analysis will be aborted\n+            Default: 25s\n+        :arg timestamp_field: Optional parameter to specify the\n+            timestamp field in the file\n+        :arg timestamp_format: Optional parameter to specify the\n+            timestamp format in the file - may be either a Joda or Java time format\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        body = _bulk_body(self.transport.serializer, body)\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_ml/find_file_structure\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"advance_time\", \"calc_interim\", \"end\", \"skip_time\", \"start\")\n+    def flush_job(self, job_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-flush-job.html>`_\n+\n+        :arg job_id: The name of the job to flush\n+        :arg body: Flush parameters\n+        :arg advance_time: Advances time to the given value generating\n+            results and updating the model for the advanced interval\n+        :arg calc_interim: Calculates interim results for the most\n+            recent bucket or all buckets within the latency period\n+        :arg end: When used in conjunction with calc_interim, specifies\n+            the range of buckets on which to calculate interim results\n+        :arg skip_time: Skips time to the given value without generating\n+            results or updating the model for the skipped interval\n+        :arg start: When used in conjunction with calc_interim,\n+            specifies the range of buckets on which to calculate interim results\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_flush\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"duration\", \"expires_in\")\n+    def forecast(self, job_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg job_id: The ID of the job to forecast for\n+        :arg duration: The duration of the forecast\n+        :arg expires_in: The time interval after which the forecast\n+            expires. Expired forecasts will be deleted at the first opportunity.\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_forecast\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"anomaly_score\",\n+        \"desc\",\n+        \"end\",\n+        \"exclude_interim\",\n+        \"expand\",\n+        \"from_\",\n+        \"size\",\n+        \"sort\",\n+        \"start\",\n+    )\n+    def get_buckets(self, job_id, body=None, timestamp=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-bucket.html>`_\n+\n+        :arg job_id: ID of the job to get bucket results from\n+        :arg body: Bucket selection details if not provided in URI\n+        :arg timestamp: The timestamp of the desired single bucket\n+            result\n+        :arg anomaly_score: Filter for the most anomalous buckets\n+        :arg desc: Set the sort direction\n+        :arg end: End time filter for buckets\n+        :arg exclude_interim: Exclude interim results\n+        :arg expand: Include anomaly records\n+        :arg from_: skips a number of buckets\n+        :arg size: specifies a max number of buckets to get\n+        :arg sort: Sort buckets by a particular field\n+        :arg start: Start time filter for buckets\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\n+                \"_ml\", \"anomaly_detectors\", job_id, \"results\", \"buckets\", timestamp\n+            ),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"end\", \"from_\", \"job_id\", \"size\", \"start\")\n+    def get_calendar_events(self, calendar_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar containing the events\n+        :arg end: Get events before this time\n+        :arg from_: Skips a number of events\n+        :arg job_id: Get events for the job. When this option is used\n+            calendar_id must be '_all'\n+        :arg size: Specifies a max number of events to get\n+        :arg start: Get events after this time\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if calendar_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'calendar_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id, \"events\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"from_\", \"size\")\n+    def get_calendars(self, body=None, calendar_id=None, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: The from and size parameters optionally sent in the\n+            body\n+        :arg calendar_id: The ID of the calendar to fetch\n+        :arg from_: skips a number of calendars\n+        :arg size: specifies a max number of calendars to get\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"from_\", \"size\")\n+    def get_categories(\n+        self, job_id, body=None, category_id=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-category.html>`_\n+\n+        :arg job_id: The name of the job\n+        :arg body: Category selection details if not provided in URI\n+        :arg category_id: The identifier of the category definition of\n+            interest\n+        :arg from_: skips a number of categories\n+        :arg size: specifies a max number of categories to get\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\n+                \"_ml\", \"anomaly_detectors\", job_id, \"results\", \"categories\", category_id\n+            ),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_datafeeds\")\n+    def get_datafeed_stats(self, datafeed_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-datafeed-stats.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeeds stats to fetch\n+        :arg allow_no_datafeeds: Whether to ignore if a wildcard\n+            expression matches no datafeeds. (This includes `_all` string or when no\n+            datafeeds have been specified)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_stats\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_datafeeds\")\n+    def get_datafeeds(self, datafeed_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeeds to fetch\n+        :arg allow_no_datafeeds: Whether to ignore if a wildcard\n+            expression matches no datafeeds. (This includes `_all` string or when no\n+            datafeeds have been specified)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"from_\", \"size\")\n+    def get_filters(self, filter_id=None, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg filter_id: The ID of the filter to fetch\n+        :arg from_: skips a number of filters\n+        :arg size: specifies a max number of filters to get\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"filters\", filter_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"desc\",\n+        \"end\",\n+        \"exclude_interim\",\n+        \"from_\",\n+        \"influencer_score\",\n+        \"size\",\n+        \"sort\",\n+        \"start\",\n+    )\n+    def get_influencers(self, job_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-influencer.html>`_\n+\n+        :arg job_id:\n+        :arg body: Influencer selection criteria\n+        :arg desc: whether the results should be sorted in decending\n+            order\n+        :arg end: end timestamp for the requested influencers\n+        :arg exclude_interim: Exclude interim results\n+        :arg from_: skips a number of influencers\n+        :arg influencer_score: influencer score threshold for the\n+            requested influencers\n+        :arg size: specifies a max number of influencers to get\n+        :arg sort: sort field for the requested influencers\n+        :arg start: start timestamp for the requested influencers\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"results\", \"influencers\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_jobs\")\n+    def get_job_stats(self, job_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-job-stats.html>`_\n+\n+        :arg job_id: The ID of the jobs stats to fetch\n+        :arg allow_no_jobs: Whether to ignore if a wildcard expression\n+            matches no jobs. (This includes `_all` string or when no jobs have been\n+            specified)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_stats\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_jobs\")\n+    def get_jobs(self, job_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-job.html>`_\n+\n+        :arg job_id: The ID of the jobs to fetch\n+        :arg allow_no_jobs: Whether to ignore if a wildcard expression\n+            matches no jobs. (This includes `_all` string or when no jobs have been\n+            specified)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"desc\", \"end\", \"from_\", \"size\", \"sort\", \"start\")\n+    def get_model_snapshots(\n+        self, job_id, body=None, snapshot_id=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-snapshot.html>`_\n+\n+        :arg job_id: The ID of the job to fetch\n+        :arg body: Model snapshot selection criteria\n+        :arg snapshot_id: The ID of the snapshot to fetch\n+        :arg desc: True if the results should be sorted in descending\n+            order\n+        :arg end: The filter 'end' query parameter\n+        :arg from_: Skips a number of documents\n+        :arg size: The default number of documents returned in queries\n+            as a string.\n+        :arg sort: Name of the field to sort on\n+        :arg start: The filter 'start' query parameter\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\n+                \"_ml\", \"anomaly_detectors\", job_id, \"model_snapshots\", snapshot_id\n+            ),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_jobs\",\n+        \"bucket_span\",\n+        \"end\",\n+        \"exclude_interim\",\n+        \"overall_score\",\n+        \"start\",\n+        \"top_n\",\n+    )\n+    def get_overall_buckets(self, job_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-overall-buckets.html>`_\n+\n+        :arg job_id: The job IDs for which to calculate overall bucket\n+            results\n+        :arg body: Overall bucket selection details if not provided in\n+            URI\n+        :arg allow_no_jobs: Whether to ignore if a wildcard expression\n+            matches no jobs. (This includes `_all` string or when no jobs have been\n+            specified)\n+        :arg bucket_span: The span of the overall buckets. Defaults to\n+            the longest job bucket_span\n+        :arg end: Returns overall buckets with timestamps earlier than\n+            this time\n+        :arg exclude_interim: If true overall buckets that include\n+            interim buckets will be excluded\n+        :arg overall_score: Returns overall buckets with overall scores\n+            higher than this value\n+        :arg start: Returns overall buckets with timestamps after this\n+            time\n+        :arg top_n: The number of top job bucket scores to be used in\n+            the overall_score calculation\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\n+                \"_ml\", \"anomaly_detectors\", job_id, \"results\", \"overall_buckets\"\n+            ),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"desc\",\n+        \"end\",\n+        \"exclude_interim\",\n+        \"from_\",\n+        \"record_score\",\n+        \"size\",\n+        \"sort\",\n+        \"start\",\n+    )\n+    def get_records(self, job_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-record.html>`_\n+\n+        :arg job_id:\n+        :arg body: Record selection criteria\n+        :arg desc: Set the sort direction\n+        :arg end: End time filter for records\n+        :arg exclude_interim: Exclude interim results\n+        :arg from_: skips a number of records\n+        :arg record_score:\n+        :arg size: specifies a max number of records to get\n+        :arg sort: Sort records by a particular field\n+        :arg start: Start time filter for records\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"results\", \"records\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def info(self, params=None, headers=None):\n+        \"\"\"\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_ml/info\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def open_job(self, job_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-open-job.html>`_\n+\n+        :arg job_id: The ID of the job to open\n+        \"\"\"\n+        if job_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_open\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def post_calendar_events(self, calendar_id, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar to modify\n+        :arg body: A list of events\n+        \"\"\"\n+        for param in (calendar_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id, \"events\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"reset_end\", \"reset_start\")\n+    def post_data(self, job_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-post-data.html>`_\n+\n+        :arg job_id: The name of the job receiving the data\n+        :arg body: The data to process\n+        :arg reset_end: Optional parameter to specify the end of the\n+            bucket resetting range\n+        :arg reset_start: Optional parameter to specify the start of the\n+            bucket resetting range\n+        \"\"\"\n+        for param in (job_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        body = _bulk_body(self.transport.serializer, body)\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_data\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def preview_datafeed(self, datafeed_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-preview-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeed to preview\n+        \"\"\"\n+        if datafeed_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'datafeed_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_preview\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def put_calendar(self, calendar_id, body=None, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar to create\n+        :arg body: The calendar details\n+        \"\"\"\n+        if calendar_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'calendar_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def put_calendar_job(self, calendar_id, job_id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg calendar_id: The ID of the calendar to modify\n+        :arg job_id: The ID of the job to add to the calendar\n+        \"\"\"\n+        for param in (calendar_id, job_id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"calendars\", calendar_id, \"jobs\", job_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def put_datafeed(self, datafeed_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeed to create\n+        :arg body: The datafeed config\n+        \"\"\"\n+        for param in (datafeed_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def put_filter(self, filter_id, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg filter_id: The ID of the filter to create\n+        :arg body: The filter details\n+        \"\"\"\n+        for param in (filter_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"filters\", filter_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def put_job(self, job_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-job.html>`_\n+\n+        :arg job_id: The ID of the job to create\n+        :arg body: The job\n+        \"\"\"\n+        for param in (job_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"delete_intervening_results\")\n+    def revert_model_snapshot(\n+        self, job_id, snapshot_id, body=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-revert-snapshot.html>`_\n+\n+        :arg job_id: The ID of the job to fetch\n+        :arg snapshot_id: The ID of the snapshot to revert to\n+        :arg body: Reversion options\n+        :arg delete_intervening_results: Should we reset the results\n+            back to the time of the snapshot?\n+        \"\"\"\n+        for param in (job_id, snapshot_id):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\n+                \"_ml\",\n+                \"anomaly_detectors\",\n+                job_id,\n+                \"model_snapshots\",\n+                snapshot_id,\n+                \"_revert\",\n+            ),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"enabled\", \"timeout\")\n+    def set_upgrade_mode(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-set-upgrade-mode.html>`_\n+\n+        :arg enabled: Whether to enable upgrade_mode ML setting or not.\n+            Defaults to false.\n+        :arg timeout: Controls the time to wait before action times out.\n+            Defaults to 30 seconds\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_ml/set_upgrade_mode\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"end\", \"start\", \"timeout\")\n+    def start_datafeed(self, datafeed_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-start-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeed to start\n+        :arg body: The start datafeed parameters\n+        :arg end: The end time when the datafeed should stop. When not\n+            set, the datafeed continues in real time\n+        :arg start: The start time from where the datafeed should begin\n+        :arg timeout: Controls the time to wait until a datafeed has\n+            started. Default to 20 seconds\n+        \"\"\"\n+        if datafeed_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'datafeed_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_start\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_datafeeds\", \"force\", \"timeout\")\n+    def stop_datafeed(self, datafeed_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-stop-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeed to stop\n+        :arg allow_no_datafeeds: Whether to ignore if a wildcard\n+            expression matches no datafeeds. (This includes `_all` string or when no\n+            datafeeds have been specified)\n+        :arg force: True if the datafeed should be forcefully stopped.\n+        :arg timeout: Controls the time to wait until a datafeed has\n+            stopped. Default to 20 seconds\n+        \"\"\"\n+        if datafeed_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'datafeed_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_stop\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def update_datafeed(self, datafeed_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-datafeed.html>`_\n+\n+        :arg datafeed_id: The ID of the datafeed to update\n+        :arg body: The datafeed update settings\n+        \"\"\"\n+        for param in (datafeed_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_update\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def update_filter(self, filter_id, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg filter_id: The ID of the filter to update\n+        :arg body: The filter update\n+        \"\"\"\n+        for param in (filter_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"filters\", filter_id, \"_update\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def update_job(self, job_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-job.html>`_\n+\n+        :arg job_id: The ID of the job to create\n+        :arg body: The job update settings\n+        \"\"\"\n+        for param in (job_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_update\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def update_model_snapshot(\n+        self, job_id, snapshot_id, body, params=None, headers=None\n+    ):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-snapshot.html>`_\n+\n+        :arg job_id: The ID of the job to fetch\n+        :arg snapshot_id: The ID of the snapshot to update\n+        :arg body: The model snapshot properties to update\n+        \"\"\"\n+        for param in (job_id, snapshot_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\n+                \"_ml\",\n+                \"anomaly_detectors\",\n+                job_id,\n+                \"model_snapshots\",\n+                snapshot_id,\n+                \"_update\",\n+            ),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def validate(self, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: The job config\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_ml/anomaly_detectors/_validate\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def validate_detector(self, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: The detector\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_ml/anomaly_detectors/_validate/detector\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"force\")\n+    def delete_data_frame_analytics(self, id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/delete-dfanalytics.html>`_\n+\n+        :arg id: The ID of the data frame analytics to delete\n+        :arg force: True if the job should be forcefully deleted\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def evaluate_data_frame(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/evaluate-dfanalytics.html>`_\n+\n+        :arg body: The evaluation definition\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            \"/_ml/data_frame/_evaluate\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_match\", \"from_\", \"size\")\n+    def get_data_frame_analytics(self, id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-dfanalytics.html>`_\n+\n+        :arg id: The ID of the data frame analytics to fetch\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no data frame analytics. (This includes `_all` string or when no\n+            data frame analytics have been specified)  Default: True\n+        :arg from_: skips a number of analytics\n+        :arg size: specifies a max number of analytics to get  Default:\n+            100\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_match\", \"from_\", \"size\")\n+    def get_data_frame_analytics_stats(self, id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-dfanalytics-stats.html>`_\n+\n+        :arg id: The ID of the data frame analytics stats to fetch\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no data frame analytics. (This includes `_all` string or when no\n+            data frame analytics have been specified)  Default: True\n+        :arg from_: skips a number of analytics\n+        :arg size: specifies a max number of analytics to get  Default:\n+            100\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id, \"_stats\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def put_data_frame_analytics(self, id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/put-dfanalytics.html>`_\n+\n+        :arg id: The ID of the data frame analytics to create\n+        :arg body: The data frame analytics configuration\n+        \"\"\"\n+        for param in (id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"timeout\")\n+    def start_data_frame_analytics(self, id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/start-dfanalytics.html>`_\n+\n+        :arg id: The ID of the data frame analytics to start\n+        :arg body: The start data frame analytics parameters\n+        :arg timeout: Controls the time to wait until the task has\n+            started. Defaults to 20 seconds\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id, \"_start\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"allow_no_match\", \"force\", \"timeout\")\n+    def stop_data_frame_analytics(self, id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/stop-dfanalytics.html>`_\n+\n+        :arg id: The ID of the data frame analytics to stop\n+        :arg body: The stop data frame analytics parameters\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no data frame analytics. (This includes `_all` string or when no\n+            data frame analytics have been specified)\n+        :arg force: True if the data frame analytics should be\n+            forcefully stopped\n+        :arg timeout: Controls the time to wait until the task has\n+            stopped. Defaults to 20 seconds\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id, \"_stop\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def delete_trained_model(self, model_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/delete-inference.html>`_\n+\n+        :arg model_id: The ID of the trained model to delete\n+        \"\"\"\n+        if model_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'model_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_ml\", \"inference\", model_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def explain_data_frame_analytics(\n+        self, body=None, id=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/explain-dfanalytics.html>`_\n+\n+        :arg body: The data frame analytics config to explain\n+        :arg id: The ID of the data frame analytics to explain\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_ml\", \"data_frame\", \"analytics\", id, \"_explain\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\n+        \"allow_no_match\",\n+        \"decompress_definition\",\n+        \"from_\",\n+        \"include_model_definition\",\n+        \"size\",\n+    )\n+    def get_trained_models(self, model_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-inference.html>`_\n+\n+        :arg model_id: The ID of the trained models to fetch\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no trained models. (This includes `_all` string or when no\n+            trained models have been specified)  Default: True\n+        :arg decompress_definition: Should the model definition be\n+            decompressed into valid JSON or returned in a custom compressed format.\n+            Defaults to true.  Default: True\n+        :arg from_: skips a number of trained models\n+        :arg include_model_definition: Should the full model definition\n+            be included in the results. These definitions can be large. So be\n+            cautious when including them. Defaults to false.\n+        :arg size: specifies a max number of trained models to get\n+            Default: 100\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"inference\", model_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_match\", \"from_\", \"size\")\n+    def get_trained_models_stats(self, model_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-inference-stats.html>`_\n+\n+        :arg model_id: The ID of the trained models stats to fetch\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no trained models. (This includes `_all` string or when no\n+            trained models have been specified)  Default: True\n+        :arg from_: skips a number of trained models\n+        :arg size: specifies a max number of trained models to get\n+            Default: 100\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_ml\", \"inference\", model_id, \"_stats\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def put_trained_model(self, model_id, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg model_id: The ID of the trained models to store\n+        :arg body: The trained model configuration\n+        \"\"\"\n+        for param in (model_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_ml\", \"inference\", model_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )"
            },
            {
                "sha": "c5bd2066c894cddcce53af2c38fc0c744f3ab64c",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/monitoring.py",
                "status": "added",
                "additions": 29,
                "deletions": 0,
                "changes": 29,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/monitoring.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/monitoring.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/monitoring.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,29 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH, _bulk_body\n+\n+\n+class MonitoringClient(NamespacedClient):\n+    @query_params(\"interval\", \"system_api_version\", \"system_id\")\n+    def bulk(self, body, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/monitor-elasticsearch-cluster.html>`_\n+\n+        :arg body: The operation definition and data (action-data\n+            pairs), separated by newlines\n+        :arg doc_type: Default document type for items which don't\n+            provide one\n+        :arg interval: Collection interval (e.g., '10s' or '10000ms') of\n+            the payload\n+        :arg system_api_version: API Version of the monitored system\n+        :arg system_id: Identifier of the monitored system\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        body = _bulk_body(self.transport.serializer, body)\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_monitoring\", doc_type, \"bulk\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )"
            },
            {
                "sha": "61b01a6e28ee900a2205bcf1966e814110d60b24",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/nodes.py",
                "status": "added",
                "additions": 151,
                "deletions": 0,
                "changes": 151,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/nodes.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/nodes.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/nodes.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,151 @@\n+from .utils import NamespacedClient, query_params, _make_path\n+\n+\n+class NodesClient(NamespacedClient):\n+    @query_params(\"timeout\")\n+    def reload_secure_settings(self, node_id=None, params=None, headers=None):\n+        \"\"\"\n+        Reloads secure settings.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/secure-settings.html#reloadable-secure-settings>`_\n+\n+        :arg node_id: A comma-separated list of node IDs to span the\n+            reload/reinit call. Should stay empty because reloading usually involves\n+            all cluster nodes.\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_nodes\", node_id, \"reload_secure_settings\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"flat_settings\", \"timeout\")\n+    def info(self, node_id=None, metric=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about nodes in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-nodes-info.html>`_\n+\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information; use `_local` to return information from\n+            the node you're connecting to, leave empty to get information from all\n+            nodes\n+        :arg metric: A comma-separated list of metrics you wish\n+            returned. Leave empty to return all.  Valid choices: settings, os,\n+            process, jvm, thread_pool, transport, http, plugins, ingest\n+        :arg flat_settings: Return settings in flat format (default:\n+            false)\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_nodes\", node_id, metric), params=params, headers=headers\n+        )\n+\n+    @query_params(\n+        \"completion_fields\",\n+        \"fielddata_fields\",\n+        \"fields\",\n+        \"groups\",\n+        \"include_segment_file_sizes\",\n+        \"level\",\n+        \"timeout\",\n+        \"types\",\n+    )\n+    def stats(\n+        self, node_id=None, metric=None, index_metric=None, params=None, headers=None\n+    ):\n+        \"\"\"\n+        Returns statistical information about nodes in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-nodes-stats.html>`_\n+\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information; use `_local` to return information from\n+            the node you're connecting to, leave empty to get information from all\n+            nodes\n+        :arg metric: Limit the information returned to the specified\n+            metrics  Valid choices: _all, breaker, fs, http, indices, jvm, os,\n+            process, thread_pool, transport, discovery\n+        :arg index_metric: Limit the information returned for `indices`\n+            metric to the specific index metrics. Isn't used if `indices` (or `all`)\n+            metric isn't specified.  Valid choices: _all, completion, docs,\n+            fielddata, query_cache, flush, get, indexing, merge, request_cache,\n+            refresh, search, segments, store, warmer, suggest\n+        :arg completion_fields: A comma-separated list of fields for\n+            `fielddata` and `suggest` index metric (supports wildcards)\n+        :arg fielddata_fields: A comma-separated list of fields for\n+            `fielddata` index metric (supports wildcards)\n+        :arg fields: A comma-separated list of fields for `fielddata`\n+            and `completion` index metric (supports wildcards)\n+        :arg groups: A comma-separated list of search groups for\n+            `search` index metric\n+        :arg include_segment_file_sizes: Whether to report the\n+            aggregated disk usage of each one of the Lucene index files (only\n+            applies if segment stats are requested)\n+        :arg level: Return indices stats aggregated at index, node or\n+            shard level  Valid choices: indices, node, shards  Default: node\n+        :arg timeout: Explicit operation timeout\n+        :arg types: A comma-separated list of document types for the\n+            `indexing` index metric\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_nodes\", node_id, \"stats\", metric, index_metric),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"doc_type\", \"ignore_idle_threads\", \"interval\", \"snapshots\", \"threads\", \"timeout\"\n+    )\n+    def hot_threads(self, node_id=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about hot threads on each node in the cluster.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-nodes-hot-threads.html>`_\n+\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information; use `_local` to return information from\n+            the node you're connecting to, leave empty to get information from all\n+            nodes\n+        :arg doc_type: The type to sample (default: cpu)  Valid choices:\n+            cpu, wait, block\n+        :arg ignore_idle_threads: Don't show threads that are in known-\n+            idle places, such as waiting on a socket select or pulling from an empty\n+            task queue (default: true)\n+        :arg interval: The interval for the second sampling of threads\n+        :arg snapshots: Number of samples of thread stacktrace (default:\n+            10)\n+        :arg threads: Specify the number of threads to provide\n+            information for (default: 3)\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        # type is a reserved word so it cannot be used, use doc_type instead\n+        if \"doc_type\" in params:\n+            params[\"type\"] = params.pop(\"doc_type\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_nodes\", node_id, \"hot_threads\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"timeout\")\n+    def usage(self, node_id=None, metric=None, params=None, headers=None):\n+        \"\"\"\n+        Returns low-level information about REST actions usage on nodes.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-nodes-usage.html>`_\n+\n+        :arg node_id: A comma-separated list of node IDs or names to\n+            limit the returned information; use `_local` to return information from\n+            the node you're connecting to, leave empty to get information from all\n+            nodes\n+        :arg metric: Limit the information returned to the specified\n+            metrics  Valid choices: _all, rest_actions\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_nodes\", node_id, \"usage\", metric),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "8590313a7e179b1383fbd5b1fb488d2639145b5b",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/remote.py",
                "status": "added",
                "additions": 12,
                "deletions": 0,
                "changes": 12,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/remote.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/remote.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/remote.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,12 @@\n+from .utils import NamespacedClient, query_params\n+\n+\n+class RemoteClient(NamespacedClient):\n+    @query_params()\n+    def info(self, params=None, headers=None):\n+        \"\"\"\n+        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-remote-info.html>`_\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_remote/info\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "e26288c6a813405480d74f9561b927a33130e583",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/rollup.py",
                "status": "added",
                "additions": 133,
                "deletions": 0,
                "changes": 133,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/rollup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/rollup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/rollup.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,133 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class RollupClient(NamespacedClient):\n+    @query_params()\n+    def delete_job(self, id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg id: The ID of the job to delete\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", _make_path(\"_rollup\", \"job\", id), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_jobs(self, id=None, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg id: The ID of the job(s) to fetch. Accepts glob patterns,\n+            or left blank for all jobs\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_rollup\", \"job\", id), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_rollup_caps(self, id=None, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg id: The ID of the index to check rollup capabilities on, or\n+            left blank for all jobs\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_rollup\", \"data\", id), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_rollup_index_caps(self, index, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg index: The rollup index or index pattern to obtain rollup\n+            capabilities from.\n+        \"\"\"\n+        if index in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(index, \"_rollup\", \"data\"), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def put_job(self, id, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg id: The ID of the job to create\n+        :arg body: The job configuration\n+        \"\"\"\n+        for param in (id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_rollup\", \"job\", id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"rest_total_hits_as_int\", \"typed_keys\")\n+    def rollup_search(self, index, body, doc_type=None, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg index: The indices or index-pattern(s) (containing rollup\n+            or regular data) that should be searched\n+        :arg body: The search request body\n+        :arg doc_type: The doc type inside the index\n+        :arg rest_total_hits_as_int: Indicates whether hits.total should\n+            be rendered as an integer or an object in the rest search response\n+        :arg typed_keys: Specify whether aggregation and suggester names\n+            should be prefixed by their respective types in the response\n+        \"\"\"\n+        for param in (index, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(index, doc_type, \"_rollup_search\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def start_job(self, id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg id: The ID of the job to start\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_rollup\", \"job\", id, \"_start\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"timeout\", \"wait_for_completion\")\n+    def stop_job(self, id, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg id: The ID of the job to stop\n+        :arg timeout: Block for (at maximum) the specified duration\n+            while waiting for the job to stop.  Defaults to 30s.\n+        :arg wait_for_completion: True if the API should block until the\n+            job has fully stopped, false if should be executed async. Defaults to\n+            false.\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_rollup\", \"job\", id, \"_stop\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "eaf8e4673d5c5b5c0814731ed7a49657ca5951d8",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/security.py",
                "status": "added",
                "additions": 464,
                "deletions": 0,
                "changes": 464,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/security.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/security.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/security.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,464 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class SecurityClient(NamespacedClient):\n+    @query_params()\n+    def authenticate(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-authenticate.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_security/_authenticate\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"refresh\")\n+    def change_password(self, body, username=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-change-password.html>`_\n+\n+        :arg body: the new password for the user\n+        :arg username: The username of the user to change the password\n+            for\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_security\", \"user\", username, \"_password\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"usernames\")\n+    def clear_cached_realms(self, realms, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-cache.html>`_\n+\n+        :arg realms: Comma-separated list of realms to clear\n+        :arg usernames: Comma-separated list of usernames to clear from\n+            the cache\n+        \"\"\"\n+        if realms in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'realms'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_security\", \"realm\", realms, \"_clear_cache\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def clear_cached_roles(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-role-cache.html>`_\n+\n+        :arg name: Role name\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_security\", \"role\", name, \"_clear_cache\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def create_api_key(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html>`_\n+\n+        :arg body: The api key request to create an API key\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\", \"/_security/api_key\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"refresh\")\n+    def delete_privileges(self, application, name, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg application: Application name\n+        :arg name: Privilege name\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        for param in (application, name):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_security\", \"privilege\", application, name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def delete_role(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-role.html>`_\n+\n+        :arg name: Role name\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_security\", \"role\", name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def delete_role_mapping(self, name, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-role-mapping.html>`_\n+\n+        :arg name: Role-mapping name\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if name in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_security\", \"role_mapping\", name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def delete_user(self, username, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-user.html>`_\n+\n+        :arg username: username\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if username in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'username'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_security\", \"user\", username),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def disable_user(self, username, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-disable-user.html>`_\n+\n+        :arg username: The username of the user to disable\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if username in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'username'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_security\", \"user\", username, \"_disable\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def enable_user(self, username, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-enable-user.html>`_\n+\n+        :arg username: The username of the user to enable\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if username in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'username'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_security\", \"user\", username, \"_enable\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"id\", \"name\", \"owner\", \"realm_name\", \"username\")\n+    def get_api_key(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-api-key.html>`_\n+\n+        :arg id: API key id of the API key to be retrieved\n+        :arg name: API key name of the API key to be retrieved\n+        :arg owner: flag to query API keys owned by the currently\n+            authenticated user\n+        :arg realm_name: realm name of the user who created this API key\n+            to be retrieved\n+        :arg username: user name of the user who created this API key to\n+            be retrieved\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_security/api_key\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_privileges(self, application=None, name=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-privileges.html>`_\n+\n+        :arg application: Application name\n+        :arg name: Privilege name\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_security\", \"privilege\", application, name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_role(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role.html>`_\n+\n+        :arg name: Role name\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_security\", \"role\", name), params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_role_mapping(self, name=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role-mapping.html>`_\n+\n+        :arg name: Role-Mapping name\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_security\", \"role_mapping\", name),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_token(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-token.html>`_\n+\n+        :arg body: The token request to get\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_security/oauth2/token\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params()\n+    def get_user(self, username=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-user.html>`_\n+\n+        :arg username: A comma-separated list of usernames\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_security\", \"user\", username),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_user_privileges(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-privileges.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_security/user/_privileges\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def has_privileges(self, body, user=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-has-privileges.html>`_\n+\n+        :arg body: The privileges to test\n+        :arg user: Username\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_security\", \"user\", user, \"_has_privileges\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def invalidate_api_key(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-invalidate-api-key.html>`_\n+\n+        :arg body: The api key request to invalidate API key(s)\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\", \"/_security/api_key\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params()\n+    def invalidate_token(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-invalidate-token.html>`_\n+\n+        :arg body: The token to invalidate\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            \"/_security/oauth2/token\",\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def put_privileges(self, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: The privilege(s) to add\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\", \"/_security/privilege/\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"refresh\")\n+    def put_role(self, name, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-role.html>`_\n+\n+        :arg name: Role name\n+        :arg body: The role to add\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        for param in (name, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_security\", \"role\", name),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def put_role_mapping(self, name, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-role-mapping.html>`_\n+\n+        :arg name: Role-mapping name\n+        :arg body: The role mapping to add\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        for param in (name, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_security\", \"role_mapping\", name),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"refresh\")\n+    def put_user(self, username, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-user.html>`_\n+\n+        :arg username: The username of the User\n+        :arg body: The user to add\n+        :arg refresh: If `true` (the default) then refresh the affected\n+            shards to make this operation visible to search, if `wait_for` then wait\n+            for a refresh to make this operation visible to search, if `false` then\n+            do nothing with refreshes.  Valid choices: true, false, wait_for\n+        \"\"\"\n+        for param in (username, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_security\", \"user\", username),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def get_builtin_privileges(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-builtin-privileges.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_security/privilege/_builtin\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "a2c0a0cab48b64ff2bf960c475f8794135890ebc",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/slm.py",
                "status": "added",
                "additions": 123,
                "deletions": 0,
                "changes": 123,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/slm.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/slm.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/slm.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,123 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class SlmClient(NamespacedClient):\n+    @query_params()\n+    def delete_lifecycle(self, policy_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-delete-policy.html>`_\n+\n+        :arg policy_id: The id of the snapshot lifecycle policy to\n+            remove\n+        \"\"\"\n+        if policy_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'policy_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_slm\", \"policy\", policy_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def execute_lifecycle(self, policy_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-execute-lifecycle.html>`_\n+\n+        :arg policy_id: The id of the snapshot lifecycle policy to be\n+            executed\n+        \"\"\"\n+        if policy_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'policy_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_slm\", \"policy\", policy_id, \"_execute\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def execute_retention(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-execute-retention.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_slm/_execute_retention\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def get_lifecycle(self, policy_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-get-policy.html>`_\n+\n+        :arg policy_id: Comma-separated list of snapshot lifecycle\n+            policies to retrieve\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_slm\", \"policy\", policy_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def get_stats(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/slm-api-get-stats.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_slm/stats\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def put_lifecycle(self, policy_id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-put-policy.html>`_\n+\n+        :arg policy_id: The id of the snapshot lifecycle policy\n+        :arg body: The snapshot lifecycle policy definition to register\n+        \"\"\"\n+        if policy_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'policy_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_slm\", \"policy\", policy_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def get_status(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-get-status.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_slm/status\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def start(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-start.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_slm/start\", params=params, headers=headers\n+        )\n+\n+    @query_params()\n+    def stop(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-api-stop.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_slm/stop\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "9fc2a41159d8e8e6581ad8e858572dfef9bdb938",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/snapshot.py",
                "status": "added",
                "additions": 229,
                "deletions": 0,
                "changes": 229,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/snapshot.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/snapshot.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/snapshot.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,229 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class SnapshotClient(NamespacedClient):\n+    @query_params(\"master_timeout\", \"wait_for_completion\")\n+    def create(self, repository, snapshot, body=None, params=None, headers=None):\n+        \"\"\"\n+        Creates a snapshot in a repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg snapshot: A snapshot name\n+        :arg body: The snapshot definition\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg wait_for_completion: Should this request wait until the\n+            operation has completed before returning\n+        \"\"\"\n+        for param in (repository, snapshot):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_snapshot\", repository, snapshot),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"master_timeout\")\n+    def delete(self, repository, snapshot, params=None, headers=None):\n+        \"\"\"\n+        Deletes a snapshot.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg snapshot: A snapshot name\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        \"\"\"\n+        for param in (repository, snapshot):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_snapshot\", repository, snapshot),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"ignore_unavailable\", \"master_timeout\", \"verbose\")\n+    def get(self, repository, snapshot, params=None, headers=None):\n+        \"\"\"\n+        Returns information about a snapshot.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg snapshot: A comma-separated list of snapshot names\n+        :arg ignore_unavailable: Whether to ignore unavailable\n+            snapshots, defaults to false which means a SnapshotMissingException is\n+            thrown\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg verbose: Whether to show verbose snapshot info or only show\n+            the basic info found in the repository index blob\n+        \"\"\"\n+        for param in (repository, snapshot):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_snapshot\", repository, snapshot),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def delete_repository(self, repository, params=None, headers=None):\n+        \"\"\"\n+        Deletes a repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A comma-separated list of repository names\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if repository in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'repository'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_snapshot\", repository),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"local\", \"master_timeout\")\n+    def get_repository(self, repository=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about a repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A comma-separated list of repository names\n+        :arg local: Return local information, do not retrieve the state\n+            from master node (default: false)\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_snapshot\", repository), params=params, headers=headers\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\", \"verify\")\n+    def create_repository(self, repository, body, params=None, headers=None):\n+        \"\"\"\n+        Creates a repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg body: The repository definition\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        :arg verify: Whether to verify the repository after creation\n+        \"\"\"\n+        for param in (repository, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_snapshot\", repository),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"master_timeout\", \"wait_for_completion\")\n+    def restore(self, repository, snapshot, body=None, params=None, headers=None):\n+        \"\"\"\n+        Restores a snapshot.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg snapshot: A snapshot name\n+        :arg body: Details of what to restore\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg wait_for_completion: Should this request wait until the\n+            operation has completed before returning\n+        \"\"\"\n+        for param in (repository, snapshot):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_snapshot\", repository, snapshot, \"_restore\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"ignore_unavailable\", \"master_timeout\")\n+    def status(self, repository=None, snapshot=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about the status of a snapshot.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg snapshot: A comma-separated list of snapshot names\n+        :arg ignore_unavailable: Whether to ignore unavailable\n+            snapshots, defaults to false which means a SnapshotMissingException is\n+            thrown\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_snapshot\", repository, snapshot, \"_status\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def verify_repository(self, repository, params=None, headers=None):\n+        \"\"\"\n+        Verifies a repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if repository in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'repository'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_snapshot\", repository, \"_verify\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"master_timeout\", \"timeout\")\n+    def cleanup_repository(self, repository, params=None, headers=None):\n+        \"\"\"\n+        Removes stale data from repository.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-snapshots.html>`_\n+\n+        :arg repository: A repository name\n+        :arg master_timeout: Explicit operation timeout for connection\n+            to master node\n+        :arg timeout: Explicit operation timeout\n+        \"\"\"\n+        if repository in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'repository'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_snapshot\", repository, \"_cleanup\"),\n+            params=params,\n+            headers=headers,\n+        )"
            },
            {
                "sha": "103f6865dcade4ea08e779eda3a4655b9e7b2fed",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/sql.py",
                "status": "added",
                "additions": 46,
                "deletions": 0,
                "changes": 46,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/sql.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/sql.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/sql.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,46 @@\n+from .utils import NamespacedClient, query_params, SKIP_IN_PATH\n+\n+\n+class SqlClient(NamespacedClient):\n+    @query_params()\n+    def clear_cursor(self, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: Specify the cursor value in the `cursor` element to\n+            clean the cursor.\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_sql/close\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"format\")\n+    def query(self, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: Use the `query` element to start a query. Use the\n+            `cursor` element to continue a query.\n+        :arg format: a short version of the Accept header, e.g. json,\n+            yaml\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_sql\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params()\n+    def translate(self, body, params=None, headers=None):\n+        \"\"\"\n+\n+        :arg body: Specify the query in the `query` element.\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_sql/translate\", params=params, headers=headers, body=body\n+        )"
            },
            {
                "sha": "66ec2855e916ff4b7ea208c1dce362a3dca07417",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ssl.py",
                "status": "added",
                "additions": 13,
                "deletions": 0,
                "changes": 13,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ssl.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ssl.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/ssl.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,13 @@\n+from .utils import NamespacedClient, query_params\n+\n+\n+class SslClient(NamespacedClient):\n+    @query_params()\n+    def certificates(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-ssl.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_ssl/certificates\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "4c4f6d1fa638ca4ad604336ad22c82c83100e50a",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/tasks.py",
                "status": "added",
                "additions": 82,
                "deletions": 0,
                "changes": 82,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/tasks.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/tasks.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/tasks.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,82 @@\n+import warnings\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class TasksClient(NamespacedClient):\n+    @query_params(\n+        \"actions\",\n+        \"detailed\",\n+        \"group_by\",\n+        \"nodes\",\n+        \"parent_task_id\",\n+        \"timeout\",\n+        \"wait_for_completion\",\n+    )\n+    def list(self, params=None, headers=None):\n+        \"\"\"\n+        Returns a list of tasks.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/tasks.html>`_\n+\n+        :arg actions: A comma-separated list of actions that should be\n+            returned. Leave empty to return all.\n+        :arg detailed: Return detailed task information (default: false)\n+        :arg group_by: Group tasks by nodes or parent/child\n+            relationships  Valid choices: nodes, parents, none  Default: nodes\n+        :arg nodes: A comma-separated list of node IDs or names to limit\n+            the returned information; use `_local` to return information from the\n+            node you're connecting to, leave empty to get information from all nodes\n+        :arg parent_task_id: Return tasks with specified parent task id\n+            (node_id:task_number). Set to -1 to return all.\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_completion: Wait for the matching tasks to\n+            complete (default: false)\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_tasks\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"actions\", \"nodes\", \"parent_task_id\")\n+    def cancel(self, task_id=None, params=None, headers=None):\n+        \"\"\"\n+        Cancels a task, if it can be cancelled through an API.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/tasks.html>`_\n+\n+        :arg task_id: Cancel the task with specified task id\n+            (node_id:task_number)\n+        :arg actions: A comma-separated list of actions that should be\n+            cancelled. Leave empty to cancel all.\n+        :arg nodes: A comma-separated list of node IDs or names to limit\n+            the returned information; use `_local` to return information from the\n+            node you're connecting to, leave empty to get information from all nodes\n+        :arg parent_task_id: Cancel tasks with specified parent task id\n+            (node_id:task_number). Set to -1 to cancel all.\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_tasks\", task_id, \"_cancel\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"timeout\", \"wait_for_completion\")\n+    def get(self, task_id=None, params=None, headers=None):\n+        \"\"\"\n+        Returns information about a task.\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/master/tasks.html>`_\n+\n+        :arg task_id: Return the task with specified id\n+            (node_id:task_number)\n+        :arg timeout: Explicit operation timeout\n+        :arg wait_for_completion: Wait for the matching tasks to\n+            complete (default: false)\n+        \"\"\"\n+        if task_id in SKIP_IN_PATH:\n+            warnings.warn(\n+                \"Calling client.tasks.get() without a task_id is deprecated \"\n+                \"and will be removed in v8.0. Use client.tasks.list() instead.\",\n+                DeprecationWarning,\n+            )\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_tasks\", task_id), params=params, headers=headers\n+        )"
            },
            {
                "sha": "6cab5377984e1a305d7f15c34a1edd315d14751f",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/transform.py",
                "status": "added",
                "additions": 196,
                "deletions": 0,
                "changes": 196,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/transform.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/transform.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/transform.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,196 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class TransformClient(NamespacedClient):\n+    @query_params(\"force\")\n+    def delete_transform(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/delete-transform.html>`_\n+\n+        :arg transform_id: The id of the transform to delete\n+        :arg force: When `true`, the transform is deleted regardless of\n+            its current state. The default value is `false`, meaning that the\n+            transform must be `stopped` before it can be deleted.\n+        \"\"\"\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_transform\", transform_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_match\", \"from_\", \"size\")\n+    def get_transform(self, transform_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-transform.html>`_\n+\n+        :arg transform_id: The id or comma delimited list of id\n+            expressions of the transforms to get, '_all' or '*' implies get all\n+            transforms\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no transforms. (This includes `_all` string or when no\n+            transforms have been specified)\n+        :arg from_: skips a number of transform configs, defaults to 0\n+        :arg size: specifies a max number of transforms to get, defaults\n+            to 100\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_transform\", transform_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"allow_no_match\", \"from_\", \"size\")\n+    def get_transform_stats(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-transform-stats.html>`_\n+\n+        :arg transform_id: The id of the transform for which to get\n+            stats. '_all' or '*' implies all transforms\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no transforms. (This includes `_all` string or when no\n+            transforms have been specified)\n+        :arg from_: skips a number of transform stats, defaults to 0\n+        :arg size: specifies a max number of transform stats to get,\n+            defaults to 100\n+        \"\"\"\n+        # from is a reserved word so it cannot be used, use from_ instead\n+        if \"from_\" in params:\n+            params[\"from\"] = params.pop(\"from_\")\n+\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_transform\", transform_id, \"_stats\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def preview_transform(self, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/preview-transform.html>`_\n+\n+        :arg body: The definition for the transform to preview\n+        \"\"\"\n+        if body in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\", \"/_transform/_preview\", params=params, headers=headers, body=body\n+        )\n+\n+    @query_params(\"defer_validation\")\n+    def put_transform(self, transform_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/put-transform.html>`_\n+\n+        :arg transform_id: The id of the new transform.\n+        :arg body: The transform definition\n+        :arg defer_validation: If validations should be deferred until\n+            transform starts, defaults to false.\n+        \"\"\"\n+        for param in (transform_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_transform\", transform_id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params(\"timeout\")\n+    def start_transform(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/start-transform.html>`_\n+\n+        :arg transform_id: The id of the transform to start\n+        :arg timeout: Controls the time to wait for the transform to\n+            start\n+        \"\"\"\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_transform\", transform_id, \"_start\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\n+        \"allow_no_match\",\n+        \"force\",\n+        \"timeout\",\n+        \"wait_for_checkpoint\",\n+        \"wait_for_completion\",\n+    )\n+    def stop_transform(self, transform_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/stop-transform.html>`_\n+\n+        :arg transform_id: The id of the transform to stop\n+        :arg allow_no_match: Whether to ignore if a wildcard expression\n+            matches no transforms. (This includes `_all` string or when no\n+            transforms have been specified)\n+        :arg force: Whether to force stop a failed transform or not.\n+            Default to false\n+        :arg timeout: Controls the time to wait until the transform has\n+            stopped. Default to 30 seconds\n+        :arg wait_for_checkpoint: Whether to wait for the transform to\n+            reach a checkpoint before stopping. Default to false\n+        :arg wait_for_completion: Whether to wait for the transform to\n+            fully stop before returning or not. Default to false\n+        \"\"\"\n+        if transform_id in SKIP_IN_PATH:\n+            raise ValueError(\n+                \"Empty value passed for a required argument 'transform_id'.\"\n+            )\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_transform\", transform_id, \"_stop\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"defer_validation\")\n+    def update_transform(self, transform_id, body, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/update-transform.html>`_\n+\n+        :arg transform_id: The id of the transform.\n+        :arg body: The update transform definition\n+        :arg defer_validation: If validations should be deferred until\n+            transform starts, defaults to false.\n+        \"\"\"\n+        for param in (transform_id, body):\n+            if param in SKIP_IN_PATH:\n+                raise ValueError(\"Empty value passed for a required argument.\")\n+\n+        return self.transport.perform_request(\n+            \"POST\",\n+            _make_path(\"_transform\", transform_id, \"_update\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )"
            },
            {
                "sha": "3baa7666b321b08d5d24b8de7a2d237766d130c5",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/utils.py",
                "status": "added",
                "additions": 125,
                "deletions": 0,
                "changes": 125,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/utils.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/utils.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/utils.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,125 @@\n+from __future__ import unicode_literals\n+\n+import weakref\n+from datetime import date, datetime\n+from functools import wraps\n+from ..compat import string_types, quote, PY2\n+\n+# parts of URL to be omitted\n+SKIP_IN_PATH = (None, \"\", b\"\", [], ())\n+\n+\n+def _escape(value):\n+    \"\"\"\n+    Escape a single value of a URL string or a query parameter. If it is a list\n+    or tuple, turn it into a comma-separated string first.\n+    \"\"\"\n+\n+    # make sequences into comma-separated stings\n+    if isinstance(value, (list, tuple)):\n+        value = \",\".join(value)\n+\n+    # dates and datetimes into isoformat\n+    elif isinstance(value, (date, datetime)):\n+        value = value.isoformat()\n+\n+    # make bools into true/false strings\n+    elif isinstance(value, bool):\n+        value = str(value).lower()\n+\n+    # don't decode bytestrings\n+    elif isinstance(value, bytes):\n+        return value\n+\n+    # encode strings to utf-8\n+    if isinstance(value, string_types):\n+        if PY2 and isinstance(value, unicode):  # noqa: F821\n+            return value.encode(\"utf-8\")\n+        if not PY2 and isinstance(value, str):\n+            return value.encode(\"utf-8\")\n+\n+    return str(value)\n+\n+\n+def _make_path(*parts):\n+    \"\"\"\n+    Create a URL string from parts, omit all `None` values and empty strings.\n+    Convert lists and tuples to comma separated values.\n+    \"\"\"\n+    # TODO: maybe only allow some parts to be lists/tuples ?\n+    return \"/\" + \"/\".join(\n+        # preserve ',' and '*' in url for nicer URLs in logs\n+        quote(_escape(p), b\",*\")\n+        for p in parts\n+        if p not in SKIP_IN_PATH\n+    )\n+\n+\n+# parameters that apply to all methods\n+GLOBAL_PARAMS = (\"pretty\", \"human\", \"error_trace\", \"format\", \"filter_path\")\n+\n+\n+def query_params(*es_query_params):\n+    \"\"\"\n+    Decorator that pops all accepted parameters from method's kwargs and puts\n+    them in the params argument.\n+    \"\"\"\n+\n+    def _wrapper(func):\n+        @wraps(func)\n+        def _wrapped(*args, **kwargs):\n+            params = {}\n+            headers = {}\n+            if \"params\" in kwargs:\n+                params = kwargs.pop(\"params\").copy()\n+            if \"headers\" in kwargs:\n+                headers = {\n+                    k.lower(): v for k, v in (kwargs.pop(\"headers\") or {}).items()\n+                }\n+            if \"opaque_id\" in kwargs:\n+                headers[\"x-opaque-id\"] = kwargs.pop(\"opaque_id\")\n+\n+            for p in es_query_params + GLOBAL_PARAMS:\n+                if p in kwargs:\n+                    v = kwargs.pop(p)\n+                    if v is not None:\n+                        params[p] = _escape(v)\n+\n+            # don't treat ignore, request_timeout, and opaque_id as other params to avoid escaping\n+            for p in (\"ignore\", \"request_timeout\"):\n+                if p in kwargs:\n+                    params[p] = kwargs.pop(p)\n+            return func(*args, params=params, headers=headers, **kwargs)\n+\n+        return _wrapped\n+\n+    return _wrapper\n+\n+\n+def _bulk_body(serializer, body):\n+    # if not passed in a string, serialize items and join by newline\n+    if not isinstance(body, string_types):\n+        body = \"\\n\".join(map(serializer.dumps, body))\n+\n+    # bulk body must end with a newline\n+    if not body.endswith(\"\\n\"):\n+        body += \"\\n\"\n+\n+    return body\n+\n+\n+class NamespacedClient(object):\n+    def __init__(self, client):\n+        self.client = client\n+\n+    @property\n+    def transport(self):\n+        return self.client.transport\n+\n+\n+class AddonClient(NamespacedClient):\n+    @classmethod\n+    def infect_client(cls, client):\n+        addon = cls(weakref.proxy(client))\n+        setattr(client, cls.namespace, addon)\n+        return client"
            },
            {
                "sha": "591d0abc9c149071036bfbc97bf94f5787289fe2",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/watcher.py",
                "status": "added",
                "additions": 171,
                "deletions": 0,
                "changes": 171,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/watcher.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/watcher.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/watcher.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,171 @@\n+from .utils import NamespacedClient, query_params, _make_path, SKIP_IN_PATH\n+\n+\n+class WatcherClient(NamespacedClient):\n+    @query_params()\n+    def ack_watch(self, watch_id, action_id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-ack-watch.html>`_\n+\n+        :arg watch_id: Watch ID\n+        :arg action_id: A comma-separated list of the action ids to be\n+            acked\n+        \"\"\"\n+        if watch_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'watch_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_watcher\", \"watch\", watch_id, \"_ack\", action_id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def activate_watch(self, watch_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-activate-watch.html>`_\n+\n+        :arg watch_id: Watch ID\n+        \"\"\"\n+        if watch_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'watch_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_watcher\", \"watch\", watch_id, \"_activate\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def deactivate_watch(self, watch_id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-deactivate-watch.html>`_\n+\n+        :arg watch_id: Watch ID\n+        \"\"\"\n+        if watch_id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'watch_id'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_watcher\", \"watch\", watch_id, \"_deactivate\"),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def delete_watch(self, id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-delete-watch.html>`_\n+\n+        :arg id: Watch ID\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"DELETE\",\n+            _make_path(\"_watcher\", \"watch\", id),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params(\"debug\")\n+    def execute_watch(self, body=None, id=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-execute-watch.html>`_\n+\n+        :arg body: Execution control\n+        :arg id: Watch ID\n+        :arg debug: indicates whether the watch should execute in debug\n+            mode\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_watcher\", \"watch\", id, \"_execute\"),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def get_watch(self, id, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-get-watch.html>`_\n+\n+        :arg id: Watch ID\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"GET\", _make_path(\"_watcher\", \"watch\", id), params=params, headers=headers\n+        )\n+\n+    @query_params(\"active\", \"if_primary_term\", \"if_seq_no\", \"version\")\n+    def put_watch(self, id, body=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-put-watch.html>`_\n+\n+        :arg id: Watch ID\n+        :arg body: The watch\n+        :arg active: Specify whether the watch is in/active by default\n+        :arg if_primary_term: only update the watch if the last\n+            operation that has changed the watch has the specified primary term\n+        :arg if_seq_no: only update the watch if the last operation that\n+            has changed the watch has the specified sequence number\n+        :arg version: Explicit version number for concurrency control\n+        \"\"\"\n+        if id in SKIP_IN_PATH:\n+            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n+\n+        return self.transport.perform_request(\n+            \"PUT\",\n+            _make_path(\"_watcher\", \"watch\", id),\n+            params=params,\n+            headers=headers,\n+            body=body,\n+        )\n+\n+    @query_params()\n+    def start(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-start.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_watcher/_start\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"emit_stacktraces\")\n+    def stats(self, metric=None, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-stats.html>`_\n+\n+        :arg metric: Controls what additional stat metrics should be\n+            include in the response  Valid choices: _all, queued_watches,\n+            current_watches, pending_watches\n+        :arg emit_stacktraces: Emits stack traces of currently running\n+            watches\n+        :arg metric: Controls what additional stat metrics should be\n+            include in the response  Valid choices: _all, queued_watches,\n+            current_watches, pending_watches\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\",\n+            _make_path(\"_watcher\", \"stats\", metric),\n+            params=params,\n+            headers=headers,\n+        )\n+\n+    @query_params()\n+    def stop(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-stop.html>`_\n+\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"POST\", \"/_watcher/_stop\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "769e681775124e13cd7fa4f0b55f82ce19a805ad",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/xpack.py",
                "status": "added",
                "additions": 30,
                "deletions": 0,
                "changes": 30,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/xpack.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/xpack.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/client/xpack.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,30 @@\n+from .utils import NamespacedClient, query_params\n+\n+\n+class XPackClient(NamespacedClient):\n+    def __getattr__(self, attr_name):\n+        return getattr(self.client, attr_name)\n+\n+    # AUTO-GENERATED-API-DEFINITIONS #\n+    @query_params(\"categories\")\n+    def info(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/info-api.html>`_\n+\n+        :arg categories: Comma-separated list of info categories. Can be\n+            any of: build, license, features\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_xpack\", params=params, headers=headers\n+        )\n+\n+    @query_params(\"master_timeout\")\n+    def usage(self, params=None, headers=None):\n+        \"\"\"\n+        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/usage-api.html>`_\n+\n+        :arg master_timeout: Specify timeout for watch write operation\n+        \"\"\"\n+        return self.transport.perform_request(\n+            \"GET\", \"/_xpack/usage\", params=params, headers=headers\n+        )"
            },
            {
                "sha": "aba63ea739416eabc6713368f277c38764ddf192",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/compat.py",
                "status": "added",
                "additions": 27,
                "deletions": 0,
                "changes": 27,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/compat.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/compat.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/compat.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,27 @@\n+import sys\n+\n+PY2 = sys.version_info[0] == 2\n+\n+if PY2:\n+    string_types = (basestring,)  # noqa: F821\n+    from urllib import quote_plus, quote, urlencode, unquote\n+    from urlparse import urlparse\n+    from itertools import imap as map\n+    from Queue import Queue\n+else:\n+    string_types = str, bytes\n+    from urllib.parse import quote, quote_plus, urlencode, urlparse, unquote\n+\n+    map = map\n+    from queue import Queue\n+\n+__all__ = [\n+    \"string_types\",\n+    \"quote_plus\",\n+    \"quote\",\n+    \"urlencode\",\n+    \"unquote\",\n+    \"urlparse\",\n+    \"map\",\n+    \"Queue\",\n+]"
            },
            {
                "sha": "e56e541d5d5e99357e23e5e8018682c89832f496",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/__init__.py",
                "status": "added",
                "additions": 10,
                "deletions": 0,
                "changes": 10,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/__init__.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,10 @@\n+from .base import Connection\n+from .http_requests import RequestsHttpConnection\n+from .http_urllib3 import Urllib3HttpConnection, create_ssl_context\n+\n+__all__ = [\n+    \"Connection\",\n+    \"RequestsHttpConnection\",\n+    \"Urllib3HttpConnection\",\n+    \"create_ssl_context\",\n+]"
            },
            {
                "sha": "bc78fc918044e072de48badb72d4dc2d33bdcc3a",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/base.py",
                "status": "added",
                "additions": 259,
                "deletions": 0,
                "changes": 259,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/base.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/base.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/base.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,259 @@\n+import logging\n+import binascii\n+import gzip\n+import io\n+from platform import python_version\n+\n+try:\n+    import simplejson as json\n+except ImportError:\n+    import json\n+\n+from ..exceptions import TransportError, ImproperlyConfigured, HTTP_EXCEPTIONS\n+from .. import __versionstr__\n+\n+logger = logging.getLogger(\"elasticsearch7\")\n+\n+# create the elasticsearch7.trace logger, but only set propagate to False if the\n+# logger hasn't already been configured\n+_tracer_already_configured = \"elasticsearch7.trace\" in logging.Logger.manager.loggerDict\n+tracer = logging.getLogger(\"elasticsearch7.trace\")\n+if not _tracer_already_configured:\n+    tracer.propagate = False\n+\n+\n+class Connection(object):\n+    \"\"\"\n+    Class responsible for maintaining a connection to an Elasticsearch node. It\n+    holds persistent connection pool to it and it's main interface\n+    (`perform_request`) is thread-safe.\n+\n+    Also responsible for logging.\n+\n+    :arg host: hostname of the node (default: localhost)\n+    :arg port: port to use (integer, default: 9200)\n+    :arg use_ssl: use ssl for the connection if `True`\n+    :arg url_prefix: optional url prefix for elasticsearch7\n+    :arg timeout: default timeout in seconds (float, default: 10)\n+    :arg http_compress: Use gzip compression\n+    :arg cloud_id: The Cloud ID from ElasticCloud. Convenient way to connect to cloud instances.\n+    :arg opaque_id: Send this value in the 'X-Opaque-Id' HTTP header\n+        For tracing all requests made by this transport.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        host=\"localhost\",\n+        port=None,\n+        use_ssl=False,\n+        url_prefix=\"\",\n+        timeout=10,\n+        headers=None,\n+        http_compress=None,\n+        cloud_id=None,\n+        api_key=None,\n+        **kwargs\n+    ):\n+\n+        if cloud_id:\n+            try:\n+                _, cloud_id = cloud_id.split(\":\")\n+                parent_dn, es_uuid = (\n+                    binascii.a2b_base64(cloud_id.encode(\"utf-8\"))\n+                    .decode(\"utf-8\")\n+                    .split(\"$\")[:2]\n+                )\n+                if \":\" in parent_dn:\n+                    parent_dn, _, parent_port = parent_dn.rpartition(\":\")\n+                    if port is None and parent_port != \"443\":\n+                        port = int(parent_port)\n+            except (ValueError, IndexError):\n+                raise ImproperlyConfigured(\"'cloud_id' is not properly formatted\")\n+\n+            host = \"%s.%s\" % (es_uuid, parent_dn)\n+            use_ssl = True\n+            if http_compress is None:\n+                http_compress = True\n+\n+        # If cloud_id isn't set and port is default then use 9200.\n+        # Cloud should use '443' by default via the 'https' scheme.\n+        elif port is None:\n+            port = 9200\n+\n+        # Work-around if the implementing class doesn't\n+        # define the headers property before calling super().__init__()\n+        if not hasattr(self, \"headers\"):\n+            self.headers = {}\n+\n+        headers = headers or {}\n+        for key in headers:\n+            self.headers[key.lower()] = headers[key]\n+\n+        self.headers.setdefault(\"content-type\", \"application/json\")\n+        self.headers.setdefault(\"user-agent\", self._get_default_user_agent())\n+\n+        if api_key is not None:\n+            self.headers[\"authorization\"] = self._get_api_key_header_val(api_key)\n+\n+        if http_compress:\n+            self.headers[\"accept-encoding\"] = \"gzip,deflate\"\n+\n+        scheme = kwargs.get(\"scheme\", \"http\")\n+        if use_ssl or scheme == \"https\":\n+            scheme = \"https\"\n+            use_ssl = True\n+        self.use_ssl = use_ssl\n+        self.http_compress = http_compress or False\n+\n+        self.hostname = host\n+        self.port = port\n+        self.host = \"%s://%s\" % (scheme, host)\n+        if self.port is not None:\n+            self.host += \":%s\" % self.port\n+        if url_prefix:\n+            url_prefix = \"/\" + url_prefix.strip(\"/\")\n+        self.url_prefix = url_prefix\n+        self.timeout = timeout\n+\n+    def __repr__(self):\n+        return \"<%s: %s>\" % (self.__class__.__name__, self.host)\n+\n+    def __eq__(self, other):\n+        if not isinstance(other, Connection):\n+            raise TypeError(\"Unsupported equality check for %s and %s\" % (self, other))\n+        return self.__hash__() == other.__hash__()\n+\n+    def __hash__(self):\n+        return id(self)\n+\n+    def _gzip_compress(self, body):\n+        buf = io.BytesIO()\n+        with gzip.GzipFile(fileobj=buf, mode=\"wb\") as f:\n+            f.write(body)\n+        return buf.getvalue()\n+\n+    def _pretty_json(self, data):\n+        # pretty JSON in tracer curl logs\n+        try:\n+            return json.dumps(\n+                json.loads(data), sort_keys=True, indent=2, separators=(\",\", \": \")\n+            ).replace(\"'\", r\"\\u0027\")\n+        except (ValueError, TypeError):\n+            # non-json data or a bulk request\n+            return data\n+\n+    def _log_trace(self, method, path, body, status_code, response, duration):\n+        if not tracer.isEnabledFor(logging.INFO) or not tracer.handlers:\n+            return\n+\n+        # include pretty in trace curls\n+        path = path.replace(\"?\", \"?pretty&\", 1) if \"?\" in path else path + \"?pretty\"\n+        if self.url_prefix:\n+            path = path.replace(self.url_prefix, \"\", 1)\n+        tracer.info(\n+            \"curl %s-X%s 'http://localhost:9200%s' -d '%s'\",\n+            \"-H 'Content-Type: application/json' \" if body else \"\",\n+            method,\n+            path,\n+            self._pretty_json(body) if body else \"\",\n+        )\n+\n+        if tracer.isEnabledFor(logging.DEBUG):\n+            tracer.debug(\n+                \"#[%s] (%.3fs)\\n#%s\",\n+                status_code,\n+                duration,\n+                self._pretty_json(response).replace(\"\\n\", \"\\n#\") if response else \"\",\n+            )\n+\n+    def log_request_success(\n+        self, method, full_url, path, body, status_code, response, duration\n+    ):\n+        \"\"\" Log a successful API call.  \"\"\"\n+        #  TODO: optionally pass in params instead of full_url and do urlencode only when needed\n+\n+        # body has already been serialized to utf-8, deserialize it for logging\n+        # TODO: find a better way to avoid (de)encoding the body back and forth\n+        if body:\n+            try:\n+                body = body.decode(\"utf-8\", \"ignore\")\n+            except AttributeError:\n+                pass\n+\n+        logger.info(\n+            \"%s %s [status:%s request:%.3fs]\", method, full_url, status_code, duration\n+        )\n+        logger.debug(\"> %s\", body)\n+        logger.debug(\"< %s\", response)\n+\n+        self._log_trace(method, path, body, status_code, response, duration)\n+\n+    def log_request_fail(\n+        self,\n+        method,\n+        full_url,\n+        path,\n+        body,\n+        duration,\n+        status_code=None,\n+        response=None,\n+        exception=None,\n+    ):\n+        \"\"\" Log an unsuccessful API call.  \"\"\"\n+        # do not log 404s on HEAD requests\n+        if method == \"HEAD\" and status_code == 404:\n+            return\n+        logger.warning(\n+            \"%s %s [status:%s request:%.3fs]\",\n+            method,\n+            full_url,\n+            status_code or \"N/A\",\n+            duration,\n+            exc_info=exception is not None,\n+        )\n+\n+        # body has already been serialized to utf-8, deserialize it for logging\n+        # TODO: find a better way to avoid (de)encoding the body back and forth\n+        if body:\n+            try:\n+                body = body.decode(\"utf-8\", \"ignore\")\n+            except AttributeError:\n+                pass\n+\n+        logger.debug(\"> %s\", body)\n+\n+        self._log_trace(method, path, body, status_code, response, duration)\n+\n+        if response is not None:\n+            logger.debug(\"< %s\", response)\n+\n+    def _raise_error(self, status_code, raw_data):\n+        \"\"\" Locate appropriate exception and raise it. \"\"\"\n+        error_message = raw_data\n+        additional_info = None\n+        try:\n+            if raw_data:\n+                additional_info = json.loads(raw_data)\n+                error_message = additional_info.get(\"error\", error_message)\n+                if isinstance(error_message, dict) and \"type\" in error_message:\n+                    error_message = error_message[\"type\"]\n+        except (ValueError, TypeError) as err:\n+            logger.warning(\"Undecodable raw error response from server: %s\", err)\n+\n+        raise HTTP_EXCEPTIONS.get(status_code, TransportError)(\n+            status_code, error_message, additional_info\n+        )\n+\n+    def _get_default_user_agent(self):\n+        return \"elasticsearch-py/%s (Python %s)\" % (__versionstr__, python_version())\n+\n+    def _get_api_key_header_val(self, api_key):\n+        \"\"\"\n+        Check the type of the passed api_key and return the correct header value\n+        for the `API Key authentication <https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html>`\n+        :arg api_key, either a tuple or a base64 encoded string\n+        \"\"\"\n+        if isinstance(api_key, (tuple, list)):\n+            s = \"{0}:{1}\".format(api_key[0], api_key[1]).encode(\"utf-8\")\n+            return \"ApiKey \" + binascii.b2a_base64(s).rstrip(b\"\\r\\n\").decode(\"utf-8\")\n+        return \"ApiKey \" + api_key"
            },
            {
                "sha": "d88fe70bb451360354342a9778377856c787cb38",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_requests.py",
                "status": "added",
                "additions": 195,
                "deletions": 0,
                "changes": 195,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_requests.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_requests.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_requests.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,195 @@\n+import time\n+import warnings\n+\n+try:\n+    import requests\n+\n+    REQUESTS_AVAILABLE = True\n+except ImportError:\n+    REQUESTS_AVAILABLE = False\n+\n+from .base import Connection\n+from ..exceptions import (\n+    ConnectionError,\n+    ImproperlyConfigured,\n+    ConnectionTimeout,\n+    SSLError,\n+)\n+from ..compat import urlencode, string_types\n+\n+\n+class RequestsHttpConnection(Connection):\n+    \"\"\"\n+    Connection using the `requests` library.\n+\n+    :arg http_auth: optional http auth information as either ':' separated\n+        string or a tuple. Any value will be passed into requests as `auth`.\n+    :arg use_ssl: use ssl for the connection if `True`\n+    :arg verify_certs: whether to verify SSL certificates\n+    :arg ssl_show_warn: show warning when verify certs is disabled\n+    :arg ca_certs: optional path to CA bundle. By default standard requests'\n+        bundle will be used.\n+    :arg client_cert: path to the file containing the private key and the\n+        certificate, or cert only if using client_key\n+    :arg client_key: path to the file containing the private key if using\n+        separate cert and key files (client_cert will contain only the cert)\n+    :arg headers: any custom http headers to be add to requests\n+    :arg http_compress: Use gzip compression\n+    :arg cloud_id: The Cloud ID from ElasticCloud. Convenient way to connect to cloud instances.\n+        Other host connection params will be ignored.\n+    :arg api_key: optional API Key authentication as either base64 encoded string or a tuple.\n+    :arg opaque_id: Send this value in the 'X-Opaque-Id' HTTP header\n+        For tracing all requests made by this transport.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        host=\"localhost\",\n+        port=None,\n+        http_auth=None,\n+        use_ssl=False,\n+        verify_certs=True,\n+        ssl_show_warn=True,\n+        ca_certs=None,\n+        client_cert=None,\n+        client_key=None,\n+        headers=None,\n+        http_compress=None,\n+        cloud_id=None,\n+        api_key=None,\n+        opaque_id=None,\n+        **kwargs\n+    ):\n+        if not REQUESTS_AVAILABLE:\n+            raise ImproperlyConfigured(\n+                \"Please install requests to use RequestsHttpConnection.\"\n+            )\n+\n+        # Initialize Session so .headers works before calling super().__init__().\n+        self.session = requests.Session()\n+        for key in list(self.session.headers):\n+            self.session.headers.pop(key)\n+\n+        super(RequestsHttpConnection, self).__init__(\n+            host=host,\n+            port=port,\n+            use_ssl=use_ssl,\n+            headers=headers,\n+            http_compress=http_compress,\n+            cloud_id=cloud_id,\n+            api_key=api_key,\n+            opaque_id=opaque_id,\n+            **kwargs\n+        )\n+\n+        if not self.http_compress:\n+            # Need to set this to 'None' otherwise Requests adds its own.\n+            self.session.headers[\"accept-encoding\"] = None\n+\n+        if http_auth is not None:\n+            if isinstance(http_auth, (tuple, list)):\n+                http_auth = tuple(http_auth)\n+            elif isinstance(http_auth, string_types):\n+                http_auth = tuple(http_auth.split(\":\", 1))\n+            self.session.auth = http_auth\n+\n+        self.base_url = \"%s%s\" % (self.host, self.url_prefix,)\n+        self.session.verify = verify_certs\n+        if not client_key:\n+            self.session.cert = client_cert\n+        elif client_cert:\n+            # cert is a tuple of (certfile, keyfile)\n+            self.session.cert = (client_cert, client_key)\n+        if ca_certs:\n+            if not verify_certs:\n+                raise ImproperlyConfigured(\n+                    \"You cannot pass CA certificates when verify SSL is off.\"\n+                )\n+            self.session.verify = ca_certs\n+\n+        if not ssl_show_warn:\n+            requests.packages.urllib3.disable_warnings()\n+\n+        if self.use_ssl and not verify_certs and ssl_show_warn:\n+            warnings.warn(\n+                \"Connecting to %s using SSL with verify_certs=False is insecure.\"\n+                % self.host\n+            )\n+\n+    def perform_request(\n+        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None\n+    ):\n+        url = self.base_url + url\n+        headers = headers or {}\n+        if params:\n+            url = \"%s?%s\" % (url, urlencode(params or {}))\n+\n+        orig_body = body\n+        if self.http_compress and body:\n+            body = self._gzip_compress(body)\n+            headers[\"content-encoding\"] = \"gzip\"\n+\n+        start = time.time()\n+        request = requests.Request(method=method, headers=headers, url=url, data=body)\n+        prepared_request = self.session.prepare_request(request)\n+        settings = self.session.merge_environment_settings(\n+            prepared_request.url, {}, None, None, None\n+        )\n+        send_kwargs = {\"timeout\": timeout or self.timeout}\n+        send_kwargs.update(settings)\n+        try:\n+            response = self.session.send(prepared_request, **send_kwargs)\n+            duration = time.time() - start\n+            raw_data = response.text\n+        except Exception as e:\n+            self.log_request_fail(\n+                method,\n+                url,\n+                prepared_request.path_url,\n+                body,\n+                time.time() - start,\n+                exception=e,\n+            )\n+            if isinstance(e, requests.exceptions.SSLError):\n+                raise SSLError(\"N/A\", str(e), e)\n+            if isinstance(e, requests.Timeout):\n+                raise ConnectionTimeout(\"TIMEOUT\", str(e), e)\n+            raise ConnectionError(\"N/A\", str(e), e)\n+\n+        # raise errors based on http status codes, let the client handle those if needed\n+        if (\n+            not (200 <= response.status_code < 300)\n+            and response.status_code not in ignore\n+        ):\n+            self.log_request_fail(\n+                method,\n+                url,\n+                response.request.path_url,\n+                orig_body,\n+                duration,\n+                response.status_code,\n+                raw_data,\n+            )\n+            self._raise_error(response.status_code, raw_data)\n+\n+        self.log_request_success(\n+            method,\n+            url,\n+            response.request.path_url,\n+            orig_body,\n+            response.status_code,\n+            raw_data,\n+            duration,\n+        )\n+\n+        return response.status_code, response.headers, raw_data\n+\n+    @property\n+    def headers(self):\n+        return self.session.headers\n+\n+    def close(self):\n+        \"\"\"\n+        Explicitly closes connections\n+        \"\"\"\n+        self.session.close()"
            },
            {
                "sha": "7ba206a78cb1897fb0706f2aceae3d91ae2a89a9",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_urllib3.py",
                "status": "added",
                "additions": 260,
                "deletions": 0,
                "changes": 260,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_urllib3.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_urllib3.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/http_urllib3.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,260 @@\n+import time\n+import ssl\n+import urllib3\n+from urllib3.exceptions import ReadTimeoutError, SSLError as UrllibSSLError\n+from urllib3.util.retry import Retry\n+import warnings\n+\n+from .base import Connection\n+from ..exceptions import (\n+    ConnectionError,\n+    ImproperlyConfigured,\n+    ConnectionTimeout,\n+    SSLError,\n+)\n+from ..compat import urlencode\n+\n+# sentinel value for `verify_certs` and `ssl_show_warn`.\n+# This is used to detect if a user is passing in a value\n+# for SSL kwargs if also using an SSLContext.\n+VERIFY_CERTS_DEFAULT = object()\n+SSL_SHOW_WARN_DEFAULT = object()\n+\n+CA_CERTS = None\n+\n+try:\n+    import certifi\n+\n+    CA_CERTS = certifi.where()\n+except ImportError:\n+    pass\n+\n+\n+def create_ssl_context(**kwargs):\n+    \"\"\"\n+    A helper function around creating an SSL context\n+\n+    https://docs.python.org/3/library/ssl.html#context-creation\n+\n+    Accepts kwargs in the same manner as `create_default_context`.\n+    \"\"\"\n+    ctx = ssl.create_default_context(**kwargs)\n+    return ctx\n+\n+\n+class Urllib3HttpConnection(Connection):\n+    \"\"\"\n+    Default connection class using the `urllib3` library and the http protocol.\n+\n+    :arg host: hostname of the node (default: localhost)\n+    :arg port: port to use (integer, default: 9200)\n+    :arg url_prefix: optional url prefix for elasticsearch7\n+    :arg timeout: default timeout in seconds (float, default: 10)\n+    :arg http_auth: optional http auth information as either ':' separated\n+        string or a tuple\n+    :arg use_ssl: use ssl for the connection if `True`\n+    :arg verify_certs: whether to verify SSL certificates\n+    :arg ssl_show_warn: show warning when verify certs is disabled\n+    :arg ca_certs: optional path to CA bundle.\n+        See https://urllib3.readthedocs.io/en/latest/security.html#using-certifi-with-urllib3\n+        for instructions how to get default set\n+    :arg client_cert: path to the file containing the private key and the\n+        certificate, or cert only if using client_key\n+    :arg client_key: path to the file containing the private key if using\n+        separate cert and key files (client_cert will contain only the cert)\n+    :arg ssl_version: version of the SSL protocol to use. Choices are:\n+        SSLv23 (default) SSLv2 SSLv3 TLSv1 (see ``PROTOCOL_*`` constants in the\n+        ``ssl`` module for exact options for your environment).\n+    :arg ssl_assert_hostname: use hostname verification if not `False`\n+    :arg ssl_assert_fingerprint: verify the supplied certificate fingerprint if not `None`\n+    :arg maxsize: the number of connections which will be kept open to this\n+        host. See https://urllib3.readthedocs.io/en/1.4/pools.html#api for more\n+        information.\n+    :arg headers: any custom http headers to be add to requests\n+    :arg http_compress: Use gzip compression\n+    :arg cloud_id: The Cloud ID from ElasticCloud. Convenient way to connect to cloud instances.\n+        Other host connection params will be ignored.\n+    :arg api_key: optional API Key authentication as either base64 encoded string or a tuple.\n+    :arg opaque_id: Send this value in the 'X-Opaque-Id' HTTP header\n+        For tracing all requests made by this transport.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        host=\"localhost\",\n+        port=None,\n+        http_auth=None,\n+        use_ssl=False,\n+        verify_certs=VERIFY_CERTS_DEFAULT,\n+        ssl_show_warn=SSL_SHOW_WARN_DEFAULT,\n+        ca_certs=None,\n+        client_cert=None,\n+        client_key=None,\n+        ssl_version=None,\n+        ssl_assert_hostname=None,\n+        ssl_assert_fingerprint=None,\n+        maxsize=10,\n+        headers=None,\n+        ssl_context=None,\n+        http_compress=None,\n+        cloud_id=None,\n+        api_key=None,\n+        opaque_id=None,\n+        **kwargs\n+    ):\n+        # Initialize headers before calling super().__init__().\n+        self.headers = urllib3.make_headers(keep_alive=True)\n+\n+        super(Urllib3HttpConnection, self).__init__(\n+            host=host,\n+            port=port,\n+            use_ssl=use_ssl,\n+            headers=headers,\n+            http_compress=http_compress,\n+            cloud_id=cloud_id,\n+            api_key=api_key,\n+            opaque_id=opaque_id,\n+            **kwargs\n+        )\n+        if http_auth is not None:\n+            if isinstance(http_auth, (tuple, list)):\n+                http_auth = \":\".join(http_auth)\n+            self.headers.update(urllib3.make_headers(basic_auth=http_auth))\n+\n+        pool_class = urllib3.HTTPConnectionPool\n+        kw = {}\n+\n+        # if providing an SSL context, raise error if any other SSL related flag is used\n+        if ssl_context and (\n+            (verify_certs is not VERIFY_CERTS_DEFAULT)\n+            or (ssl_show_warn is not SSL_SHOW_WARN_DEFAULT)\n+            or ca_certs\n+            or client_cert\n+            or client_key\n+            or ssl_version\n+        ):\n+            warnings.warn(\n+                \"When using `ssl_context`, all other SSL related kwargs are ignored\"\n+            )\n+\n+        # if ssl_context provided use SSL by default\n+        if ssl_context and self.use_ssl:\n+            pool_class = urllib3.HTTPSConnectionPool\n+            kw.update(\n+                {\n+                    \"assert_fingerprint\": ssl_assert_fingerprint,\n+                    \"ssl_context\": ssl_context,\n+                }\n+            )\n+\n+        elif self.use_ssl:\n+            pool_class = urllib3.HTTPSConnectionPool\n+            kw.update(\n+                {\n+                    \"ssl_version\": ssl_version,\n+                    \"assert_hostname\": ssl_assert_hostname,\n+                    \"assert_fingerprint\": ssl_assert_fingerprint,\n+                }\n+            )\n+\n+            # Convert all sentinel values to their actual default\n+            # values if not using an SSLContext.\n+            if verify_certs is VERIFY_CERTS_DEFAULT:\n+                verify_certs = True\n+            if ssl_show_warn is SSL_SHOW_WARN_DEFAULT:\n+                ssl_show_warn = True\n+\n+            ca_certs = CA_CERTS if ca_certs is None else ca_certs\n+            if verify_certs:\n+                if not ca_certs:\n+                    raise ImproperlyConfigured(\n+                        \"Root certificates are missing for certificate \"\n+                        \"validation. Either pass them in using the ca_certs parameter or \"\n+                        \"install certifi to use it automatically.\"\n+                    )\n+\n+                kw.update(\n+                    {\n+                        \"cert_reqs\": \"CERT_REQUIRED\",\n+                        \"ca_certs\": ca_certs,\n+                        \"cert_file\": client_cert,\n+                        \"key_file\": client_key,\n+                    }\n+                )\n+            else:\n+                kw[\"cert_reqs\"] = \"CERT_NONE\"\n+                if ssl_show_warn:\n+                    warnings.warn(\n+                        \"Connecting to %s using SSL with verify_certs=False is insecure.\"\n+                        % self.host\n+                    )\n+                if not ssl_show_warn:\n+                    urllib3.disable_warnings()\n+\n+        self.pool = pool_class(\n+            self.hostname, port=self.port, timeout=self.timeout, maxsize=maxsize, **kw\n+        )\n+\n+    def perform_request(\n+        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None\n+    ):\n+        url = self.url_prefix + url\n+        if params:\n+            url = \"%s?%s\" % (url, urlencode(params))\n+        full_url = self.host + url\n+\n+        start = time.time()\n+        orig_body = body\n+        try:\n+            kw = {}\n+            if timeout:\n+                kw[\"timeout\"] = timeout\n+\n+            # in python2 we need to make sure the url and method are not\n+            # unicode. Otherwise the body will be decoded into unicode too and\n+            # that will fail (#133, #201).\n+            if not isinstance(url, str):\n+                url = url.encode(\"utf-8\")\n+            if not isinstance(method, str):\n+                method = method.encode(\"utf-8\")\n+\n+            request_headers = self.headers.copy()\n+            request_headers.update(headers or ())\n+\n+            if self.http_compress and body:\n+                body = self._gzip_compress(body)\n+                request_headers[\"content-encoding\"] = \"gzip\"\n+\n+            response = self.pool.urlopen(\n+                method, url, body, retries=Retry(False), headers=request_headers, **kw\n+            )\n+            duration = time.time() - start\n+            raw_data = response.data.decode(\"utf-8\")\n+        except Exception as e:\n+            self.log_request_fail(\n+                method, full_url, url, orig_body, time.time() - start, exception=e\n+            )\n+            if isinstance(e, UrllibSSLError):\n+                raise SSLError(\"N/A\", str(e), e)\n+            if isinstance(e, ReadTimeoutError):\n+                raise ConnectionTimeout(\"TIMEOUT\", str(e), e)\n+            raise ConnectionError(\"N/A\", str(e), e)\n+\n+        # raise errors based on http status codes, let the client handle those if needed\n+        if not (200 <= response.status < 300) and response.status not in ignore:\n+            self.log_request_fail(\n+                method, full_url, url, orig_body, duration, response.status, raw_data\n+            )\n+            self._raise_error(response.status, raw_data)\n+\n+        self.log_request_success(\n+            method, full_url, url, orig_body, response.status, raw_data, duration\n+        )\n+\n+        return response.status, response.getheaders(), raw_data\n+\n+    def close(self):\n+        \"\"\"\n+        Explicitly closes connection\n+        \"\"\"\n+        self.pool.close()"
            },
            {
                "sha": "dd5431e1517103f1b5ca7a7865736c614ebfdb8f",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/pooling.py",
                "status": "added",
                "additions": 33,
                "deletions": 0,
                "changes": 33,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/pooling.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/pooling.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection/pooling.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,33 @@\n+try:\n+    import queue\n+except ImportError:\n+    import Queue as queue\n+from .base import Connection\n+\n+\n+class PoolingConnection(Connection):\n+    \"\"\"\n+    Base connection class for connections that use libraries without thread\n+    safety and no capacity for connection pooling. To use this just implement a\n+    ``_make_connection`` method that constructs a new connection and returns\n+    it.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        self._free_connections = queue.Queue()\n+        super(PoolingConnection, self).__init__(*args, **kwargs)\n+\n+    def _get_connection(self):\n+        try:\n+            return self._free_connections.get_nowait()\n+        except queue.Empty:\n+            return self._make_connection()\n+\n+    def _release_connection(self, con):\n+        self._free_connections.put(con)\n+\n+    def close(self):\n+        \"\"\"\n+        Explicitly close connection\n+        \"\"\"\n+        pass"
            },
            {
                "sha": "d6851786b320d2bc8ff5c393f686a8860d87b1a2",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection_pool.py",
                "status": "added",
                "additions": 282,
                "deletions": 0,
                "changes": 282,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection_pool.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection_pool.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/connection_pool.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,282 @@\n+import time\n+import random\n+import logging\n+import threading\n+\n+try:\n+    from Queue import PriorityQueue, Empty\n+except ImportError:\n+    from queue import PriorityQueue, Empty\n+\n+from .exceptions import ImproperlyConfigured\n+\n+logger = logging.getLogger(\"elasticsearch7\")\n+\n+\n+class ConnectionSelector(object):\n+    \"\"\"\n+    Simple class used to select a connection from a list of currently live\n+    connection instances. In init time it is passed a dictionary containing all\n+    the connections' options which it can then use during the selection\n+    process. When the `select` method is called it is given a list of\n+    *currently* live connections to choose from.\n+\n+    The options dictionary is the one that has been passed to\n+    :class:`~elasticsearch7.Transport` as `hosts` param and the same that is\n+    used to construct the Connection object itself. When the Connection was\n+    created from information retrieved from the cluster via the sniffing\n+    process it will be the dictionary returned by the `host_info_callback`.\n+\n+    Example of where this would be useful is a zone-aware selector that would\n+    only select connections from it's own zones and only fall back to other\n+    connections where there would be none in it's zones.\n+    \"\"\"\n+\n+    def __init__(self, opts):\n+        \"\"\"\n+        :arg opts: dictionary of connection instances and their options\n+        \"\"\"\n+        self.connection_opts = opts\n+\n+    def select(self, connections):\n+        \"\"\"\n+        Select a connection from the given list.\n+\n+        :arg connections: list of live connections to choose from\n+        \"\"\"\n+        pass\n+\n+\n+class RandomSelector(ConnectionSelector):\n+    \"\"\"\n+    Select a connection at random\n+    \"\"\"\n+\n+    def select(self, connections):\n+        return random.choice(connections)\n+\n+\n+class RoundRobinSelector(ConnectionSelector):\n+    \"\"\"\n+    Selector using round-robin.\n+    \"\"\"\n+\n+    def __init__(self, opts):\n+        super(RoundRobinSelector, self).__init__(opts)\n+        self.data = threading.local()\n+\n+    def select(self, connections):\n+        self.data.rr = getattr(self.data, \"rr\", -1) + 1\n+        self.data.rr %= len(connections)\n+        return connections[self.data.rr]\n+\n+\n+class ConnectionPool(object):\n+    \"\"\"\n+    Container holding the :class:`~elasticsearch7.Connection` instances,\n+    managing the selection process (via a\n+    :class:`~elasticsearch7.ConnectionSelector`) and dead connections.\n+\n+    It's only interactions are with the :class:`~elasticsearch7.Transport` class\n+    that drives all the actions within `ConnectionPool`.\n+\n+    Initially connections are stored on the class as a list and, along with the\n+    connection options, get passed to the `ConnectionSelector` instance for\n+    future reference.\n+\n+    Upon each request the `Transport` will ask for a `Connection` via the\n+    `get_connection` method. If the connection fails (it's `perform_request`\n+    raises a `ConnectionError`) it will be marked as dead (via `mark_dead`) and\n+    put on a timeout (if it fails N times in a row the timeout is exponentially\n+    longer - the formula is `default_timeout * 2 ** (fail_count - 1)`). When\n+    the timeout is over the connection will be resurrected and returned to the\n+    live pool. A connection that has been previously marked as dead and\n+    succeeds will be marked as live (its fail count will be deleted).\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        connections,\n+        dead_timeout=60,\n+        timeout_cutoff=5,\n+        selector_class=RoundRobinSelector,\n+        randomize_hosts=True,\n+        **kwargs\n+    ):\n+        \"\"\"\n+        :arg connections: list of tuples containing the\n+            :class:`~elasticsearch7.Connection` instance and it's options\n+        :arg dead_timeout: number of seconds a connection should be retired for\n+            after a failure, increases on consecutive failures\n+        :arg timeout_cutoff: number of consecutive failures after which the\n+            timeout doesn't increase\n+        :arg selector_class: :class:`~elasticsearch7.ConnectionSelector`\n+            subclass to use if more than one connection is live\n+        :arg randomize_hosts: shuffle the list of connections upon arrival to\n+            avoid dog piling effect across processes\n+        \"\"\"\n+        if not connections:\n+            raise ImproperlyConfigured(\n+                \"No defined connections, you need to \" \"specify at least one host.\"\n+            )\n+        self.connection_opts = connections\n+        self.connections = [c for (c, opts) in connections]\n+        # remember original connection list for resurrect(force=True)\n+        self.orig_connections = tuple(self.connections)\n+        # PriorityQueue for thread safety and ease of timeout management\n+        self.dead = PriorityQueue(len(self.connections))\n+        self.dead_count = {}\n+\n+        if randomize_hosts:\n+            # randomize the connection list to avoid all clients hitting same node\n+            # after startup/restart\n+            random.shuffle(self.connections)\n+\n+        # default timeout after which to try resurrecting a connection\n+        self.dead_timeout = dead_timeout\n+        self.timeout_cutoff = timeout_cutoff\n+\n+        self.selector = selector_class(dict(connections))\n+\n+    def mark_dead(self, connection, now=None):\n+        \"\"\"\n+        Mark the connection as dead (failed). Remove it from the live pool and\n+        put it on a timeout.\n+\n+        :arg connection: the failed instance\n+        \"\"\"\n+        # allow inject for testing purposes\n+        now = now if now else time.time()\n+        try:\n+            self.connections.remove(connection)\n+        except ValueError:\n+            logger.info(\n+                \"Attempted to remove %r, but it does not exist in the connection pool.\",\n+                connection,\n+            )\n+            # connection not alive or another thread marked it already, ignore\n+            return\n+        else:\n+            dead_count = self.dead_count.get(connection, 0) + 1\n+            self.dead_count[connection] = dead_count\n+            timeout = self.dead_timeout * 2 ** min(dead_count - 1, self.timeout_cutoff)\n+            self.dead.put((now + timeout, connection))\n+            logger.warning(\n+                \"Connection %r has failed for %i times in a row, putting on %i second timeout.\",\n+                connection,\n+                dead_count,\n+                timeout,\n+            )\n+\n+    def mark_live(self, connection):\n+        \"\"\"\n+        Mark connection as healthy after a resurrection. Resets the fail\n+        counter for the connection.\n+\n+        :arg connection: the connection to redeem\n+        \"\"\"\n+        try:\n+            del self.dead_count[connection]\n+        except KeyError:\n+            # race condition, safe to ignore\n+            pass\n+\n+    def resurrect(self, force=False):\n+        \"\"\"\n+        Attempt to resurrect a connection from the dead pool. It will try to\n+        locate one (not all) eligible (it's timeout is over) connection to\n+        return to the live pool. Any resurrected connection is also returned.\n+\n+        :arg force: resurrect a connection even if there is none eligible (used\n+            when we have no live connections). If force is specified resurrect\n+            always returns a connection.\n+\n+        \"\"\"\n+        # no dead connections\n+        if self.dead.empty():\n+            # we are forced to return a connection, take one from the original\n+            # list. This is to avoid a race condition where get_connection can\n+            # see no live connections but when it calls resurrect self.dead is\n+            # also empty. We assume that other threat has resurrected all\n+            # available connections so we can safely return one at random.\n+            if force:\n+                return random.choice(self.orig_connections)\n+            return\n+\n+        try:\n+            # retrieve a connection to check\n+            timeout, connection = self.dead.get(block=False)\n+        except Empty:\n+            # other thread has been faster and the queue is now empty. If we\n+            # are forced, return a connection at random again.\n+            if force:\n+                return random.choice(self.orig_connections)\n+            return\n+\n+        if not force and timeout > time.time():\n+            # return it back if not eligible and not forced\n+            self.dead.put((timeout, connection))\n+            return\n+\n+        # either we were forced or the connection is elligible to be retried\n+        self.connections.append(connection)\n+        logger.info(\"Resurrecting connection %r (force=%s).\", connection, force)\n+        return connection\n+\n+    def get_connection(self):\n+        \"\"\"\n+        Return a connection from the pool using the `ConnectionSelector`\n+        instance.\n+\n+        It tries to resurrect eligible connections, forces a resurrection when\n+        no connections are availible and passes the list of live connections to\n+        the selector instance to choose from.\n+\n+        Returns a connection instance and it's current fail count.\n+        \"\"\"\n+        self.resurrect()\n+        connections = self.connections[:]\n+\n+        # no live nodes, resurrect one by force and return it\n+        if not connections:\n+            return self.resurrect(True)\n+\n+        # only call selector if we have a selection\n+        if len(connections) > 1:\n+            return self.selector.select(connections)\n+\n+        # only one connection, no need for a selector\n+        return connections[0]\n+\n+    def close(self):\n+        \"\"\"\n+        Explicitly closes connections\n+        \"\"\"\n+        for conn in self.orig_connections:\n+            conn.close()\n+\n+\n+class DummyConnectionPool(ConnectionPool):\n+    def __init__(self, connections, **kwargs):\n+        if len(connections) != 1:\n+            raise ImproperlyConfigured(\n+                \"DummyConnectionPool needs exactly one \" \"connection defined.\"\n+            )\n+        # we need connection opts for sniffing logic\n+        self.connection_opts = connections\n+        self.connection = connections[0][0]\n+        self.connections = (self.connection,)\n+\n+    def get_connection(self):\n+        return self.connection\n+\n+    def close(self):\n+        \"\"\"\n+        Explicitly closes connections\n+        \"\"\"\n+        self.connection.close()\n+\n+    def _noop(self, *args, **kwargs):\n+        pass\n+\n+    mark_dead = mark_live = resurrect = _noop"
            },
            {
                "sha": "7068f2cb1ed1f1654ad25be156b4d70c52ed4ba2",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/exceptions.py",
                "status": "added",
                "additions": 146,
                "deletions": 0,
                "changes": 146,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/exceptions.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/exceptions.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/exceptions.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,146 @@\n+__all__ = [\n+    \"ImproperlyConfigured\",\n+    \"ElasticsearchException\",\n+    \"SerializationError\",\n+    \"TransportError\",\n+    \"NotFoundError\",\n+    \"ConflictError\",\n+    \"RequestError\",\n+    \"ConnectionError\",\n+    \"SSLError\",\n+    \"ConnectionTimeout\",\n+    \"AuthenticationException\",\n+    \"AuthorizationException\",\n+]\n+\n+\n+class ImproperlyConfigured(Exception):\n+    \"\"\"\n+    Exception raised when the config passed to the client is inconsistent or invalid.\n+    \"\"\"\n+\n+\n+class ElasticsearchException(Exception):\n+    \"\"\"\n+    Base class for all exceptions raised by this package's operations (doesn't\n+    apply to :class:`~elasticsearch7.ImproperlyConfigured`).\n+    \"\"\"\n+\n+\n+class SerializationError(ElasticsearchException):\n+    \"\"\"\n+    Data passed in failed to serialize properly in the ``Serializer`` being\n+    used.\n+    \"\"\"\n+\n+\n+class TransportError(ElasticsearchException):\n+    \"\"\"\n+    Exception raised when ES returns a non-OK (>=400) HTTP status code. Or when\n+    an actual connection error happens; in that case the ``status_code`` will\n+    be set to ``'N/A'``.\n+    \"\"\"\n+\n+    @property\n+    def status_code(self):\n+        \"\"\"\n+        The HTTP status code of the response that precipitated the error or\n+        ``'N/A'`` if not applicable.\n+        \"\"\"\n+        return self.args[0]\n+\n+    @property\n+    def error(self):\n+        \"\"\" A string error message. \"\"\"\n+        return self.args[1]\n+\n+    @property\n+    def info(self):\n+        \"\"\"\n+        Dict of returned error info from ES, where available, underlying\n+        exception when not.\n+        \"\"\"\n+        return self.args[2]\n+\n+    def __str__(self):\n+        cause = \"\"\n+        try:\n+            if self.info and \"error\" in self.info:\n+                if isinstance(self.info[\"error\"], dict):\n+                    root_cause = self.info[\"error\"][\"root_cause\"][0]\n+                    cause = \", \".join(\n+                        filter(\n+                            None,\n+                            [\n+                                repr(root_cause[\"reason\"]),\n+                                root_cause.get(\"resource.id\"),\n+                                root_cause.get(\"resource.type\"),\n+                            ],\n+                        )\n+                    )\n+\n+                else:\n+                    cause = repr(self.info[\"error\"])\n+        except LookupError:\n+            pass\n+        msg = \", \".join(filter(None, [str(self.status_code), repr(self.error), cause]))\n+        return \"%s(%s)\" % (self.__class__.__name__, msg)\n+\n+\n+class ConnectionError(TransportError):\n+    \"\"\"\n+    Error raised when there was an exception while talking to ES. Original\n+    exception from the underlying :class:`~elasticsearch7.Connection`\n+    implementation is available as ``.info``.\n+    \"\"\"\n+\n+    def __str__(self):\n+        return \"ConnectionError(%s) caused by: %s(%s)\" % (\n+            self.error,\n+            self.info.__class__.__name__,\n+            self.info,\n+        )\n+\n+\n+class SSLError(ConnectionError):\n+    \"\"\" Error raised when encountering SSL errors. \"\"\"\n+\n+\n+class ConnectionTimeout(ConnectionError):\n+    \"\"\" A network timeout. Doesn't cause a node retry by default. \"\"\"\n+\n+    def __str__(self):\n+        return \"ConnectionTimeout caused by - %s(%s)\" % (\n+            self.info.__class__.__name__,\n+            self.info,\n+        )\n+\n+\n+class NotFoundError(TransportError):\n+    \"\"\" Exception representing a 404 status code. \"\"\"\n+\n+\n+class ConflictError(TransportError):\n+    \"\"\" Exception representing a 409 status code. \"\"\"\n+\n+\n+class RequestError(TransportError):\n+    \"\"\" Exception representing a 400 status code. \"\"\"\n+\n+\n+class AuthenticationException(TransportError):\n+    \"\"\" Exception representing a 401 status code. \"\"\"\n+\n+\n+class AuthorizationException(TransportError):\n+    \"\"\" Exception representing a 403 status code. \"\"\"\n+\n+\n+# more generic mappings from status_code to python exceptions\n+HTTP_EXCEPTIONS = {\n+    400: RequestError,\n+    401: AuthenticationException,\n+    403: AuthorizationException,\n+    404: NotFoundError,\n+    409: ConflictError,\n+}"
            },
            {
                "sha": "28a11c303e776ad8df4051c7e990e567ce8cc7dc",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/__init__.py",
                "status": "added",
                "additions": 17,
                "deletions": 0,
                "changes": 17,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/__init__.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,17 @@\n+from .errors import BulkIndexError, ScanError\n+from .actions import expand_action, streaming_bulk, bulk, parallel_bulk\n+from .actions import scan, reindex\n+from .actions import _chunk_actions, _process_bulk_chunk\n+\n+__all__ = [\n+    \"BulkIndexError\",\n+    \"ScanError\",\n+    \"expand_action\",\n+    \"streaming_bulk\",\n+    \"bulk\",\n+    \"parallel_bulk\",\n+    \"scan\",\n+    \"reindex\",\n+    \"_chunk_actions\",\n+    \"_process_bulk_chunk\",\n+]"
            },
            {
                "sha": "b664671593836a08bb6675eb4806cfaf29342d9a",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/actions.py",
                "status": "added",
                "additions": 543,
                "deletions": 0,
                "changes": 543,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/actions.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/actions.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/actions.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,543 @@\n+from operator import methodcaller\n+import time\n+\n+from ..exceptions import TransportError\n+from ..compat import map, string_types, Queue\n+\n+from .errors import ScanError, BulkIndexError\n+\n+import logging\n+\n+\n+logger = logging.getLogger(\"elasticsearch7.helpers\")\n+\n+\n+def expand_action(data):\n+    \"\"\"\n+    From one document or action definition passed in by the user extract the\n+    action/data lines needed for elasticsearch7's\n+    :meth:`~elasticsearch7.Elasticsearch.bulk` api.\n+    \"\"\"\n+    # when given a string, assume user wants to index raw json\n+    if isinstance(data, string_types):\n+        return '{\"index\":{}}', data\n+\n+    # make sure we don't alter the action\n+    data = data.copy()\n+    op_type = data.pop(\"_op_type\", \"index\")\n+    action = {op_type: {}}\n+    for key in (\n+        \"_id\",\n+        \"_index\",\n+        \"_parent\",\n+        \"_percolate\",\n+        \"_retry_on_conflict\",\n+        \"_routing\",\n+        \"_timestamp\",\n+        \"_type\",\n+        \"_version\",\n+        \"_version_type\",\n+        \"parent\",\n+        \"pipeline\",\n+        \"retry_on_conflict\",\n+        \"routing\",\n+        \"version\",\n+        \"version_type\",\n+    ):\n+        if key in data:\n+            if key in [\n+                \"_parent\",\n+                \"_retry_on_conflict\",\n+                \"_routing\",\n+                \"_version\",\n+                \"_version_type\",\n+            ]:\n+                action[op_type][key[1:]] = data.pop(key)\n+            else:\n+                action[op_type][key] = data.pop(key)\n+\n+    # no data payload for delete\n+    if op_type == \"delete\":\n+        return action, None\n+\n+    return action, data.get(\"_source\", data)\n+\n+\n+def _chunk_actions(actions, chunk_size, max_chunk_bytes, serializer):\n+    \"\"\"\n+    Split actions into chunks by number or size, serialize them into strings in\n+    the process.\n+    \"\"\"\n+    bulk_actions, bulk_data = [], []\n+    size, action_count = 0, 0\n+    for action, data in actions:\n+        raw_data, raw_action = data, action\n+        action = serializer.dumps(action)\n+        # +1 to account for the trailing new line character\n+        cur_size = len(action.encode(\"utf-8\")) + 1\n+\n+        if data is not None:\n+            data = serializer.dumps(data)\n+            cur_size += len(data.encode(\"utf-8\")) + 1\n+\n+        # full chunk, send it and start a new one\n+        if bulk_actions and (\n+            size + cur_size > max_chunk_bytes or action_count == chunk_size\n+        ):\n+            yield bulk_data, bulk_actions\n+            bulk_actions, bulk_data = [], []\n+            size, action_count = 0, 0\n+\n+        bulk_actions.append(action)\n+        if data is not None:\n+            bulk_actions.append(data)\n+            bulk_data.append((raw_action, raw_data))\n+        else:\n+            bulk_data.append((raw_action,))\n+\n+        size += cur_size\n+        action_count += 1\n+\n+    if bulk_actions:\n+        yield bulk_data, bulk_actions\n+\n+\n+def _process_bulk_chunk(\n+    client,\n+    bulk_actions,\n+    bulk_data,\n+    raise_on_exception=True,\n+    raise_on_error=True,\n+    *args,\n+    **kwargs\n+):\n+    \"\"\"\n+    Send a bulk request to elasticsearch7 and process the output.\n+    \"\"\"\n+    # if raise on error is set, we need to collect errors per chunk before raising them\n+    errors = []\n+\n+    try:\n+        # send the actual request\n+        resp = client.bulk(\"\\n\".join(bulk_actions) + \"\\n\", *args, **kwargs)\n+    except TransportError as e:\n+        # default behavior - just propagate exception\n+        if raise_on_exception:\n+            raise e\n+\n+        # if we are not propagating, mark all actions in current chunk as failed\n+        err_message = str(e)\n+        exc_errors = []\n+\n+        for data in bulk_data:\n+            # collect all the information about failed actions\n+            op_type, action = data[0].copy().popitem()\n+            info = {\"error\": err_message, \"status\": e.status_code, \"exception\": e}\n+            if op_type != \"delete\":\n+                info[\"data\"] = data[1]\n+            info.update(action)\n+            exc_errors.append({op_type: info})\n+\n+        # emulate standard behavior for failed actions\n+        if raise_on_error:\n+            raise BulkIndexError(\n+                \"%i document(s) failed to index.\" % len(exc_errors), exc_errors\n+            )\n+        else:\n+            for err in exc_errors:\n+                yield False, err\n+            return\n+\n+    # go through request-response pairs and detect failures\n+    for data, (op_type, item) in zip(\n+        bulk_data, map(methodcaller(\"popitem\"), resp[\"items\"])\n+    ):\n+        ok = 200 <= item.get(\"status\", 500) < 300\n+        if not ok and raise_on_error:\n+            # include original document source\n+            if len(data) > 1:\n+                item[\"data\"] = data[1]\n+            errors.append({op_type: item})\n+\n+        if ok or not errors:\n+            # if we are not just recording all errors to be able to raise\n+            # them all at once, yield items individually\n+            yield ok, {op_type: item}\n+\n+    if errors:\n+        raise BulkIndexError(\"%i document(s) failed to index.\" % len(errors), errors)\n+\n+\n+def streaming_bulk(\n+    client,\n+    actions,\n+    chunk_size=500,\n+    max_chunk_bytes=100 * 1024 * 1024,\n+    raise_on_error=True,\n+    expand_action_callback=expand_action,\n+    raise_on_exception=True,\n+    max_retries=0,\n+    initial_backoff=2,\n+    max_backoff=600,\n+    yield_ok=True,\n+    *args,\n+    **kwargs\n+):\n+\n+    \"\"\"\n+    Streaming bulk consumes actions from the iterable passed in and yields\n+    results per action. For non-streaming usecases use\n+    :func:`~elasticsearch7.helpers.bulk` which is a wrapper around streaming\n+    bulk that returns summary information about the bulk operation once the\n+    entire input is consumed and sent.\n+\n+    If you specify ``max_retries`` it will also retry any documents that were\n+    rejected with a ``429`` status code. To do this it will wait (**by calling\n+    time.sleep which will block**) for ``initial_backoff`` seconds and then,\n+    every subsequent rejection for the same chunk, for double the time every\n+    time up to ``max_backoff`` seconds.\n+\n+    :arg client: instance of :class:`~elasticsearch7.Elasticsearch` to use\n+    :arg actions: iterable containing the actions to be executed\n+    :arg chunk_size: number of docs in one chunk sent to es (default: 500)\n+    :arg max_chunk_bytes: the maximum size of the request in bytes (default: 100MB)\n+    :arg raise_on_error: raise ``BulkIndexError`` containing errors (as `.errors`)\n+        from the execution of the last chunk when some occur. By default we raise.\n+    :arg raise_on_exception: if ``False`` then don't propagate exceptions from\n+        call to ``bulk`` and just report the items that failed as failed.\n+    :arg expand_action_callback: callback executed on each action passed in,\n+        should return a tuple containing the action line and the data line\n+        (`None` if data line should be omitted).\n+    :arg max_retries: maximum number of times a document will be retried when\n+        ``429`` is received, set to 0 (default) for no retries on ``429``\n+    :arg initial_backoff: number of seconds we should wait before the first\n+        retry. Any subsequent retries will be powers of ``initial_backoff *\n+        2**retry_number``\n+    :arg max_backoff: maximum number of seconds a retry will wait\n+    :arg yield_ok: if set to False will skip successful documents in the output\n+    \"\"\"\n+    actions = map(expand_action_callback, actions)\n+\n+    for bulk_data, bulk_actions in _chunk_actions(\n+        actions, chunk_size, max_chunk_bytes, client.transport.serializer\n+    ):\n+\n+        for attempt in range(max_retries + 1):\n+            to_retry, to_retry_data = [], []\n+            if attempt:\n+                time.sleep(min(max_backoff, initial_backoff * 2 ** (attempt - 1)))\n+\n+            try:\n+                for data, (ok, info) in zip(\n+                    bulk_data,\n+                    _process_bulk_chunk(\n+                        client,\n+                        bulk_actions,\n+                        bulk_data,\n+                        raise_on_exception,\n+                        raise_on_error,\n+                        *args,\n+                        **kwargs\n+                    ),\n+                ):\n+\n+                    if not ok:\n+                        action, info = info.popitem()\n+                        # retry if retries enabled, we get 429, and we are not\n+                        # in the last attempt\n+                        if (\n+                            max_retries\n+                            and info[\"status\"] == 429\n+                            and (attempt + 1) <= max_retries\n+                        ):\n+                            # _process_bulk_chunk expects strings so we need to\n+                            # re-serialize the data\n+                            to_retry.extend(\n+                                map(client.transport.serializer.dumps, data)\n+                            )\n+                            to_retry_data.append(data)\n+                        else:\n+                            yield ok, {action: info}\n+                    elif yield_ok:\n+                        yield ok, info\n+\n+            except TransportError as e:\n+                # suppress 429 errors since we will retry them\n+                if attempt == max_retries or e.status_code != 429:\n+                    raise\n+            else:\n+                if not to_retry:\n+                    break\n+                # retry only subset of documents that didn't succeed\n+                bulk_actions, bulk_data = to_retry, to_retry_data\n+\n+\n+def bulk(client, actions, stats_only=False, *args, **kwargs):\n+    \"\"\"\n+    Helper for the :meth:`~elasticsearch7.Elasticsearch.bulk` api that provides\n+    a more human friendly interface - it consumes an iterator of actions and\n+    sends them to elasticsearch7 in chunks. It returns a tuple with summary\n+    information - number of successfully executed actions and either list of\n+    errors or number of errors if ``stats_only`` is set to ``True``. Note that\n+    by default we raise a ``BulkIndexError`` when we encounter an error so\n+    options like ``stats_only`` only apply when ``raise_on_error`` is set to\n+    ``False``.\n+\n+    When errors are being collected original document data is included in the\n+    error dictionary which can lead to an extra high memory usage. If you need\n+    to process a lot of data and want to ignore/collect errors please consider\n+    using the :func:`~elasticsearch7.helpers.streaming_bulk` helper which will\n+    just return the errors and not store them in memory.\n+\n+\n+    :arg client: instance of :class:`~elasticsearch7.Elasticsearch` to use\n+    :arg actions: iterator containing the actions\n+    :arg stats_only: if `True` only report number of successful/failed\n+        operations instead of just number of successful and a list of error responses\n+\n+    Any additional keyword arguments will be passed to\n+    :func:`~elasticsearch7.helpers.streaming_bulk` which is used to execute\n+    the operation, see :func:`~elasticsearch7.helpers.streaming_bulk` for more\n+    accepted parameters.\n+    \"\"\"\n+    success, failed = 0, 0\n+\n+    # list of errors to be collected is not stats_only\n+    errors = []\n+\n+    # make streaming_bulk yield successful results so we can count them\n+    kwargs[\"yield_ok\"] = True\n+    for ok, item in streaming_bulk(client, actions, *args, **kwargs):\n+        # go through request-response pairs and detect failures\n+        if not ok:\n+            if not stats_only:\n+                errors.append(item)\n+            failed += 1\n+        else:\n+            success += 1\n+\n+    return success, failed if stats_only else errors\n+\n+\n+def parallel_bulk(\n+    client,\n+    actions,\n+    thread_count=4,\n+    chunk_size=500,\n+    max_chunk_bytes=100 * 1024 * 1024,\n+    queue_size=4,\n+    expand_action_callback=expand_action,\n+    *args,\n+    **kwargs\n+):\n+    \"\"\"\n+    Parallel version of the bulk helper run in multiple threads at once.\n+\n+    :arg client: instance of :class:`~elasticsearch7.Elasticsearch` to use\n+    :arg actions: iterator containing the actions\n+    :arg thread_count: size of the threadpool to use for the bulk requests\n+    :arg chunk_size: number of docs in one chunk sent to es (default: 500)\n+    :arg max_chunk_bytes: the maximum size of the request in bytes (default: 100MB)\n+    :arg raise_on_error: raise ``BulkIndexError`` containing errors (as `.errors`)\n+        from the execution of the last chunk when some occur. By default we raise.\n+    :arg raise_on_exception: if ``False`` then don't propagate exceptions from\n+        call to ``bulk`` and just report the items that failed as failed.\n+    :arg expand_action_callback: callback executed on each action passed in,\n+        should return a tuple containing the action line and the data line\n+        (`None` if data line should be omitted).\n+    :arg queue_size: size of the task queue between the main thread (producing\n+        chunks to send) and the processing threads.\n+    \"\"\"\n+    # Avoid importing multiprocessing unless parallel_bulk is used\n+    # to avoid exceptions on restricted environments like App Engine\n+    from multiprocessing.pool import ThreadPool\n+\n+    actions = map(expand_action_callback, actions)\n+\n+    class BlockingPool(ThreadPool):\n+        def _setup_queues(self):\n+            super(BlockingPool, self)._setup_queues()\n+            # The queue must be at least the size of the number of threads to\n+            # prevent hanging when inserting sentinel values during teardown.\n+            self._inqueue = Queue(max(queue_size, thread_count))\n+            self._quick_put = self._inqueue.put\n+\n+    pool = BlockingPool(thread_count)\n+\n+    try:\n+        for result in pool.imap(\n+            lambda bulk_chunk: list(\n+                _process_bulk_chunk(\n+                    client, bulk_chunk[1], bulk_chunk[0], *args, **kwargs\n+                )\n+            ),\n+            _chunk_actions(\n+                actions, chunk_size, max_chunk_bytes, client.transport.serializer\n+            ),\n+        ):\n+            for item in result:\n+                yield item\n+\n+    finally:\n+        pool.close()\n+        pool.join()\n+\n+\n+def scan(\n+    client,\n+    query=None,\n+    scroll=\"5m\",\n+    raise_on_error=True,\n+    preserve_order=False,\n+    size=1000,\n+    request_timeout=None,\n+    clear_scroll=True,\n+    scroll_kwargs=None,\n+    **kwargs\n+):\n+    \"\"\"\n+    Simple abstraction on top of the\n+    :meth:`~elasticsearch7.Elasticsearch.scroll` api - a simple iterator that\n+    yields all hits as returned by underlining scroll requests.\n+\n+    By default scan does not return results in any pre-determined order. To\n+    have a standard order in the returned documents (either by score or\n+    explicit sort definition) when scrolling, use ``preserve_order=True``. This\n+    may be an expensive operation and will negate the performance benefits of\n+    using ``scan``.\n+\n+    :arg client: instance of :class:`~elasticsearch7.Elasticsearch` to use\n+    :arg query: body for the :meth:`~elasticsearch7.Elasticsearch.search` api\n+    :arg scroll: Specify how long a consistent view of the index should be\n+        maintained for scrolled search\n+    :arg raise_on_error: raises an exception (``ScanError``) if an error is\n+        encountered (some shards fail to execute). By default we raise.\n+    :arg preserve_order: don't set the ``search_type`` to ``scan`` - this will\n+        cause the scroll to paginate with preserving the order. Note that this\n+        can be an extremely expensive operation and can easily lead to\n+        unpredictable results, use with caution.\n+    :arg size: size (per shard) of the batch send at each iteration.\n+    :arg request_timeout: explicit timeout for each call to ``scan``\n+    :arg clear_scroll: explicitly calls delete on the scroll id via the clear\n+        scroll API at the end of the method on completion or error, defaults\n+        to true.\n+    :arg scroll_kwargs: additional kwargs to be passed to\n+        :meth:`~elasticsearch7.Elasticsearch.scroll`\n+\n+    Any additional keyword arguments will be passed to the initial\n+    :meth:`~elasticsearch7.Elasticsearch.search` call::\n+\n+        scan(es,\n+            query={\"query\": {\"match\": {\"title\": \"python\"}}},\n+            index=\"orders-*\",\n+            doc_type=\"books\"\n+        )\n+\n+    \"\"\"\n+    scroll_kwargs = scroll_kwargs or {}\n+\n+    if not preserve_order:\n+        query = query.copy() if query else {}\n+        query[\"sort\"] = \"_doc\"\n+\n+    # initial search\n+    resp = client.search(\n+        body=query, scroll=scroll, size=size, request_timeout=request_timeout, **kwargs\n+    )\n+    scroll_id = resp.get(\"_scroll_id\")\n+\n+    try:\n+        while scroll_id and resp[\"hits\"][\"hits\"]:\n+            for hit in resp[\"hits\"][\"hits\"]:\n+                yield hit\n+\n+            # check if we have any errors\n+            if (resp[\"_shards\"][\"successful\"] + resp[\"_shards\"][\"skipped\"]) < resp[\n+                \"_shards\"\n+            ][\"total\"]:\n+                logger.warning(\n+                    \"Scroll request has only succeeded on %d (+%d skipped) shards out of %d.\",\n+                    resp[\"_shards\"][\"successful\"],\n+                    resp[\"_shards\"][\"skipped\"],\n+                    resp[\"_shards\"][\"total\"],\n+                )\n+                if raise_on_error:\n+                    raise ScanError(\n+                        scroll_id,\n+                        \"Scroll request has only succeeded on %d (+%d skiped) shards out of %d.\"\n+                        % (\n+                            resp[\"_shards\"][\"successful\"],\n+                            resp[\"_shards\"][\"skipped\"],\n+                            resp[\"_shards\"][\"total\"],\n+                        ),\n+                    )\n+            resp = client.scroll(\n+                body={\"scroll_id\": scroll_id, \"scroll\": scroll}, **scroll_kwargs\n+            )\n+            scroll_id = resp.get(\"_scroll_id\")\n+\n+    finally:\n+        if scroll_id and clear_scroll:\n+            client.clear_scroll(body={\"scroll_id\": [scroll_id]}, ignore=(404,))\n+\n+\n+def reindex(\n+    client,\n+    source_index,\n+    target_index,\n+    query=None,\n+    target_client=None,\n+    chunk_size=500,\n+    scroll=\"5m\",\n+    scan_kwargs={},\n+    bulk_kwargs={},\n+):\n+\n+    \"\"\"\n+    Reindex all documents from one index that satisfy a given query\n+    to another, potentially (if `target_client` is specified) on a different cluster.\n+    If you don't specify the query you will reindex all the documents.\n+\n+    Since ``2.3`` a :meth:`~elasticsearch7.Elasticsearch.reindex` api is\n+    available as part of elasticsearch7 itself. It is recommended to use the api\n+    instead of this helper wherever possible. The helper is here mostly for\n+    backwards compatibility and for situations where more flexibility is\n+    needed.\n+\n+    .. note::\n+\n+        This helper doesn't transfer mappings, just the data.\n+\n+    :arg client: instance of :class:`~elasticsearch7.Elasticsearch` to use (for\n+        read if `target_client` is specified as well)\n+    :arg source_index: index (or list of indices) to read documents from\n+    :arg target_index: name of the index in the target cluster to populate\n+    :arg query: body for the :meth:`~elasticsearch7.Elasticsearch.search` api\n+    :arg target_client: optional, is specified will be used for writing (thus\n+        enabling reindex between clusters)\n+    :arg chunk_size: number of docs in one chunk sent to es (default: 500)\n+    :arg scroll: Specify how long a consistent view of the index should be\n+        maintained for scrolled search\n+    :arg scan_kwargs: additional kwargs to be passed to\n+        :func:`~elasticsearch7.helpers.scan`\n+    :arg bulk_kwargs: additional kwargs to be passed to\n+        :func:`~elasticsearch7.helpers.bulk`\n+    \"\"\"\n+    target_client = client if target_client is None else target_client\n+    docs = scan(client, query=query, index=source_index, scroll=scroll, **scan_kwargs)\n+\n+    def _change_doc_index(hits, index):\n+        for h in hits:\n+            h[\"_index\"] = index\n+            if \"fields\" in h:\n+                h.update(h.pop(\"fields\"))\n+            yield h\n+\n+    kwargs = {\"stats_only\": True}\n+    kwargs.update(bulk_kwargs)\n+    return bulk(\n+        target_client,\n+        _change_doc_index(docs, target_index),\n+        chunk_size=chunk_size,\n+        **kwargs\n+    )"
            },
            {
                "sha": "6261822e51707e111761126b83e2b904d4db8235",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/errors.py",
                "status": "added",
                "additions": 14,
                "deletions": 0,
                "changes": 14,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/errors.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/errors.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/errors.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,14 @@\n+from ..exceptions import ElasticsearchException\n+\n+\n+class BulkIndexError(ElasticsearchException):\n+    @property\n+    def errors(self):\n+        \"\"\" List of errors from execution of the last chunk. \"\"\"\n+        return self.args[1]\n+\n+\n+class ScanError(ElasticsearchException):\n+    def __init__(self, scroll_id, *args, **kwargs):\n+        super(ScanError, self).__init__(*args, **kwargs)\n+        self.scroll_id = scroll_id"
            },
            {
                "sha": "938f27576bff5bd9c38e09670fb4c89c490f55b0",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/test.py",
                "status": "added",
                "additions": 63,
                "deletions": 0,
                "changes": 63,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/test.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/test.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/helpers/test.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,63 @@\n+import time\n+import os\n+from unittest import TestCase, SkipTest\n+\n+from elasticsearch7 import Elasticsearch\n+from elasticsearch7.exceptions import ConnectionError\n+\n+\n+def get_test_client(nowait=False, **kwargs):\n+    # construct kwargs from the environment\n+    kw = {\"timeout\": 30, \"ca_certs\": \".ci/certs/ca.pem\"}\n+\n+    if \"PYTHON_CONNECTION_CLASS\" in os.environ:\n+        from elasticsearch7 import connection\n+\n+        kw[\"connection_class\"] = getattr(\n+            connection, os.environ[\"PYTHON_CONNECTION_CLASS\"]\n+        )\n+\n+    kw.update(kwargs)\n+    client = Elasticsearch([os.environ.get(\"ELASTICSEARCH_HOST\", {})], **kw)\n+\n+    # wait for yellow status\n+    for _ in range(1 if nowait else 100):\n+        try:\n+            client.cluster.health(wait_for_status=\"yellow\")\n+            return client\n+        except ConnectionError:\n+            time.sleep(0.1)\n+    else:\n+        # timeout\n+        raise SkipTest(\"Elasticsearch failed to start.\")\n+\n+\n+def _get_version(version_string):\n+    if \".\" not in version_string:\n+        return ()\n+    version = version_string.strip().split(\".\")\n+    return tuple(int(v) if v.isdigit() else 999 for v in version)\n+\n+\n+class ElasticsearchTestCase(TestCase):\n+    @staticmethod\n+    def _get_client():\n+        return get_test_client()\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        super(ElasticsearchTestCase, cls).setUpClass()\n+        cls.client = cls._get_client()\n+\n+    def tearDown(self):\n+        super(ElasticsearchTestCase, self).tearDown()\n+        self.client.indices.delete(index=\"*\", ignore=404)\n+        self.client.indices.delete_template(name=\"*\", ignore=404)\n+        self.client.indices.delete_alias(index=\"_all\", name=\"_all\", ignore=404)\n+\n+    @property\n+    def es_version(self):\n+        if not hasattr(self, \"_es_version\"):\n+            version_string = self.client.info()[\"version\"][\"number\"]\n+            self._es_version = _get_version(version_string)\n+        return self._es_version"
            },
            {
                "sha": "dd7d0dc87ac835d0f9400829013cc2a7400e2e92",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/serializer.py",
                "status": "added",
                "additions": 86,
                "deletions": 0,
                "changes": 86,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/serializer.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/serializer.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/serializer.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,86 @@\n+try:\n+    import simplejson as json\n+except ImportError:\n+    import json\n+import uuid\n+from datetime import date, datetime\n+from decimal import Decimal\n+\n+from .exceptions import SerializationError, ImproperlyConfigured\n+from .compat import string_types\n+\n+\n+class TextSerializer(object):\n+    mimetype = \"text/plain\"\n+\n+    def loads(self, s):\n+        return s\n+\n+    def dumps(self, data):\n+        if isinstance(data, string_types):\n+            return data\n+\n+        raise SerializationError(\"Cannot serialize %r into text.\" % data)\n+\n+\n+class JSONSerializer(object):\n+    mimetype = \"application/json\"\n+\n+    def default(self, data):\n+        if isinstance(data, (date, datetime)):\n+            return data.isoformat()\n+        elif isinstance(data, Decimal):\n+            return float(data)\n+        elif isinstance(data, uuid.UUID):\n+            return str(data)\n+        raise TypeError(\"Unable to serialize %r (type: %s)\" % (data, type(data)))\n+\n+    def loads(self, s):\n+        try:\n+            return json.loads(s)\n+        except (ValueError, TypeError) as e:\n+            raise SerializationError(s, e)\n+\n+    def dumps(self, data):\n+        # don't serialize strings\n+        if isinstance(data, string_types):\n+            return data\n+\n+        try:\n+            return json.dumps(\n+                data, default=self.default, ensure_ascii=False, separators=(\",\", \":\")\n+            )\n+        except (ValueError, TypeError) as e:\n+            raise SerializationError(data, e)\n+\n+\n+DEFAULT_SERIALIZERS = {\n+    JSONSerializer.mimetype: JSONSerializer(),\n+    TextSerializer.mimetype: TextSerializer(),\n+}\n+\n+\n+class Deserializer(object):\n+    def __init__(self, serializers, default_mimetype=\"application/json\"):\n+        try:\n+            self.default = serializers[default_mimetype]\n+        except KeyError:\n+            raise ImproperlyConfigured(\n+                \"Cannot find default serializer (%s)\" % default_mimetype\n+            )\n+        self.serializers = serializers\n+\n+    def loads(self, s, mimetype=None):\n+        if not mimetype:\n+            deserializer = self.default\n+        else:\n+            # split out charset\n+            mimetype, _, _ = mimetype.partition(\";\")\n+            try:\n+                deserializer = self.serializers[mimetype]\n+            except KeyError:\n+                raise SerializationError(\n+                    \"Unknown mimetype, unable to deserialize: %s\" % mimetype\n+                )\n+\n+        return deserializer.loads(s)"
            },
            {
                "sha": "2c3a89cf97003c4d5e1f81459820d062397b24e8",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/transport.py",
                "status": "added",
                "additions": 403,
                "deletions": 0,
                "changes": 403,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/transport.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/transport.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/elasticsearch7/transport.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,403 @@\n+import time\n+from itertools import chain\n+\n+from .connection import Urllib3HttpConnection\n+from .connection_pool import ConnectionPool, DummyConnectionPool\n+from .serializer import JSONSerializer, Deserializer, DEFAULT_SERIALIZERS\n+from .exceptions import (\n+    ConnectionError,\n+    TransportError,\n+    SerializationError,\n+    ConnectionTimeout,\n+)\n+\n+\n+def get_host_info(node_info, host):\n+    \"\"\"\n+    Simple callback that takes the node info from `/_cluster/nodes` and a\n+    parsed connection information and return the connection information. If\n+    `None` is returned this node will be skipped.\n+\n+    Useful for filtering nodes (by proximity for example) or if additional\n+    information needs to be provided for the :class:`~elasticsearch7.Connection`\n+    class. By default master only nodes are filtered out since they shouldn't\n+    typically be used for API operations.\n+\n+    :arg node_info: node information from `/_cluster/nodes`\n+    :arg host: connection information (host, port) extracted from the node info\n+    \"\"\"\n+    # ignore master only nodes\n+    if node_info.get(\"roles\", []) == [\"master\"]:\n+        return None\n+    return host\n+\n+\n+class Transport(object):\n+    \"\"\"\n+    Encapsulation of transport-related to logic. Handles instantiation of the\n+    individual connections as well as creating a connection pool to hold them.\n+\n+    Main interface is the `perform_request` method.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        hosts,\n+        connection_class=Urllib3HttpConnection,\n+        connection_pool_class=ConnectionPool,\n+        host_info_callback=get_host_info,\n+        sniff_on_start=False,\n+        sniffer_timeout=None,\n+        sniff_timeout=0.1,\n+        sniff_on_connection_fail=False,\n+        serializer=JSONSerializer(),\n+        serializers=None,\n+        default_mimetype=\"application/json\",\n+        max_retries=3,\n+        retry_on_status=(502, 503, 504),\n+        retry_on_timeout=False,\n+        send_get_body_as=\"GET\",\n+        **kwargs\n+    ):\n+        \"\"\"\n+        :arg hosts: list of dictionaries, each containing keyword arguments to\n+            create a `connection_class` instance\n+        :arg connection_class: subclass of :class:`~elasticsearch7.Connection` to use\n+        :arg connection_pool_class: subclass of :class:`~elasticsearch7.ConnectionPool` to use\n+        :arg host_info_callback: callback responsible for taking the node information from\n+            `/_cluster/nodes`, along with already extracted information, and\n+            producing a list of arguments (same as `hosts` parameter)\n+        :arg sniff_on_start: flag indicating whether to obtain a list of nodes\n+            from the cluster at startup time\n+        :arg sniffer_timeout: number of seconds between automatic sniffs\n+        :arg sniff_on_connection_fail: flag controlling if connection failure triggers a sniff\n+        :arg sniff_timeout: timeout used for the sniff request - it should be a\n+            fast api call and we are talking potentially to more nodes so we want\n+            to fail quickly. Not used during initial sniffing (if\n+            ``sniff_on_start`` is on) when the connection still isn't\n+            initialized.\n+        :arg serializer: serializer instance\n+        :arg serializers: optional dict of serializer instances that will be\n+            used for deserializing data coming from the server. (key is the mimetype)\n+        :arg default_mimetype: when no mimetype is specified by the server\n+            response assume this mimetype, defaults to `'application/json'`\n+        :arg max_retries: maximum number of retries before an exception is propagated\n+        :arg retry_on_status: set of HTTP status codes on which we should retry\n+            on a different node. defaults to ``(502, 503, 504)``\n+        :arg retry_on_timeout: should timeout trigger a retry on different\n+            node? (default `False`)\n+        :arg send_get_body_as: for GET requests with body this option allows\n+            you to specify an alternate way of execution for environments that\n+            don't support passing bodies with GET requests. If you set this to\n+            'POST' a POST method will be used instead, if to 'source' then the body\n+            will be serialized and passed as a query parameter `source`.\n+\n+        Any extra keyword arguments will be passed to the `connection_class`\n+        when creating and instance unless overridden by that connection's\n+        options provided as part of the hosts parameter.\n+        \"\"\"\n+\n+        # serialization config\n+        _serializers = DEFAULT_SERIALIZERS.copy()\n+        # if a serializer has been specified, use it for deserialization as well\n+        _serializers[serializer.mimetype] = serializer\n+        # if custom serializers map has been supplied, override the defaults with it\n+        if serializers:\n+            _serializers.update(serializers)\n+        # create a deserializer with our config\n+        self.deserializer = Deserializer(_serializers, default_mimetype)\n+\n+        self.max_retries = max_retries\n+        self.retry_on_timeout = retry_on_timeout\n+        self.retry_on_status = retry_on_status\n+        self.send_get_body_as = send_get_body_as\n+\n+        # data serializer\n+        self.serializer = serializer\n+\n+        # store all strategies...\n+        self.connection_pool_class = connection_pool_class\n+        self.connection_class = connection_class\n+\n+        # ...save kwargs to be passed to the connections\n+        self.kwargs = kwargs\n+        self.hosts = hosts\n+\n+        # ...and instantiate them\n+        self.set_connections(hosts)\n+        # retain the original connection instances for sniffing\n+        self.seed_connections = self.connection_pool.connections[:]\n+\n+        # Don't enable sniffing on Cloud instances.\n+        if kwargs.get(\"cloud_id\", False):\n+            sniff_on_start = False\n+            sniff_on_connection_fail = False\n+\n+        # sniffing data\n+        self.sniffer_timeout = sniffer_timeout\n+        self.sniff_on_connection_fail = sniff_on_connection_fail\n+        self.last_sniff = time.time()\n+        self.sniff_timeout = sniff_timeout\n+\n+        # callback to construct host dict from data in /_cluster/nodes\n+        self.host_info_callback = host_info_callback\n+\n+        if sniff_on_start:\n+            self.sniff_hosts(True)\n+\n+    def add_connection(self, host):\n+        \"\"\"\n+        Create a new :class:`~elasticsearch7.Connection` instance and add it to the pool.\n+\n+        :arg host: kwargs that will be used to create the instance\n+        \"\"\"\n+        self.hosts.append(host)\n+        self.set_connections(self.hosts)\n+\n+    def set_connections(self, hosts):\n+        \"\"\"\n+        Instantiate all the connections and create new connection pool to hold them.\n+        Tries to identify unchanged hosts and re-use existing\n+        :class:`~elasticsearch7.Connection` instances.\n+\n+        :arg hosts: same as `__init__`\n+        \"\"\"\n+        # construct the connections\n+        def _create_connection(host):\n+            # if this is not the initial setup look at the existing connection\n+            # options and identify connections that haven't changed and can be\n+            # kept around.\n+            if hasattr(self, \"connection_pool\"):\n+                for (connection, old_host) in self.connection_pool.connection_opts:\n+                    if old_host == host:\n+                        return connection\n+\n+            # previously unseen params, create new connection\n+            kwargs = self.kwargs.copy()\n+            kwargs.update(host)\n+            return self.connection_class(**kwargs)\n+\n+        connections = map(_create_connection, hosts)\n+\n+        connections = list(zip(connections, hosts))\n+        if len(connections) == 1:\n+            self.connection_pool = DummyConnectionPool(connections)\n+        else:\n+            # pass the hosts dicts to the connection pool to optionally extract parameters from\n+            self.connection_pool = self.connection_pool_class(\n+                connections, **self.kwargs\n+            )\n+\n+    def get_connection(self):\n+        \"\"\"\n+        Retrieve a :class:`~elasticsearch7.Connection` instance from the\n+        :class:`~elasticsearch7.ConnectionPool` instance.\n+        \"\"\"\n+        if self.sniffer_timeout:\n+            if time.time() >= self.last_sniff + self.sniffer_timeout:\n+                self.sniff_hosts()\n+        return self.connection_pool.get_connection()\n+\n+    def _get_sniff_data(self, initial=False):\n+        \"\"\"\n+        Perform the request to get sniffing information. Returns a list of\n+        dictionaries (one per node) containing all the information from the\n+        cluster.\n+\n+        It also sets the last_sniff attribute in case of a successful attempt.\n+\n+        In rare cases it might be possible to override this method in your\n+        custom Transport class to serve data from alternative source like\n+        configuration management.\n+        \"\"\"\n+        previous_sniff = self.last_sniff\n+\n+        try:\n+            # reset last_sniff timestamp\n+            self.last_sniff = time.time()\n+            # go through all current connections as well as the\n+            # seed_connections for good measure\n+            for c in chain(self.connection_pool.connections, self.seed_connections):\n+                try:\n+                    # use small timeout for the sniffing request, should be a fast api call\n+                    _, headers, node_info = c.perform_request(\n+                        \"GET\",\n+                        \"/_nodes/_all/http\",\n+                        timeout=self.sniff_timeout if not initial else None,\n+                    )\n+                    node_info = self.deserializer.loads(\n+                        node_info, headers.get(\"content-type\")\n+                    )\n+                    break\n+                except (ConnectionError, SerializationError):\n+                    pass\n+            else:\n+                raise TransportError(\"N/A\", \"Unable to sniff hosts.\")\n+        except Exception:\n+            # keep the previous value on error\n+            self.last_sniff = previous_sniff\n+            raise\n+\n+        return list(node_info[\"nodes\"].values())\n+\n+    def _get_host_info(self, host_info):\n+        host = {}\n+        address = host_info.get(\"http\", {}).get(\"publish_address\")\n+\n+        # malformed or no address given\n+        if not address or \":\" not in address:\n+            return None\n+\n+        if \"/\" in address:\n+            # Support 7.x host/ip:port behavior where http.publish_host has been set.\n+            fqdn, ipaddress = address.split(\"/\", 1)\n+            host[\"host\"] = fqdn\n+            _, host[\"port\"] = ipaddress.rsplit(\":\", 1)\n+            host[\"port\"] = int(host[\"port\"])\n+\n+        else:\n+            host[\"host\"], host[\"port\"] = address.rsplit(\":\", 1)\n+            host[\"port\"] = int(host[\"port\"])\n+\n+        return self.host_info_callback(host_info, host)\n+\n+    def sniff_hosts(self, initial=False):\n+        \"\"\"\n+        Obtain a list of nodes from the cluster and create a new connection\n+        pool using the information retrieved.\n+\n+        To extract the node connection parameters use the ``nodes_to_host_callback``.\n+\n+        :arg initial: flag indicating if this is during startup\n+            (``sniff_on_start``), ignore the ``sniff_timeout`` if ``True``\n+        \"\"\"\n+        node_info = self._get_sniff_data(initial)\n+\n+        hosts = list(filter(None, (self._get_host_info(n) for n in node_info)))\n+\n+        # we weren't able to get any nodes or host_info_callback blocked all -\n+        # raise error.\n+        if not hosts:\n+            raise TransportError(\n+                \"N/A\", \"Unable to sniff hosts - no viable hosts found.\"\n+            )\n+\n+        self.set_connections(hosts)\n+\n+    def mark_dead(self, connection):\n+        \"\"\"\n+        Mark a connection as dead (failed) in the connection pool. If sniffing\n+        on failure is enabled this will initiate the sniffing process.\n+\n+        :arg connection: instance of :class:`~elasticsearch7.Connection` that failed\n+        \"\"\"\n+        # mark as dead even when sniffing to avoid hitting this host during the sniff process\n+        self.connection_pool.mark_dead(connection)\n+        if self.sniff_on_connection_fail:\n+            self.sniff_hosts()\n+\n+    def perform_request(self, method, url, headers=None, params=None, body=None):\n+        \"\"\"\n+        Perform the actual request. Retrieve a connection from the connection\n+        pool, pass all the information to it's perform_request method and\n+        return the data.\n+\n+        If an exception was raised, mark the connection as failed and retry (up\n+        to `max_retries` times).\n+\n+        If the operation was successful and the connection used was previously\n+        marked as dead, mark it as live, resetting it's failure count.\n+\n+        :arg method: HTTP method to use\n+        :arg url: absolute url (without host) to target\n+        :arg headers: dictionary of headers, will be handed over to the\n+            underlying :class:`~elasticsearch7.Connection` class\n+        :arg params: dictionary of query parameters, will be handed over to the\n+            underlying :class:`~elasticsearch7.Connection` class for serialization\n+        :arg body: body of the request, will be serialized using serializer and\n+            passed to the connection\n+        \"\"\"\n+        if body is not None:\n+            body = self.serializer.dumps(body)\n+\n+            # some clients or environments don't support sending GET with body\n+            if method in (\"HEAD\", \"GET\") and self.send_get_body_as != \"GET\":\n+                # send it as post instead\n+                if self.send_get_body_as == \"POST\":\n+                    method = \"POST\"\n+\n+                # or as source parameter\n+                elif self.send_get_body_as == \"source\":\n+                    if params is None:\n+                        params = {}\n+                    params[\"source\"] = body\n+                    body = None\n+\n+        if body is not None:\n+            try:\n+                body = body.encode(\"utf-8\", \"surrogatepass\")\n+            except (UnicodeDecodeError, AttributeError):\n+                # bytes/str - no need to re-encode\n+                pass\n+\n+        ignore = ()\n+        timeout = None\n+        if params:\n+            timeout = params.pop(\"request_timeout\", None)\n+            ignore = params.pop(\"ignore\", ())\n+            if isinstance(ignore, int):\n+                ignore = (ignore,)\n+\n+        for attempt in range(self.max_retries + 1):\n+            connection = self.get_connection()\n+\n+            try:\n+                status, headers_response, data = connection.perform_request(\n+                    method,\n+                    url,\n+                    params,\n+                    body,\n+                    headers=headers,\n+                    ignore=ignore,\n+                    timeout=timeout,\n+                )\n+\n+            except TransportError as e:\n+                if method == \"HEAD\" and e.status_code == 404:\n+                    return False\n+\n+                retry = False\n+                if isinstance(e, ConnectionTimeout):\n+                    retry = self.retry_on_timeout\n+                elif isinstance(e, ConnectionError):\n+                    retry = True\n+                elif e.status_code in self.retry_on_status:\n+                    retry = True\n+\n+                if retry:\n+                    # only mark as dead if we are retrying\n+                    self.mark_dead(connection)\n+                    # raise exception on last retry\n+                    if attempt == self.max_retries:\n+                        raise\n+                else:\n+                    raise\n+\n+            else:\n+                # connection didn't fail, confirm it's live status\n+                self.connection_pool.mark_live(connection)\n+\n+                if method == \"HEAD\":\n+                    return 200 <= status < 300\n+\n+                if data:\n+                    data = self.deserializer.loads(\n+                        data, headers_response.get(\"content-type\")\n+                    )\n+                return data\n+\n+    def close(self):\n+        \"\"\"\n+        Explicitly closes connections\n+        \"\"\"\n+        self.connection_pool.close()"
            },
            {
                "sha": "6a40f5332707f1a1197f898fd97b172438acab9d",
                "filename": "github-analytics/libs/elasticsearch7-7.6.0/setup.py",
                "status": "added",
                "additions": 66,
                "deletions": 0,
                "changes": 66,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/setup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/734cadd4cd38fd334af09ba24fe48194d04e6d60/github-analytics/libs/elasticsearch7-7.6.0/setup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/libs/elasticsearch7-7.6.0/setup.py?ref=734cadd4cd38fd334af09ba24fe48194d04e6d60",
                "patch": "@@ -0,0 +1,66 @@\n+# -*- coding: utf-8 -*-\n+from os.path import join, dirname\n+from setuptools import setup, find_packages\n+import sys\n+\n+VERSION = (7, 6, 0)\n+__version__ = VERSION\n+__versionstr__ = \"7.6.0\"\n+\n+with open(join(dirname(__file__), \"README\")) as f:\n+    long_description = f.read().strip()\n+\n+install_requires = [\"urllib3>=1.21.1\"]\n+tests_require = [\n+    \"requests>=2.0.0, <3.0.0\",\n+    \"nose\",\n+    \"coverage\",\n+    \"mock\",\n+    \"pyyaml\",\n+    \"nosexcover\",\n+]\n+\n+docs_require = [\"sphinx<1.7\", \"sphinx_rtd_theme\"]\n+generate_require = [\"black\", \"jinja2\"]\n+\n+setup(\n+    name=\"elasticsearch7\",\n+    description=\"Python client for Elasticsearch\",\n+    license=\"Apache-2.0\",\n+    url=\"https://github.com/elastic/elasticsearch-py\",\n+    download_url=\"https://github.com/shdkpr2008/elasticsearch-py/archive/7.6.0.tar.gz\",\n+    long_description=long_description,\n+    long_description_content_type=\"text/x-rst\",\n+    version=__versionstr__,\n+    author=\"Honza Kr\u00e1l, Nick Lang\",\n+    author_email=\"honza.kral@gmail.com, nick@nicklang.com\",\n+    maintainer=\"Seth Michael Larson\",\n+    maintainer_email=\"seth.larson@elastic.co\",\n+    packages=find_packages(where=\".\", exclude=(\"test_elasticsearch*\",)),\n+    classifiers=[\n+        \"Development Status :: 5 - Production/Stable\",\n+        \"License :: OSI Approved :: Apache Software License\",\n+        \"Intended Audience :: Developers\",\n+        \"Operating System :: OS Independent\",\n+        \"Programming Language :: Python\",\n+        \"Programming Language :: Python :: 2\",\n+        \"Programming Language :: Python :: 2.7\",\n+        \"Programming Language :: Python :: 3\",\n+        \"Programming Language :: Python :: 3.4\",\n+        \"Programming Language :: Python :: 3.5\",\n+        \"Programming Language :: Python :: 3.6\",\n+        \"Programming Language :: Python :: 3.7\",\n+        \"Programming Language :: Python :: 3.8\",\n+        \"Programming Language :: Python :: Implementation :: CPython\",\n+        \"Programming Language :: Python :: Implementation :: PyPy\",\n+    ],\n+    python_requires=\">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, <4\",\n+    install_requires=install_requires,\n+    test_suite=\"test_elasticsearch7.run_tests.run_all\",\n+    tests_require=tests_require,\n+    extras_require={\n+        \"develop\": tests_require + docs_require + generate_require,\n+        \"docs\": docs_require,\n+        \"requests\": [\"requests>=2.4.0, <3.0.0\"],\n+    },\n+)"
            }
        ]
    },
    {
        "sha": "bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmJkZGIzYTdlNmJjYWI0YzBmMDk0OGY4MTA3NDU4ZDhjNWQzZTNmMTQ=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-18T18:51:49Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-18T18:51:49Z"
            },
            "message": "Removing .pyc files",
            "tree": {
                "sha": "641d52246feaec8cc39a0e9c75704c3e03e25f06",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/641d52246feaec8cc39a0e9c75704c3e03e25f06"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "e73075335014adc52d52d35d24b7f00da588c4dd",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/e73075335014adc52d52d35d24b7f00da588c4dd",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/e73075335014adc52d52d35d24b7f00da588c4dd"
            }
        ],
        "stats": {
            "total": 0,
            "additions": 0,
            "deletions": 0
        },
        "files": [
            {
                "sha": "dbf6b196b731844d58da03d2e3074adb6fc42c9a",
                "filename": "django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc?ref=e73075335014adc52d52d35d24b7f00da588c4dd"
            },
            {
                "sha": "b2d7d6592a36fb06c1b9651c07f1732412140302",
                "filename": "django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc?ref=e73075335014adc52d52d35d24b7f00da588c4dd"
            },
            {
                "sha": "6dff987ab1fe0796006b6985024176363b001cee",
                "filename": "django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc?ref=e73075335014adc52d52d35d24b7f00da588c4dd"
            },
            {
                "sha": "ac5e7b3240fa9a57f5725d27bb0c66c0a0414594",
                "filename": "django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc?ref=e73075335014adc52d52d35d24b7f00da588c4dd"
            },
            {
                "sha": "57f000ea0b2ec023842ae3100069b40c43f07a59",
                "filename": "django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc?ref=e73075335014adc52d52d35d24b7f00da588c4dd"
            },
            {
                "sha": "c1822c41af31cf69213a8254754370766dbea5cf",
                "filename": "django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14/django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14/django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc?ref=bddb3a7e6bcab4c0f0948f8107458d8c5d3e3f14"
            },
            {
                "sha": "ef448b20dac8ff827a0b3cb100f906e75060ff4c",
                "filename": "django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc?ref=e73075335014adc52d52d35d24b7f00da588c4dd"
            },
            {
                "sha": "1a0871e09e88291d2cce6e6a409a2decdcc3550b",
                "filename": "django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc?ref=e73075335014adc52d52d35d24b7f00da588c4dd"
            },
            {
                "sha": "abf7528accc4cd0182652f8d597491fd778aca7a",
                "filename": "django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc?ref=e73075335014adc52d52d35d24b7f00da588c4dd"
            },
            {
                "sha": "45f5ff925cdf0aa1dac25e82e4b0a797c54b4c98",
                "filename": "django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc?ref=e73075335014adc52d52d35d24b7f00da588c4dd"
            }
        ]
    },
    {
        "sha": "e73075335014adc52d52d35d24b7f00da588c4dd",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmU3MzA3NTMzNTAxNGFkYzUyZDUyZDM1ZDI0YjdmMDBkYTU4OGM0ZGQ=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-18T18:49:19Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-18T18:49:19Z"
            },
            "message": "Updating commits with addition and deletion",
            "tree": {
                "sha": "3fea5f0c1a7ebd32583f1e71b9608fbe202398b8",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/3fea5f0c1a7ebd32583f1e71b9608fbe202398b8"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/e73075335014adc52d52d35d24b7f00da588c4dd",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/e73075335014adc52d52d35d24b7f00da588c4dd",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/e73075335014adc52d52d35d24b7f00da588c4dd",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/e73075335014adc52d52d35d24b7f00da588c4dd/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "82f391cf4919c01c15e67a71bb6765c1306da31c",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/82f391cf4919c01c15e67a71bb6765c1306da31c",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/82f391cf4919c01c15e67a71bb6765c1306da31c"
            }
        ],
        "stats": {
            "total": 7966,
            "additions": 7960,
            "deletions": 6
        },
        "files": [
            {
                "sha": "cf9dfc09c6708f5109e1dc64473da13cacb06ad8",
                "filename": "github-crawler/Templates_commands/single_commit.txt",
                "status": "added",
                "additions": 404,
                "deletions": 0,
                "changes": 404,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/github-crawler/Templates_commands/single_commit.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/github-crawler/Templates_commands/single_commit.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/single_commit.txt?ref=e73075335014adc52d52d35d24b7f00da588c4dd",
                "patch": "@@ -0,0 +1,404 @@\n+PUT commit\n+{\n+\t\"mappings\": {\n+\t\t\"properties\": {\n+\t\t\t\"sha\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"node_id\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"commit\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"author\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"name\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"email\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"date\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\t\"committer\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"name\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"email\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"date\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\t\"message\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"tree\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"sha\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"comment_count\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"verification\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"verified\": {\n+\t\t\t\t\t\t\t\t\"type\": \"boolean\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"reason\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"signature\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"payload\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\t\"url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"html_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"comments_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"author\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"login\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"id\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"node_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"avatar_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gravatar_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"html_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"followers_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"following_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gists_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"starred_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"subscriptions_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"organizations_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"repos_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"received_events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"type\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"site_admin\": {\n+\t\t\t\t\t\t\"type\": \"boolean\"\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\t\"committer\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"login\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"id\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"node_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"avatar_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gravatar_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"html_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"followers_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"following_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gists_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"starred_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"subscriptions_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"organizations_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"repos_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"received_events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"type\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"site_admin\": {\n+\t\t\t\t\t\t\"type\": \"boolean\"\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\t\"parents\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"sha\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"html_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\t\t\"stat\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"total\": {\n+\t\t\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"additions\": {\n+\t\t\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"deletions\": {\n+\t\t\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\t\"files\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"sha\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+          \"filename\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+                \"status\":{\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+                \"additions\":{\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+                \"deletions\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+                \"changes\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+                \"blob_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+                \"raw_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+                \"contents_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+POST commit/_doc/1\n+{\n+        \"sha\": \"8d46f35175b0deac76cfc940d0f37c2d0266feb9\",\n+        \"node_id\": \"MDY6Q29tbWl0MjM5MzczNzkxOjhkNDZmMzUxNzViMGRlYWM3NmNmYzk0MGQwZjM3YzJkMDI2NmZlYjk=\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"vishwakulkarni\",\n+                \"email\": \"vishwa.kulkarni@gmail.com\",\n+                \"date\": \"2020-04-17T23:43:05Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"vishwakulkarni\",\n+                \"email\": \"vishwa.kulkarni@gmail.com\",\n+                \"date\": \"2020-04-17T23:43:52Z\"\n+            },\n+            \"message\": \"sending each commit to commit elastic search\",\n+            \"tree\": {\n+                \"sha\": \"d63d8d00bddbeeeb34c68395dee3e2a8b3b9fe2e\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/d63d8d00bddbeeeb34c68395dee3e2a8b3b9fe2e\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/8d46f35175b0deac76cfc940d0f37c2d0266feb9\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/8d46f35175b0deac76cfc940d0f37c2d0266feb9\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/commit/8d46f35175b0deac76cfc940d0f37c2d0266feb9\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/8d46f35175b0deac76cfc940d0f37c2d0266feb9/comments\",\n+        \"author\": {\n+            \"login\": \"vishwakulkarni\",\n+            \"id\": 5782419,\n+            \"node_id\": \"MDQ6VXNlcjU3ODI0MTk=\",\n+            \"avatar_url\": \"https://avatars2.githubusercontent.com/u/5782419?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/vishwakulkarni\",\n+            \"html_url\": \"https://github.com/vishwakulkarni\",\n+            \"followers_url\": \"https://api.github.com/users/vishwakulkarni/followers\",\n+            \"following_url\": \"https://api.github.com/users/vishwakulkarni/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/vishwakulkarni/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/vishwakulkarni/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/vishwakulkarni/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/vishwakulkarni/repos\",\n+            \"events_url\": \"https://api.github.com/users/vishwakulkarni/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/vishwakulkarni/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"vishwakulkarni\",\n+            \"id\": 5782419,\n+            \"node_id\": \"MDQ6VXNlcjU3ODI0MTk=\",\n+            \"avatar_url\": \"https://avatars2.githubusercontent.com/u/5782419?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/vishwakulkarni\",\n+            \"html_url\": \"https://github.com/vishwakulkarni\",\n+            \"followers_url\": \"https://api.github.com/users/vishwakulkarni/followers\",\n+            \"following_url\": \"https://api.github.com/users/vishwakulkarni/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/vishwakulkarni/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/vishwakulkarni/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/vishwakulkarni/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/vishwakulkarni/repos\",\n+            \"events_url\": \"https://api.github.com/users/vishwakulkarni/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/vishwakulkarni/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"7e0ce0a9e453f38906631c18bffd2259520ccc4a\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/7e0ce0a9e453f38906631c18bffd2259520ccc4a\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/commit/7e0ce0a9e453f38906631c18bffd2259520ccc4a\"\n+            }\n+        ],\n+        \"stats\": {\n+            \"total\": 37,\n+            \"additions\": 30,\n+            \"deletions\": 7\n+        },\n+        \"files\": [\n+            {\n+                \"sha\": \"ea825674a4ea96dfc360b8604cf348be17c77862\",\n+                \"filename\": \"github-crawler/lib/__pycache__/config.cpython-37.pyc\",\n+                \"status\": \"modified\",\n+                \"additions\": 0,\n+                \"deletions\": 0,\n+                \"changes\": 0,\n+                \"blob_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/blob/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/__pycache__/config.cpython-37.pyc\",\n+                \"raw_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/raw/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/__pycache__/config.cpython-37.pyc\",\n+                \"contents_url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/config.cpython-37.pyc?ref=8d46f35175b0deac76cfc940d0f37c2d0266feb9\"\n+            },\n+            {\n+                \"sha\": \"3d0afa90c8139736e73d85fdfc7a62ecae9e9bb9\",\n+                \"filename\": \"github-crawler/lib/__pycache__/helper.cpython-37.pyc\",\n+                \"status\": \"modified\",\n+                \"additions\": 0,\n+                \"deletions\": 0,\n+                \"changes\": 0,\n+                \"blob_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/blob/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/__pycache__/helper.cpython-37.pyc\",\n+                \"raw_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/raw/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/__pycache__/helper.cpython-37.pyc\",\n+                \"contents_url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/helper.cpython-37.pyc?ref=8d46f35175b0deac76cfc940d0f37c2d0266feb9\"\n+            },\n+            {\n+                \"sha\": \"c53f6f839bbf8e1821f46baef596036946a02353\",\n+                \"filename\": \"github-crawler/lib/config.py\",\n+                \"status\": \"modified\",\n+                \"additions\": 2,\n+                \"deletions\": 2,\n+                \"changes\": 4,\n+                \"blob_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/blob/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/config.py\",\n+                \"raw_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/raw/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/config.py\",\n+                \"contents_url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/config.py?ref=8d46f35175b0deac76cfc940d0f37c2d0266feb9\",\n+                \"patch\": \"@@ -1,5 +1,5 @@\\n GITEA_APP_URL = 'YOUR_GITEA_API'\\n-GITEA_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\\n+GITEA_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\\n GITHUB_USERNAME = 'vishwakulkarni'\\n-GITHUB_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\\n+GITHUB_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\\n SQL_ALCHEMY_STRING = ''\"\n+            },\n+            {\n+                \"sha\": \"0d6d1134efc800f1e44f14e710929e09e24b9514\",\n+                \"filename\": \"github-crawler/lib/helper.py\",\n+                \"status\": \"modified\",\n+                \"additions\": 28,\n+                \"deletions\": 5,\n+                \"changes\": 33,\n+                \"blob_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/blob/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/helper.py\",\n+                \"raw_url\": \"https://github.com/CUBigDataClass/Kode-Kallas/raw/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/helper.py\",\n+                \"contents_url\": \"https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/helper.py?ref=8d46f35175b0deac76cfc940d0f37c2d0266feb9\",\n+                \"patch\": \"@@ -12,9 +12,9 @@\\n from elasticsearch import Elasticsearch\\n \\n GITEA_APP_URL = 'YOUR_GITEA_API'\\n-GITEA_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\\n+GITEA_TOKEN = 'aff8945f859b696bf05932045f0be7e1b8379ddb'\\n GITHUB_USERNAME = 'vishwakulkarni'\\n-GITHUB_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\\n+GITHUB_TOKEN = 'aff8945f859b696bf05932045f0be7e1b8379ddb'\\n SQL_ALCHEMY_STRING = ''\\n \\n \\n@@ -78,14 +78,37 @@ def get_org_users(self,owner,api):\\n         \\n         return members_list\\n \\n+    #save commits of repos as json\\n+    def commits_of_repo_github(self,repo, owner, api):\\n+        commits = []\\n+        next = True\\n+        i = 1\\n+        while next == True:\\n+            url = api + '/repos/{}/{}/commits?page={}&per_page=100'.format(owner, repo, i)\\n+            commit_pg = self.gh_session.get(url = url)\\n+            commit_tp = json.loads(commit_pg.content)\\n+            for commit in commit_tp:\\n+                commits.append(commit) \\n+                print(commit)\\n+            if 'Link' in commit_pg.headers:\\n+                if 'rel=\\\"next\\\"' not in commit_pg.headers['Link']:\\n+                    next = False\\n+            i = i + 1\\n+        return commits\\n+\\n+\\n \\n \\n #testing comment after use\\n h = Helper()\\n github_api = \\\"https://api.github.com\\\"\\n h.set_org_name(\\\"CUBigDataClass\\\")\\n #print(h.get_org_information(\\\"vishwakulkarni\\\",github_api))\\n-#k=h.get_repositories('vishwakulkarni',github_api)\\n+k=h.get_repositories('vishwakulkarni',github_api)\\n+commits = h.commits_of_repo_github('kode-kallas','cubigdataclass',github_api)\\n+for commit in commits:\\n+    h.send_to_elasticInstance(commit,'commit',commit['sha'])\\n #print(len(k))\\n-#for mem in k:\\n-#    print(mem['name'])\\n+for mem in k:\\n+    print(mem['name'])\\n+    #commits = h.commits_of_repo_github(mem['name'],'vishwakulkarni',github_api)\\n\\\\ No newline at end of file\"\n+            }\n+        ]\n+    }"
            },
            {
                "sha": "ee3f72b649851d53e136e9b44dbe6e9d42127767",
                "filename": "github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/config.cpython-37.pyc?ref=e73075335014adc52d52d35d24b7f00da588c4dd"
            },
            {
                "sha": "284d76f5429a504d3eeb16e9fd2316969817f711",
                "filename": "github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/helper.cpython-37.pyc?ref=e73075335014adc52d52d35d24b7f00da588c4dd"
            },
            {
                "sha": "c1bdbf4fc514200e2add8c9fa94e061374bc329f",
                "filename": "github-crawler/lib/commits.json",
                "status": "added",
                "additions": 7544,
                "deletions": 0,
                "changes": 7544,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/github-crawler/lib/commits.json",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/github-crawler/lib/commits.json",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/commits.json?ref=e73075335014adc52d52d35d24b7f00da588c4dd"
            },
            {
                "sha": "46de99cb50435a0169426ee53f861f57a4db2b50",
                "filename": "github-crawler/lib/helper.py",
                "status": "modified",
                "additions": 12,
                "deletions": 6,
                "changes": 18,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e73075335014adc52d52d35d24b7f00da588c4dd/github-crawler/lib/helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e73075335014adc52d52d35d24b7f00da588c4dd/github-crawler/lib/helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/helper.py?ref=e73075335014adc52d52d35d24b7f00da588c4dd",
                "patch": "@@ -12,9 +12,9 @@\n from elasticsearch import Elasticsearch\n \n GITEA_APP_URL = 'YOUR_GITEA_API'\n-GITEA_TOKEN = 'aff8945f859b696bf05932045f0be7e1b8379ddb'\n+GITEA_TOKEN = '48fb007c866e8fb377a4d61a60af3dc881707df4'\n GITHUB_USERNAME = 'vishwakulkarni'\n-GITHUB_TOKEN = 'aff8945f859b696bf05932045f0be7e1b8379ddb'\n+GITHUB_TOKEN = '48fb007c866e8fb377a4d61a60af3dc881707df4'\n SQL_ALCHEMY_STRING = ''\n \n \n@@ -88,8 +88,11 @@ def commits_of_repo_github(self,repo, owner, api):\n             commit_pg = self.gh_session.get(url = url)\n             commit_tp = json.loads(commit_pg.content)\n             for commit in commit_tp:\n-                commits.append(commit) \n-                print(commit)\n+                url = commit['url']\n+                single_commit = self.gh_session.get(url = url)\n+                single_commit = json.loads(single_commit.content)\n+                commits.append(single_commit)\n+                #print(commit)\n             if 'Link' in commit_pg.headers:\n                 if 'rel=\"next\"' not in commit_pg.headers['Link']:\n                     next = False\n@@ -100,15 +103,18 @@ def commits_of_repo_github(self,repo, owner, api):\n \n \n #testing comment after use\n-h = Helper()\n+'''h = Helper()\n github_api = \"https://api.github.com\"\n h.set_org_name(\"CUBigDataClass\")\n #print(h.get_org_information(\"vishwakulkarni\",github_api))\n k=h.get_repositories('vishwakulkarni',github_api)\n commits = h.commits_of_repo_github('kode-kallas','cubigdataclass',github_api)\n+with open(\"commits.json\", \"w\") as outfile: \n+    outfile.write(json.dumps(commits,indent=4)) \n+#print(commits[0]['stat'])\n for commit in commits:\n     h.send_to_elasticInstance(commit,'commit',commit['sha'])\n #print(len(k))\n for mem in k:\n     print(mem['name'])\n-    #commits = h.commits_of_repo_github(mem['name'],'vishwakulkarni',github_api)\n\\ No newline at end of file\n+    #commits = h.commits_of_repo_github(mem['name'],'vishwakulkarni',github_api)'''\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "34510f3de8d1271fa3c008cbd4f2680083393c3c",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjM0NTEwZjNkZThkMTI3MWZhM2MwMDhjYmQ0ZjI2ODAwODMzOTNjM2M=",
        "commit": {
            "author": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-18T09:41:22Z"
            },
            "committer": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-18T09:41:22Z"
            },
            "message": "Merge remote-tracking branch 'origin/master' into shreyas",
            "tree": {
                "sha": "0e14be7095baaad71c6ccfd59217f74ae85099fe",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/0e14be7095baaad71c6ccfd59217f74ae85099fe"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/34510f3de8d1271fa3c008cbd4f2680083393c3c",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/34510f3de8d1271fa3c008cbd4f2680083393c3c",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/34510f3de8d1271fa3c008cbd4f2680083393c3c",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/34510f3de8d1271fa3c008cbd4f2680083393c3c/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "41ec137138e1925df37f312b039ab9bf88e09a3f",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/41ec137138e1925df37f312b039ab9bf88e09a3f",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/41ec137138e1925df37f312b039ab9bf88e09a3f"
            },
            {
                "sha": "82f391cf4919c01c15e67a71bb6765c1306da31c",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/82f391cf4919c01c15e67a71bb6765c1306da31c",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/82f391cf4919c01c15e67a71bb6765c1306da31c"
            }
        ],
        "stats": {
            "total": 43,
            "additions": 36,
            "deletions": 7
        },
        "files": [
            {
                "sha": "2347feab2e7185c530287a0a29d8cbfd130ae725",
                "filename": "github-crawler/app.py",
                "status": "modified",
                "additions": 6,
                "deletions": 0,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/34510f3de8d1271fa3c008cbd4f2680083393c3c/github-crawler/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/34510f3de8d1271fa3c008cbd4f2680083393c3c/github-crawler/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/app.py?ref=34510f3de8d1271fa3c008cbd4f2680083393c3c",
                "patch": "@@ -34,6 +34,12 @@ def org_parser(orgname):\n     print(\"sending user info to elasticsearch\")\n     for member in member_list:\n         helper.send_to_elasticInstance(member,'users',member['id'])\n+    print(\"Getting Commits from each repository\")\n+    for repo in repo_list:\n+        commits = helper.commits_of_repo_github(repo['name'],orgname,github_api)\n+        print(\"sending to ES commits of\",repo['name'])\n+        for commit in commits:\n+            helper.send_to_elasticInstance(commit,'commit',commit['sha'])\n     print(\"Done!!!!!!!!!\")\n     return 'We got your org name ' + orgname + ' give us some time to process your request, please check server output for progress'\n "
            },
            {
                "sha": "ea825674a4ea96dfc360b8604cf348be17c77862",
                "filename": "github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/34510f3de8d1271fa3c008cbd4f2680083393c3c/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/34510f3de8d1271fa3c008cbd4f2680083393c3c/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/config.cpython-37.pyc?ref=34510f3de8d1271fa3c008cbd4f2680083393c3c"
            },
            {
                "sha": "3d0afa90c8139736e73d85fdfc7a62ecae9e9bb9",
                "filename": "github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/34510f3de8d1271fa3c008cbd4f2680083393c3c/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/34510f3de8d1271fa3c008cbd4f2680083393c3c/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/helper.cpython-37.pyc?ref=34510f3de8d1271fa3c008cbd4f2680083393c3c"
            },
            {
                "sha": "c53f6f839bbf8e1821f46baef596036946a02353",
                "filename": "github-crawler/lib/config.py",
                "status": "modified",
                "additions": 2,
                "deletions": 2,
                "changes": 4,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/34510f3de8d1271fa3c008cbd4f2680083393c3c/github-crawler/lib/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/34510f3de8d1271fa3c008cbd4f2680083393c3c/github-crawler/lib/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/config.py?ref=34510f3de8d1271fa3c008cbd4f2680083393c3c",
                "patch": "@@ -1,5 +1,5 @@\n GITEA_APP_URL = 'YOUR_GITEA_API'\n-GITEA_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+GITEA_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n GITHUB_USERNAME = 'vishwakulkarni'\n-GITHUB_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+GITHUB_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n SQL_ALCHEMY_STRING = ''"
            },
            {
                "sha": "0d6d1134efc800f1e44f14e710929e09e24b9514",
                "filename": "github-crawler/lib/helper.py",
                "status": "modified",
                "additions": 28,
                "deletions": 5,
                "changes": 33,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/34510f3de8d1271fa3c008cbd4f2680083393c3c/github-crawler/lib/helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/34510f3de8d1271fa3c008cbd4f2680083393c3c/github-crawler/lib/helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/helper.py?ref=34510f3de8d1271fa3c008cbd4f2680083393c3c",
                "patch": "@@ -12,9 +12,9 @@\n from elasticsearch import Elasticsearch\n \n GITEA_APP_URL = 'YOUR_GITEA_API'\n-GITEA_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n+GITEA_TOKEN = 'aff8945f859b696bf05932045f0be7e1b8379ddb'\n GITHUB_USERNAME = 'vishwakulkarni'\n-GITHUB_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n+GITHUB_TOKEN = 'aff8945f859b696bf05932045f0be7e1b8379ddb'\n SQL_ALCHEMY_STRING = ''\n \n \n@@ -78,14 +78,37 @@ def get_org_users(self,owner,api):\n         \n         return members_list\n \n+    #save commits of repos as json\n+    def commits_of_repo_github(self,repo, owner, api):\n+        commits = []\n+        next = True\n+        i = 1\n+        while next == True:\n+            url = api + '/repos/{}/{}/commits?page={}&per_page=100'.format(owner, repo, i)\n+            commit_pg = self.gh_session.get(url = url)\n+            commit_tp = json.loads(commit_pg.content)\n+            for commit in commit_tp:\n+                commits.append(commit) \n+                print(commit)\n+            if 'Link' in commit_pg.headers:\n+                if 'rel=\"next\"' not in commit_pg.headers['Link']:\n+                    next = False\n+            i = i + 1\n+        return commits\n+\n+\n \n \n #testing comment after use\n h = Helper()\n github_api = \"https://api.github.com\"\n h.set_org_name(\"CUBigDataClass\")\n #print(h.get_org_information(\"vishwakulkarni\",github_api))\n-#k=h.get_repositories('vishwakulkarni',github_api)\n+k=h.get_repositories('vishwakulkarni',github_api)\n+commits = h.commits_of_repo_github('kode-kallas','cubigdataclass',github_api)\n+for commit in commits:\n+    h.send_to_elasticInstance(commit,'commit',commit['sha'])\n #print(len(k))\n-#for mem in k:\n-#    print(mem['name'])\n+for mem in k:\n+    print(mem['name'])\n+    #commits = h.commits_of_repo_github(mem['name'],'vishwakulkarni',github_api)\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "41ec137138e1925df37f312b039ab9bf88e09a3f",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjQxZWMxMzcxMzhlMTkyNWRmMzdmMzEyYjAzOWFiOWJmODhlMDlhM2Y=",
        "commit": {
            "author": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-18T09:37:58Z"
            },
            "committer": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-18T09:37:58Z"
            },
            "message": "Creating Repo data with contributors list",
            "tree": {
                "sha": "1e0ef274b5776d43d6d77ec30534da75589bb96a",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/1e0ef274b5776d43d6d77ec30534da75589bb96a"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/41ec137138e1925df37f312b039ab9bf88e09a3f",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/41ec137138e1925df37f312b039ab9bf88e09a3f",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/41ec137138e1925df37f312b039ab9bf88e09a3f",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/41ec137138e1925df37f312b039ab9bf88e09a3f/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "7e0ce0a9e453f38906631c18bffd2259520ccc4a",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/7e0ce0a9e453f38906631c18bffd2259520ccc4a",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/7e0ce0a9e453f38906631c18bffd2259520ccc4a"
            }
        ],
        "stats": {
            "total": 142,
            "additions": 124,
            "deletions": 18
        },
        "files": [
            {
                "sha": "4dcd3773728d9efe07d1417a28bae11c076a319e",
                "filename": "github-analytics/CassandraHelper/CassandraOrgData.py",
                "status": "added",
                "additions": 24,
                "deletions": 0,
                "changes": 24,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/CassandraHelper/CassandraOrgData.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/CassandraHelper/CassandraOrgData.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/CassandraOrgData.py?ref=41ec137138e1925df37f312b039ab9bf88e09a3f",
                "patch": "@@ -0,0 +1,24 @@\n+class CassandraOrgData():\n+    def __init__(self, elasticOrgData):\n+        data = elasticOrgData['hits']['hits'][0]['_source']\n+        self.data = {\n+            \"avatar_url\": data['avatar_url'],\n+            \"description\": data['description'],\n+            \"email\": data['email'],\n+            \"followers\": data['followers'],\n+            \"following\": data['following'],\n+            \"html_url\": data['html_url'],\n+            \"id\": data['id'],\n+            \"issues_url\": data['issues_url'],\n+            \"location\": data['location'],\n+            \"login\": data['login'],\n+            \"members_url\": data['members_url'],\n+            \"name\": data['name'],\n+            \"node_id\": data['node_id'],\n+            \"public_members_url\": data['public_members_url'],\n+            \"public_repos\": data['public_repos'],\n+            \"repos_url\": data['repos_url'],\n+            \"type\": data['type'],\n+            \"updated_at\": data['updated_at'],\n+            \"url\": data['url']\n+        }\n\\ No newline at end of file"
            },
            {
                "sha": "453c10084f8f49a3c680f88972e916ee7c0c9acc",
                "filename": "github-analytics/CassandraHelper/CassandraRepoData.py",
                "status": "added",
                "additions": 34,
                "deletions": 0,
                "changes": 34,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/CassandraHelper/CassandraRepoData.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/CassandraHelper/CassandraRepoData.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/CassandraRepoData.py?ref=41ec137138e1925df37f312b039ab9bf88e09a3f",
                "patch": "@@ -0,0 +1,34 @@\n+from CassandraHelper import Utils as utils\n+\n+class CassandraRepoData():\n+    def __init__(self, elasticOrgData):\n+        dataList = elasticOrgData['hits']['hits']\n+        self.data = list()\n+        for i in range(len(dataList)):\n+            repoDataItem = dataList[i]['_source']\n+            repoData = {\n+                \"contributors\": utils.get(utils.getContributorsList, repoDataItem['contributors_url']),\n+                \"name\": repoDataItem['name'],\n+                \"commits_url\": repoDataItem['commits_url'],\n+                \"created_at\": repoDataItem['created_at'],\n+                \"issues_url\": repoDataItem['issues_url'],\n+                \"id\": repoDataItem['id'],\n+                \"watchers_count\": repoDataItem['watchers_count'],\n+                \"description\": repoDataItem['description'],\n+                \"forks_count\": repoDataItem['forks_count'],\n+                \"forks_url\": repoDataItem['forks_url'],\n+                \"full_name\": repoDataItem['full_name'],\n+                \"html_url\": repoDataItem['html_url'],\n+                \"languages_url\": repoDataItem['languages_url'],\n+                \"owner\":{\n+                    \"login\": repoDataItem['owner']['login'],\n+                    \"avatar_url\": repoDataItem['owner']['avatar_url'],\n+                    \"html_url\": repoDataItem['owner']['html_url'],\n+                    \"id\": repoDataItem['owner']['id'],\n+                    \"organizations_url\": repoDataItem['owner']['organizations_url']\n+                },\n+                \"open_issues_count\": repoDataItem['open_issues_count'],\n+                \"tags_url\": repoDataItem['tags_url'],\n+                \"updated_at\": repoDataItem['updated_at']\n+            }\n+            self.data.append(repoData)\n\\ No newline at end of file"
            },
            {
                "sha": "174db917aa1ad238aba46e5817897bfe29c4fffc",
                "filename": "github-analytics/CassandraHelper/Utils.py",
                "status": "added",
                "additions": 25,
                "deletions": 0,
                "changes": 25,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/CassandraHelper/Utils.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/CassandraHelper/Utils.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/Utils.py?ref=41ec137138e1925df37f312b039ab9bf88e09a3f",
                "patch": "@@ -0,0 +1,25 @@\n+import requests\n+from requests.auth import HTTPBasicAuth\n+from CassandraHelper import config\n+import json\n+\n+# Http get request with callback\n+def get(function, url):\n+    try:\n+        r = requests.get(url, auth=HTTPBasicAuth(config.GITHUB_USER, config.GITHUB_TOKEN))\n+        return function(r.json())\n+    except:\n+        return []\n+\n+def getContributorsList(data):\n+    contributorsList = list()\n+    for i in range(len(data)):\n+        # print(\"99999999999999999\", data, \"\\n\\n\\n\")\n+        contributor = dict()\n+        contributor['login'] = data[i]['login']\n+        contributor['id'] = data[i]['id']\n+        contributor['avatar_url'] = data[i]['avatar_url']\n+        contributor['html_url'] = data[i]['html_url']\n+        contributor['contributions'] = data[i]['contributions']\n+        contributorsList.append(contributor)\n+    return contributorsList\n\\ No newline at end of file"
            },
            {
                "sha": "8f55617b58c4563a6a5da005d1229eaffbdaee4d",
                "filename": "github-analytics/CassandraHelper/config.py",
                "status": "modified",
                "additions": 6,
                "deletions": 1,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/CassandraHelper/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/CassandraHelper/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/config.py?ref=41ec137138e1925df37f312b039ab9bf88e09a3f",
                "patch": "@@ -1,6 +1,11 @@\n+# Cassandra Config\n CASSANDRA_HOST = 'localhost'\n CASSANDRA_PORT = 5000\n \n CASSANDRA_URL = \"http://\" + CASSANDRA_HOST + \":\" + str(CASSANDRA_PORT) + \"/\"\n \n-CASSANDRA_URL_INSERT = CASSANDRA_URL + \"insert/\"\n\\ No newline at end of file\n+CASSANDRA_URL_INSERT = CASSANDRA_URL + \"insert/\"\n+\n+# Github Config\n+GITHUB_USER = \"shreyas-gopalakrishna\"\n+GITHUB_TOKEN = \"f02831928492a6bfe9f1bca49693f69d1d0e44b3\"\n\\ No newline at end of file"
            },
            {
                "sha": "0eda6e2c71d08288d68fa6edec8496bba73e6100",
                "filename": "github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "status": "modified",
                "additions": 7,
                "deletions": 9,
                "changes": 16,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py?ref=41ec137138e1925df37f312b039ab9bf88e09a3f",
                "patch": "@@ -9,17 +9,15 @@ class ElasticSearchHelper():\n     def __init__(self):\n         pass\n \n-    def getOrgData(self,orgname):\n-        res = es.search(index=orgname, body={\"query\": {\"match_all\": {}}})\n-        print(res)\n+    def getOrgData(self, orgname):\n+        res = es.search(index='org1', body={\"query\": {\"match\": {\"login\":orgname}}, \"_source\": config.ELASTIC_ORG_DATA_FIELDS, 'size': 1000})\n         return res\n \n-    def getUserData(self,username):\n+    def getUserData(self, username):\n         res = es.search(index=username, body={\"query\": {\"match_all\": {}}})\n-        print(res)\n         return res\n \n-    def getRepoData(self,reponame):\n-        res = es.search(index=reponame, body={\"query\": {\"match_all\": {}}})\n-        print(res)\n-        return res\n\\ No newline at end of file\n+    def getRepoData(self, orgname):\n+        res = es.search(index='repos', body={\"query\": {\"match\": {\"owner.login\": orgname}}, \"_source\": config.ELASTIC_REPO_DATA_FIELDS, 'size': 1000})\n+        return res\n+"
            },
            {
                "sha": "caa2c54fb74dfc8565f153fae7cf1c4360113ae0",
                "filename": "github-analytics/ElasticSearchHelper/config.py",
                "status": "modified",
                "additions": 5,
                "deletions": 1,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/ElasticSearchHelper/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/ElasticSearchHelper/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/config.py?ref=41ec137138e1925df37f312b039ab9bf88e09a3f",
                "patch": "@@ -1,2 +1,6 @@\n ELASTIC_HOST = 'localhost'\n-ELASTIC_PORT = 9200\n\\ No newline at end of file\n+ELASTIC_PORT = 9200\n+\n+ELASTIC_ORG_DATA_FIELDS = [\"_id\",\"avatar_url\",\"description\",\"email\",\"followers\",\"following\",\"html_url\",\"id\",\"issues_url\",\"location\",\"members_url\",\"name\",\"node_id\",\"login\",\"url\",\"updated_at\",\"repos_url\",\"public_repos\",\"public_members_url\",\"type\"]\n+\n+ELASTIC_REPO_DATA_FIELDS = [\"contributors_url\",\"name\",\"owner.login\",\"owner.organizations_url\",\"commits_url\",\"created_at\",\"issues_url\",\"id\",\"watchers_count\",\"description\",\"forks_count\",\"forks_url\",\"full_name\",\"html_url\",\"languages_url\",\"open_issues_count\",\"owner.avatar_url\",\"owner.html_url\",\"owner.id\",\"tags_url\",\"updated_at\"]"
            },
            {
                "sha": "2482d9d60342674ac10f1e906bef0ea6edc22798",
                "filename": "github-analytics/app.py",
                "status": "modified",
                "additions": 23,
                "deletions": 7,
                "changes": 30,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/41ec137138e1925df37f312b039ab9bf88e09a3f/github-analytics/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/app.py?ref=41ec137138e1925df37f312b039ab9bf88e09a3f",
                "patch": "@@ -1,28 +1,44 @@\n from flask import Flask\n+from flask import jsonify\n \n from config import config\n from ElasticSearchHelper import ElasticSearchHelper as esh\n from CassandraHelper import CassandraHelper as ch\n+from CassandraHelper import CassandraOrgData as cod\n+from CassandraHelper import CassandraRepoData as crd\n+import json\n \n app = Flask(__name__)\n \n elasticSearchHelper = esh.ElasticSearchHelper()\n cassandraHelper = ch.CassandraHelper()\n \n+\n @app.route('/')\n def home():\n-    return 'Github Analytics - Use APIs to process data'\n+    return 'Github Analytics - Use APIs to process data. /org/{organization_name}'\n+\n \n @app.route('/org/<orgname>')\n-def org_retrive(orgname):\n-    return elasticSearchHelper.getOrgData(orgname)\n+def orgRetrieve(orgname):\n+    elasticOrgData = elasticSearchHelper.getOrgData(orgname)\n+    # return elasticOrgData\n+    cassandraOrgData = cod.CassandraOrgData(elasticOrgData)\n+    return cassandraOrgData.data\n+    # return cassandraHelper.insertOrgData(cassandraOrgData.data)\n+\n+\n+@app.route('/repo/<orgname>')\n+def repoRetrieve(orgname):\n+    elasticRepoData = elasticSearchHelper.getRepoData(orgname)\n+    # return elasticRepoData\n+    cassandraRepoData = crd.CassandraRepoData(elasticRepoData)\n+    print(\"Done Done!\")\n+    return jsonify(cassandraRepoData.data)\n \n-@app.route('/repo/<reponame>')\n-def repo_retrive(reponame):\n-    return elasticSearchHelper.getRepoData(reponame)\n \n @app.route('/user/<username>')\n-def user_retrive(username):\n+def userRetrieve(username):\n     return elasticSearchHelper.getUserData(username)\n \n "
            }
        ]
    },
    {
        "sha": "82f391cf4919c01c15e67a71bb6765c1306da31c",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjgyZjM5MWNmNDkxOWMwMWMxNWU2N2E3MWJiNjc2NWMxMzA2ZGEzMWM=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-17T23:43:46Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-17T23:43:52Z"
            },
            "message": "sending each commit to commit elastic search with server",
            "tree": {
                "sha": "e21651d546ce67f836b3b9391e97f6dbf6699e8d",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/e21651d546ce67f836b3b9391e97f6dbf6699e8d"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/82f391cf4919c01c15e67a71bb6765c1306da31c",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/82f391cf4919c01c15e67a71bb6765c1306da31c",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/82f391cf4919c01c15e67a71bb6765c1306da31c",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/82f391cf4919c01c15e67a71bb6765c1306da31c/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "8d46f35175b0deac76cfc940d0f37c2d0266feb9",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/8d46f35175b0deac76cfc940d0f37c2d0266feb9",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/8d46f35175b0deac76cfc940d0f37c2d0266feb9"
            }
        ],
        "stats": {
            "total": 6,
            "additions": 6,
            "deletions": 0
        },
        "files": [
            {
                "sha": "2347feab2e7185c530287a0a29d8cbfd130ae725",
                "filename": "github-crawler/app.py",
                "status": "modified",
                "additions": 6,
                "deletions": 0,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/82f391cf4919c01c15e67a71bb6765c1306da31c/github-crawler/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/82f391cf4919c01c15e67a71bb6765c1306da31c/github-crawler/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/app.py?ref=82f391cf4919c01c15e67a71bb6765c1306da31c",
                "patch": "@@ -34,6 +34,12 @@ def org_parser(orgname):\n     print(\"sending user info to elasticsearch\")\n     for member in member_list:\n         helper.send_to_elasticInstance(member,'users',member['id'])\n+    print(\"Getting Commits from each repository\")\n+    for repo in repo_list:\n+        commits = helper.commits_of_repo_github(repo['name'],orgname,github_api)\n+        print(\"sending to ES commits of\",repo['name'])\n+        for commit in commits:\n+            helper.send_to_elasticInstance(commit,'commit',commit['sha'])\n     print(\"Done!!!!!!!!!\")\n     return 'We got your org name ' + orgname + ' give us some time to process your request, please check server output for progress'\n "
            }
        ]
    },
    {
        "sha": "8d46f35175b0deac76cfc940d0f37c2d0266feb9",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjhkNDZmMzUxNzViMGRlYWM3NmNmYzk0MGQwZjM3YzJkMDI2NmZlYjk=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-17T23:43:05Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-17T23:43:52Z"
            },
            "message": "sending each commit to commit elastic search",
            "tree": {
                "sha": "d63d8d00bddbeeeb34c68395dee3e2a8b3b9fe2e",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/d63d8d00bddbeeeb34c68395dee3e2a8b3b9fe2e"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/8d46f35175b0deac76cfc940d0f37c2d0266feb9",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/8d46f35175b0deac76cfc940d0f37c2d0266feb9",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/8d46f35175b0deac76cfc940d0f37c2d0266feb9",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/8d46f35175b0deac76cfc940d0f37c2d0266feb9/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "7e0ce0a9e453f38906631c18bffd2259520ccc4a",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/7e0ce0a9e453f38906631c18bffd2259520ccc4a",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/7e0ce0a9e453f38906631c18bffd2259520ccc4a"
            }
        ],
        "stats": {
            "total": 37,
            "additions": 30,
            "deletions": 7
        },
        "files": [
            {
                "sha": "ea825674a4ea96dfc360b8604cf348be17c77862",
                "filename": "github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/config.cpython-37.pyc?ref=8d46f35175b0deac76cfc940d0f37c2d0266feb9"
            },
            {
                "sha": "3d0afa90c8139736e73d85fdfc7a62ecae9e9bb9",
                "filename": "github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/helper.cpython-37.pyc?ref=8d46f35175b0deac76cfc940d0f37c2d0266feb9"
            },
            {
                "sha": "c53f6f839bbf8e1821f46baef596036946a02353",
                "filename": "github-crawler/lib/config.py",
                "status": "modified",
                "additions": 2,
                "deletions": 2,
                "changes": 4,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/config.py?ref=8d46f35175b0deac76cfc940d0f37c2d0266feb9",
                "patch": "@@ -1,5 +1,5 @@\n GITEA_APP_URL = 'YOUR_GITEA_API'\n-GITEA_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+GITEA_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n GITHUB_USERNAME = 'vishwakulkarni'\n-GITHUB_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+GITHUB_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n SQL_ALCHEMY_STRING = ''"
            },
            {
                "sha": "0d6d1134efc800f1e44f14e710929e09e24b9514",
                "filename": "github-crawler/lib/helper.py",
                "status": "modified",
                "additions": 28,
                "deletions": 5,
                "changes": 33,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/8d46f35175b0deac76cfc940d0f37c2d0266feb9/github-crawler/lib/helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/helper.py?ref=8d46f35175b0deac76cfc940d0f37c2d0266feb9",
                "patch": "@@ -12,9 +12,9 @@\n from elasticsearch import Elasticsearch\n \n GITEA_APP_URL = 'YOUR_GITEA_API'\n-GITEA_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n+GITEA_TOKEN = 'aff8945f859b696bf05932045f0be7e1b8379ddb'\n GITHUB_USERNAME = 'vishwakulkarni'\n-GITHUB_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n+GITHUB_TOKEN = 'aff8945f859b696bf05932045f0be7e1b8379ddb'\n SQL_ALCHEMY_STRING = ''\n \n \n@@ -78,14 +78,37 @@ def get_org_users(self,owner,api):\n         \n         return members_list\n \n+    #save commits of repos as json\n+    def commits_of_repo_github(self,repo, owner, api):\n+        commits = []\n+        next = True\n+        i = 1\n+        while next == True:\n+            url = api + '/repos/{}/{}/commits?page={}&per_page=100'.format(owner, repo, i)\n+            commit_pg = self.gh_session.get(url = url)\n+            commit_tp = json.loads(commit_pg.content)\n+            for commit in commit_tp:\n+                commits.append(commit) \n+                print(commit)\n+            if 'Link' in commit_pg.headers:\n+                if 'rel=\"next\"' not in commit_pg.headers['Link']:\n+                    next = False\n+            i = i + 1\n+        return commits\n+\n+\n \n \n #testing comment after use\n h = Helper()\n github_api = \"https://api.github.com\"\n h.set_org_name(\"CUBigDataClass\")\n #print(h.get_org_information(\"vishwakulkarni\",github_api))\n-#k=h.get_repositories('vishwakulkarni',github_api)\n+k=h.get_repositories('vishwakulkarni',github_api)\n+commits = h.commits_of_repo_github('kode-kallas','cubigdataclass',github_api)\n+for commit in commits:\n+    h.send_to_elasticInstance(commit,'commit',commit['sha'])\n #print(len(k))\n-#for mem in k:\n-#    print(mem['name'])\n+for mem in k:\n+    print(mem['name'])\n+    #commits = h.commits_of_repo_github(mem['name'],'vishwakulkarni',github_api)\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "7e0ce0a9e453f38906631c18bffd2259520ccc4a",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjdlMGNlMGE5ZTQ1M2YzODkwNjYzMWMxOGJmZmQyMjU5NTIwY2NjNGE=",
        "commit": {
            "author": {
                "name": "Shreyas Gopalakrishna",
                "email": "11889130+shreyas-gopalakrishna@users.noreply.github.com",
                "date": "2020-04-17T06:56:43Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-17T06:56:43Z"
            },
            "message": "Delete .DS_Store",
            "tree": {
                "sha": "f7e0d59fbca9cfc15f1e37a50bc2a12dbbd05d24",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/f7e0d59fbca9cfc15f1e37a50bc2a12dbbd05d24"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/7e0ce0a9e453f38906631c18bffd2259520ccc4a",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJemVMrCRBK7hj4Ov3rIwAAdHIIAEvOY1eRUQHRBsh+Nj1MBV6b\nQfPdoPwjfzxfDzD4ZDZ5JRzWIfOh8LUfM8kUsjkYCy0C4wk4gm+dM9CJfNPQ4w8E\nuWiVlW3BaAAiIF9XGimHIHdeTdafnoJtv0Xl+0dBGdbsasm+U6zUhR0hJih/QXVp\nxHt0NJdJmRwYlNZbLWrnuXIisydhHqBg/MdlbY7QisbOyu0Mp2xwb7Bv9gQA755D\nzZU6VyTc7yBjzsQgbdkgZ8GB+6lUmS6AkqpRxZjYtIt7uV3dz3fgs/GTVn/hfR8J\n1T2EC1PZoukT1Wqv1U26p4f/iK42X5Sw+oKf8yIg3o7SmI37woaxhiJTD29/4vg=\n=4p6n\n-----END PGP SIGNATURE-----\n",
                "payload": "tree f7e0d59fbca9cfc15f1e37a50bc2a12dbbd05d24\nparent 30850ae4ad0775bd9539242457173e7d8f32d4f2\nauthor Shreyas Gopalakrishna <11889130+shreyas-gopalakrishna@users.noreply.github.com> 1587106603 -0600\ncommitter GitHub <noreply@github.com> 1587106603 -0600\n\nDelete .DS_Store"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/7e0ce0a9e453f38906631c18bffd2259520ccc4a",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/7e0ce0a9e453f38906631c18bffd2259520ccc4a",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/7e0ce0a9e453f38906631c18bffd2259520ccc4a/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "30850ae4ad0775bd9539242457173e7d8f32d4f2",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/30850ae4ad0775bd9539242457173e7d8f32d4f2",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/30850ae4ad0775bd9539242457173e7d8f32d4f2"
            }
        ],
        "stats": {
            "total": 0,
            "additions": 0,
            "deletions": 0
        },
        "files": [
            {
                "sha": "d7c3f3ec3dfbf653c865061115a91d9fcb6fc47a",
                "filename": "github-analytics/ElasticSearchHelper/.DS_Store",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/30850ae4ad0775bd9539242457173e7d8f32d4f2/github-analytics/ElasticSearchHelper/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/30850ae4ad0775bd9539242457173e7d8f32d4f2/github-analytics/ElasticSearchHelper/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/.DS_Store?ref=30850ae4ad0775bd9539242457173e7d8f32d4f2"
            }
        ]
    },
    {
        "sha": "30850ae4ad0775bd9539242457173e7d8f32d4f2",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjMwODUwYWU0YWQwNzc1YmQ5NTM5MjQyNDU3MTczZTdkOGYzMmQ0ZjI=",
        "commit": {
            "author": {
                "name": "Shreyas Gopalakrishna",
                "email": "11889130+shreyas-gopalakrishna@users.noreply.github.com",
                "date": "2020-04-17T06:56:27Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-17T06:56:27Z"
            },
            "message": "Delete .DS_Store",
            "tree": {
                "sha": "22d40e6a84ffb1a0826f09ae607995bcc6bc21ee",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/22d40e6a84ffb1a0826f09ae607995bcc6bc21ee"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/30850ae4ad0775bd9539242457173e7d8f32d4f2",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJemVMbCRBK7hj4Ov3rIwAAdHIIAEU6w+EX4mSw0K9GzYw+gABX\nrS5kXVFRaJS0I1wi7wnm86FAM7MwjPNY/OusereM4Ig9oMkRdyA0jGqC9EYbfltE\nGcPx7ZzqUfPBLJ/9hBIEUDjX2G5vRyDJUjcjkWbwKnV+tCXwkpWHefubd7QH+5J3\n081HTFdfkD3kNZGDYPQrjjZJQkVPky0OFXBMk70GJuJOGNrsvt+2jexyejFJ51Q0\nxM2ZQqgFuAODyrvkpdqwItJX77JG/1U274S1DNfrHL3BaSqfDpSLkD4Zh5o2X+2I\nZe328IUEmTWva9KSEu7xvy19q4D87yaxHdBNvE0qa8uR+cSb69ThEmxjBflsp7A=\n=qbQT\n-----END PGP SIGNATURE-----\n",
                "payload": "tree 22d40e6a84ffb1a0826f09ae607995bcc6bc21ee\nparent eeadb6f5ae4e83f010ce9dba92fb125ffb51c606\nauthor Shreyas Gopalakrishna <11889130+shreyas-gopalakrishna@users.noreply.github.com> 1587106587 -0600\ncommitter GitHub <noreply@github.com> 1587106587 -0600\n\nDelete .DS_Store"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/30850ae4ad0775bd9539242457173e7d8f32d4f2",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/30850ae4ad0775bd9539242457173e7d8f32d4f2",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/30850ae4ad0775bd9539242457173e7d8f32d4f2/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606"
            }
        ],
        "stats": {
            "total": 0,
            "additions": 0,
            "deletions": 0
        },
        "files": [
            {
                "sha": "c3b9dfcc688b1bb30c39e6eb39171ceeb5ce4c2a",
                "filename": "github-analytics/.DS_Store",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/github-analytics/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/github-analytics/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/.DS_Store?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606"
            }
        ]
    },
    {
        "sha": "eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmVlYWRiNmY1YWU0ZTgzZjAxMGNlOWRiYTkyZmIxMjVmZmI1MWM2MDY=",
        "commit": {
            "author": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-17T05:52:25Z"
            },
            "committer": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-17T05:52:25Z"
            },
            "message": "Merge remote-tracking branch 'origin/master'",
            "tree": {
                "sha": "3494df938481cffe2f56ebb737ce5e26958a52c7",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/3494df938481cffe2f56ebb737ce5e26958a52c7"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "b6aa424bcf095751dc15bc66737400de173dfe60",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/b6aa424bcf095751dc15bc66737400de173dfe60",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/b6aa424bcf095751dc15bc66737400de173dfe60"
            },
            {
                "sha": "9fcf0c2accec95da34ef5c2840e9a4a7dc550b35",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/9fcf0c2accec95da34ef5c2840e9a4a7dc550b35",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/9fcf0c2accec95da34ef5c2840e9a4a7dc550b35"
            }
        ],
        "stats": {
            "total": 289,
            "additions": 259,
            "deletions": 30
        },
        "files": [
            {
                "sha": "33292d50bf69ff1eb0bc9c06b6e7ff5b64900b23",
                "filename": "db-apis/app.py",
                "status": "modified",
                "additions": 8,
                "deletions": 0,
                "changes": 8,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/db-apis/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/db-apis/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/app.py?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
                "patch": "@@ -50,6 +50,14 @@ def get_data():\n     return core.get_data(request)\n \n \n+@app.route('/get_all', methods=['POST'])\n+def get_all_data():\n+    # TODO: populate response P0\n+    # TODO: Catch exceptions P1\n+    haha = core.get_all_data(request)\n+    return core.get_all_data(request)\n+\n+\n # If we're running in stand alone mode, run the application\n if __name__ == '__main__':\n     app.run(debug=True)"
            },
            {
                "sha": "ee585a56e12546d11b2089cd8cd3a6fa30064a21",
                "filename": "db-apis/dao/core.py",
                "status": "modified",
                "additions": 22,
                "deletions": 2,
                "changes": 24,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/db-apis/dao/core.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/db-apis/dao/core.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/core.py?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
                "patch": "@@ -1,13 +1,18 @@\n from cassandra.cqlengine import connection\n from dao.config import CASSANDRA_HOSTS\n from dao.models import Users, Data\n+from flask import jsonify\n+\n \n ORG = \"org\"\n TABLE = \"table\"\n BODY = \"body\"\n+START_ID = \"start_id\"\n+END_ID = \"end_id\"\n USERS = \"users\"\n \n \n+\n # TODO: Enable quorum\n def insert(request):\n     # TODO: Data Validation should be done?\n@@ -23,5 +28,20 @@ def insert(request):\n def get_data(request):\n     content = request.get_json()\n     connection.setup(CASSANDRA_HOSTS, content[ORG])\n-    obj = eval(content[TABLE]).objects.filter(**(content[BODY]))\n-    return dict(obj.get())\n+    obj = eval(content[TABLE]).objects.filter(**(content[BODY])).allow_filtering()\n+    obj_list = []\n+    for o in obj:\n+        obj_list.append(dict(o))\n+    return jsonify(obj_list)\n+\n+\n+def get_all_data(request):\n+    content = request.get_json()\n+    connection.setup(CASSANDRA_HOSTS, content[ORG])\n+    obj = eval(content[TABLE]).objects.all()\n+    obj_list = []\n+    for o in obj:\n+        obj_list.append(dict(o))\n+    if content[START_ID]:\n+        return jsonify(obj_list[int(content[START_ID]):int(content[END_ID])])\n+    return jsonify(obj_list)"
            },
            {
                "sha": "bcff9413591204059431ef1bdf5c56433d99278f",
                "filename": "db-apis/dao/models.py",
                "status": "modified",
                "additions": 7,
                "deletions": 1,
                "changes": 8,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/db-apis/dao/models.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/db-apis/dao/models.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/models.py?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
                "patch": "@@ -9,7 +9,7 @@ def __init__(self, url):\n \n class Users(Model):\n     uid = columns.UUID(primary_key=True, default=uuid.uuid4)\n-    name = columns.Text(required=True)\n+    name = columns.Text(primary_key=True, required=True)\n     repos = columns.List(value_type=columns.Text, required=False)\n \n \n@@ -18,3 +18,9 @@ class Data(Model):\n     name = columns.Text(primary_key=True,required=True)\n     repo = columns.Text(primary_key=True,required=True)\n     commit_num = columns.Integer(required=False, default=0)\n+\n+\n+class Repos(Model):\n+    uid = columns.UUID(primary_key=True, default=uuid.uuid4)\n+    name = columns.Text(primary_key=True, required=True)\n+    users = columns.List(value_type=columns.Text, required=False)"
            },
            {
                "sha": "b45edbfdde79c13d94aa4ff3ef21f1daa64bee3b",
                "filename": "django/bdaSite/.DS_Store",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/b6aa424bcf095751dc15bc66737400de173dfe60/django/bdaSite/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/b6aa424bcf095751dc15bc66737400de173dfe60/django/bdaSite/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/.DS_Store?ref=b6aa424bcf095751dc15bc66737400de173dfe60"
            },
            {
                "sha": "744381f3bd26406868047a0c54e8853e1af9573d",
                "filename": "django/bdaSite/bdaProject/.DS_Store",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/b6aa424bcf095751dc15bc66737400de173dfe60/django/bdaSite/bdaProject/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/b6aa424bcf095751dc15bc66737400de173dfe60/django/bdaSite/bdaProject/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/.DS_Store?ref=b6aa424bcf095751dc15bc66737400de173dfe60"
            },
            {
                "sha": "ac5e7b3240fa9a57f5725d27bb0c66c0a0414594",
                "filename": "django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606"
            },
            {
                "sha": "57f000ea0b2ec023842ae3100069b40c43f07a59",
                "filename": "django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606"
            },
            {
                "sha": "bae78bbb3a0f954332508a5735d5d16ffd111e6a",
                "filename": "django/bdaSite/bdaProject/static/.DS_Store",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/.DS_Store?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606"
            },
            {
                "sha": "d9f3f519cad89cdc25f934fc9a51b46180b7e65b",
                "filename": "django/bdaSite/bdaProject/static/css/main.css",
                "status": "added",
                "additions": 28,
                "deletions": 0,
                "changes": 28,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/css/main.css",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/css/main.css",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/css/main.css?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
                "patch": "@@ -0,0 +1,28 @@\n+#loader {\n+    width: 100%;\n+    height: 100vh;\n+    background: #F5F5F5 url('../img/loader.gif') no-repeat center;\n+    z-index: 99999;\n+}\n+\n+#loader2 {\n+    width: 100%;\n+    height: 100vh;\n+    background: #E7EFF1 url('../img/loader2.gif') no-repeat center;\n+    z-index: 99999;\n+}\n+\n+#loader3 {\n+    width: 100%;\n+    height: 100vh;\n+    background: #F5F5F5 url('../img/loader3.gif') no-repeat center;\n+    z-index: 99999;\n+}\n+\n+#con {\n+    display: flex;\n+    justify-content: center;\n+    align-items: center;\n+    width: 100vw;\n+    height: 100vh;\n+}\n\\ No newline at end of file"
            },
            {
                "sha": "9ce5ed28b0ef218e122c400455a205387a94d29c",
                "filename": "django/bdaSite/bdaProject/static/img/.DS_Store",
                "status": "renamed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/img/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/img/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/img/.DS_Store?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
                "previous_filename": "django/.DS_Store"
            },
            {
                "sha": "977cf9d7e59d99946025906c104f12b529907345",
                "filename": "django/bdaSite/bdaProject/static/img/github.svg",
                "status": "added",
                "additions": 59,
                "deletions": 0,
                "changes": 59,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/img/github.svg",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/img/github.svg",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/img/github.svg?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
                "patch": "@@ -0,0 +1,59 @@\n+<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\r\n+<!-- Generator: Adobe Illustrator 16.0.0, SVG Export Plug-In . SVG Version: 6.00 Build 0)  -->\r\n+<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n+<svg version=\"1.1\" id=\"Capa_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\"\r\n+\t width=\"438.549px\" height=\"438.549px\" viewBox=\"0 0 438.549 438.549\" style=\"enable-background:new 0 0 438.549 438.549;\"\r\n+\t xml:space=\"preserve\">\r\n+<g>\r\n+\t<path d=\"M409.132,114.573c-19.608-33.596-46.205-60.194-79.798-79.8C295.736,15.166,259.057,5.365,219.271,5.365\r\n+\t\tc-39.781,0-76.472,9.804-110.063,29.408c-33.596,19.605-60.192,46.204-79.8,79.8C9.803,148.168,0,184.854,0,224.63\r\n+\t\tc0,47.78,13.94,90.745,41.827,128.906c27.884,38.164,63.906,64.572,108.063,79.227c5.14,0.954,8.945,0.283,11.419-1.996\r\n+\t\tc2.475-2.282,3.711-5.14,3.711-8.562c0-0.571-0.049-5.708-0.144-15.417c-0.098-9.709-0.144-18.179-0.144-25.406l-6.567,1.136\r\n+\t\tc-4.187,0.767-9.469,1.092-15.846,1c-6.374-0.089-12.991-0.757-19.842-1.999c-6.854-1.231-13.229-4.086-19.13-8.559\r\n+\t\tc-5.898-4.473-10.085-10.328-12.56-17.556l-2.855-6.57c-1.903-4.374-4.899-9.233-8.992-14.559\r\n+\t\tc-4.093-5.331-8.232-8.945-12.419-10.848l-1.999-1.431c-1.332-0.951-2.568-2.098-3.711-3.429c-1.142-1.331-1.997-2.663-2.568-3.997\r\n+\t\tc-0.572-1.335-0.098-2.43,1.427-3.289c1.525-0.859,4.281-1.276,8.28-1.276l5.708,0.853c3.807,0.763,8.516,3.042,14.133,6.851\r\n+\t\tc5.614,3.806,10.229,8.754,13.846,14.842c4.38,7.806,9.657,13.754,15.846,17.847c6.184,4.093,12.419,6.136,18.699,6.136\r\n+\t\tc6.28,0,11.704-0.476,16.274-1.423c4.565-0.952,8.848-2.383,12.847-4.285c1.713-12.758,6.377-22.559,13.988-29.41\r\n+\t\tc-10.848-1.14-20.601-2.857-29.264-5.14c-8.658-2.286-17.605-5.996-26.835-11.14c-9.235-5.137-16.896-11.516-22.985-19.126\r\n+\t\tc-6.09-7.614-11.088-17.61-14.987-29.979c-3.901-12.374-5.852-26.648-5.852-42.826c0-23.035,7.52-42.637,22.557-58.817\r\n+\t\tc-7.044-17.318-6.379-36.732,1.997-58.24c5.52-1.715,13.706-0.428,24.554,3.853c10.85,4.283,18.794,7.952,23.84,10.994\r\n+\t\tc5.046,3.041,9.089,5.618,12.135,7.708c17.705-4.947,35.976-7.421,54.818-7.421s37.117,2.474,54.823,7.421l10.849-6.849\r\n+\t\tc7.419-4.57,16.18-8.758,26.262-12.565c10.088-3.805,17.802-4.853,23.134-3.138c8.562,21.509,9.325,40.922,2.279,58.24\r\n+\t\tc15.036,16.18,22.559,35.787,22.559,58.817c0,16.178-1.958,30.497-5.853,42.966c-3.9,12.471-8.941,22.457-15.125,29.979\r\n+\t\tc-6.191,7.521-13.901,13.85-23.131,18.986c-9.232,5.14-18.182,8.85-26.84,11.136c-8.662,2.286-18.415,4.004-29.263,5.146\r\n+\t\tc9.894,8.562,14.842,22.077,14.842,40.539v60.237c0,3.422,1.19,6.279,3.572,8.562c2.379,2.279,6.136,2.95,11.276,1.995\r\n+\t\tc44.163-14.653,80.185-41.062,108.068-79.226c27.88-38.161,41.825-81.126,41.825-128.906\r\n+\t\tC438.536,184.851,428.728,148.168,409.132,114.573z\"/>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+</svg>\r"
            },
            {
                "sha": "0091e5742fc6aa5d98ed76d3174beea517a6afed",
                "filename": "django/bdaSite/bdaProject/static/img/loader.gif",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/img/loader.gif",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/img/loader.gif",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/img/loader.gif?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606"
            },
            {
                "sha": "61c2566644d0be35d44cbcce332f9b2405785098",
                "filename": "django/bdaSite/bdaProject/static/img/loader2.gif",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/img/loader2.gif",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/img/loader2.gif",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/img/loader2.gif?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606"
            },
            {
                "sha": "5ed9c51332b344b968df8cec7f7e85076bc85ef7",
                "filename": "django/bdaSite/bdaProject/static/img/loader3.gif",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/img/loader3.gif",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/static/img/loader3.gif",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/img/loader3.gif?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606"
            },
            {
                "sha": "32a08657fefcc176984fa91fe61ee00e4a1face6",
                "filename": "django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "status": "modified",
                "additions": 74,
                "deletions": 27,
                "changes": 101,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/bdaProject/index.html?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
                "patch": "@@ -1,35 +1,82 @@\n {% load static %}\n <!DOCTYPE html>\n <html lang=\"en\" dir=\"ltr\">\n-  <head>\n+\n+<head>\n+    <title>Welcome - GitHub Analyser</title>\n     <meta charset=\"utf-8\">\n     <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n-    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n+    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\" />\n+    <link href={% static 'css/main.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\" />\n     <style>\n-     @font-face\n-     { font-family: R; src: url({% static 'fonts/HomepageBaukasten-Book.ttf' %}); }\n-\n+         ::placeholder {\n+            color: black;\n+        }\n+        \n+        @font-face {\n+            font-family: R;\n+            src: url({% static 'fonts/HomepageBaukasten-Book.ttf' %});\n+        }\n     </style>\n-    <title></title>\n-  </head>\n-  <body id='body' style=\"font-family:R\">\n-    <!-- Navbar goes here -->\n-    <nav>\n-      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n-        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform</a>\n-      </div>\n-    </nav>\n-\n-    <!-- Page Layout here -->\n-  \n-    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n-    <script src={% static 'js/materialize.min.js' %}></script>\n-    <script src={% static 'js/chart.min.js' %}></script>\n-    <script type=\"text/javascript\">\n-      navHeight = $('#nav').height();\n-      columnHeight = (window.innerHeight - navHeight);\n-      $('#col1').height(columnHeight);\n-      $('#col2').height(columnHeight);\n-    </script>\n-  </body>\n+</head>\n+\n+<body id='body' style=\"font-family:R; background-color: #00171F;\">\n+\n+    <!-- Loader -->\n+    <div id='loader'></div>\n+\n+    <div id=\"con\">\n+        <div class=\"container z-depth-3\" style=\"background-color:#E5F3FF;width:60vw;height:60vh;outline:0;border-radius: 90px;\">\n+            <div class=\"row\">\n+                <div class=\"center\">\n+                    <br><br><br>\n+                    <img src=\"{% static 'img/github.svg' %}\" style=\"width:100px;height:100px\">\n+                    <h4 style=\"font-weight:bold;margin:10px;padding:0px;font-size:220%;letter-spacing:3pt;line-height:30px\">\n+                        Welcome to GitHub Analyser\n+                    </h4>\n+                    <h4 id='error' style=\"font-weight:bold;margin:10px;padding:0px;font-size:150%;line-height:30px\">\n+                        Enter an Organization Name to continue:\n+                    </h4>\n+                    <h4 style=\"font-weight:bold;margin-top:10px;padding:0px;font-size:150%;line-height:30px\">\n+                        <input id='org_input' type=\"text\" placeholder=\"Type to search\" class=\"center\" style=\"width:40%; color: black; placeholder: black; margin: 10px;\">\n+                        <br>\n+                        <btn id='search' type=\"submit\" class=\"btn-floating btn-large waves-effect waves-light\" style=\"background-color: #00171F;\"><i class=\"material-icons\">search</i></btn>\n+                    </h4>\n+                </div>\n+\n+            </div>\n+        </div>\n+\n+        <script src={% static 'js/jquery-3.1.1.js' %}></script>\n+        <script src={% static 'js/materialize.min.js' %}></script>\n+        <script src={% static 'js/chart.min.js' %}></script>\n+        <script type=\" text/javascript \">\n+            navHeight = $('#nav').height();\n+            columnHeight = (window.innerHeight - navHeight);\n+            $('#col1').height(columnHeight);\n+            $('#col2').height(columnHeight);\n+\n+            $(window).on('load', function() {\n+                $('#loader').fadeOut();\n+            });\n+\n+            $('#search').click(function() {\n+                $.term = $('#org_input').val()\n+                if ($.term == '' || $.term == null || $.term == undefined) {\n+                    $('#error').text(\"Please Enter an Organization Name to continue:\").css(\"color\", \"red\");\n+                } else {\n+                    location.assign('org/' + $.term);\n+                }\n+            });\n+\n+            $(\"#org_input\").keyup(function(event) {\n+                if (event.keyCode === 13) {\n+                    $(\"#search\").click();\n+                }\n+            });\n+        </script>\n+</body>\n+\n </html>\n+\n+</html>\n\\ No newline at end of file"
            },
            {
                "sha": "47913391331cfe17605315a8116498c33bd5da78",
                "filename": "django/bdaSite/bdaProject/templates/bdaProject/organization.html",
                "status": "added",
                "additions": 53,
                "deletions": 0,
                "changes": 53,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/templates/bdaProject/organization.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/templates/bdaProject/organization.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/bdaProject/organization.html?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
                "patch": "@@ -0,0 +1,53 @@\n+{% load static %}\n+<!DOCTYPE html>\n+<html lang=\"en\" dir=\"ltr\">\n+\n+<head>\n+    <title>{{ org_name }} - GitHub Analyser</title>\n+    <meta charset=\"utf-8\">\n+    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n+    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\" />\n+    <link href={% static 'css/main.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\" />\n+    <style>\n+        @font-face {\n+            font-family: R;\n+            src: url({% static 'fonts/HomepageBaukasten-Book.ttf' %});\n+        }\n+    </style>\n+</head>\n+\n+<body id='body' style=\"font-family:R; background-color: #08163F;\">\n+\n+    <!-- Loader  -->\n+    <div id='loader2'></div>\n+\n+    <!-- Navbar goes here -->\n+    <nav>\n+        <div id='nav' style=\"background:#050E28\" class=\"nav-wrapper\">\n+            <a href=\"#\" class=\"brand-logo center\">GitHub Analyser</a>\n+        </div>\n+    </nav>\n+\n+    <!-- Page Layout here -->\n+    <div class=\"row\">\n+        <div class=\"col s12 m6 offset-m3 center white-text\">\n+            <h3>Organization: {{ org_name }}</h3>\n+        </div>\n+    </div>\n+\n+    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n+    <script src={% static 'js/materialize.min.js' %}></script>\n+    <script src={% static 'js/chart.min.js' %}></script>\n+    <script type=\"text/javascript\">\n+        navHeight = $('#nav').height();\n+        columnHeight = (window.innerHeight - navHeight);\n+        $('#col1').height(columnHeight);\n+        $('#col2').height(columnHeight);\n+\n+        $(window).on('load', function() {\n+            $('#loader2').fadeOut();\n+        });\n+    </script>\n+</body>\n+\n+</html>\n\\ No newline at end of file"
            },
            {
                "sha": "9a978f049c53265ea71ea3ad8cd0acd904d3d678",
                "filename": "django/bdaSite/bdaProject/urls.py",
                "status": "modified",
                "additions": 1,
                "deletions": 0,
                "changes": 1,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/urls.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/urls.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/urls.py?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
                "patch": "@@ -3,4 +3,5 @@\n \n urlpatterns = [\n     path('', views.index, name='index'),\n+    path('org/<str:org>/', views.org, name='organization'),\n ]\n\\ No newline at end of file"
            },
            {
                "sha": "a85940eee69347ae4b7244b31f8637e128e68493",
                "filename": "django/bdaSite/bdaProject/views.py",
                "status": "modified",
                "additions": 7,
                "deletions": 0,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/views.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/eeadb6f5ae4e83f010ce9dba92fb125ffb51c606/django/bdaSite/bdaProject/views.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/views.py?ref=eeadb6f5ae4e83f010ce9dba92fb125ffb51c606",
                "patch": "@@ -6,3 +6,10 @@\n def index(request):\n     context = {}\n     return render(request, 'bdaProject/index.html', context)\n+\n+def org(request, org):\n+    \n+    context = {\n+        'org_name' : org\n+    }\n+    return render(request, 'bdaProject/organization.html', context)"
            }
        ]
    },
    {
        "sha": "b6aa424bcf095751dc15bc66737400de173dfe60",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmI2YWE0MjRiY2YwOTU3NTFkYzE1YmM2NjczNzQwMGRlMTczZGZlNjA=",
        "commit": {
            "author": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-17T05:51:34Z"
            },
            "committer": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-17T05:51:34Z"
            },
            "message": "Insert data to Cassandra using CassandraHelper",
            "tree": {
                "sha": "5d68f39247a20ea9114edeca99460ad0c23160b9",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/5d68f39247a20ea9114edeca99460ad0c23160b9"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/b6aa424bcf095751dc15bc66737400de173dfe60",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/b6aa424bcf095751dc15bc66737400de173dfe60",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/b6aa424bcf095751dc15bc66737400de173dfe60",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/b6aa424bcf095751dc15bc66737400de173dfe60/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "31153feb806ae4b4ee12b3eb592de20355f7561e",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/31153feb806ae4b4ee12b3eb592de20355f7561e",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/31153feb806ae4b4ee12b3eb592de20355f7561e"
            }
        ],
        "stats": {
            "total": 31,
            "additions": 24,
            "deletions": 7
        },
        "files": [
            {
                "sha": "c3b9dfcc688b1bb30c39e6eb39171ceeb5ce4c2a",
                "filename": "github-analytics/.DS_Store",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/.DS_Store?ref=b6aa424bcf095751dc15bc66737400de173dfe60"
            },
            {
                "sha": "4d99e46e3f3634420996dafc71112ede80624b5f",
                "filename": "github-analytics/CassandraHelper/CassandraHelper.py",
                "status": "modified",
                "additions": 15,
                "deletions": 4,
                "changes": 19,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/CassandraHelper/CassandraHelper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/CassandraHelper/CassandraHelper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/CassandraHelper.py?ref=b6aa424bcf095751dc15bc66737400de173dfe60",
                "patch": "@@ -1,7 +1,18 @@\n+import requests\n+from CassandraHelper import config as config\n+\n class CassandraHelper():\n     def __init__(self):\n-        self.orgname = \"\"\n+        pass\n     \n-    def set_org_name(self,orgname):\n-        self.orgname = orgname\n-        return\n\\ No newline at end of file\n+    def insertOrgData(self,orgdata):\n+        r = requests.post(config.CASSANDRA_URL_INSERT, json = orgdata)\n+        return r.status_code\n+\n+    def insertUserData(self,userdata):\n+        r = requests.post(config.CASSANDRA_URL_INSERT, json = userdata)\n+        return r.status_code\n+\n+    def insertRepoData(self,repodata):\n+        r = requests.post(config.CASSANDRA_URL_INSERT, json = repodata)\n+        return r.status_code\n\\ No newline at end of file"
            },
            {
                "sha": "3127495ef782956591095bc956148d83765a3881",
                "filename": "github-analytics/CassandraHelper/__pycache__/CassandraHelper.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/CassandraHelper/__pycache__/CassandraHelper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/CassandraHelper/__pycache__/CassandraHelper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/__pycache__/CassandraHelper.cpython-37.pyc?ref=b6aa424bcf095751dc15bc66737400de173dfe60"
            },
            {
                "sha": "0889d6850466f806a7f114f15114a50ac5de2a19",
                "filename": "github-analytics/CassandraHelper/__pycache__/config.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/CassandraHelper/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/CassandraHelper/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/__pycache__/config.cpython-37.pyc?ref=b6aa424bcf095751dc15bc66737400de173dfe60"
            },
            {
                "sha": "a54974685242d1f7dc2121bd890e57a0e46bea26",
                "filename": "github-analytics/CassandraHelper/config.py",
                "status": "added",
                "additions": 6,
                "deletions": 0,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/CassandraHelper/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/CassandraHelper/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/config.py?ref=b6aa424bcf095751dc15bc66737400de173dfe60",
                "patch": "@@ -0,0 +1,6 @@\n+CASSANDRA_HOST = 'localhost'\n+CASSANDRA_PORT = 5000\n+\n+CASSANDRA_URL = \"http://\" + CASSANDRA_HOST + \":\" + str(CASSANDRA_PORT) + \"/\"\n+\n+CASSANDRA_URL_INSERT = CASSANDRA_URL + \"insert/\"\n\\ No newline at end of file"
            },
            {
                "sha": "d7c3f3ec3dfbf653c865061115a91d9fcb6fc47a",
                "filename": "github-analytics/ElasticSearchHelper/.DS_Store",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/ElasticSearchHelper/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/ElasticSearchHelper/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/.DS_Store?ref=b6aa424bcf095751dc15bc66737400de173dfe60"
            },
            {
                "sha": "b37264274f704c57d04c2244e06e25c812c28b92",
                "filename": "github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "status": "modified",
                "additions": 3,
                "deletions": 3,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py?ref=b6aa424bcf095751dc15bc66737400de173dfe60",
                "patch": "@@ -12,14 +12,14 @@ def __init__(self):\n     def getOrgData(self,orgname):\n         res = es.search(index=orgname, body={\"query\": {\"match_all\": {}}})\n         print(res)\n-        return json.dumps(res)\n+        return res\n \n     def getUserData(self,username):\n         res = es.search(index=username, body={\"query\": {\"match_all\": {}}})\n         print(res)\n-        return json.dumps(res)\n+        return res\n \n     def getRepoData(self,reponame):\n         res = es.search(index=reponame, body={\"query\": {\"match_all\": {}}})\n         print(res)\n-        return json.dumps(res)\n\\ No newline at end of file\n+        return res\n\\ No newline at end of file"
            },
            {
                "sha": "b37cb566dcb921a13d6def51656ed920d5b6441d",
                "filename": "github-analytics/ElasticSearchHelper/__pycache__/.DS_Store",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/ElasticSearchHelper/__pycache__/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/ElasticSearchHelper/__pycache__/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/__pycache__/.DS_Store?ref=b6aa424bcf095751dc15bc66737400de173dfe60"
            },
            {
                "sha": "106e6ef98b2e86d4f2fa49914b3a148ba3767b55",
                "filename": "github-analytics/ElasticSearchHelper/__pycache__/ElasticSearchHelper.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/ElasticSearchHelper/__pycache__/ElasticSearchHelper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/ElasticSearchHelper/__pycache__/ElasticSearchHelper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/__pycache__/ElasticSearchHelper.cpython-37.pyc?ref=b6aa424bcf095751dc15bc66737400de173dfe60"
            },
            {
                "sha": "7ae1cb99bd3ad019ba410a45724334827de774fa",
                "filename": "github-analytics/ElasticSearchHelper/__pycache__/config.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/ElasticSearchHelper/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/ElasticSearchHelper/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/__pycache__/config.cpython-37.pyc?ref=b6aa424bcf095751dc15bc66737400de173dfe60"
            },
            {
                "sha": "5294ef55fee8ee6edfb191879fbbd17a706447f6",
                "filename": "github-analytics/config/__pycache__/config.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/config/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/b6aa424bcf095751dc15bc66737400de173dfe60/github-analytics/config/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/config/__pycache__/config.cpython-37.pyc?ref=b6aa424bcf095751dc15bc66737400de173dfe60"
            }
        ]
    },
    {
        "sha": "9fcf0c2accec95da34ef5c2840e9a4a7dc550b35",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjlmY2YwYzJhY2NlYzk1ZGEzNGVmNWMyODQwZTlhNGE3ZGM1NTBiMzU=",
        "commit": {
            "author": {
                "name": "Karthik S",
                "email": "karthiks1995@gmail.com",
                "date": "2020-04-17T03:43:01Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-17T03:43:01Z"
            },
            "message": "Merge pull request #1 from CUBigDataClass/karthik\n\nAdded get_all with index specs: Updated postman apis can be found in the readme link",
            "tree": {
                "sha": "51923188a4addc89fff896538d2838339c306458",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/51923188a4addc89fff896538d2838339c306458"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/9fcf0c2accec95da34ef5c2840e9a4a7dc550b35",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJemSXFCRBK7hj4Ov3rIwAAdHIIAB8yhv2zwKaB/d6Q7pz90X2V\n7a1wrxVo2VGZIycCEtUCOvleo0AbTK6dwEHfuC2aX5QcFkWtjj66wVZT3ge7+6AN\ntmqjI20PDOLvjnU5GzqNsJ9xMcnVX3aOJaD2Buc9lxXyBEqIh4uPPnedqlOUUVUb\nTCy6PLDTWf38AITehGRd0478YWlfCeFwoGl7ibze+kyYSi5lstoPUiq10nYLHZgp\nyzVkHx2Lh9bXOckRekqtsAxEl/ASVZEiwCmKIVNV7rTPcNaKj9K7aVeNwQ+QKfE5\n/FqhGibxL7UXlXOUyW+S9RiDXGN5be9MCpNkPxFdFiCPSpkSjw+D0WNdYdOkZvY=\n=ucNv\n-----END PGP SIGNATURE-----\n",
                "payload": "tree 51923188a4addc89fff896538d2838339c306458\nparent 3aab875bcf53a150e921f8b0a8a6fd4618d31475\nparent ae2b9f700484cb447a448dd556dbdb1e353bd377\nauthor Karthik S <karthiks1995@gmail.com> 1587094981 -0700\ncommitter GitHub <noreply@github.com> 1587094981 -0700\n\nMerge pull request #1 from CUBigDataClass/karthik\n\nAdded get_all with index specs: Updated postman apis can be found in the readme link"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/9fcf0c2accec95da34ef5c2840e9a4a7dc550b35",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/9fcf0c2accec95da34ef5c2840e9a4a7dc550b35",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/9fcf0c2accec95da34ef5c2840e9a4a7dc550b35/comments",
        "author": {
            "login": "karthiks1995",
            "id": 7661540,
            "node_id": "MDQ6VXNlcjc2NjE1NDA=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/7661540?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/karthiks1995",
            "html_url": "https://github.com/karthiks1995",
            "followers_url": "https://api.github.com/users/karthiks1995/followers",
            "following_url": "https://api.github.com/users/karthiks1995/following{/other_user}",
            "gists_url": "https://api.github.com/users/karthiks1995/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/karthiks1995/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/karthiks1995/subscriptions",
            "organizations_url": "https://api.github.com/users/karthiks1995/orgs",
            "repos_url": "https://api.github.com/users/karthiks1995/repos",
            "events_url": "https://api.github.com/users/karthiks1995/events{/privacy}",
            "received_events_url": "https://api.github.com/users/karthiks1995/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "3aab875bcf53a150e921f8b0a8a6fd4618d31475",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/3aab875bcf53a150e921f8b0a8a6fd4618d31475",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/3aab875bcf53a150e921f8b0a8a6fd4618d31475"
            },
            {
                "sha": "ae2b9f700484cb447a448dd556dbdb1e353bd377",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/ae2b9f700484cb447a448dd556dbdb1e353bd377",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/ae2b9f700484cb447a448dd556dbdb1e353bd377"
            }
        ],
        "stats": {
            "total": 40,
            "additions": 37,
            "deletions": 3
        },
        "files": [
            {
                "sha": "33292d50bf69ff1eb0bc9c06b6e7ff5b64900b23",
                "filename": "db-apis/app.py",
                "status": "modified",
                "additions": 8,
                "deletions": 0,
                "changes": 8,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/9fcf0c2accec95da34ef5c2840e9a4a7dc550b35/db-apis/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/9fcf0c2accec95da34ef5c2840e9a4a7dc550b35/db-apis/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/app.py?ref=9fcf0c2accec95da34ef5c2840e9a4a7dc550b35",
                "patch": "@@ -50,6 +50,14 @@ def get_data():\n     return core.get_data(request)\n \n \n+@app.route('/get_all', methods=['POST'])\n+def get_all_data():\n+    # TODO: populate response P0\n+    # TODO: Catch exceptions P1\n+    haha = core.get_all_data(request)\n+    return core.get_all_data(request)\n+\n+\n # If we're running in stand alone mode, run the application\n if __name__ == '__main__':\n     app.run(debug=True)"
            },
            {
                "sha": "ee585a56e12546d11b2089cd8cd3a6fa30064a21",
                "filename": "db-apis/dao/core.py",
                "status": "modified",
                "additions": 22,
                "deletions": 2,
                "changes": 24,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/9fcf0c2accec95da34ef5c2840e9a4a7dc550b35/db-apis/dao/core.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/9fcf0c2accec95da34ef5c2840e9a4a7dc550b35/db-apis/dao/core.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/core.py?ref=9fcf0c2accec95da34ef5c2840e9a4a7dc550b35",
                "patch": "@@ -1,13 +1,18 @@\n from cassandra.cqlengine import connection\n from dao.config import CASSANDRA_HOSTS\n from dao.models import Users, Data\n+from flask import jsonify\n+\n \n ORG = \"org\"\n TABLE = \"table\"\n BODY = \"body\"\n+START_ID = \"start_id\"\n+END_ID = \"end_id\"\n USERS = \"users\"\n \n \n+\n # TODO: Enable quorum\n def insert(request):\n     # TODO: Data Validation should be done?\n@@ -23,5 +28,20 @@ def insert(request):\n def get_data(request):\n     content = request.get_json()\n     connection.setup(CASSANDRA_HOSTS, content[ORG])\n-    obj = eval(content[TABLE]).objects.filter(**(content[BODY]))\n-    return dict(obj.get())\n+    obj = eval(content[TABLE]).objects.filter(**(content[BODY])).allow_filtering()\n+    obj_list = []\n+    for o in obj:\n+        obj_list.append(dict(o))\n+    return jsonify(obj_list)\n+\n+\n+def get_all_data(request):\n+    content = request.get_json()\n+    connection.setup(CASSANDRA_HOSTS, content[ORG])\n+    obj = eval(content[TABLE]).objects.all()\n+    obj_list = []\n+    for o in obj:\n+        obj_list.append(dict(o))\n+    if content[START_ID]:\n+        return jsonify(obj_list[int(content[START_ID]):int(content[END_ID])])\n+    return jsonify(obj_list)"
            },
            {
                "sha": "bcff9413591204059431ef1bdf5c56433d99278f",
                "filename": "db-apis/dao/models.py",
                "status": "modified",
                "additions": 7,
                "deletions": 1,
                "changes": 8,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/9fcf0c2accec95da34ef5c2840e9a4a7dc550b35/db-apis/dao/models.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/9fcf0c2accec95da34ef5c2840e9a4a7dc550b35/db-apis/dao/models.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/models.py?ref=9fcf0c2accec95da34ef5c2840e9a4a7dc550b35",
                "patch": "@@ -9,7 +9,7 @@ def __init__(self, url):\n \n class Users(Model):\n     uid = columns.UUID(primary_key=True, default=uuid.uuid4)\n-    name = columns.Text(required=True)\n+    name = columns.Text(primary_key=True, required=True)\n     repos = columns.List(value_type=columns.Text, required=False)\n \n \n@@ -18,3 +18,9 @@ class Data(Model):\n     name = columns.Text(primary_key=True,required=True)\n     repo = columns.Text(primary_key=True,required=True)\n     commit_num = columns.Integer(required=False, default=0)\n+\n+\n+class Repos(Model):\n+    uid = columns.UUID(primary_key=True, default=uuid.uuid4)\n+    name = columns.Text(primary_key=True, required=True)\n+    users = columns.List(value_type=columns.Text, required=False)"
            }
        ]
    },
    {
        "sha": "ae2b9f700484cb447a448dd556dbdb1e353bd377",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmFlMmI5ZjcwMDQ4NGNiNDQ3YTQ0OGRkNTU2ZGJkYjFlMzUzYmQzNzc=",
        "commit": {
            "author": {
                "name": "karthiks1995",
                "email": "karthiks@Karthiks-MBP.lan",
                "date": "2020-04-17T03:39:58Z"
            },
            "committer": {
                "name": "karthiks1995",
                "email": "karthiks@Karthiks-MBP.lan",
                "date": "2020-04-17T03:40:56Z"
            },
            "message": "Added get_all with index specs",
            "tree": {
                "sha": "51923188a4addc89fff896538d2838339c306458",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/51923188a4addc89fff896538d2838339c306458"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/ae2b9f700484cb447a448dd556dbdb1e353bd377",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/ae2b9f700484cb447a448dd556dbdb1e353bd377",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/ae2b9f700484cb447a448dd556dbdb1e353bd377",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/ae2b9f700484cb447a448dd556dbdb1e353bd377/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "3aab875bcf53a150e921f8b0a8a6fd4618d31475",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/3aab875bcf53a150e921f8b0a8a6fd4618d31475",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/3aab875bcf53a150e921f8b0a8a6fd4618d31475"
            }
        ],
        "stats": {
            "total": 40,
            "additions": 37,
            "deletions": 3
        },
        "files": [
            {
                "sha": "33292d50bf69ff1eb0bc9c06b6e7ff5b64900b23",
                "filename": "db-apis/app.py",
                "status": "modified",
                "additions": 8,
                "deletions": 0,
                "changes": 8,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/ae2b9f700484cb447a448dd556dbdb1e353bd377/db-apis/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/ae2b9f700484cb447a448dd556dbdb1e353bd377/db-apis/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/app.py?ref=ae2b9f700484cb447a448dd556dbdb1e353bd377",
                "patch": "@@ -50,6 +50,14 @@ def get_data():\n     return core.get_data(request)\n \n \n+@app.route('/get_all', methods=['POST'])\n+def get_all_data():\n+    # TODO: populate response P0\n+    # TODO: Catch exceptions P1\n+    haha = core.get_all_data(request)\n+    return core.get_all_data(request)\n+\n+\n # If we're running in stand alone mode, run the application\n if __name__ == '__main__':\n     app.run(debug=True)"
            },
            {
                "sha": "ee585a56e12546d11b2089cd8cd3a6fa30064a21",
                "filename": "db-apis/dao/core.py",
                "status": "modified",
                "additions": 22,
                "deletions": 2,
                "changes": 24,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/ae2b9f700484cb447a448dd556dbdb1e353bd377/db-apis/dao/core.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/ae2b9f700484cb447a448dd556dbdb1e353bd377/db-apis/dao/core.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/core.py?ref=ae2b9f700484cb447a448dd556dbdb1e353bd377",
                "patch": "@@ -1,13 +1,18 @@\n from cassandra.cqlengine import connection\n from dao.config import CASSANDRA_HOSTS\n from dao.models import Users, Data\n+from flask import jsonify\n+\n \n ORG = \"org\"\n TABLE = \"table\"\n BODY = \"body\"\n+START_ID = \"start_id\"\n+END_ID = \"end_id\"\n USERS = \"users\"\n \n \n+\n # TODO: Enable quorum\n def insert(request):\n     # TODO: Data Validation should be done?\n@@ -23,5 +28,20 @@ def insert(request):\n def get_data(request):\n     content = request.get_json()\n     connection.setup(CASSANDRA_HOSTS, content[ORG])\n-    obj = eval(content[TABLE]).objects.filter(**(content[BODY]))\n-    return dict(obj.get())\n+    obj = eval(content[TABLE]).objects.filter(**(content[BODY])).allow_filtering()\n+    obj_list = []\n+    for o in obj:\n+        obj_list.append(dict(o))\n+    return jsonify(obj_list)\n+\n+\n+def get_all_data(request):\n+    content = request.get_json()\n+    connection.setup(CASSANDRA_HOSTS, content[ORG])\n+    obj = eval(content[TABLE]).objects.all()\n+    obj_list = []\n+    for o in obj:\n+        obj_list.append(dict(o))\n+    if content[START_ID]:\n+        return jsonify(obj_list[int(content[START_ID]):int(content[END_ID])])\n+    return jsonify(obj_list)"
            },
            {
                "sha": "bcff9413591204059431ef1bdf5c56433d99278f",
                "filename": "db-apis/dao/models.py",
                "status": "modified",
                "additions": 7,
                "deletions": 1,
                "changes": 8,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/ae2b9f700484cb447a448dd556dbdb1e353bd377/db-apis/dao/models.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/ae2b9f700484cb447a448dd556dbdb1e353bd377/db-apis/dao/models.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/models.py?ref=ae2b9f700484cb447a448dd556dbdb1e353bd377",
                "patch": "@@ -9,7 +9,7 @@ def __init__(self, url):\n \n class Users(Model):\n     uid = columns.UUID(primary_key=True, default=uuid.uuid4)\n-    name = columns.Text(required=True)\n+    name = columns.Text(primary_key=True, required=True)\n     repos = columns.List(value_type=columns.Text, required=False)\n \n \n@@ -18,3 +18,9 @@ class Data(Model):\n     name = columns.Text(primary_key=True,required=True)\n     repo = columns.Text(primary_key=True,required=True)\n     commit_num = columns.Integer(required=False, default=0)\n+\n+\n+class Repos(Model):\n+    uid = columns.UUID(primary_key=True, default=uuid.uuid4)\n+    name = columns.Text(primary_key=True, required=True)\n+    users = columns.List(value_type=columns.Text, required=False)"
            }
        ]
    },
    {
        "sha": "3aab875bcf53a150e921f8b0a8a6fd4618d31475",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjNhYWI4NzViY2Y1M2ExNTBlOTIxZjhiMGE4YTZmZDQ2MThkMzE0NzU=",
        "commit": {
            "author": {
                "name": "sish2654",
                "email": "55568643+sish2654@users.noreply.github.com",
                "date": "2020-04-16T22:56:23Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-16T22:56:23Z"
            },
            "message": "Delete .DS_Store",
            "tree": {
                "sha": "ec26ee64bf45fb10086a34bbdcc7d99bc0f35782",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/ec26ee64bf45fb10086a34bbdcc7d99bc0f35782"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/3aab875bcf53a150e921f8b0a8a6fd4618d31475",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJemOKXCRBK7hj4Ov3rIwAAdHIIAK3GxOsMD7/3YVGm5qQYgpdV\nxwmD0yez8kxFmS/I3B7KZ9XQHCNgOXSsfAW8VMWPeJ/6PMXH5efgAxHeF/JhXH2V\nQ5++vwzmc1khPTGii/yCHrXdZWdXBv+S+nRQOagp08S6KeMqWRlJ8f7eGiCKeblL\nkB90Oe7MFulESCxw1jINT73LvF/zXYBNkUvVqr1xL52l5quC5VLtXy7zHTgUOgAD\nZPIu9LAx3YD+TM01LVzz4msEObfiwvOODZk5t4AgsWetPPJqUCsKXEKgrbgf9fGw\nC2+EwBWbxiWm4YS/q6DhtI2rhzz48efaKWZSaXD+0p3KAOTY2eMm6+J7jZ5JKWU=\n=l5cT\n-----END PGP SIGNATURE-----\n",
                "payload": "tree ec26ee64bf45fb10086a34bbdcc7d99bc0f35782\nparent a30f0845b2cbf280618de294a4413081953ae2a1\nauthor sish2654 <55568643+sish2654@users.noreply.github.com> 1587077783 -0600\ncommitter GitHub <noreply@github.com> 1587077783 -0600\n\nDelete .DS_Store"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/3aab875bcf53a150e921f8b0a8a6fd4618d31475",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/3aab875bcf53a150e921f8b0a8a6fd4618d31475",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/3aab875bcf53a150e921f8b0a8a6fd4618d31475/comments",
        "author": {
            "login": "sish2654",
            "id": 55568643,
            "node_id": "MDQ6VXNlcjU1NTY4NjQz",
            "avatar_url": "https://avatars1.githubusercontent.com/u/55568643?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sish2654",
            "html_url": "https://github.com/sish2654",
            "followers_url": "https://api.github.com/users/sish2654/followers",
            "following_url": "https://api.github.com/users/sish2654/following{/other_user}",
            "gists_url": "https://api.github.com/users/sish2654/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sish2654/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sish2654/subscriptions",
            "organizations_url": "https://api.github.com/users/sish2654/orgs",
            "repos_url": "https://api.github.com/users/sish2654/repos",
            "events_url": "https://api.github.com/users/sish2654/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sish2654/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "a30f0845b2cbf280618de294a4413081953ae2a1",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/a30f0845b2cbf280618de294a4413081953ae2a1",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/a30f0845b2cbf280618de294a4413081953ae2a1"
            }
        ],
        "stats": {
            "total": 0,
            "additions": 0,
            "deletions": 0
        },
        "files": [
            {
                "sha": "744381f3bd26406868047a0c54e8853e1af9573d",
                "filename": "django/bdaSite/bdaProject/.DS_Store",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/a30f0845b2cbf280618de294a4413081953ae2a1/django/bdaSite/bdaProject/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/a30f0845b2cbf280618de294a4413081953ae2a1/django/bdaSite/bdaProject/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/.DS_Store?ref=a30f0845b2cbf280618de294a4413081953ae2a1"
            }
        ]
    },
    {
        "sha": "a30f0845b2cbf280618de294a4413081953ae2a1",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmEzMGYwODQ1YjJjYmYyODA2MThkZTI5NGE0NDEzMDgxOTUzYWUyYTE=",
        "commit": {
            "author": {
                "name": "sish2654",
                "email": "55568643+sish2654@users.noreply.github.com",
                "date": "2020-04-16T22:56:09Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-16T22:56:09Z"
            },
            "message": "Delete .DS_Store",
            "tree": {
                "sha": "e79a62614d7dd8e5c1a49b8099322a92621c418b",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/e79a62614d7dd8e5c1a49b8099322a92621c418b"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/a30f0845b2cbf280618de294a4413081953ae2a1",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJemOKJCRBK7hj4Ov3rIwAAdHIIAH/SYQPZzD4hIUqDMerfmhGJ\n34H5vWSU/0wU7ImFRB7JX+jrkGcY0b1mvaLNZd557ttEszx54LSD05SgLFIUg8YD\n66id7jeiANgdQWzoS/Va+6+Lha+B58pqGphOPV7XYWcD1P/dxVVXDz+XIFZjWNAg\njtiVSLMcBJ78qBucvCevFQj/VHqqLjcoQ+IH7+9EFEzypxMTqJ39pdlTt4p3A6Ay\n+6Enxa8VndzxR41wzqAY13r9De9KTgS9z9LkfYt3Y0Z4Mz7uUy/xzOHn3EbvC2IG\nkQp5jS05Ur3Uh2Un2cLOHfQlEH5XfYC1s49rJnOzBJh7lf+xc8n/mBcL+6AoR1w=\n=goYg\n-----END PGP SIGNATURE-----\n",
                "payload": "tree e79a62614d7dd8e5c1a49b8099322a92621c418b\nparent 27b05355891d2319ae2d69feedbb6ba5f8912c08\nauthor sish2654 <55568643+sish2654@users.noreply.github.com> 1587077769 -0600\ncommitter GitHub <noreply@github.com> 1587077769 -0600\n\nDelete .DS_Store"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/a30f0845b2cbf280618de294a4413081953ae2a1",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/a30f0845b2cbf280618de294a4413081953ae2a1",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/a30f0845b2cbf280618de294a4413081953ae2a1/comments",
        "author": {
            "login": "sish2654",
            "id": 55568643,
            "node_id": "MDQ6VXNlcjU1NTY4NjQz",
            "avatar_url": "https://avatars1.githubusercontent.com/u/55568643?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sish2654",
            "html_url": "https://github.com/sish2654",
            "followers_url": "https://api.github.com/users/sish2654/followers",
            "following_url": "https://api.github.com/users/sish2654/following{/other_user}",
            "gists_url": "https://api.github.com/users/sish2654/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sish2654/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sish2654/subscriptions",
            "organizations_url": "https://api.github.com/users/sish2654/orgs",
            "repos_url": "https://api.github.com/users/sish2654/repos",
            "events_url": "https://api.github.com/users/sish2654/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sish2654/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "27b05355891d2319ae2d69feedbb6ba5f8912c08",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/27b05355891d2319ae2d69feedbb6ba5f8912c08",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/27b05355891d2319ae2d69feedbb6ba5f8912c08"
            }
        ],
        "stats": {
            "total": 0,
            "additions": 0,
            "deletions": 0
        },
        "files": [
            {
                "sha": "b45edbfdde79c13d94aa4ff3ef21f1daa64bee3b",
                "filename": "django/bdaSite/.DS_Store",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/27b05355891d2319ae2d69feedbb6ba5f8912c08/django/bdaSite/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/27b05355891d2319ae2d69feedbb6ba5f8912c08/django/bdaSite/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/.DS_Store?ref=27b05355891d2319ae2d69feedbb6ba5f8912c08"
            }
        ]
    },
    {
        "sha": "27b05355891d2319ae2d69feedbb6ba5f8912c08",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjI3YjA1MzU1ODkxZDIzMTlhZTJkNjlmZWVkYmI2YmE1Zjg5MTJjMDg=",
        "commit": {
            "author": {
                "name": "sish2654",
                "email": "55568643+sish2654@users.noreply.github.com",
                "date": "2020-04-16T22:56:00Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-16T22:56:00Z"
            },
            "message": "Delete .DS_Store",
            "tree": {
                "sha": "d1df9848d0bcad0bdebb1ba471e3345ec2845ab3",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/d1df9848d0bcad0bdebb1ba471e3345ec2845ab3"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/27b05355891d2319ae2d69feedbb6ba5f8912c08",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJemOKACRBK7hj4Ov3rIwAAdHIIAAAVG0hePRDNx2Q9YMxp7K4R\neiQCobUHO2giEjksMkqZOjvKbadwWDoiv7uWDG6cDm8M8rcl6hWqSu1Dbq2HE1rG\nb0uoiH/417n818HHzNth9MUwO6k2Z8lICL13VdK5/e8aPFIWcfAcsjZLR08MkSdu\nsh2cRtbZR+b2Omrf97u1QSwJzuyntR4ynY0VzMZLr3DfEoHYsxuNUqY8cJhPVZOA\nh1HCFgXn9ZzcIoDuI4UNj1gsZpyJ2yYllnedjTYSu49u7tVyzEiGMWyD5ltN/ySS\naNETxpq5mpYv0rUhFZ2avrNi3ktzeuLZ91uGBwOmFpJnAoDADzyvCb9g++Vhcgg=\n=xYUC\n-----END PGP SIGNATURE-----\n",
                "payload": "tree d1df9848d0bcad0bdebb1ba471e3345ec2845ab3\nparent 907881cb0de06534a8d0dee4acead8bac62a96af\nauthor sish2654 <55568643+sish2654@users.noreply.github.com> 1587077760 -0600\ncommitter GitHub <noreply@github.com> 1587077760 -0600\n\nDelete .DS_Store"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/27b05355891d2319ae2d69feedbb6ba5f8912c08",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/27b05355891d2319ae2d69feedbb6ba5f8912c08",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/27b05355891d2319ae2d69feedbb6ba5f8912c08/comments",
        "author": {
            "login": "sish2654",
            "id": 55568643,
            "node_id": "MDQ6VXNlcjU1NTY4NjQz",
            "avatar_url": "https://avatars1.githubusercontent.com/u/55568643?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sish2654",
            "html_url": "https://github.com/sish2654",
            "followers_url": "https://api.github.com/users/sish2654/followers",
            "following_url": "https://api.github.com/users/sish2654/following{/other_user}",
            "gists_url": "https://api.github.com/users/sish2654/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sish2654/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sish2654/subscriptions",
            "organizations_url": "https://api.github.com/users/sish2654/orgs",
            "repos_url": "https://api.github.com/users/sish2654/repos",
            "events_url": "https://api.github.com/users/sish2654/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sish2654/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "907881cb0de06534a8d0dee4acead8bac62a96af",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/907881cb0de06534a8d0dee4acead8bac62a96af",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/907881cb0de06534a8d0dee4acead8bac62a96af"
            }
        ],
        "stats": {
            "total": 0,
            "additions": 0,
            "deletions": 0
        },
        "files": [
            {
                "sha": "275a113e16335210def75f91320c2efc184ebf54",
                "filename": "django/.DS_Store",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/907881cb0de06534a8d0dee4acead8bac62a96af/django/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/907881cb0de06534a8d0dee4acead8bac62a96af/django/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/.DS_Store?ref=907881cb0de06534a8d0dee4acead8bac62a96af"
            }
        ]
    },
    {
        "sha": "907881cb0de06534a8d0dee4acead8bac62a96af",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjkwNzg4MWNiMGRlMDY1MzRhOGQwZGVlNGFjZWFkOGJhYzYyYTk2YWY=",
        "commit": {
            "author": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T22:55:31Z"
            },
            "committer": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T22:55:31Z"
            },
            "message": "UI 4",
            "tree": {
                "sha": "dc72398e70302820113242ce1a9722f6fd6a1ba0",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/dc72398e70302820113242ce1a9722f6fd6a1ba0"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/907881cb0de06534a8d0dee4acead8bac62a96af",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/907881cb0de06534a8d0dee4acead8bac62a96af",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/907881cb0de06534a8d0dee4acead8bac62a96af",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/907881cb0de06534a8d0dee4acead8bac62a96af/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "8ed42d01441bfad02c9480fe6dcc3113ece811f5",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/8ed42d01441bfad02c9480fe6dcc3113ece811f5",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/8ed42d01441bfad02c9480fe6dcc3113ece811f5"
            }
        ],
        "stats": {
            "total": 62,
            "additions": 39,
            "deletions": 23
        },
        "files": [
            {
                "sha": "ac5e7b3240fa9a57f5725d27bb0c66c0a0414594",
                "filename": "django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc?ref=907881cb0de06534a8d0dee4acead8bac62a96af"
            },
            {
                "sha": "57f000ea0b2ec023842ae3100069b40c43f07a59",
                "filename": "django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc?ref=907881cb0de06534a8d0dee4acead8bac62a96af"
            },
            {
                "sha": "bae78bbb3a0f954332508a5735d5d16ffd111e6a",
                "filename": "django/bdaSite/bdaProject/static/.DS_Store",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/static/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/static/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/.DS_Store?ref=907881cb0de06534a8d0dee4acead8bac62a96af"
            },
            {
                "sha": "d9f3f519cad89cdc25f934fc9a51b46180b7e65b",
                "filename": "django/bdaSite/bdaProject/static/css/main.css",
                "status": "modified",
                "additions": 15,
                "deletions": 1,
                "changes": 16,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/static/css/main.css",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/static/css/main.css",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/css/main.css?ref=907881cb0de06534a8d0dee4acead8bac62a96af",
                "patch": "@@ -1,7 +1,21 @@\n #loader {\n     width: 100%;\n     height: 100vh;\n-    background: #f5f5f5 url('../img/loader.gif') no-repeat center;\n+    background: #F5F5F5 url('../img/loader.gif') no-repeat center;\n+    z-index: 99999;\n+}\n+\n+#loader2 {\n+    width: 100%;\n+    height: 100vh;\n+    background: #E7EFF1 url('../img/loader2.gif') no-repeat center;\n+    z-index: 99999;\n+}\n+\n+#loader3 {\n+    width: 100%;\n+    height: 100vh;\n+    background: #F5F5F5 url('../img/loader3.gif') no-repeat center;\n     z-index: 99999;\n }\n "
            },
            {
                "sha": "9ce5ed28b0ef218e122c400455a205387a94d29c",
                "filename": "django/bdaSite/bdaProject/static/img/.DS_Store",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/static/img/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/static/img/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/img/.DS_Store?ref=907881cb0de06534a8d0dee4acead8bac62a96af"
            },
            {
                "sha": "61c2566644d0be35d44cbcce332f9b2405785098",
                "filename": "django/bdaSite/bdaProject/static/img/loader2.gif",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/static/img/loader2.gif",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/static/img/loader2.gif",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/img/loader2.gif?ref=907881cb0de06534a8d0dee4acead8bac62a96af"
            },
            {
                "sha": "5ed9c51332b344b968df8cec7f7e85076bc85ef7",
                "filename": "django/bdaSite/bdaProject/static/img/loader3.gif",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/static/img/loader3.gif",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/static/img/loader3.gif",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/img/loader3.gif?ref=907881cb0de06534a8d0dee4acead8bac62a96af"
            },
            {
                "sha": "32a08657fefcc176984fa91fe61ee00e4a1face6",
                "filename": "django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "status": "modified",
                "additions": 5,
                "deletions": 8,
                "changes": 13,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/bdaProject/index.html?ref=907881cb0de06534a8d0dee4acead8bac62a96af",
                "patch": "@@ -3,6 +3,7 @@\n <html lang=\"en\" dir=\"ltr\">\n \n <head>\n+    <title>Welcome - GitHub Analyser</title>\n     <meta charset=\"utf-8\">\n     <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n     <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\" />\n@@ -17,7 +18,6 @@\n             src: url({% static 'fonts/HomepageBaukasten-Book.ttf' %});\n         }\n     </style>\n-    <title></title>\n </head>\n \n <body id='body' style=\"font-family:R; background-color: #00171F;\">\n@@ -60,15 +60,12 @@ <h4 style=\"font-weight:bold;margin-top:10px;padding:0px;font-size:150%;line-heig\n                 $('#loader').fadeOut();\n             });\n \n-            $('#search').click( function() {\n+            $('#search').click(function() {\n                 $.term = $('#org_input').val()\n-                if($.term == '' || $.term == null || $.term == undefined)\n-                {\n+                if ($.term == '' || $.term == null || $.term == undefined) {\n                     $('#error').text(\"Please Enter an Organization Name to continue:\").css(\"color\", \"red\");\n-                }\n-                else\n-                {\n-                    location.assign('org/'+$.term);\n+                } else {\n+                    location.assign('org/' + $.term);\n                 }\n             });\n "
            },
            {
                "sha": "47913391331cfe17605315a8116498c33bd5da78",
                "filename": "django/bdaSite/bdaProject/templates/bdaProject/organization.html",
                "status": "modified",
                "additions": 11,
                "deletions": 14,
                "changes": 25,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/templates/bdaProject/organization.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/templates/bdaProject/organization.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/bdaProject/organization.html?ref=907881cb0de06534a8d0dee4acead8bac62a96af",
                "patch": "@@ -3,6 +3,7 @@\n <html lang=\"en\" dir=\"ltr\">\n \n <head>\n+    <title>{{ org_name }} - GitHub Analyser</title>\n     <meta charset=\"utf-8\">\n     <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n     <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\" />\n@@ -12,31 +13,27 @@\n             font-family: R;\n             src: url({% static 'fonts/HomepageBaukasten-Book.ttf' %});\n         }\n-        \n-        #loader {\n-            width: 100%;\n-            height: 100vh;\n-            background: #f5f5f5 url({% static 'img/loader.gif' %}) no-repeat center;\n-            z-index: 99999\n-        }\n     </style>\n-    <title></title>\n </head>\n \n-<body id='body' style=\"font-family:R; background-color: #2C3747;\">\n-\n-    <div id='loader'>\n+<body id='body' style=\"font-family:R; background-color: #08163F;\">\n \n-    </div>\n+    <!-- Loader  -->\n+    <div id='loader2'></div>\n \n     <!-- Navbar goes here -->\n     <nav>\n-        <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n+        <div id='nav' style=\"background:#050E28\" class=\"nav-wrapper\">\n             <a href=\"#\" class=\"brand-logo center\">GitHub Analyser</a>\n         </div>\n     </nav>\n \n     <!-- Page Layout here -->\n+    <div class=\"row\">\n+        <div class=\"col s12 m6 offset-m3 center white-text\">\n+            <h3>Organization: {{ org_name }}</h3>\n+        </div>\n+    </div>\n \n     <script src={% static 'js/jquery-3.1.1.js' %}></script>\n     <script src={% static 'js/materialize.min.js' %}></script>\n@@ -48,7 +45,7 @@\n         $('#col2').height(columnHeight);\n \n         $(window).on('load', function() {\n-            $('#loader').fadeOut();\n+            $('#loader2').fadeOut();\n         });\n     </script>\n </body>"
            },
            {
                "sha": "9a978f049c53265ea71ea3ad8cd0acd904d3d678",
                "filename": "django/bdaSite/bdaProject/urls.py",
                "status": "modified",
                "additions": 1,
                "deletions": 0,
                "changes": 1,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/urls.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/urls.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/urls.py?ref=907881cb0de06534a8d0dee4acead8bac62a96af",
                "patch": "@@ -3,4 +3,5 @@\n \n urlpatterns = [\n     path('', views.index, name='index'),\n+    path('org/<str:org>/', views.org, name='organization'),\n ]\n\\ No newline at end of file"
            },
            {
                "sha": "a85940eee69347ae4b7244b31f8637e128e68493",
                "filename": "django/bdaSite/bdaProject/views.py",
                "status": "modified",
                "additions": 7,
                "deletions": 0,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/views.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/907881cb0de06534a8d0dee4acead8bac62a96af/django/bdaSite/bdaProject/views.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/views.py?ref=907881cb0de06534a8d0dee4acead8bac62a96af",
                "patch": "@@ -6,3 +6,10 @@\n def index(request):\n     context = {}\n     return render(request, 'bdaProject/index.html', context)\n+\n+def org(request, org):\n+    \n+    context = {\n+        'org_name' : org\n+    }\n+    return render(request, 'bdaProject/organization.html', context)"
            }
        ]
    },
    {
        "sha": "8ed42d01441bfad02c9480fe6dcc3113ece811f5",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjhlZDQyZDAxNDQxYmZhZDAyYzk0ODBmZTZkY2MzMTEzZWNlODExZjU=",
        "commit": {
            "author": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T22:18:12Z"
            },
            "committer": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T22:18:12Z"
            },
            "message": "Merge branch 'master' of https://github.com/CUBigDataClass/Kode-Kallas",
            "tree": {
                "sha": "4e93ec44a829469184e24a52e6a19f7e0a32aef2",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/4e93ec44a829469184e24a52e6a19f7e0a32aef2"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/8ed42d01441bfad02c9480fe6dcc3113ece811f5",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/8ed42d01441bfad02c9480fe6dcc3113ece811f5",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/8ed42d01441bfad02c9480fe6dcc3113ece811f5",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/8ed42d01441bfad02c9480fe6dcc3113ece811f5/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "005dc357732a38e9dc49d389fd785e299f14a544",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/005dc357732a38e9dc49d389fd785e299f14a544",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/005dc357732a38e9dc49d389fd785e299f14a544"
            },
            {
                "sha": "31153feb806ae4b4ee12b3eb592de20355f7561e",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/31153feb806ae4b4ee12b3eb592de20355f7561e",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/31153feb806ae4b4ee12b3eb592de20355f7561e"
            }
        ],
        "stats": {
            "total": 35,
            "additions": 28,
            "deletions": 7
        },
        "files": [
            {
                "sha": "38c2b30d9334da350cbfff7e87edc02fe7a62d24",
                "filename": ".DS_Store",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/005dc357732a38e9dc49d389fd785e299f14a544/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/005dc357732a38e9dc49d389fd785e299f14a544/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/.DS_Store?ref=005dc357732a38e9dc49d389fd785e299f14a544"
            },
            {
                "sha": "e05088d56f4aca71bfc840bc9b869c75aebcf1ad",
                "filename": "github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "status": "modified",
                "additions": 23,
                "deletions": 3,
                "changes": 26,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/8ed42d01441bfad02c9480fe6dcc3113ece811f5/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/8ed42d01441bfad02c9480fe6dcc3113ece811f5/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py?ref=8ed42d01441bfad02c9480fe6dcc3113ece811f5",
                "patch": "@@ -1,5 +1,25 @@\n+import json\n+from elasticsearch import Elasticsearch\n+from ElasticSearchHelper import config as config\n+\n+es = Elasticsearch([{'host': config.ELASTIC_HOST, 'port': config.ELASTIC_PORT}])\n+\n+\n class ElasticSearchHelper():\n     def __init__(self):\n-    \n-    def getOrgName(self,orgname):\n-        return orgname\n\\ No newline at end of file\n+        pass\n+\n+    def getOrgData(self,orgname):\n+        res = es.search(index=orgname, body={\"query\": {\"match_all\": {}}})\n+        print(res)\n+        return json.dumps(res)\n+\n+    def getUserData(self,username):\n+        res = es.search(index=username, body={\"query\": {\"match_all\": {}}})\n+        print(res)\n+        return json.dumps(res)\n+\n+    def getRepoData(self,reponame):\n+        res = es.search(index=reponame, body={\"query\": {\"match_all\": {}}})\n+        print(res)\n+        return json.dumps(res)\n\\ No newline at end of file"
            },
            {
                "sha": "d11701f9db5497081903d6f4b5e67f006a3a81cf",
                "filename": "github-analytics/ElasticSearchHelper/config.py",
                "status": "added",
                "additions": 2,
                "deletions": 0,
                "changes": 2,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/8ed42d01441bfad02c9480fe6dcc3113ece811f5/github-analytics/ElasticSearchHelper/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/8ed42d01441bfad02c9480fe6dcc3113ece811f5/github-analytics/ElasticSearchHelper/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/config.py?ref=8ed42d01441bfad02c9480fe6dcc3113ece811f5",
                "patch": "@@ -0,0 +1,2 @@\n+ELASTIC_HOST = 'localhost'\n+ELASTIC_PORT = 9200\n\\ No newline at end of file"
            },
            {
                "sha": "5f2d050a9cca23dfde989ebac940d07263d85086",
                "filename": "github-analytics/app.py",
                "status": "modified",
                "additions": 3,
                "deletions": 4,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/8ed42d01441bfad02c9480fe6dcc3113ece811f5/github-analytics/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/8ed42d01441bfad02c9480fe6dcc3113ece811f5/github-analytics/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/app.py?ref=8ed42d01441bfad02c9480fe6dcc3113ece811f5",
                "patch": "@@ -13,18 +13,17 @@\n def home():\n     return 'Github Analytics - Use APIs to process data'\n \n-\n @app.route('/org/<orgname>')\n def org_retrive(orgname):\n-    return elasticSearchHelper.getOrgName(orgname)\n+    return elasticSearchHelper.getOrgData(orgname)\n \n @app.route('/repo/<reponame>')\n def repo_retrive(reponame):\n-    return reponame\n+    return elasticSearchHelper.getRepoData(reponame)\n \n @app.route('/user/<username>')\n def user_retrive(username):\n-    return username\n+    return elasticSearchHelper.getUserData(username)\n \n \n if __name__ == '__main__':"
            }
        ]
    },
    {
        "sha": "005dc357732a38e9dc49d389fd785e299f14a544",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjAwNWRjMzU3NzMyYTM4ZTlkYzQ5ZDM4OWZkNzg1ZTI5OWYxNGE1NDQ=",
        "commit": {
            "author": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T22:13:29Z"
            },
            "committer": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T22:13:29Z"
            },
            "message": "UI 3",
            "tree": {
                "sha": "9201c4dc0886ac95bcfcfb86b1f8db6c673d732f",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/9201c4dc0886ac95bcfcfb86b1f8db6c673d732f"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/005dc357732a38e9dc49d389fd785e299f14a544",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/005dc357732a38e9dc49d389fd785e299f14a544",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/005dc357732a38e9dc49d389fd785e299f14a544",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/005dc357732a38e9dc49d389fd785e299f14a544/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "116995791daecdabb510ed0db74f56843bbb7ca0",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/116995791daecdabb510ed0db74f56843bbb7ca0",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/116995791daecdabb510ed0db74f56843bbb7ca0"
            }
        ],
        "stats": {
            "total": 231,
            "additions": 205,
            "deletions": 26
        },
        "files": [
            {
                "sha": "538cc14087b95b4f542cf89ccfe2c922524f7c1d",
                "filename": "django/bdaSite/bdaProject/static/css/main.css",
                "status": "added",
                "additions": 14,
                "deletions": 0,
                "changes": 14,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/005dc357732a38e9dc49d389fd785e299f14a544/django/bdaSite/bdaProject/static/css/main.css",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/005dc357732a38e9dc49d389fd785e299f14a544/django/bdaSite/bdaProject/static/css/main.css",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/css/main.css?ref=005dc357732a38e9dc49d389fd785e299f14a544",
                "patch": "@@ -0,0 +1,14 @@\n+#loader {\n+    width: 100%;\n+    height: 100vh;\n+    background: #f5f5f5 url('../img/loader.gif') no-repeat center;\n+    z-index: 99999;\n+}\n+\n+#con {\n+    display: flex;\n+    justify-content: center;\n+    align-items: center;\n+    width: 100vw;\n+    height: 100vh;\n+}\n\\ No newline at end of file"
            },
            {
                "sha": "977cf9d7e59d99946025906c104f12b529907345",
                "filename": "django/bdaSite/bdaProject/static/img/github.svg",
                "status": "added",
                "additions": 59,
                "deletions": 0,
                "changes": 59,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/005dc357732a38e9dc49d389fd785e299f14a544/django/bdaSite/bdaProject/static/img/github.svg",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/005dc357732a38e9dc49d389fd785e299f14a544/django/bdaSite/bdaProject/static/img/github.svg",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/img/github.svg?ref=005dc357732a38e9dc49d389fd785e299f14a544",
                "patch": "@@ -0,0 +1,59 @@\n+<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\r\n+<!-- Generator: Adobe Illustrator 16.0.0, SVG Export Plug-In . SVG Version: 6.00 Build 0)  -->\r\n+<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n+<svg version=\"1.1\" id=\"Capa_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\"\r\n+\t width=\"438.549px\" height=\"438.549px\" viewBox=\"0 0 438.549 438.549\" style=\"enable-background:new 0 0 438.549 438.549;\"\r\n+\t xml:space=\"preserve\">\r\n+<g>\r\n+\t<path d=\"M409.132,114.573c-19.608-33.596-46.205-60.194-79.798-79.8C295.736,15.166,259.057,5.365,219.271,5.365\r\n+\t\tc-39.781,0-76.472,9.804-110.063,29.408c-33.596,19.605-60.192,46.204-79.8,79.8C9.803,148.168,0,184.854,0,224.63\r\n+\t\tc0,47.78,13.94,90.745,41.827,128.906c27.884,38.164,63.906,64.572,108.063,79.227c5.14,0.954,8.945,0.283,11.419-1.996\r\n+\t\tc2.475-2.282,3.711-5.14,3.711-8.562c0-0.571-0.049-5.708-0.144-15.417c-0.098-9.709-0.144-18.179-0.144-25.406l-6.567,1.136\r\n+\t\tc-4.187,0.767-9.469,1.092-15.846,1c-6.374-0.089-12.991-0.757-19.842-1.999c-6.854-1.231-13.229-4.086-19.13-8.559\r\n+\t\tc-5.898-4.473-10.085-10.328-12.56-17.556l-2.855-6.57c-1.903-4.374-4.899-9.233-8.992-14.559\r\n+\t\tc-4.093-5.331-8.232-8.945-12.419-10.848l-1.999-1.431c-1.332-0.951-2.568-2.098-3.711-3.429c-1.142-1.331-1.997-2.663-2.568-3.997\r\n+\t\tc-0.572-1.335-0.098-2.43,1.427-3.289c1.525-0.859,4.281-1.276,8.28-1.276l5.708,0.853c3.807,0.763,8.516,3.042,14.133,6.851\r\n+\t\tc5.614,3.806,10.229,8.754,13.846,14.842c4.38,7.806,9.657,13.754,15.846,17.847c6.184,4.093,12.419,6.136,18.699,6.136\r\n+\t\tc6.28,0,11.704-0.476,16.274-1.423c4.565-0.952,8.848-2.383,12.847-4.285c1.713-12.758,6.377-22.559,13.988-29.41\r\n+\t\tc-10.848-1.14-20.601-2.857-29.264-5.14c-8.658-2.286-17.605-5.996-26.835-11.14c-9.235-5.137-16.896-11.516-22.985-19.126\r\n+\t\tc-6.09-7.614-11.088-17.61-14.987-29.979c-3.901-12.374-5.852-26.648-5.852-42.826c0-23.035,7.52-42.637,22.557-58.817\r\n+\t\tc-7.044-17.318-6.379-36.732,1.997-58.24c5.52-1.715,13.706-0.428,24.554,3.853c10.85,4.283,18.794,7.952,23.84,10.994\r\n+\t\tc5.046,3.041,9.089,5.618,12.135,7.708c17.705-4.947,35.976-7.421,54.818-7.421s37.117,2.474,54.823,7.421l10.849-6.849\r\n+\t\tc7.419-4.57,16.18-8.758,26.262-12.565c10.088-3.805,17.802-4.853,23.134-3.138c8.562,21.509,9.325,40.922,2.279,58.24\r\n+\t\tc15.036,16.18,22.559,35.787,22.559,58.817c0,16.178-1.958,30.497-5.853,42.966c-3.9,12.471-8.941,22.457-15.125,29.979\r\n+\t\tc-6.191,7.521-13.901,13.85-23.131,18.986c-9.232,5.14-18.182,8.85-26.84,11.136c-8.662,2.286-18.415,4.004-29.263,5.146\r\n+\t\tc9.894,8.562,14.842,22.077,14.842,40.539v60.237c0,3.422,1.19,6.279,3.572,8.562c2.379,2.279,6.136,2.95,11.276,1.995\r\n+\t\tc44.163-14.653,80.185-41.062,108.068-79.226c27.88-38.161,41.825-81.126,41.825-128.906\r\n+\t\tC438.536,184.851,428.728,148.168,409.132,114.573z\"/>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+<g>\r\n+</g>\r\n+</svg>\r"
            },
            {
                "sha": "0091e5742fc6aa5d98ed76d3174beea517a6afed",
                "filename": "django/bdaSite/bdaProject/static/img/loader.gif",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/005dc357732a38e9dc49d389fd785e299f14a544/django/bdaSite/bdaProject/static/img/loader.gif",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/005dc357732a38e9dc49d389fd785e299f14a544/django/bdaSite/bdaProject/static/img/loader.gif",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/img/loader.gif?ref=005dc357732a38e9dc49d389fd785e299f14a544"
            },
            {
                "sha": "9dc57cffe36e595c8f17e900990ac934159bfc49",
                "filename": "django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "status": "modified",
                "additions": 76,
                "deletions": 26,
                "changes": 102,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/005dc357732a38e9dc49d389fd785e299f14a544/django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/005dc357732a38e9dc49d389fd785e299f14a544/django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/bdaProject/index.html?ref=005dc357732a38e9dc49d389fd785e299f14a544",
                "patch": "@@ -1,35 +1,85 @@\n {% load static %}\n <!DOCTYPE html>\n <html lang=\"en\" dir=\"ltr\">\n-  <head>\n+\n+<head>\n     <meta charset=\"utf-8\">\n     <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n-    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n+    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\" />\n+    <link href={% static 'css/main.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\" />\n     <style>\n-     @font-face\n-     { font-family: R; src: url({% static 'fonts/HomepageBaukasten-Book.ttf' %}); }\n-\n+         ::placeholder {\n+            color: black;\n+        }\n+        \n+        @font-face {\n+            font-family: R;\n+            src: url({% static 'fonts/HomepageBaukasten-Book.ttf' %});\n+        }\n     </style>\n     <title></title>\n-  </head>\n-  <body id='body' style=\"font-family:R\">\n-    <!-- Navbar goes here -->\n-    <nav>\n-      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n-        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform</a>\n-      </div>\n-    </nav>\n-\n-    <!-- Page Layout here -->\n-  \n-    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n-    <script src={% static 'js/materialize.min.js' %}></script>\n-    <script src={% static 'js/chart.min.js' %}></script>\n-    <script type=\"text/javascript\">\n-      navHeight = $('#nav').height();\n-      columnHeight = (window.innerHeight - navHeight);\n-      $('#col1').height(columnHeight);\n-      $('#col2').height(columnHeight);\n-    </script>\n-  </body>\n+</head>\n+\n+<body id='body' style=\"font-family:R; background-color: #00171F;\">\n+\n+    <!-- Loader -->\n+    <div id='loader'></div>\n+\n+    <div id=\"con\">\n+        <div class=\"container z-depth-3\" style=\"background-color:#E5F3FF;width:60vw;height:60vh;outline:0;border-radius: 90px;\">\n+            <div class=\"row\">\n+                <div class=\"center\">\n+                    <br><br><br>\n+                    <img src=\"{% static 'img/github.svg' %}\" style=\"width:100px;height:100px\">\n+                    <h4 style=\"font-weight:bold;margin:10px;padding:0px;font-size:220%;letter-spacing:3pt;line-height:30px\">\n+                        Welcome to GitHub Analyser\n+                    </h4>\n+                    <h4 id='error' style=\"font-weight:bold;margin:10px;padding:0px;font-size:150%;line-height:30px\">\n+                        Enter an Organization Name to continue:\n+                    </h4>\n+                    <h4 style=\"font-weight:bold;margin-top:10px;padding:0px;font-size:150%;line-height:30px\">\n+                        <input id='org_input' type=\"text\" placeholder=\"Type to search\" class=\"center\" style=\"width:40%; color: black; placeholder: black; margin: 10px;\">\n+                        <br>\n+                        <btn id='search' type=\"submit\" class=\"btn-floating btn-large waves-effect waves-light\" style=\"background-color: #00171F;\"><i class=\"material-icons\">search</i></btn>\n+                    </h4>\n+                </div>\n+\n+            </div>\n+        </div>\n+\n+        <script src={% static 'js/jquery-3.1.1.js' %}></script>\n+        <script src={% static 'js/materialize.min.js' %}></script>\n+        <script src={% static 'js/chart.min.js' %}></script>\n+        <script type=\" text/javascript \">\n+            navHeight = $('#nav').height();\n+            columnHeight = (window.innerHeight - navHeight);\n+            $('#col1').height(columnHeight);\n+            $('#col2').height(columnHeight);\n+\n+            $(window).on('load', function() {\n+                $('#loader').fadeOut();\n+            });\n+\n+            $('#search').click( function() {\n+                $.term = $('#org_input').val()\n+                if($.term == '' || $.term == null || $.term == undefined)\n+                {\n+                    $('#error').text(\"Please Enter an Organization Name to continue:\").css(\"color\", \"red\");\n+                }\n+                else\n+                {\n+                    location.assign('org/'+$.term);\n+                }\n+            });\n+\n+            $(\"#org_input\").keyup(function(event) {\n+                if (event.keyCode === 13) {\n+                    $(\"#search\").click();\n+                }\n+            });\n+        </script>\n+</body>\n+\n </html>\n+\n+</html>\n\\ No newline at end of file"
            },
            {
                "sha": "98adbce9b273d30a9947807b807c99b92ab2c2da",
                "filename": "django/bdaSite/bdaProject/templates/bdaProject/organization.html",
                "status": "added",
                "additions": 56,
                "deletions": 0,
                "changes": 56,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/005dc357732a38e9dc49d389fd785e299f14a544/django/bdaSite/bdaProject/templates/bdaProject/organization.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/005dc357732a38e9dc49d389fd785e299f14a544/django/bdaSite/bdaProject/templates/bdaProject/organization.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/bdaProject/organization.html?ref=005dc357732a38e9dc49d389fd785e299f14a544",
                "patch": "@@ -0,0 +1,56 @@\n+{% load static %}\n+<!DOCTYPE html>\n+<html lang=\"en\" dir=\"ltr\">\n+\n+<head>\n+    <meta charset=\"utf-8\">\n+    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n+    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\" />\n+    <link href={% static 'css/main.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\" />\n+    <style>\n+        @font-face {\n+            font-family: R;\n+            src: url({% static 'fonts/HomepageBaukasten-Book.ttf' %});\n+        }\n+        \n+        #loader {\n+            width: 100%;\n+            height: 100vh;\n+            background: #f5f5f5 url({% static 'img/loader.gif' %}) no-repeat center;\n+            z-index: 99999\n+        }\n+    </style>\n+    <title></title>\n+</head>\n+\n+<body id='body' style=\"font-family:R; background-color: #2C3747;\">\n+\n+    <div id='loader'>\n+\n+    </div>\n+\n+    <!-- Navbar goes here -->\n+    <nav>\n+        <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n+            <a href=\"#\" class=\"brand-logo center\">GitHub Analyser</a>\n+        </div>\n+    </nav>\n+\n+    <!-- Page Layout here -->\n+\n+    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n+    <script src={% static 'js/materialize.min.js' %}></script>\n+    <script src={% static 'js/chart.min.js' %}></script>\n+    <script type=\"text/javascript\">\n+        navHeight = $('#nav').height();\n+        columnHeight = (window.innerHeight - navHeight);\n+        $('#col1').height(columnHeight);\n+        $('#col2').height(columnHeight);\n+\n+        $(window).on('load', function() {\n+            $('#loader').fadeOut();\n+        });\n+    </script>\n+</body>\n+\n+</html>\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "31153feb806ae4b4ee12b3eb592de20355f7561e",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjMxMTUzZmViODA2YWU0YjRlZTEyYjNlYjU5MmRlMjAzNTVmNzU2MWU=",
        "commit": {
            "author": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-16T07:50:05Z"
            },
            "committer": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-16T07:50:05Z"
            },
            "message": "Merge remote-tracking branch 'refs/remotes/origin/master'",
            "tree": {
                "sha": "3f0b17b46cae33318655689febef7ed47b3d5e49",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/3f0b17b46cae33318655689febef7ed47b3d5e49"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/31153feb806ae4b4ee12b3eb592de20355f7561e",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/31153feb806ae4b4ee12b3eb592de20355f7561e",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/31153feb806ae4b4ee12b3eb592de20355f7561e",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/31153feb806ae4b4ee12b3eb592de20355f7561e/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "5041465f33f67981aeb297f60c985414ac34481f",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/5041465f33f67981aeb297f60c985414ac34481f",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/5041465f33f67981aeb297f60c985414ac34481f"
            },
            {
                "sha": "2db13188e3be11adb9b2b63868fa0a2ec41956e6",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/2db13188e3be11adb9b2b63868fa0a2ec41956e6",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/2db13188e3be11adb9b2b63868fa0a2ec41956e6"
            }
        ],
        "stats": {
            "total": 418,
            "additions": 40,
            "deletions": 378
        },
        "files": [
            {
                "sha": "275a113e16335210def75f91320c2efc184ebf54",
                "filename": "django/.DS_Store",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/.DS_Store?ref=31153feb806ae4b4ee12b3eb592de20355f7561e"
            },
            {
                "sha": "b2d7d6592a36fb06c1b9651c07f1732412140302",
                "filename": "django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc?ref=31153feb806ae4b4ee12b3eb592de20355f7561e"
            },
            {
                "sha": "6dff987ab1fe0796006b6985024176363b001cee",
                "filename": "django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc?ref=31153feb806ae4b4ee12b3eb592de20355f7561e"
            },
            {
                "sha": "df3d10193b9b486356f5d15c3fbfe599a7d25aad",
                "filename": "django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc?ref=31153feb806ae4b4ee12b3eb592de20355f7561e"
            },
            {
                "sha": "af79c2749820ca2bdb5d8abae09b0351f7a635b6",
                "filename": "django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc?ref=31153feb806ae4b4ee12b3eb592de20355f7561e"
            },
            {
                "sha": "44d9377d95b83303e59424ab1d602b73747ff2f6",
                "filename": "django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc?ref=31153feb806ae4b4ee12b3eb592de20355f7561e"
            },
            {
                "sha": "52cfb2d0f7affd5fd165c53e2691ddf0c4fdc290",
                "filename": "django/bdaSite/bdaProject/static/.DS_Store",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/static/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/static/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/.DS_Store?ref=31153feb806ae4b4ee12b3eb592de20355f7561e"
            },
            {
                "sha": "886ab67a581247fe732635e5c0c672fd1e52e770",
                "filename": "django/bdaSite/bdaProject/static/fonts/HomepageBaukasten-Book.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/static/fonts/HomepageBaukasten-Book.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/static/fonts/HomepageBaukasten-Book.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/HomepageBaukasten-Book.ttf?ref=31153feb806ae4b4ee12b3eb592de20355f7561e"
            },
            {
                "sha": "6f3ab754446ed2e240a21ada98c9c75b65019658",
                "filename": "django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "status": "added",
                "additions": 35,
                "deletions": 0,
                "changes": 35,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/bdaProject/index.html?ref=31153feb806ae4b4ee12b3eb592de20355f7561e",
                "patch": "@@ -0,0 +1,35 @@\n+{% load static %}\n+<!DOCTYPE html>\n+<html lang=\"en\" dir=\"ltr\">\n+  <head>\n+    <meta charset=\"utf-8\">\n+    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n+    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n+    <style>\n+     @font-face\n+     { font-family: R; src: url({% static 'fonts/HomepageBaukasten-Book.ttf' %}); }\n+\n+    </style>\n+    <title></title>\n+  </head>\n+  <body id='body' style=\"font-family:R\">\n+    <!-- Navbar goes here -->\n+    <nav>\n+      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n+        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform</a>\n+      </div>\n+    </nav>\n+\n+    <!-- Page Layout here -->\n+  \n+    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n+    <script src={% static 'js/materialize.min.js' %}></script>\n+    <script src={% static 'js/chart.min.js' %}></script>\n+    <script type=\"text/javascript\">\n+      navHeight = $('#nav').height();\n+      columnHeight = (window.innerHeight - navHeight);\n+      $('#col1').height(columnHeight);\n+      $('#col2').height(columnHeight);\n+    </script>\n+  </body>\n+</html>"
            },
            {
                "sha": "b41ad27c60f86b852a9cf345885c611bbef7776c",
                "filename": "django/bdaSite/bdaProject/templates/iotdp/graph.html",
                "status": "removed",
                "additions": 0,
                "deletions": 129,
                "changes": 129,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/5041465f33f67981aeb297f60c985414ac34481f/django/bdaSite/bdaProject/templates/iotdp/graph.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/5041465f33f67981aeb297f60c985414ac34481f/django/bdaSite/bdaProject/templates/iotdp/graph.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/iotdp/graph.html?ref=5041465f33f67981aeb297f60c985414ac34481f",
                "patch": "@@ -1,129 +0,0 @@\n-{% load static %}\n-<!DOCTYPE html>\n-<html lang=\"en\" dir=\"ltr\">\n-  <head>\n-    <meta charset=\"utf-8\">\n-    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n-    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n-    <style>\n-     @font-face\n-     { font-family: R; src: url({% static 'fonts/Roboto/Roboto-Regular.ttf' %}); }\n-\n-    </style>\n-    <title></title>\n-  </head>\n-  <body id='body' style=\"font-family:R\">\n-    <!-- Navbar goes here -->\n-    <nav>\n-      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n-        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform</a>\n-      </div>\n-    </nav>\n-\n-    <!-- Page Layout here -->\n-    <div class=\"row\">\n-      <div id='col1' class=\"col s3\" style=\"background:#336699;\">\n-        <a href=\"/iotdp/{{ user }}\">\n-          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n-            OVERALL VIEW\n-          </div>\n-        </a>\n-        <a href=\"/iotdp/{{ user }}/graph\">\n-          <div class=\"col s12 m12 l12\" style=\"color:#17365D;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px;  background:#F1BA65\">\n-            GRAPH\n-          </div>\n-        </a>\n-        <a href=\"/iotdp/{{ user }}/logs\">\n-          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n-            LOGS\n-          </div>\n-        </a>\n-      </div>\n-\n-      <div id='col2' class=\"col s9\" style=\"background: #EBEBD3;\">\n-        <h4 class=\"center\">POWER CONSUMPTION IN THE LAST 30 MINUTES</h4>\n-        <canvas id=\"chart1\" class=\"\" style=\"width:100%;\"></canvas>\n-        <div class=\"row\">\n-          <div class=\"col m8 offset-m2\">\n-            <div class=\"card center\" style=\"background: #EBEBD3;\">\n-              <h5>Generated Power from Solar and other Sources: {{ gen_pow }} kW</h5>\n-            </div>\n-          </div>\n-          <div class=\"col m8 offset-m2\">\n-            <div class=\"card center\" style=\"background: #EBEBD3;\">\n-              <table>\n-                <tr>\n-                  <th>Small Appliances</th>\n-                  <td>Microwave, Dishwasher, Fridge</td>\n-                </tr>\n-                <tr>\n-                  <th>Big Appliances</th>\n-                  <td>Garage Door, Well, Barn, Furnaces</td>\n-                </tr>\n-                <tr>\n-                  <th>Rooms</th>\n-                  <td>Home Office, Wine Cellar, Living Room, Kitchens</td>\n-                </tr>\n-              </table>\n-            </div>\n-          </div>\n-        </div>\n-      </div>\n-\n-    </div>\n-\n-    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n-    <script src={% static 'js/materialize.min.js' %}></script>\n-    <script src={% static 'js/chart.min.js' %}></script>\n-    <script type=\"text/javascript\">\n-      navHeight = $('#nav').height();\n-      columnHeight = (window.innerHeight - navHeight);\n-      $('#col1').height(columnHeight);\n-      $('#col2').height(columnHeight);\n-    </script>\n-    <script type=\"text/javascript\">\n-      var ctx3 = document.getElementById(\"chart1\");\n-      var myChart = new Chart(ctx3, {\n-          type: 'bar',\n-          data: {\n-              labels: {% autoescape off %} {{ graph_labels }} {% endautoescape %},\n-              datasets: [{\n-                  label: 'Number of Units [kW]',\n-                  data: {% autoescape off %} {{ graph_data }} {% endautoescape %},\n-                  backgroundColor: [\n-                      '#FDB44B',\n-                      '#FDB44B',\n-                      '#FDB44B'\n-                  ],\n-                  borderColor: [\n-                      '#FDB44B',\n-                      '#FDB44B',\n-                      '#FDB44B'\n-                  ],\n-                  borderWidth: 1\n-              }]\n-          },\n-          options: {\n-              legend: {\n-                  labels: {\n-                      fontColor:\"black\"\n-                  }\n-              },\n-              scales: {\n-                xAxes: [{\n-                    ticks: {\n-                        fontColor:\"black\"\n-                    }\n-                }],\n-                yAxes: [{\n-                    ticks: {\n-                        beginAtZero:true,\n-                        fontColor:\"black\"\n-                    }\n-                }]\n-              }\n-          }\n-      });\n-    </script>\n-  </body>\n-</html>"
            },
            {
                "sha": "ab918e5c83d9f39a659db66c4957aa57ccfcd967",
                "filename": "django/bdaSite/bdaProject/templates/iotdp/index.html",
                "status": "removed",
                "additions": 0,
                "deletions": 170,
                "changes": 170,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/5041465f33f67981aeb297f60c985414ac34481f/django/bdaSite/bdaProject/templates/iotdp/index.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/5041465f33f67981aeb297f60c985414ac34481f/django/bdaSite/bdaProject/templates/iotdp/index.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/iotdp/index.html?ref=5041465f33f67981aeb297f60c985414ac34481f",
                "patch": "@@ -1,170 +0,0 @@\n-{% load static %}\n-<!DOCTYPE html>\n-<html lang=\"en\" dir=\"ltr\">\n-  <head>\n-    <meta charset=\"utf-8\">\n-    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n-    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n-    <style>\n-     @font-face\n-     { font-family: R; src: url({% static 'fonts/Roboto/Roboto-Regular.ttf' %}); }\n-\n-    </style>\n-    <title></title>\n-  </head>\n-  <body id='body' style=\"font-family:R\">\n-    <!-- Navbar goes here -->\n-    <nav>\n-      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n-        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform</a>\n-      </div>\n-    </nav>\n-\n-    <!-- Page Layout here -->\n-    <div class=\"row\">\n-\n-      <div id='col1' class=\"col s3\" style=\"background:#336699;\">\n-        <a href=\"/iotdp/{{ user }}\">\n-          <div class=\"col s12 m12 l12\" style=\"color:#17365D;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; background:#F1BA65; padding:0px\">\n-            OVERALL VIEW\n-          </div>\n-        </a>\n-        <a href=\"/iotdp/{{ user }}/graph\">\n-          <div class=\"col s12 m12 l12 white-text\" style=\"display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px\">\n-            GRAPH\n-          </div>\n-        </a>\n-        <a href=\"/iotdp/{{ user }}/logs\">\n-          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n-            LOGS\n-          </div>\n-        </a>\n-      </div>\n-\n-      <div id='col2' class=\"col s9 center\" style=\"background: #EBEBD3;\">\n-        <div style=\"width:100%; overflow-x:scroll\">\n-          <table style=\"\">\n-            <thead id='tableHead'>\n-              <th>Timestamp</th>\n-              <th>Use</th>\n-              <th>General</th>\n-              <th>House Overall</th>\n-              <th>Dishwasher</th>\n-              <th>Furnace1</th>\n-              <th>Furnace2</th>\n-              <th>Home Office</th>\n-              <th>Fridge</th>\n-              <th>Wine Cellar</th>\n-              <th>Garage Door</th>\n-              <th>Kitchen 12</th>\n-              <th>Kitchen 14</th>\n-              <th>Kitchen 38</th>\n-              <th>Barn</th>\n-              <th>Well</th>\n-              <th>Microwave</th>\n-              <th>Living Room</th>\n-              <th>Solar</th>\n-              <th>Temperature</th>\n-              <th>Icon</th>\n-              <th>Humidity</th>\n-              <th>Visibility</th>\n-              <th>Summary</th>\n-              <th>Apparent Temperature</th>\n-              <th>Pressure</th>\n-              <th>Wind Speed</th>\n-              <th>Cloud Cover</th>\n-              <th>Wind Bearing</th>\n-              <th>Precipitation Intensity</th>\n-              <th>Dew Point</th>\n-              <th>Precipitation Porbab</th>\n-            </thead>\n-            <tbody id='tableBody'>\n-\n-            </tbody>\n-          </table>\n-          <a id='prev' class=\"btn-floating\" style=\"background: #17365D\"><i class=\"material-icons\">keyboard_arrow_left</i></a>\n-          <a id='next' class=\"btn-floating\" style=\"background: #17365D\"><i class=\"material-icons\">keyboard_arrow_right</i></a>\n-        </div>\n-      </div>\n-\n-    </div>\n-\n-    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n-    <script src={% static 'js/materialize.min.js' %}></script>\n-    <script src={% static 'js/chart.min.js' %}></script>\n-    <script type=\"text/javascript\">\n-      navHeight = $('#nav').height();\n-      columnHeight = (window.innerHeight - navHeight);\n-      $('#col1').height(columnHeight);\n-      $('#col2').height(columnHeight);\n-    </script>\n-    <script type=\"text/javascript\">\n-        current_index = 0;\n-        size = {{ size }};\n-        function dataUpdate(current_index)\n-        {\n-          $.get('http://35.230.40.217:5000/get_data/{{ user }}/'+current_index, function(data, status) {\n-            for(i=0; i<data['result'].length; i++)\n-            {\n-              row = \"<tr>\";\n-              date = new Date(parseInt(data['result'][i]['time'])*1000).toLocaleString();\n-              row += \"<td>\"+date+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['use1']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['gen']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['house_overall']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['dishwasher']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['furnace1']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['furnace2']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['home_office']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['fridge']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['wine_cellar']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['garage_door']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['kitchen12']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['kitchen14']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['kitchen38']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['barn']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['well']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['microwave']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['living_room']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['solar']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['temperature']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+data['result'][i]['icon']+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['humidity']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['visibility']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+data['result'][i]['summary']+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['apparent_temp']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['pressure']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['wind_speed']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+data['result'][i]['cloud_cover']+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['wind_bearing']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['precip_intensity']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['dew_point']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['precip_probability']* 10000)) / 10000)+\"</td>\";\n-              row += \"</tr>\";\n-              $('#tableBody').append(row);\n-            }\n-          });\n-        }\n-\n-        dataUpdate(0);\n-        // alert(size);\n-        $('#prev').click(function(){\n-            if(current_index > 0)\n-            {\n-              $('#tableBody').empty();\n-              current_index = current_index - 7;\n-              dataUpdate(current_index);\n-            }\n-        });\n-        $('#next').click(function(){\n-            if (current_index <= size-7)\n-            {\n-              $('#tableBody').empty();\n-              current_index = current_index + 7;\n-              dataUpdate(current_index);\n-            }\n-        });\n-\n-    </script>\n-  </body>\n-</html>"
            },
            {
                "sha": "0df695c397fe3d580ee82eb06165abce940e55cf",
                "filename": "django/bdaSite/bdaProject/templates/iotdp/logs.html",
                "status": "removed",
                "additions": 0,
                "deletions": 74,
                "changes": 74,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/5041465f33f67981aeb297f60c985414ac34481f/django/bdaSite/bdaProject/templates/iotdp/logs.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/5041465f33f67981aeb297f60c985414ac34481f/django/bdaSite/bdaProject/templates/iotdp/logs.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/iotdp/logs.html?ref=5041465f33f67981aeb297f60c985414ac34481f",
                "patch": "@@ -1,74 +0,0 @@\n-{% load static %}\n-<!DOCTYPE html>\n-<html lang=\"en\" dir=\"ltr\">\n-  <head>\n-    <meta charset=\"utf-8\">\n-    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n-    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n-    <style>\n-     @font-face\n-     { font-family: R; src: url({% static 'fonts/Roboto/Roboto-Regular.ttf' %}); }\n-\n-    </style>\n-    <script src=\"{% static 'js/terminal.js' %}\"></script>\n-    <script type=\"text/javascript\">\n-\n-    </script>\n-    <title></title>\n-  </head>\n-  <body id='body' style=\"font-family:R\">\n-    <!-- Navbar goes here -->\n-    <nav>\n-      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n-        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform - Logs</a>\n-      </div>\n-    </nav>\n-\n-    <div class=\"row\">\n-      <div id='col1' class=\"col s3\" style=\"background:#336699;\">\n-        <a href=\"/iotdp/{{ user }}\">\n-          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n-            OVERALL VIEW\n-          </div>\n-        </a>\n-        <a href=\"/iotdp/{{ user }}/graph\">\n-          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px;\">\n-            GRAPH\n-          </div>\n-        </a>\n-        <a href=\"/iotdp/{{ user }}/logs\">\n-          <div class=\"col s12 m12 l12\" style=\"color:#17365D;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px;  background:#F1BA65\">\n-            LOGS\n-          </div>\n-        </a>\n-      </div>\n-\n-      <div id='col2' class=\"col s9 black\" style=\"height:750px;background: #EBEBD3; margin:0px; padding:0px; overflow-y:scroll\">\n-        <div id='term' class=\"row\" style=\"height:750px;display:flex; margin:0px; padding:0px; overflow-y:scroll;\">\n-\n-        </div>\n-      </div>\n-\n-    <!-- Page Layout here -->\n-\n-\n-    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n-    <script src={% static 'js/materialize.min.js' %}></script>\n-    <script src={% static 'js/chart.min.js' %}></script>\n-    <script type=\"text/javascript\">\n-      navHeight = $('#nav').height();\n-      columnHeight = (window.innerHeight - navHeight);\n-      $('#col1').height(columnHeight);\n-    </script>\n-    <script type=\"text/javascript\">\n-      var term = new Terminal();\n-      var logs = {% autoescape off %} {{ logs }} {% endautoescape %}\n-      var logLines = logs.split(';;;')\n-      for(var i=0; i<logLines.length; i++)\n-      {\n-          term.print(logLines[i]);\n-      }\n-      $('#term').append(term.html)\n-    </script>\n-  </body>\n-</html>"
            },
            {
                "sha": "c486297ddccdfdf774fe9b8e0dc4584043b02652",
                "filename": "django/bdaSite/bdaProject/urls.py",
                "status": "modified",
                "additions": 0,
                "deletions": 1,
                "changes": 1,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/urls.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/urls.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/urls.py?ref=31153feb806ae4b4ee12b3eb592de20355f7561e",
                "patch": "@@ -3,5 +3,4 @@\n \n urlpatterns = [\n     path('', views.index, name='index'),\n-    path('trial/', views.trial, name='trial')\n ]\n\\ No newline at end of file"
            },
            {
                "sha": "84da6ec3082bd1c158fd02b6415e30cd9ee22493",
                "filename": "django/bdaSite/bdaProject/views.py",
                "status": "modified",
                "additions": 3,
                "deletions": 4,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/views.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaProject/views.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/views.py?ref=31153feb806ae4b4ee12b3eb592de20355f7561e",
                "patch": "@@ -1,9 +1,8 @@\n from django.shortcuts import render\n from django.http import HttpResponse\n+from django.template import loader\n \n \n def index(request):\n-    return HttpResponse(\"Hello, world. You're at the bdaProject index.\")\n-\n-def trial(request):\n-    return HttpResponse(\"Trial\")\n+    context = {}\n+    return render(request, 'bdaProject/index.html', context)"
            },
            {
                "sha": "1a0871e09e88291d2cce6e6a409a2decdcc3550b",
                "filename": "django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc?ref=31153feb806ae4b4ee12b3eb592de20355f7561e"
            },
            {
                "sha": "048412ac61833b220350fa1182176f443702497c",
                "filename": "django/bdaSite/bdaSite/settings.py",
                "status": "modified",
                "additions": 2,
                "deletions": 0,
                "changes": 2,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaSite/settings.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/bdaSite/settings.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/settings.py?ref=31153feb806ae4b4ee12b3eb592de20355f7561e",
                "patch": "@@ -37,6 +37,7 @@\n     'django.contrib.sessions',\n     'django.contrib.messages',\n     'django.contrib.staticfiles',\n+    'bdaProject'\n ]\n \n MIDDLEWARE = [\n@@ -118,3 +119,4 @@\n # https://docs.djangoproject.com/en/3.0/howto/static-files/\n \n STATIC_URL = '/static/'\n+"
            },
            {
                "sha": "3ed785da46cd65daa196bd65ab33f5a8c6ed2882",
                "filename": "django/bdaSite/db.sqlite3",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/db.sqlite3",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/31153feb806ae4b4ee12b3eb592de20355f7561e/django/bdaSite/db.sqlite3",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/db.sqlite3?ref=31153feb806ae4b4ee12b3eb592de20355f7561e"
            }
        ]
    },
    {
        "sha": "5041465f33f67981aeb297f60c985414ac34481f",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjUwNDE0NjVmMzNmNjc5ODFhZWIyOTdmNjBjOTg1NDE0YWMzNDQ4MWY=",
        "commit": {
            "author": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-16T07:49:46Z"
            },
            "committer": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-16T07:49:46Z"
            },
            "message": "Retrieving data from Elastic search",
            "tree": {
                "sha": "f1bfb631ade4d559c759d5483aaa4ae294758c58",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/f1bfb631ade4d559c759d5483aaa4ae294758c58"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/5041465f33f67981aeb297f60c985414ac34481f",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/5041465f33f67981aeb297f60c985414ac34481f",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/5041465f33f67981aeb297f60c985414ac34481f",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/5041465f33f67981aeb297f60c985414ac34481f/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "f312dc10571ddc19cdfaaa98edda7f6db6432aee",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/f312dc10571ddc19cdfaaa98edda7f6db6432aee",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/f312dc10571ddc19cdfaaa98edda7f6db6432aee"
            }
        ],
        "stats": {
            "total": 35,
            "additions": 28,
            "deletions": 7
        },
        "files": [
            {
                "sha": "e05088d56f4aca71bfc840bc9b869c75aebcf1ad",
                "filename": "github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "status": "modified",
                "additions": 23,
                "deletions": 3,
                "changes": 26,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/5041465f33f67981aeb297f60c985414ac34481f/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/5041465f33f67981aeb297f60c985414ac34481f/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py?ref=5041465f33f67981aeb297f60c985414ac34481f",
                "patch": "@@ -1,5 +1,25 @@\n+import json\n+from elasticsearch import Elasticsearch\n+from ElasticSearchHelper import config as config\n+\n+es = Elasticsearch([{'host': config.ELASTIC_HOST, 'port': config.ELASTIC_PORT}])\n+\n+\n class ElasticSearchHelper():\n     def __init__(self):\n-    \n-    def getOrgName(self,orgname):\n-        return orgname\n\\ No newline at end of file\n+        pass\n+\n+    def getOrgData(self,orgname):\n+        res = es.search(index=orgname, body={\"query\": {\"match_all\": {}}})\n+        print(res)\n+        return json.dumps(res)\n+\n+    def getUserData(self,username):\n+        res = es.search(index=username, body={\"query\": {\"match_all\": {}}})\n+        print(res)\n+        return json.dumps(res)\n+\n+    def getRepoData(self,reponame):\n+        res = es.search(index=reponame, body={\"query\": {\"match_all\": {}}})\n+        print(res)\n+        return json.dumps(res)\n\\ No newline at end of file"
            },
            {
                "sha": "d11701f9db5497081903d6f4b5e67f006a3a81cf",
                "filename": "github-analytics/ElasticSearchHelper/config.py",
                "status": "added",
                "additions": 2,
                "deletions": 0,
                "changes": 2,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/5041465f33f67981aeb297f60c985414ac34481f/github-analytics/ElasticSearchHelper/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/5041465f33f67981aeb297f60c985414ac34481f/github-analytics/ElasticSearchHelper/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/config.py?ref=5041465f33f67981aeb297f60c985414ac34481f",
                "patch": "@@ -0,0 +1,2 @@\n+ELASTIC_HOST = 'localhost'\n+ELASTIC_PORT = 9200\n\\ No newline at end of file"
            },
            {
                "sha": "5f2d050a9cca23dfde989ebac940d07263d85086",
                "filename": "github-analytics/app.py",
                "status": "modified",
                "additions": 3,
                "deletions": 4,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/5041465f33f67981aeb297f60c985414ac34481f/github-analytics/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/5041465f33f67981aeb297f60c985414ac34481f/github-analytics/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/app.py?ref=5041465f33f67981aeb297f60c985414ac34481f",
                "patch": "@@ -13,18 +13,17 @@\n def home():\n     return 'Github Analytics - Use APIs to process data'\n \n-\n @app.route('/org/<orgname>')\n def org_retrive(orgname):\n-    return elasticSearchHelper.getOrgName(orgname)\n+    return elasticSearchHelper.getOrgData(orgname)\n \n @app.route('/repo/<reponame>')\n def repo_retrive(reponame):\n-    return reponame\n+    return elasticSearchHelper.getRepoData(reponame)\n \n @app.route('/user/<username>')\n def user_retrive(username):\n-    return username\n+    return elasticSearchHelper.getUserData(username)\n \n \n if __name__ == '__main__':"
            }
        ]
    },
    {
        "sha": "2db13188e3be11adb9b2b63868fa0a2ec41956e6",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjJkYjEzMTg4ZTNiZTExYWRiOWIyYjYzODY4ZmEwYTJlYzQxOTU2ZTY=",
        "commit": {
            "author": {
                "name": "sish2654",
                "email": "55568643+sish2654@users.noreply.github.com",
                "date": "2020-04-16T06:15:45Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-16T06:15:45Z"
            },
            "message": "Delete .DS_Store",
            "tree": {
                "sha": "00a470a683d56ae21cfe3d7a20ba1e85bcd335bf",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/00a470a683d56ae21cfe3d7a20ba1e85bcd335bf"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/2db13188e3be11adb9b2b63868fa0a2ec41956e6",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJel/gRCRBK7hj4Ov3rIwAAdHIIAI9teS67hLVD9YfznEtyMi4J\n/S7ssYelGviZFuf69qIA69BP4A6UoQ4hASj9zha+1a7Ig0QRapKiUw35Xd7l/KXH\nMPrbh9xlJuKk6FcvfmHDv+kLKTP72U0hvmw8yo4ndLsiCg2hYhx//rXLSKRWokrv\nS+gOXnLVTakvH1wiDqVjVUwHOHUr7gCV5uSUqTQGGGW6KYToJhA46k5u6k4EcnKx\nxQhI93JnO5kwDOc0iLG0w+zZxOk5aBUpXd0FfY8Uueq16JHAfK+TPQy86ViayswL\noIjDYnpheY0DId2ZZZQNP4vjWkY/wp0jzWezqDtVWUqGztQGfH5nYGoiUPY8qQg=\n=ur3Z\n-----END PGP SIGNATURE-----\n",
                "payload": "tree 00a470a683d56ae21cfe3d7a20ba1e85bcd335bf\nparent 116995791daecdabb510ed0db74f56843bbb7ca0\nauthor sish2654 <55568643+sish2654@users.noreply.github.com> 1587017745 -0600\ncommitter GitHub <noreply@github.com> 1587017745 -0600\n\nDelete .DS_Store"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/2db13188e3be11adb9b2b63868fa0a2ec41956e6",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/2db13188e3be11adb9b2b63868fa0a2ec41956e6",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/2db13188e3be11adb9b2b63868fa0a2ec41956e6/comments",
        "author": {
            "login": "sish2654",
            "id": 55568643,
            "node_id": "MDQ6VXNlcjU1NTY4NjQz",
            "avatar_url": "https://avatars1.githubusercontent.com/u/55568643?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sish2654",
            "html_url": "https://github.com/sish2654",
            "followers_url": "https://api.github.com/users/sish2654/followers",
            "following_url": "https://api.github.com/users/sish2654/following{/other_user}",
            "gists_url": "https://api.github.com/users/sish2654/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sish2654/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sish2654/subscriptions",
            "organizations_url": "https://api.github.com/users/sish2654/orgs",
            "repos_url": "https://api.github.com/users/sish2654/repos",
            "events_url": "https://api.github.com/users/sish2654/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sish2654/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "116995791daecdabb510ed0db74f56843bbb7ca0",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/116995791daecdabb510ed0db74f56843bbb7ca0",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/116995791daecdabb510ed0db74f56843bbb7ca0"
            }
        ],
        "stats": {
            "total": 0,
            "additions": 0,
            "deletions": 0
        },
        "files": [
            {
                "sha": "38c2b30d9334da350cbfff7e87edc02fe7a62d24",
                "filename": ".DS_Store",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/116995791daecdabb510ed0db74f56843bbb7ca0/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/116995791daecdabb510ed0db74f56843bbb7ca0/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/.DS_Store?ref=116995791daecdabb510ed0db74f56843bbb7ca0"
            }
        ]
    },
    {
        "sha": "116995791daecdabb510ed0db74f56843bbb7ca0",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjExNjk5NTc5MWRhZWNkYWJiNTEwZWQwZGI3NGY1Njg0M2JiYjdjYTA=",
        "commit": {
            "author": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T06:14:34Z"
            },
            "committer": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T06:14:34Z"
            },
            "message": "Merge branch 'master' of https://github.com/CUBigDataClass/Kode-Kallas",
            "tree": {
                "sha": "96c95e34eca2cc33275dc09818490920484a9aed",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/96c95e34eca2cc33275dc09818490920484a9aed"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/116995791daecdabb510ed0db74f56843bbb7ca0",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/116995791daecdabb510ed0db74f56843bbb7ca0",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/116995791daecdabb510ed0db74f56843bbb7ca0",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/116995791daecdabb510ed0db74f56843bbb7ca0/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "2990dd361573c2a3e3d88fb56cc92780e8dfc8de",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/2990dd361573c2a3e3d88fb56cc92780e8dfc8de",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/2990dd361573c2a3e3d88fb56cc92780e8dfc8de"
            },
            {
                "sha": "f312dc10571ddc19cdfaaa98edda7f6db6432aee",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/f312dc10571ddc19cdfaaa98edda7f6db6432aee",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/f312dc10571ddc19cdfaaa98edda7f6db6432aee"
            }
        ],
        "stats": {
            "total": 67,
            "additions": 63,
            "deletions": 4
        },
        "files": [
            {
                "sha": "1e4a027a38a6e90d40dbcac2a8c987aeb6259900",
                "filename": "README.md",
                "status": "modified",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/116995791daecdabb510ed0db74f56843bbb7ca0/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/116995791daecdabb510ed0db74f56843bbb7ca0/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=116995791daecdabb510ed0db74f56843bbb7ca0",
                "patch": "@@ -139,6 +139,9 @@ navigate to\n     Prereq: Install flask and cassandra-driver using pip.\n \n \t    python3 app.py\n+\t   \n+Link to postman apis: https://www.getpostman.com/collections/034caa33048a99cfb99a\t   \n+\t   \n \n \n "
            },
            {
                "sha": "4e35b7f6eefc4832b34f2bbe4644e78cfb2372c0",
                "filename": "db-apis/app.py",
                "status": "modified",
                "additions": 7,
                "deletions": 1,
                "changes": 8,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/116995791daecdabb510ed0db74f56843bbb7ca0/db-apis/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/116995791daecdabb510ed0db74f56843bbb7ca0/db-apis/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/app.py?ref=116995791daecdabb510ed0db74f56843bbb7ca0",
                "patch": "@@ -41,7 +41,13 @@ def insert():\n     # TODO: populate response P0\n     # TODO: Catch exceptions P1\n     response = core.insert(request)\n-    # return response\n+    return response\n+\n+@app.route('/get', methods=['POST'])\n+def get_data():\n+    # TODO: populate response P0\n+    # TODO: Catch exceptions P1\n+    return core.get_data(request)\n \n \n # If we're running in stand alone mode, run the application"
            },
            {
                "sha": "bb548fc9875bdc24ca474caff4ca63b5f18777aa",
                "filename": "db-apis/dao/core.py",
                "status": "modified",
                "additions": 6,
                "deletions": 0,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/116995791daecdabb510ed0db74f56843bbb7ca0/db-apis/dao/core.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/116995791daecdabb510ed0db74f56843bbb7ca0/db-apis/dao/core.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/core.py?ref=116995791daecdabb510ed0db74f56843bbb7ca0",
                "patch": "@@ -19,3 +19,9 @@ def insert(request):\n     obj = eval(content[TABLE]).create(**(content[BODY]))\n     return obj\n \n+\n+def get_data(request):\n+    content = request.get_json()\n+    connection.setup(CASSANDRA_HOSTS, content[ORG])\n+    obj = eval(content[TABLE]).objects.filter(**(content[BODY]))\n+    return dict(obj.get())"
            },
            {
                "sha": "6344922a391657e963edb0e7d820c84a57b0cedc",
                "filename": "db-apis/dao/models.py",
                "status": "modified",
                "additions": 2,
                "deletions": 3,
                "changes": 5,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/116995791daecdabb510ed0db74f56843bbb7ca0/db-apis/dao/models.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/116995791daecdabb510ed0db74f56843bbb7ca0/db-apis/dao/models.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/models.py?ref=116995791daecdabb510ed0db74f56843bbb7ca0",
                "patch": "@@ -14,8 +14,7 @@ class Users(Model):\n \n \n class Data(Model):\n-    uid = columns.UUID(primary_key=True, default=uuid.uuid4)\n     gitid = columns.Text(required=True)\n-    name = columns.Text(required=True)\n-    repo = columns.Text(required=True)\n+    name = columns.Text(primary_key=True,required=True)\n+    repo = columns.Text(primary_key=True,required=True)\n     commit_num = columns.Integer(required=False, default=0)"
            },
            {
                "sha": "f4395e078ef5da3ecbfc6fa9bd0ed0d3f0aaebe9",
                "filename": "github-analytics/CassandraHelper/CassandraHelper.py",
                "status": "added",
                "additions": 7,
                "deletions": 0,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/CassandraHelper/CassandraHelper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/CassandraHelper/CassandraHelper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/CassandraHelper.py?ref=116995791daecdabb510ed0db74f56843bbb7ca0",
                "patch": "@@ -0,0 +1,7 @@\n+class CassandraHelper():\n+    def __init__(self):\n+        self.orgname = \"\"\n+    \n+    def set_org_name(self,orgname):\n+        self.orgname = orgname\n+        return\n\\ No newline at end of file"
            },
            {
                "sha": "65cd6dbe5aa05b9d97d259056b3411ded392302f",
                "filename": "github-analytics/CassandraHelper/__pycache__/CassandraHelper.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/CassandraHelper/__pycache__/CassandraHelper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/CassandraHelper/__pycache__/CassandraHelper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/__pycache__/CassandraHelper.cpython-37.pyc?ref=116995791daecdabb510ed0db74f56843bbb7ca0"
            },
            {
                "sha": "7da34057a0060337c7054cece276df914bc958f9",
                "filename": "github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "status": "added",
                "additions": 5,
                "deletions": 0,
                "changes": 5,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py?ref=116995791daecdabb510ed0db74f56843bbb7ca0",
                "patch": "@@ -0,0 +1,5 @@\n+class ElasticSearchHelper():\n+    def __init__(self):\n+    \n+    def getOrgName(self,orgname):\n+        return orgname\n\\ No newline at end of file"
            },
            {
                "sha": "dfb8738a2c5e7996b762c0eaaccc0fa6ba0ea124",
                "filename": "github-analytics/ElasticSearchHelper/__pycache__/ElasticSearchHelper.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/ElasticSearchHelper/__pycache__/ElasticSearchHelper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/ElasticSearchHelper/__pycache__/ElasticSearchHelper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/__pycache__/ElasticSearchHelper.cpython-37.pyc?ref=116995791daecdabb510ed0db74f56843bbb7ca0"
            },
            {
                "sha": "6dd5827a970ac79410042a52a4a451354c799fe1",
                "filename": "github-analytics/app.py",
                "status": "added",
                "additions": 31,
                "deletions": 0,
                "changes": 31,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/app.py?ref=116995791daecdabb510ed0db74f56843bbb7ca0",
                "patch": "@@ -0,0 +1,31 @@\n+from flask import Flask\n+\n+from config import config\n+from ElasticSearchHelper import ElasticSearchHelper as esh\n+from CassandraHelper import CassandraHelper as ch\n+\n+app = Flask(__name__)\n+\n+elasticSearchHelper = esh.ElasticSearchHelper()\n+cassandraHelper = ch.CassandraHelper()\n+\n+@app.route('/')\n+def home():\n+    return 'Github Analytics - Use APIs to process data'\n+\n+\n+@app.route('/org/<orgname>')\n+def org_retrive(orgname):\n+    return elasticSearchHelper.getOrgName(orgname)\n+\n+@app.route('/repo/<reponame>')\n+def repo_retrive(reponame):\n+    return reponame\n+\n+@app.route('/user/<username>')\n+def user_retrive(username):\n+    return username\n+\n+\n+if __name__ == '__main__':\n+    app.run(host=config.FLASK_HOST, port=config.FLASK_PORT, debug=True)"
            },
            {
                "sha": "a291ad4c6f2b79181cd17c106290c663e5c856b0",
                "filename": "github-analytics/config/__pycache__/config.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/config/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/config/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/config/__pycache__/config.cpython-37.pyc?ref=116995791daecdabb510ed0db74f56843bbb7ca0"
            },
            {
                "sha": "b27201c2edf737dc3a0fd17b8c0d88b123892c46",
                "filename": "github-analytics/config/config.py",
                "status": "added",
                "additions": 2,
                "deletions": 0,
                "changes": 2,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/config/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/116995791daecdabb510ed0db74f56843bbb7ca0/github-analytics/config/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/config/config.py?ref=116995791daecdabb510ed0db74f56843bbb7ca0",
                "patch": "@@ -0,0 +1,2 @@\n+FLASK_HOST = 'localhost'\n+FLASK_PORT = 4000\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "2990dd361573c2a3e3d88fb56cc92780e8dfc8de",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjI5OTBkZDM2MTU3M2MyYTNlM2Q4OGZiNTZjYzkyNzgwZThkZmM4ZGU=",
        "commit": {
            "author": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T06:14:22Z"
            },
            "committer": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T06:14:22Z"
            },
            "message": "UI 2",
            "tree": {
                "sha": "0abf56431f39d12fba94004ba4c7881b16ff89aa",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/0abf56431f39d12fba94004ba4c7881b16ff89aa"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/2990dd361573c2a3e3d88fb56cc92780e8dfc8de",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/2990dd361573c2a3e3d88fb56cc92780e8dfc8de",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/2990dd361573c2a3e3d88fb56cc92780e8dfc8de",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            }
        ],
        "stats": {
            "total": 418,
            "additions": 40,
            "deletions": 378
        },
        "files": [
            {
                "sha": "38c2b30d9334da350cbfff7e87edc02fe7a62d24",
                "filename": ".DS_Store",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/.DS_Store?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de"
            },
            {
                "sha": "275a113e16335210def75f91320c2efc184ebf54",
                "filename": "django/.DS_Store",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/.DS_Store?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de"
            },
            {
                "sha": "b2d7d6592a36fb06c1b9651c07f1732412140302",
                "filename": "django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/admin.cpython-37.pyc?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de"
            },
            {
                "sha": "6dff987ab1fe0796006b6985024176363b001cee",
                "filename": "django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/models.cpython-37.pyc?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de"
            },
            {
                "sha": "df3d10193b9b486356f5d15c3fbfe599a7d25aad",
                "filename": "django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de"
            },
            {
                "sha": "af79c2749820ca2bdb5d8abae09b0351f7a635b6",
                "filename": "django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de"
            },
            {
                "sha": "44d9377d95b83303e59424ab1d602b73747ff2f6",
                "filename": "django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/migrations/__pycache__/__init__.cpython-37.pyc?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de"
            },
            {
                "sha": "52cfb2d0f7affd5fd165c53e2691ddf0c4fdc290",
                "filename": "django/bdaSite/bdaProject/static/.DS_Store",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/static/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/static/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/.DS_Store?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de"
            },
            {
                "sha": "886ab67a581247fe732635e5c0c672fd1e52e770",
                "filename": "django/bdaSite/bdaProject/static/fonts/HomepageBaukasten-Book.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/static/fonts/HomepageBaukasten-Book.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/static/fonts/HomepageBaukasten-Book.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/HomepageBaukasten-Book.ttf?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de"
            },
            {
                "sha": "6f3ab754446ed2e240a21ada98c9c75b65019658",
                "filename": "django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "status": "added",
                "additions": 35,
                "deletions": 0,
                "changes": 35,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/templates/bdaProject/index.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/bdaProject/index.html?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de",
                "patch": "@@ -0,0 +1,35 @@\n+{% load static %}\n+<!DOCTYPE html>\n+<html lang=\"en\" dir=\"ltr\">\n+  <head>\n+    <meta charset=\"utf-8\">\n+    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n+    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n+    <style>\n+     @font-face\n+     { font-family: R; src: url({% static 'fonts/HomepageBaukasten-Book.ttf' %}); }\n+\n+    </style>\n+    <title></title>\n+  </head>\n+  <body id='body' style=\"font-family:R\">\n+    <!-- Navbar goes here -->\n+    <nav>\n+      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n+        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform</a>\n+      </div>\n+    </nav>\n+\n+    <!-- Page Layout here -->\n+  \n+    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n+    <script src={% static 'js/materialize.min.js' %}></script>\n+    <script src={% static 'js/chart.min.js' %}></script>\n+    <script type=\"text/javascript\">\n+      navHeight = $('#nav').height();\n+      columnHeight = (window.innerHeight - navHeight);\n+      $('#col1').height(columnHeight);\n+      $('#col2').height(columnHeight);\n+    </script>\n+  </body>\n+</html>"
            },
            {
                "sha": "b41ad27c60f86b852a9cf345885c611bbef7776c",
                "filename": "django/bdaSite/bdaProject/templates/iotdp/graph.html",
                "status": "removed",
                "additions": 0,
                "deletions": 129,
                "changes": 129,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/templates/iotdp/graph.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/templates/iotdp/graph.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/iotdp/graph.html?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
                "patch": "@@ -1,129 +0,0 @@\n-{% load static %}\n-<!DOCTYPE html>\n-<html lang=\"en\" dir=\"ltr\">\n-  <head>\n-    <meta charset=\"utf-8\">\n-    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n-    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n-    <style>\n-     @font-face\n-     { font-family: R; src: url({% static 'fonts/Roboto/Roboto-Regular.ttf' %}); }\n-\n-    </style>\n-    <title></title>\n-  </head>\n-  <body id='body' style=\"font-family:R\">\n-    <!-- Navbar goes here -->\n-    <nav>\n-      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n-        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform</a>\n-      </div>\n-    </nav>\n-\n-    <!-- Page Layout here -->\n-    <div class=\"row\">\n-      <div id='col1' class=\"col s3\" style=\"background:#336699;\">\n-        <a href=\"/iotdp/{{ user }}\">\n-          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n-            OVERALL VIEW\n-          </div>\n-        </a>\n-        <a href=\"/iotdp/{{ user }}/graph\">\n-          <div class=\"col s12 m12 l12\" style=\"color:#17365D;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px;  background:#F1BA65\">\n-            GRAPH\n-          </div>\n-        </a>\n-        <a href=\"/iotdp/{{ user }}/logs\">\n-          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n-            LOGS\n-          </div>\n-        </a>\n-      </div>\n-\n-      <div id='col2' class=\"col s9\" style=\"background: #EBEBD3;\">\n-        <h4 class=\"center\">POWER CONSUMPTION IN THE LAST 30 MINUTES</h4>\n-        <canvas id=\"chart1\" class=\"\" style=\"width:100%;\"></canvas>\n-        <div class=\"row\">\n-          <div class=\"col m8 offset-m2\">\n-            <div class=\"card center\" style=\"background: #EBEBD3;\">\n-              <h5>Generated Power from Solar and other Sources: {{ gen_pow }} kW</h5>\n-            </div>\n-          </div>\n-          <div class=\"col m8 offset-m2\">\n-            <div class=\"card center\" style=\"background: #EBEBD3;\">\n-              <table>\n-                <tr>\n-                  <th>Small Appliances</th>\n-                  <td>Microwave, Dishwasher, Fridge</td>\n-                </tr>\n-                <tr>\n-                  <th>Big Appliances</th>\n-                  <td>Garage Door, Well, Barn, Furnaces</td>\n-                </tr>\n-                <tr>\n-                  <th>Rooms</th>\n-                  <td>Home Office, Wine Cellar, Living Room, Kitchens</td>\n-                </tr>\n-              </table>\n-            </div>\n-          </div>\n-        </div>\n-      </div>\n-\n-    </div>\n-\n-    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n-    <script src={% static 'js/materialize.min.js' %}></script>\n-    <script src={% static 'js/chart.min.js' %}></script>\n-    <script type=\"text/javascript\">\n-      navHeight = $('#nav').height();\n-      columnHeight = (window.innerHeight - navHeight);\n-      $('#col1').height(columnHeight);\n-      $('#col2').height(columnHeight);\n-    </script>\n-    <script type=\"text/javascript\">\n-      var ctx3 = document.getElementById(\"chart1\");\n-      var myChart = new Chart(ctx3, {\n-          type: 'bar',\n-          data: {\n-              labels: {% autoescape off %} {{ graph_labels }} {% endautoescape %},\n-              datasets: [{\n-                  label: 'Number of Units [kW]',\n-                  data: {% autoescape off %} {{ graph_data }} {% endautoescape %},\n-                  backgroundColor: [\n-                      '#FDB44B',\n-                      '#FDB44B',\n-                      '#FDB44B'\n-                  ],\n-                  borderColor: [\n-                      '#FDB44B',\n-                      '#FDB44B',\n-                      '#FDB44B'\n-                  ],\n-                  borderWidth: 1\n-              }]\n-          },\n-          options: {\n-              legend: {\n-                  labels: {\n-                      fontColor:\"black\"\n-                  }\n-              },\n-              scales: {\n-                xAxes: [{\n-                    ticks: {\n-                        fontColor:\"black\"\n-                    }\n-                }],\n-                yAxes: [{\n-                    ticks: {\n-                        beginAtZero:true,\n-                        fontColor:\"black\"\n-                    }\n-                }]\n-              }\n-          }\n-      });\n-    </script>\n-  </body>\n-</html>"
            },
            {
                "sha": "ab918e5c83d9f39a659db66c4957aa57ccfcd967",
                "filename": "django/bdaSite/bdaProject/templates/iotdp/index.html",
                "status": "removed",
                "additions": 0,
                "deletions": 170,
                "changes": 170,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/templates/iotdp/index.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/templates/iotdp/index.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/iotdp/index.html?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
                "patch": "@@ -1,170 +0,0 @@\n-{% load static %}\n-<!DOCTYPE html>\n-<html lang=\"en\" dir=\"ltr\">\n-  <head>\n-    <meta charset=\"utf-8\">\n-    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n-    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n-    <style>\n-     @font-face\n-     { font-family: R; src: url({% static 'fonts/Roboto/Roboto-Regular.ttf' %}); }\n-\n-    </style>\n-    <title></title>\n-  </head>\n-  <body id='body' style=\"font-family:R\">\n-    <!-- Navbar goes here -->\n-    <nav>\n-      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n-        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform</a>\n-      </div>\n-    </nav>\n-\n-    <!-- Page Layout here -->\n-    <div class=\"row\">\n-\n-      <div id='col1' class=\"col s3\" style=\"background:#336699;\">\n-        <a href=\"/iotdp/{{ user }}\">\n-          <div class=\"col s12 m12 l12\" style=\"color:#17365D;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; background:#F1BA65; padding:0px\">\n-            OVERALL VIEW\n-          </div>\n-        </a>\n-        <a href=\"/iotdp/{{ user }}/graph\">\n-          <div class=\"col s12 m12 l12 white-text\" style=\"display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px\">\n-            GRAPH\n-          </div>\n-        </a>\n-        <a href=\"/iotdp/{{ user }}/logs\">\n-          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n-            LOGS\n-          </div>\n-        </a>\n-      </div>\n-\n-      <div id='col2' class=\"col s9 center\" style=\"background: #EBEBD3;\">\n-        <div style=\"width:100%; overflow-x:scroll\">\n-          <table style=\"\">\n-            <thead id='tableHead'>\n-              <th>Timestamp</th>\n-              <th>Use</th>\n-              <th>General</th>\n-              <th>House Overall</th>\n-              <th>Dishwasher</th>\n-              <th>Furnace1</th>\n-              <th>Furnace2</th>\n-              <th>Home Office</th>\n-              <th>Fridge</th>\n-              <th>Wine Cellar</th>\n-              <th>Garage Door</th>\n-              <th>Kitchen 12</th>\n-              <th>Kitchen 14</th>\n-              <th>Kitchen 38</th>\n-              <th>Barn</th>\n-              <th>Well</th>\n-              <th>Microwave</th>\n-              <th>Living Room</th>\n-              <th>Solar</th>\n-              <th>Temperature</th>\n-              <th>Icon</th>\n-              <th>Humidity</th>\n-              <th>Visibility</th>\n-              <th>Summary</th>\n-              <th>Apparent Temperature</th>\n-              <th>Pressure</th>\n-              <th>Wind Speed</th>\n-              <th>Cloud Cover</th>\n-              <th>Wind Bearing</th>\n-              <th>Precipitation Intensity</th>\n-              <th>Dew Point</th>\n-              <th>Precipitation Porbab</th>\n-            </thead>\n-            <tbody id='tableBody'>\n-\n-            </tbody>\n-          </table>\n-          <a id='prev' class=\"btn-floating\" style=\"background: #17365D\"><i class=\"material-icons\">keyboard_arrow_left</i></a>\n-          <a id='next' class=\"btn-floating\" style=\"background: #17365D\"><i class=\"material-icons\">keyboard_arrow_right</i></a>\n-        </div>\n-      </div>\n-\n-    </div>\n-\n-    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n-    <script src={% static 'js/materialize.min.js' %}></script>\n-    <script src={% static 'js/chart.min.js' %}></script>\n-    <script type=\"text/javascript\">\n-      navHeight = $('#nav').height();\n-      columnHeight = (window.innerHeight - navHeight);\n-      $('#col1').height(columnHeight);\n-      $('#col2').height(columnHeight);\n-    </script>\n-    <script type=\"text/javascript\">\n-        current_index = 0;\n-        size = {{ size }};\n-        function dataUpdate(current_index)\n-        {\n-          $.get('http://35.230.40.217:5000/get_data/{{ user }}/'+current_index, function(data, status) {\n-            for(i=0; i<data['result'].length; i++)\n-            {\n-              row = \"<tr>\";\n-              date = new Date(parseInt(data['result'][i]['time'])*1000).toLocaleString();\n-              row += \"<td>\"+date+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['use1']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['gen']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['house_overall']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['dishwasher']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['furnace1']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['furnace2']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['home_office']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['fridge']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['wine_cellar']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['garage_door']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['kitchen12']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['kitchen14']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['kitchen38']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['barn']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['well']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['microwave']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['living_room']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['solar']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['temperature']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+data['result'][i]['icon']+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['humidity']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['visibility']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+data['result'][i]['summary']+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['apparent_temp']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['pressure']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['wind_speed']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+data['result'][i]['cloud_cover']+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['wind_bearing']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['precip_intensity']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['dew_point']* 10000)) / 10000)+\"</td>\";\n-              row += \"<td>\"+(Math.round((data['result'][i]['precip_probability']* 10000)) / 10000)+\"</td>\";\n-              row += \"</tr>\";\n-              $('#tableBody').append(row);\n-            }\n-          });\n-        }\n-\n-        dataUpdate(0);\n-        // alert(size);\n-        $('#prev').click(function(){\n-            if(current_index > 0)\n-            {\n-              $('#tableBody').empty();\n-              current_index = current_index - 7;\n-              dataUpdate(current_index);\n-            }\n-        });\n-        $('#next').click(function(){\n-            if (current_index <= size-7)\n-            {\n-              $('#tableBody').empty();\n-              current_index = current_index + 7;\n-              dataUpdate(current_index);\n-            }\n-        });\n-\n-    </script>\n-  </body>\n-</html>"
            },
            {
                "sha": "0df695c397fe3d580ee82eb06165abce940e55cf",
                "filename": "django/bdaSite/bdaProject/templates/iotdp/logs.html",
                "status": "removed",
                "additions": 0,
                "deletions": 74,
                "changes": 74,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/templates/iotdp/logs.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/templates/iotdp/logs.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/iotdp/logs.html?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
                "patch": "@@ -1,74 +0,0 @@\n-{% load static %}\n-<!DOCTYPE html>\n-<html lang=\"en\" dir=\"ltr\">\n-  <head>\n-    <meta charset=\"utf-8\">\n-    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n-    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n-    <style>\n-     @font-face\n-     { font-family: R; src: url({% static 'fonts/Roboto/Roboto-Regular.ttf' %}); }\n-\n-    </style>\n-    <script src=\"{% static 'js/terminal.js' %}\"></script>\n-    <script type=\"text/javascript\">\n-\n-    </script>\n-    <title></title>\n-  </head>\n-  <body id='body' style=\"font-family:R\">\n-    <!-- Navbar goes here -->\n-    <nav>\n-      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n-        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform - Logs</a>\n-      </div>\n-    </nav>\n-\n-    <div class=\"row\">\n-      <div id='col1' class=\"col s3\" style=\"background:#336699;\">\n-        <a href=\"/iotdp/{{ user }}\">\n-          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n-            OVERALL VIEW\n-          </div>\n-        </a>\n-        <a href=\"/iotdp/{{ user }}/graph\">\n-          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px;\">\n-            GRAPH\n-          </div>\n-        </a>\n-        <a href=\"/iotdp/{{ user }}/logs\">\n-          <div class=\"col s12 m12 l12\" style=\"color:#17365D;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px;  background:#F1BA65\">\n-            LOGS\n-          </div>\n-        </a>\n-      </div>\n-\n-      <div id='col2' class=\"col s9 black\" style=\"height:750px;background: #EBEBD3; margin:0px; padding:0px; overflow-y:scroll\">\n-        <div id='term' class=\"row\" style=\"height:750px;display:flex; margin:0px; padding:0px; overflow-y:scroll;\">\n-\n-        </div>\n-      </div>\n-\n-    <!-- Page Layout here -->\n-\n-\n-    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n-    <script src={% static 'js/materialize.min.js' %}></script>\n-    <script src={% static 'js/chart.min.js' %}></script>\n-    <script type=\"text/javascript\">\n-      navHeight = $('#nav').height();\n-      columnHeight = (window.innerHeight - navHeight);\n-      $('#col1').height(columnHeight);\n-    </script>\n-    <script type=\"text/javascript\">\n-      var term = new Terminal();\n-      var logs = {% autoescape off %} {{ logs }} {% endautoescape %}\n-      var logLines = logs.split(';;;')\n-      for(var i=0; i<logLines.length; i++)\n-      {\n-          term.print(logLines[i]);\n-      }\n-      $('#term').append(term.html)\n-    </script>\n-  </body>\n-</html>"
            },
            {
                "sha": "c486297ddccdfdf774fe9b8e0dc4584043b02652",
                "filename": "django/bdaSite/bdaProject/urls.py",
                "status": "modified",
                "additions": 0,
                "deletions": 1,
                "changes": 1,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/urls.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/urls.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/urls.py?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de",
                "patch": "@@ -3,5 +3,4 @@\n \n urlpatterns = [\n     path('', views.index, name='index'),\n-    path('trial/', views.trial, name='trial')\n ]\n\\ No newline at end of file"
            },
            {
                "sha": "84da6ec3082bd1c158fd02b6415e30cd9ee22493",
                "filename": "django/bdaSite/bdaProject/views.py",
                "status": "modified",
                "additions": 3,
                "deletions": 4,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/views.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaProject/views.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/views.py?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de",
                "patch": "@@ -1,9 +1,8 @@\n from django.shortcuts import render\n from django.http import HttpResponse\n+from django.template import loader\n \n \n def index(request):\n-    return HttpResponse(\"Hello, world. You're at the bdaProject index.\")\n-\n-def trial(request):\n-    return HttpResponse(\"Trial\")\n+    context = {}\n+    return render(request, 'bdaProject/index.html', context)"
            },
            {
                "sha": "1a0871e09e88291d2cce6e6a409a2decdcc3550b",
                "filename": "django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de"
            },
            {
                "sha": "048412ac61833b220350fa1182176f443702497c",
                "filename": "django/bdaSite/bdaSite/settings.py",
                "status": "modified",
                "additions": 2,
                "deletions": 0,
                "changes": 2,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaSite/settings.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/bdaSite/settings.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/settings.py?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de",
                "patch": "@@ -37,6 +37,7 @@\n     'django.contrib.sessions',\n     'django.contrib.messages',\n     'django.contrib.staticfiles',\n+    'bdaProject'\n ]\n \n MIDDLEWARE = [\n@@ -118,3 +119,4 @@\n # https://docs.djangoproject.com/en/3.0/howto/static-files/\n \n STATIC_URL = '/static/'\n+"
            },
            {
                "sha": "3ed785da46cd65daa196bd65ab33f5a8c6ed2882",
                "filename": "django/bdaSite/db.sqlite3",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/db.sqlite3",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/2990dd361573c2a3e3d88fb56cc92780e8dfc8de/django/bdaSite/db.sqlite3",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/db.sqlite3?ref=2990dd361573c2a3e3d88fb56cc92780e8dfc8de"
            }
        ]
    },
    {
        "sha": "f312dc10571ddc19cdfaaa98edda7f6db6432aee",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmYzMTJkYzEwNTcxZGRjMTljZGZhYWE5OGVkZGE3ZjZkYjY0MzJhZWU=",
        "commit": {
            "author": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-16T05:33:23Z"
            },
            "committer": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-16T05:33:23Z"
            },
            "message": "Github analytics flask app layout",
            "tree": {
                "sha": "312c5ce522be227c015139932944bd904a4708b1",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/312c5ce522be227c015139932944bd904a4708b1"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/f312dc10571ddc19cdfaaa98edda7f6db6432aee",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/f312dc10571ddc19cdfaaa98edda7f6db6432aee",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/f312dc10571ddc19cdfaaa98edda7f6db6432aee",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/f312dc10571ddc19cdfaaa98edda7f6db6432aee/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "7a08a0ffb450c205cf2000dd90785b18ec82d3da",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/7a08a0ffb450c205cf2000dd90785b18ec82d3da",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/7a08a0ffb450c205cf2000dd90785b18ec82d3da"
            }
        ],
        "stats": {
            "total": 45,
            "additions": 45,
            "deletions": 0
        },
        "files": [
            {
                "sha": "f4395e078ef5da3ecbfc6fa9bd0ed0d3f0aaebe9",
                "filename": "github-analytics/CassandraHelper/CassandraHelper.py",
                "status": "added",
                "additions": 7,
                "deletions": 0,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/CassandraHelper/CassandraHelper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/CassandraHelper/CassandraHelper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/CassandraHelper.py?ref=f312dc10571ddc19cdfaaa98edda7f6db6432aee",
                "patch": "@@ -0,0 +1,7 @@\n+class CassandraHelper():\n+    def __init__(self):\n+        self.orgname = \"\"\n+    \n+    def set_org_name(self,orgname):\n+        self.orgname = orgname\n+        return\n\\ No newline at end of file"
            },
            {
                "sha": "65cd6dbe5aa05b9d97d259056b3411ded392302f",
                "filename": "github-analytics/CassandraHelper/__pycache__/CassandraHelper.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/CassandraHelper/__pycache__/CassandraHelper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/CassandraHelper/__pycache__/CassandraHelper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/CassandraHelper/__pycache__/CassandraHelper.cpython-37.pyc?ref=f312dc10571ddc19cdfaaa98edda7f6db6432aee"
            },
            {
                "sha": "7da34057a0060337c7054cece276df914bc958f9",
                "filename": "github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "status": "added",
                "additions": 5,
                "deletions": 0,
                "changes": 5,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/ElasticSearchHelper.py?ref=f312dc10571ddc19cdfaaa98edda7f6db6432aee",
                "patch": "@@ -0,0 +1,5 @@\n+class ElasticSearchHelper():\n+    def __init__(self):\n+    \n+    def getOrgName(self,orgname):\n+        return orgname\n\\ No newline at end of file"
            },
            {
                "sha": "dfb8738a2c5e7996b762c0eaaccc0fa6ba0ea124",
                "filename": "github-analytics/ElasticSearchHelper/__pycache__/ElasticSearchHelper.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/ElasticSearchHelper/__pycache__/ElasticSearchHelper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/ElasticSearchHelper/__pycache__/ElasticSearchHelper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/ElasticSearchHelper/__pycache__/ElasticSearchHelper.cpython-37.pyc?ref=f312dc10571ddc19cdfaaa98edda7f6db6432aee"
            },
            {
                "sha": "6dd5827a970ac79410042a52a4a451354c799fe1",
                "filename": "github-analytics/app.py",
                "status": "added",
                "additions": 31,
                "deletions": 0,
                "changes": 31,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/app.py?ref=f312dc10571ddc19cdfaaa98edda7f6db6432aee",
                "patch": "@@ -0,0 +1,31 @@\n+from flask import Flask\n+\n+from config import config\n+from ElasticSearchHelper import ElasticSearchHelper as esh\n+from CassandraHelper import CassandraHelper as ch\n+\n+app = Flask(__name__)\n+\n+elasticSearchHelper = esh.ElasticSearchHelper()\n+cassandraHelper = ch.CassandraHelper()\n+\n+@app.route('/')\n+def home():\n+    return 'Github Analytics - Use APIs to process data'\n+\n+\n+@app.route('/org/<orgname>')\n+def org_retrive(orgname):\n+    return elasticSearchHelper.getOrgName(orgname)\n+\n+@app.route('/repo/<reponame>')\n+def repo_retrive(reponame):\n+    return reponame\n+\n+@app.route('/user/<username>')\n+def user_retrive(username):\n+    return username\n+\n+\n+if __name__ == '__main__':\n+    app.run(host=config.FLASK_HOST, port=config.FLASK_PORT, debug=True)"
            },
            {
                "sha": "a291ad4c6f2b79181cd17c106290c663e5c856b0",
                "filename": "github-analytics/config/__pycache__/config.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/config/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/config/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/config/__pycache__/config.cpython-37.pyc?ref=f312dc10571ddc19cdfaaa98edda7f6db6432aee"
            },
            {
                "sha": "b27201c2edf737dc3a0fd17b8c0d88b123892c46",
                "filename": "github-analytics/config/config.py",
                "status": "added",
                "additions": 2,
                "deletions": 0,
                "changes": 2,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/config/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/f312dc10571ddc19cdfaaa98edda7f6db6432aee/github-analytics/config/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-analytics/config/config.py?ref=f312dc10571ddc19cdfaaa98edda7f6db6432aee",
                "patch": "@@ -0,0 +1,2 @@\n+FLASK_HOST = 'localhost'\n+FLASK_PORT = 4000\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "7a08a0ffb450c205cf2000dd90785b18ec82d3da",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjdhMDhhMGZmYjQ1MGMyMDVjZjIwMDBkZDkwNzg1YjE4ZWM4MmQzZGE=",
        "commit": {
            "author": {
                "name": "Karthik S",
                "email": "karthiks1995@gmail.com",
                "date": "2020-04-16T05:20:51Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-16T05:20:51Z"
            },
            "message": "Update README.md",
            "tree": {
                "sha": "f03b8d11c5c9e974558f8a5e4b1bab89fcd63897",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/f03b8d11c5c9e974558f8a5e4b1bab89fcd63897"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/7a08a0ffb450c205cf2000dd90785b18ec82d3da",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJel+szCRBK7hj4Ov3rIwAAdHIIAD1+4Z/BbJdDNoUeJ2kUIXis\n0ui8OMAQVuIoKJP6Nj32Nu9KqU/oF8M+ZZnNEBwy2MNmhBL7BSXUV3yrBF2u0K3T\nY1FVKoOOkw2au/fnZlYJFUhCNyihgHugGunIPp2aa/IPygRvAPtRmmu3VLCZYwlK\nV8sh3SKY2panHOugvSmbr7rI/Aj019sjkxILAXlZ4Ivxe5v6RkNgAfQ5OfKK8Zwy\nGkj2EtkQD0SwWz9UC7o30wEodGAWkZm1iLJxUds3ZWpwm4wXmhRG47SiF+Rei5gp\nS2Pkrlb4r4KB8BaT7DRkKmCqIYKVYdUVpt1XMohTgYOmv/F5v6SAlbf43EIusoI=\n=gMZ5\n-----END PGP SIGNATURE-----\n",
                "payload": "tree f03b8d11c5c9e974558f8a5e4b1bab89fcd63897\nparent c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b\nauthor Karthik S <karthiks1995@gmail.com> 1587014451 -0700\ncommitter GitHub <noreply@github.com> 1587014451 -0700\n\nUpdate README.md"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/7a08a0ffb450c205cf2000dd90785b18ec82d3da",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/7a08a0ffb450c205cf2000dd90785b18ec82d3da",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/7a08a0ffb450c205cf2000dd90785b18ec82d3da/comments",
        "author": {
            "login": "karthiks1995",
            "id": 7661540,
            "node_id": "MDQ6VXNlcjc2NjE1NDA=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/7661540?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/karthiks1995",
            "html_url": "https://github.com/karthiks1995",
            "followers_url": "https://api.github.com/users/karthiks1995/followers",
            "following_url": "https://api.github.com/users/karthiks1995/following{/other_user}",
            "gists_url": "https://api.github.com/users/karthiks1995/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/karthiks1995/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/karthiks1995/subscriptions",
            "organizations_url": "https://api.github.com/users/karthiks1995/orgs",
            "repos_url": "https://api.github.com/users/karthiks1995/repos",
            "events_url": "https://api.github.com/users/karthiks1995/events{/privacy}",
            "received_events_url": "https://api.github.com/users/karthiks1995/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            }
        ],
        "stats": {
            "total": 3,
            "additions": 3,
            "deletions": 0
        },
        "files": [
            {
                "sha": "1e4a027a38a6e90d40dbcac2a8c987aeb6259900",
                "filename": "README.md",
                "status": "modified",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/7a08a0ffb450c205cf2000dd90785b18ec82d3da/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/7a08a0ffb450c205cf2000dd90785b18ec82d3da/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=7a08a0ffb450c205cf2000dd90785b18ec82d3da",
                "patch": "@@ -139,6 +139,9 @@ navigate to\n     Prereq: Install flask and cassandra-driver using pip.\n \n \t    python3 app.py\n+\t   \n+Link to postman apis: https://www.getpostman.com/collections/034caa33048a99cfb99a\t   \n+\t   \n \n \n "
            }
        ]
    },
    {
        "sha": "c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmMxNzFlNDMzYjdiYzEzNTNmYWI5YjFjNGIwYWU3MTYyYTdlNGFhOWI=",
        "commit": {
            "author": {
                "name": "Karthik Siddaramanna",
                "email": "karthiks@Karthiks-MBP.lan",
                "date": "2020-04-16T05:15:15Z"
            },
            "committer": {
                "name": "Karthik Siddaramanna",
                "email": "karthiks@Karthiks-MBP.lan",
                "date": "2020-04-16T05:15:15Z"
            },
            "message": "Merge branch 'master' of https://github.com/CUBigDataClass/Kode-Kallas",
            "tree": {
                "sha": "1890aaf9c5502347cb02fc52f9d0296b46edcd1e",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/1890aaf9c5502347cb02fc52f9d0296b46edcd1e"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
            "comment_count": 1,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "cff04dbe8bc646295decebc6b5b295830267e300",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/cff04dbe8bc646295decebc6b5b295830267e300",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/cff04dbe8bc646295decebc6b5b295830267e300"
            },
            {
                "sha": "496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            }
        ],
        "stats": {
            "total": 28745,
            "additions": 28719,
            "deletions": 26
        },
        "files": [
            {
                "sha": "a86165d1469792833005cbe18ed0b2a58468201f",
                "filename": "Architecture-Diagram.pdf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/Architecture-Diagram.pdf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/Architecture-Diagram.pdf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/Architecture-Diagram.pdf?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "2b63cace2f5e424d58ceaf5eb70ffae4f5d7f121",
                "filename": "README.md",
                "status": "modified",
                "additions": 79,
                "deletions": 2,
                "changes": 81,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -18,6 +18,72 @@\n  1) Install and keep following software\n  \n \n+ - Docker\n+ - Python-Flask\n+ - Python3\n+ - ElasticSearch\n+ - \n+\n+2) Run Below Comands after starting Docker\n+\n+\t\tdocker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.5.2\n+\t```\n+\tdocker run -d --name kibana --net somenetwork -p 5601:5601 kibana:tag\n+\t```\n+3) Set-up ElasticSearch (Important)\n+   navigate to [http://localhost:5601/app/kibana#/dev_tools/console?_g=()](http://localhost:5601/app/kibana#/dev_tools/console?_g=())\n+   copy and past each and every file from **github-crawler/Templates_commands** into above console (list of files is given below)\n+\n+         * comments_command.txt\n+         * repos_cammand.txt\n+         * put_org_index.txt\n+    \n+\n+3) Starting github-crawler and usage\n+\n+\t\tgit clone https://github.com/CUBigDataClass/Kode-Kallas.git\n+\t\tcd Kode-Kallas/github-crawler\n+\t    python3 app.py\n+navigate to\n+-  http://127.0.0.1:5000  -- you will be able to see proper message\n+- http://127.0.0.1:5000/org/<!orgname_should be added here>  - Just check server logs  \n+\n+4) For stopping and starting your docker images\n+\n+\t     $ docker ps -a    \n+\t     $ docker stop 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\n+\t     $ docker stop d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\n+\t     $ docker start 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\n+\t     $ docker start d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\n+\n+\n+4) Below some Documentation\n+\n+\t\t Work Under progress\n+\n+|**Module**  | **Usage** | **Requirements**|\n+|--|--|--|\n+|Github-Crawler|Used to get Github organization info |Docker and Python3|\n+# Kode - Kallas\n+\n+\n+  \n+\n+## Modules\n+\n+|Name  | Progress |\n+|--|--|\n+|  Github-Crawler| In-Progress|\n+| Github-Analyser|In-Progress|\n+|Dashboard-backend| In-progress|\n+|Dashboard-Frontend||\n+\n+\n+    Github-Crawler Setup\n+\n+ 1) Install and keep following software\n+ \n+\n  - Docker\n  - Python-Flask\n  - Python3\n@@ -65,11 +131,22 @@ navigate to\n \t     $ docker start d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\t\n \n \n+5) Cassandra api setup\n+\n+\t    docker run -e DS_LICENSE=accept --memory 4g -p 7000:7000 -p 7001:7001 -p 7199:7199 -p 9042:9042 -p 9160:9160 -p 9404:9404 --name my-dse -e CASSANDRA_START_RPC=true -d datastax/dse-server:6.8.0\n+\t    docker start my-dse\n+\t \n+    Prereq: Install flask and cassandra-driver using pip.\n+\n+\t    python3 app.py\n+\n \n-5) Below some Documentation\n \n-\t\t Work Under progress\n+\n+6) Below some Documentation\n+\n \n |**Module**  | **Usage** | **Requirements**|\n |--|--|--|\n |Github-Crawler|Used to get Github organization info |Docker and Python3|\n+"
            },
            {
                "sha": "b45edbfdde79c13d94aa4ff3ef21f1daa64bee3b",
                "filename": "django/bdaSite/.DS_Store",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/.DS_Store?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "744381f3bd26406868047a0c54e8853e1af9573d",
                "filename": "django/bdaSite/bdaProject/.DS_Store",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/.DS_Store?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "django/bdaSite/bdaProject/__init__.py",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__init__.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "dbf6b196b731844d58da03d2e3074adb6fc42c9a",
                "filename": "django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "22171cf1ea8eadc37bf270bfc0db75e52d83d73e",
                "filename": "django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "94a0e6d75398a265f03825e36ada9ad3b8b54df9",
                "filename": "django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "8c38f3f3dad51e4585f3984282c2a4bec5349c1e",
                "filename": "django/bdaSite/bdaProject/admin.py",
                "status": "added",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/admin.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/admin.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/admin.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,3 @@\n+from django.contrib import admin\n+\n+# Register your models here."
            },
            {
                "sha": "1e3be5584a0ced1e461a728fd279e46f78be69f6",
                "filename": "django/bdaSite/bdaProject/apps.py",
                "status": "added",
                "additions": 5,
                "deletions": 0,
                "changes": 5,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/apps.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/apps.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/apps.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,5 @@\n+from django.apps import AppConfig\n+\n+\n+class BdaprojectConfig(AppConfig):\n+    name = 'bdaProject'"
            },
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "django/bdaSite/bdaProject/migrations/__init__.py",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/migrations/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/migrations/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/migrations/__init__.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "71a836239075aa6e6e4ecb700e9c42c95c022d91",
                "filename": "django/bdaSite/bdaProject/models.py",
                "status": "added",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/models.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/models.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/models.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,3 @@\n+from django.db import models\n+\n+# Create your models here."
            },
            {
                "sha": "9fe0613e3a9f7304fda2644019cb7de55a86b800",
                "filename": "django/bdaSite/bdaProject/static/.DS_Store",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/.DS_Store?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "77ff7497d4d132cb736615709b107fb006a1ec6e",
                "filename": "django/bdaSite/bdaProject/static/css/materialize.css",
                "status": "added",
                "additions": 8952,
                "deletions": 0,
                "changes": 8952,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/css/materialize.css",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/css/materialize.css",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/css/materialize.css?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "103b4dffbe9201084b96b18129be2bc6a042a134",
                "filename": "django/bdaSite/bdaProject/static/css/materialize.min.css",
                "status": "added",
                "additions": 16,
                "deletions": 0,
                "changes": 16,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/css/materialize.min.css",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/css/materialize.min.css",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/css/materialize.min.css?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "75b52484ea471f882c29e02693b4f02dba175b5e",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/LICENSE.txt",
                "status": "added",
                "additions": 202,
                "deletions": 0,
                "changes": 202,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/LICENSE.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/LICENSE.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/LICENSE.txt?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,202 @@\n+\r\n+                                 Apache License\r\n+                           Version 2.0, January 2004\r\n+                        http://www.apache.org/licenses/\r\n+\r\n+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\r\n+\r\n+   1. Definitions.\r\n+\r\n+      \"License\" shall mean the terms and conditions for use, reproduction,\r\n+      and distribution as defined by Sections 1 through 9 of this document.\r\n+\r\n+      \"Licensor\" shall mean the copyright owner or entity authorized by\r\n+      the copyright owner that is granting the License.\r\n+\r\n+      \"Legal Entity\" shall mean the union of the acting entity and all\r\n+      other entities that control, are controlled by, or are under common\r\n+      control with that entity. For the purposes of this definition,\r\n+      \"control\" means (i) the power, direct or indirect, to cause the\r\n+      direction or management of such entity, whether by contract or\r\n+      otherwise, or (ii) ownership of fifty percent (50%) or more of the\r\n+      outstanding shares, or (iii) beneficial ownership of such entity.\r\n+\r\n+      \"You\" (or \"Your\") shall mean an individual or Legal Entity\r\n+      exercising permissions granted by this License.\r\n+\r\n+      \"Source\" form shall mean the preferred form for making modifications,\r\n+      including but not limited to software source code, documentation\r\n+      source, and configuration files.\r\n+\r\n+      \"Object\" form shall mean any form resulting from mechanical\r\n+      transformation or translation of a Source form, including but\r\n+      not limited to compiled object code, generated documentation,\r\n+      and conversions to other media types.\r\n+\r\n+      \"Work\" shall mean the work of authorship, whether in Source or\r\n+      Object form, made available under the License, as indicated by a\r\n+      copyright notice that is included in or attached to the work\r\n+      (an example is provided in the Appendix below).\r\n+\r\n+      \"Derivative Works\" shall mean any work, whether in Source or Object\r\n+      form, that is based on (or derived from) the Work and for which the\r\n+      editorial revisions, annotations, elaborations, or other modifications\r\n+      represent, as a whole, an original work of authorship. For the purposes\r\n+      of this License, Derivative Works shall not include works that remain\r\n+      separable from, or merely link (or bind by name) to the interfaces of,\r\n+      the Work and Derivative Works thereof.\r\n+\r\n+      \"Contribution\" shall mean any work of authorship, including\r\n+      the original version of the Work and any modifications or additions\r\n+      to that Work or Derivative Works thereof, that is intentionally\r\n+      submitted to Licensor for inclusion in the Work by the copyright owner\r\n+      or by an individual or Legal Entity authorized to submit on behalf of\r\n+      the copyright owner. For the purposes of this definition, \"submitted\"\r\n+      means any form of electronic, verbal, or written communication sent\r\n+      to the Licensor or its representatives, including but not limited to\r\n+      communication on electronic mailing lists, source code control systems,\r\n+      and issue tracking systems that are managed by, or on behalf of, the\r\n+      Licensor for the purpose of discussing and improving the Work, but\r\n+      excluding communication that is conspicuously marked or otherwise\r\n+      designated in writing by the copyright owner as \"Not a Contribution.\"\r\n+\r\n+      \"Contributor\" shall mean Licensor and any individual or Legal Entity\r\n+      on behalf of whom a Contribution has been received by Licensor and\r\n+      subsequently incorporated within the Work.\r\n+\r\n+   2. Grant of Copyright License. Subject to the terms and conditions of\r\n+      this License, each Contributor hereby grants to You a perpetual,\r\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\r\n+      copyright license to reproduce, prepare Derivative Works of,\r\n+      publicly display, publicly perform, sublicense, and distribute the\r\n+      Work and such Derivative Works in Source or Object form.\r\n+\r\n+   3. Grant of Patent License. Subject to the terms and conditions of\r\n+      this License, each Contributor hereby grants to You a perpetual,\r\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\r\n+      (except as stated in this section) patent license to make, have made,\r\n+      use, offer to sell, sell, import, and otherwise transfer the Work,\r\n+      where such license applies only to those patent claims licensable\r\n+      by such Contributor that are necessarily infringed by their\r\n+      Contribution(s) alone or by combination of their Contribution(s)\r\n+      with the Work to which such Contribution(s) was submitted. If You\r\n+      institute patent litigation against any entity (including a\r\n+      cross-claim or counterclaim in a lawsuit) alleging that the Work\r\n+      or a Contribution incorporated within the Work constitutes direct\r\n+      or contributory patent infringement, then any patent licenses\r\n+      granted to You under this License for that Work shall terminate\r\n+      as of the date such litigation is filed.\r\n+\r\n+   4. Redistribution. You may reproduce and distribute copies of the\r\n+      Work or Derivative Works thereof in any medium, with or without\r\n+      modifications, and in Source or Object form, provided that You\r\n+      meet the following conditions:\r\n+\r\n+      (a) You must give any other recipients of the Work or\r\n+          Derivative Works a copy of this License; and\r\n+\r\n+      (b) You must cause any modified files to carry prominent notices\r\n+          stating that You changed the files; and\r\n+\r\n+      (c) You must retain, in the Source form of any Derivative Works\r\n+          that You distribute, all copyright, patent, trademark, and\r\n+          attribution notices from the Source form of the Work,\r\n+          excluding those notices that do not pertain to any part of\r\n+          the Derivative Works; and\r\n+\r\n+      (d) If the Work includes a \"NOTICE\" text file as part of its\r\n+          distribution, then any Derivative Works that You distribute must\r\n+          include a readable copy of the attribution notices contained\r\n+          within such NOTICE file, excluding those notices that do not\r\n+          pertain to any part of the Derivative Works, in at least one\r\n+          of the following places: within a NOTICE text file distributed\r\n+          as part of the Derivative Works; within the Source form or\r\n+          documentation, if provided along with the Derivative Works; or,\r\n+          within a display generated by the Derivative Works, if and\r\n+          wherever such third-party notices normally appear. The contents\r\n+          of the NOTICE file are for informational purposes only and\r\n+          do not modify the License. You may add Your own attribution\r\n+          notices within Derivative Works that You distribute, alongside\r\n+          or as an addendum to the NOTICE text from the Work, provided\r\n+          that such additional attribution notices cannot be construed\r\n+          as modifying the License.\r\n+\r\n+      You may add Your own copyright statement to Your modifications and\r\n+      may provide additional or different license terms and conditions\r\n+      for use, reproduction, or distribution of Your modifications, or\r\n+      for any such Derivative Works as a whole, provided Your use,\r\n+      reproduction, and distribution of the Work otherwise complies with\r\n+      the conditions stated in this License.\r\n+\r\n+   5. Submission of Contributions. Unless You explicitly state otherwise,\r\n+      any Contribution intentionally submitted for inclusion in the Work\r\n+      by You to the Licensor shall be under the terms and conditions of\r\n+      this License, without any additional terms or conditions.\r\n+      Notwithstanding the above, nothing herein shall supersede or modify\r\n+      the terms of any separate license agreement you may have executed\r\n+      with Licensor regarding such Contributions.\r\n+\r\n+   6. Trademarks. This License does not grant permission to use the trade\r\n+      names, trademarks, service marks, or product names of the Licensor,\r\n+      except as required for reasonable and customary use in describing the\r\n+      origin of the Work and reproducing the content of the NOTICE file.\r\n+\r\n+   7. Disclaimer of Warranty. Unless required by applicable law or\r\n+      agreed to in writing, Licensor provides the Work (and each\r\n+      Contributor provides its Contributions) on an \"AS IS\" BASIS,\r\n+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\r\n+      implied, including, without limitation, any warranties or conditions\r\n+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\r\n+      PARTICULAR PURPOSE. You are solely responsible for determining the\r\n+      appropriateness of using or redistributing the Work and assume any\r\n+      risks associated with Your exercise of permissions under this License.\r\n+\r\n+   8. Limitation of Liability. In no event and under no legal theory,\r\n+      whether in tort (including negligence), contract, or otherwise,\r\n+      unless required by applicable law (such as deliberate and grossly\r\n+      negligent acts) or agreed to in writing, shall any Contributor be\r\n+      liable to You for damages, including any direct, indirect, special,\r\n+      incidental, or consequential damages of any character arising as a\r\n+      result of this License or out of the use or inability to use the\r\n+      Work (including but not limited to damages for loss of goodwill,\r\n+      work stoppage, computer failure or malfunction, or any and all\r\n+      other commercial damages or losses), even if such Contributor\r\n+      has been advised of the possibility of such damages.\r\n+\r\n+   9. Accepting Warranty or Additional Liability. While redistributing\r\n+      the Work or Derivative Works thereof, You may choose to offer,\r\n+      and charge a fee for, acceptance of support, warranty, indemnity,\r\n+      or other liability obligations and/or rights consistent with this\r\n+      License. However, in accepting such obligations, You may act only\r\n+      on Your own behalf and on Your sole responsibility, not on behalf\r\n+      of any other Contributor, and only if You agree to indemnify,\r\n+      defend, and hold each Contributor harmless for any liability\r\n+      incurred by, or claims asserted against, such Contributor by reason\r\n+      of your accepting any such warranty or additional liability.\r\n+\r\n+   END OF TERMS AND CONDITIONS\r\n+\r\n+   APPENDIX: How to apply the Apache License to your work.\r\n+\r\n+      To apply the Apache License to your work, attach the following\r\n+      boilerplate notice, with the fields enclosed by brackets \"[]\"\r\n+      replaced with your own identifying information. (Don't include\r\n+      the brackets!)  The text should be enclosed in the appropriate\r\n+      comment syntax for the file format. We also recommend that a\r\n+      file or class name and description of purpose be included on the\r\n+      same \"printed page\" as the copyright notice for easier\r\n+      identification within third-party archives.\r\n+\r\n+   Copyright [yyyy] [name of copyright owner]\r\n+\r\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\r\n+   you may not use this file except in compliance with the License.\r\n+   You may obtain a copy of the License at\r\n+\r\n+       http://www.apache.org/licenses/LICENSE-2.0\r\n+\r\n+   Unless required by applicable law or agreed to in writing, software\r\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+   See the License for the specific language governing permissions and\r\n+   limitations under the License.\r"
            },
            {
                "sha": "689fe5cb3c715f2944fec30e43ccb8a2b10625d3",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Black.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Black.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Black.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Black.ttf?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "0b4e0ee108899ddfef739f48d2aa9475b8b41a03",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BlackItalic.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BlackItalic.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BlackItalic.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BlackItalic.ttf?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "d3f01ad245b628f386ac95786f53167038720eb2",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Bold.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Bold.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Bold.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Bold.ttf?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "41cc1e753153e343a0ab73a341f545fec9eb7816",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BoldItalic.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BoldItalic.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BoldItalic.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BoldItalic.ttf?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "6a1cee5b2948dbddf8fe6bb050a5cdca1c206dbf",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Italic.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Italic.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Italic.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Italic.ttf?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "219063a578a486b7c00262057efcbc44ebab0eeb",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Light.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Light.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Light.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Light.ttf?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "0e81e876fcab86b7cc99356aa0e19c0916a907bf",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-LightItalic.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-LightItalic.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-LightItalic.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-LightItalic.ttf?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "1a7f3b0bba45b7470a4240c3ec67595eeeb02192",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Medium.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Medium.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Medium.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Medium.ttf?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "003029527cc651faa12b18f02ce81d74f5031756",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-MediumItalic.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-MediumItalic.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-MediumItalic.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-MediumItalic.ttf?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "2c97eeadffe1a34bd67d3ff1c3887fd53e22c2ca",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Regular.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Regular.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Regular.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Regular.ttf?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "b74a4fd1a2ed1960da1d0f8f0e9b8d05a5819000",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Thin.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Thin.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Thin.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Thin.ttf?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "dd0ddb852645db815ce4f3d45ec4a37b601ccb5f",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-ThinItalic.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-ThinItalic.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-ThinItalic.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-ThinItalic.ttf?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "7c16b0d1287d0e9e69efd568f85bbc7989d3e631",
                "filename": "django/bdaSite/bdaProject/static/js/chart.min.js",
                "status": "added",
                "additions": 7,
                "deletions": 0,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/js/chart.min.js",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/js/chart.min.js",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/js/chart.min.js?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "072e308110fcf5d9e1c32750aba69557909dc82f",
                "filename": "django/bdaSite/bdaProject/static/js/jquery-3.1.1.js",
                "status": "added",
                "additions": 10220,
                "deletions": 0,
                "changes": 10220,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/js/jquery-3.1.1.js",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/js/jquery-3.1.1.js",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/js/jquery-3.1.1.js?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "f7b76adde388fa12f5163d37ab4f55fa707e0fdd",
                "filename": "django/bdaSite/bdaProject/static/js/main.js",
                "status": "added",
                "additions": 29,
                "deletions": 0,
                "changes": 29,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/js/main.js",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/js/main.js",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/js/main.js?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,29 @@\n+function show(val, element, file) {\n+  //element = '#' + element;\n+\n+  if (val == \"\") {\n+    //$(element).html(\"\");\n+    document.getElementById(element).innerHTML = \"\";\n+    return;\n+  }\n+  else {\n+    if (window.XMLHttpRequest) {\n+      // code for IE7+, Firefox, Chrome, Opera, Safari\n+      xmlhttp = new XMLHttpRequest();\n+    }\n+    else {\n+      // code for IE6, IE5\n+      xmlhttp = new ActiveXObject(\"Microsoft.XMLHTTP\");\n+    }\n+    xmlhttp.onreadystatechange = function() {\n+      if (xmlhttp.readyState == 4 && xmlhttp.status == 200) {\n+        //$(element).html(xmlhttp.responseText);\n+        document.getElementById(element).innerHTML = xmlhttp.responseText;\n+      }\n+    }\n+    xmlhttp.open(\"GET\", file+\"?q=\"+val, true);\n+    xmlhttp.send();\n+    return val;\n+    console.log(val+\" \"+element+\" \"+file);\n+  }\n+}"
            },
            {
                "sha": "fc440ee58184fa72e377315a0a466358e0cc3025",
                "filename": "django/bdaSite/bdaProject/static/js/materialize.js",
                "status": "added",
                "additions": 8297,
                "deletions": 0,
                "changes": 8297,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/js/materialize.js",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/js/materialize.js",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/js/materialize.js?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "57dfd28c3e173e1fd8013d5eec938e2f3c927969",
                "filename": "django/bdaSite/bdaProject/static/js/materialize.min.js",
                "status": "added",
                "additions": 10,
                "deletions": 0,
                "changes": 10,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/js/materialize.min.js",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/js/materialize.min.js",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/js/materialize.min.js?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "5b695942c0ac1b758ea6591c97778062a0a98201",
                "filename": "django/bdaSite/bdaProject/static/js/terminal.js",
                "status": "added",
                "additions": 185,
                "deletions": 0,
                "changes": 185,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/js/terminal.js",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/static/js/terminal.js",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/js/terminal.js?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,185 @@\n+/*! terminal.js v2.0 | (c) 2014 Erik \u00d6sterberg | https://github.com/eosterberg/terminaljs */\n+\n+var Terminal = (function () {\n+\t// PROMPT_TYPE\n+\tvar PROMPT_INPUT = 1, PROMPT_PASSWORD = 2, PROMPT_CONFIRM = 3\n+\n+\tvar fireCursorInterval = function (inputField, terminalObj) {\n+\t\tvar cursor = terminalObj._cursor\n+\t\tsetTimeout(function () {\n+\t\t\tif (inputField.parentElement && terminalObj._shouldBlinkCursor) {\n+\t\t\t\tcursor.style.visibility = cursor.style.visibility === 'visible' ? 'hidden' : 'visible'\n+\t\t\t\tfireCursorInterval(inputField, terminalObj)\n+\t\t\t} else {\n+\t\t\t\tcursor.style.visibility = 'visible'\n+\t\t\t}\n+\t\t}, 500)\n+\t}\n+\n+\tvar firstPrompt = true;\n+\tpromptInput = function (terminalObj, message, PROMPT_TYPE, callback) {\n+\t\tvar shouldDisplayInput = (PROMPT_TYPE === PROMPT_INPUT)\n+\t\tvar inputField = document.createElement('input')\n+\n+\t\tinputField.style.position = 'absolute'\n+\t\tinputField.style.zIndex = '-100'\n+\t\tinputField.style.outline = 'none'\n+\t\tinputField.style.border = 'none'\n+\t\tinputField.style.opacity = '0'\n+\t\tinputField.style.fontSize = '0.2em'\n+\n+\t\tterminalObj._inputLine.textContent = ''\n+\t\tterminalObj._input.style.display = 'block'\n+\t\tterminalObj.html.appendChild(inputField)\n+\t\tfireCursorInterval(inputField, terminalObj)\n+\n+\t\tif (message.length) terminalObj.print(PROMPT_TYPE === PROMPT_CONFIRM ? message + ' (y/n)' : message)\n+\n+\t\tinputField.onblur = function () {\n+\t\t\tterminalObj._cursor.style.display = 'none'\n+\t\t}\n+\n+\t\tinputField.onfocus = function () {\n+\t\t\tinputField.value = terminalObj._inputLine.textContent\n+\t\t\tterminalObj._cursor.style.display = 'inline'\n+\t\t}\n+\n+\t\tterminalObj.html.onclick = function () {\n+\t\t\tinputField.focus()\n+\t\t}\n+\n+\t\tinputField.onkeydown = function (e) {\n+\t\t\tif (e.which === 37 || e.which === 39 || e.which === 38 || e.which === 40 || e.which === 9) {\n+\t\t\t\te.preventDefault()\n+\t\t\t} else if (shouldDisplayInput && e.which !== 13) {\n+\t\t\t\tsetTimeout(function () {\n+\t\t\t\t\tterminalObj._inputLine.textContent = inputField.value\n+\t\t\t\t}, 1)\n+\t\t\t}\n+\t\t}\n+\t\tinputField.onkeyup = function (e) {\n+\t\t\tif (PROMPT_TYPE === PROMPT_CONFIRM || e.which === 13) {\n+\t\t\t\tterminalObj._input.style.display = 'none'\n+\t\t\t\tvar inputValue = inputField.value\n+\t\t\t\tif (shouldDisplayInput) terminalObj.print(inputValue)\n+\t\t\t\tterminalObj.html.removeChild(inputField)\n+\t\t\t\tif (typeof(callback) === 'function') {\n+\t\t\t\t\tif (PROMPT_TYPE === PROMPT_CONFIRM) {\n+\t\t\t\t\t\tcallback(inputValue.toUpperCase()[0] === 'Y' ? true : false)\n+\t\t\t\t\t} else callback(inputValue)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif (firstPrompt) {\n+\t\t\tfirstPrompt = false\n+\t\t\tsetTimeout(function () { inputField.focus()\t}, 50)\n+\t\t} else {\n+\t\t\tinputField.focus()\n+\t\t}\n+\t}\n+\n+\tvar terminalBeep\n+\n+\tvar TerminalConstructor = function (id) {\n+\t\tif (!terminalBeep) {\n+\t\t\tterminalBeep = document.createElement('audio')\n+\t\t\tvar source = '<source src=\"http://www.erikosterberg.com/terminaljs/beep.'\n+\t\t\tterminalBeep.innerHTML = source + 'mp3\" type=\"audio/mpeg\">' + source + 'ogg\" type=\"audio/ogg\">'\n+\t\t\tterminalBeep.volume = 0.05\n+\t\t}\n+\n+\t\tthis.html = document.createElement('div')\n+\t\tthis.html.className = 'Terminal'\n+\t\tif (typeof(id) === 'string') { this.html.id = id }\n+\n+\t\tthis._innerWindow = document.createElement('div')\n+\t\tthis._output = document.createElement('p')\n+\t\tthis._inputLine = document.createElement('span') //the span element where the users input is put\n+\t\tthis._cursor = document.createElement('span')\n+\t\tthis._input = document.createElement('p') //the full element administering the user input, including cursor\n+\n+\t\tthis._shouldBlinkCursor = true\n+\n+\t\tthis.beep = function () {\n+\t\t\tterminalBeep.load()\n+\t\t\tterminalBeep.play()\n+\t\t}\n+\n+\t\tthis.print = function (message) {\n+\t\t\tvar newLine = document.createElement('div')\n+\t\t\tnewLine.textContent = message\n+\t\t\tthis._output.appendChild(newLine)\n+\t\t}\n+\n+\t\tthis.input = function (message, callback) {\n+\t\t\tpromptInput(this, message, PROMPT_INPUT, callback)\n+\t\t}\n+\n+\t\tthis.password = function (message, callback) {\n+\t\t\tpromptInput(this, message, PROMPT_PASSWORD, callback)\n+\t\t}\n+\n+\t\tthis.confirm = function (message, callback) {\n+\t\t\tpromptInput(this, message, PROMPT_CONFIRM, callback)\n+\t\t}\n+\n+\t\tthis.clear = function () {\n+\t\t\tthis._output.innerHTML = ''\n+\t\t}\n+\n+\t\tthis.sleep = function (milliseconds, callback) {\n+\t\t\tsetTimeout(callback, milliseconds)\n+\t\t}\n+\n+\t\tthis.setTextSize = function (size) {\n+\t\t\tthis._output.style.fontSize = size\n+\t\t\tthis._input.style.fontSize = size\n+\t\t}\n+\n+\t\tthis.setTextColor = function (col) {\n+\t\t\tthis.html.style.color = col\n+\t\t\tthis._cursor.style.background = col\n+\t\t}\n+\n+\t\tthis.setBackgroundColor = function (col) {\n+\t\t\tthis.html.style.background = col\n+\t\t}\n+\n+\t\tthis.setWidth = function (width) {\n+\t\t\tthis.html.style.width = width\n+\t\t}\n+\n+\t\tthis.setHeight = function (height) {\n+\t\t\tthis.html.style.height = height\n+\t\t}\n+\n+\t\tthis.blinkingCursor = function (bool) {\n+\t\t\tbool = bool.toString().toUpperCase()\n+\t\t\tthis._shouldBlinkCursor = (bool === 'TRUE' || bool === '1' || bool === 'YES')\n+\t\t}\n+\n+\t\tthis._input.appendChild(this._inputLine)\n+\t\tthis._input.appendChild(this._cursor)\n+\t\tthis._innerWindow.appendChild(this._output)\n+\t\tthis._innerWindow.appendChild(this._input)\n+\t\tthis.html.appendChild(this._innerWindow)\n+\n+\t\tthis.setBackgroundColor('black')\n+\t\tthis.setTextColor('white')\n+\t\tthis.setTextSize('1em')\n+\t\tthis.setWidth('100%')\n+\t\tthis.setHeight('100%')\n+\n+\t\tthis.html.style.fontFamily = 'Monaco, Courier'\n+\t\tthis.html.style.margin = '0'\n+\t\tthis._innerWindow.style.padding = '10px'\n+\t\tthis._input.style.margin = '0'\n+\t\tthis._output.style.margin = '0'\n+\t\tthis._cursor.style.background = 'white'\n+\t\tthis._cursor.innerHTML = 'C' //put something in the cursor..\n+\t\tthis._cursor.style.display = 'none' //then hide it\n+\t\tthis._input.style.display = 'none'\n+\t}\n+\n+\treturn TerminalConstructor\n+}())\n\\ No newline at end of file"
            },
            {
                "sha": "b41ad27c60f86b852a9cf345885c611bbef7776c",
                "filename": "django/bdaSite/bdaProject/templates/iotdp/graph.html",
                "status": "added",
                "additions": 129,
                "deletions": 0,
                "changes": 129,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/templates/iotdp/graph.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/templates/iotdp/graph.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/iotdp/graph.html?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,129 @@\n+{% load static %}\n+<!DOCTYPE html>\n+<html lang=\"en\" dir=\"ltr\">\n+  <head>\n+    <meta charset=\"utf-8\">\n+    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n+    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n+    <style>\n+     @font-face\n+     { font-family: R; src: url({% static 'fonts/Roboto/Roboto-Regular.ttf' %}); }\n+\n+    </style>\n+    <title></title>\n+  </head>\n+  <body id='body' style=\"font-family:R\">\n+    <!-- Navbar goes here -->\n+    <nav>\n+      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n+        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform</a>\n+      </div>\n+    </nav>\n+\n+    <!-- Page Layout here -->\n+    <div class=\"row\">\n+      <div id='col1' class=\"col s3\" style=\"background:#336699;\">\n+        <a href=\"/iotdp/{{ user }}\">\n+          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n+            OVERALL VIEW\n+          </div>\n+        </a>\n+        <a href=\"/iotdp/{{ user }}/graph\">\n+          <div class=\"col s12 m12 l12\" style=\"color:#17365D;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px;  background:#F1BA65\">\n+            GRAPH\n+          </div>\n+        </a>\n+        <a href=\"/iotdp/{{ user }}/logs\">\n+          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n+            LOGS\n+          </div>\n+        </a>\n+      </div>\n+\n+      <div id='col2' class=\"col s9\" style=\"background: #EBEBD3;\">\n+        <h4 class=\"center\">POWER CONSUMPTION IN THE LAST 30 MINUTES</h4>\n+        <canvas id=\"chart1\" class=\"\" style=\"width:100%;\"></canvas>\n+        <div class=\"row\">\n+          <div class=\"col m8 offset-m2\">\n+            <div class=\"card center\" style=\"background: #EBEBD3;\">\n+              <h5>Generated Power from Solar and other Sources: {{ gen_pow }} kW</h5>\n+            </div>\n+          </div>\n+          <div class=\"col m8 offset-m2\">\n+            <div class=\"card center\" style=\"background: #EBEBD3;\">\n+              <table>\n+                <tr>\n+                  <th>Small Appliances</th>\n+                  <td>Microwave, Dishwasher, Fridge</td>\n+                </tr>\n+                <tr>\n+                  <th>Big Appliances</th>\n+                  <td>Garage Door, Well, Barn, Furnaces</td>\n+                </tr>\n+                <tr>\n+                  <th>Rooms</th>\n+                  <td>Home Office, Wine Cellar, Living Room, Kitchens</td>\n+                </tr>\n+              </table>\n+            </div>\n+          </div>\n+        </div>\n+      </div>\n+\n+    </div>\n+\n+    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n+    <script src={% static 'js/materialize.min.js' %}></script>\n+    <script src={% static 'js/chart.min.js' %}></script>\n+    <script type=\"text/javascript\">\n+      navHeight = $('#nav').height();\n+      columnHeight = (window.innerHeight - navHeight);\n+      $('#col1').height(columnHeight);\n+      $('#col2').height(columnHeight);\n+    </script>\n+    <script type=\"text/javascript\">\n+      var ctx3 = document.getElementById(\"chart1\");\n+      var myChart = new Chart(ctx3, {\n+          type: 'bar',\n+          data: {\n+              labels: {% autoescape off %} {{ graph_labels }} {% endautoescape %},\n+              datasets: [{\n+                  label: 'Number of Units [kW]',\n+                  data: {% autoescape off %} {{ graph_data }} {% endautoescape %},\n+                  backgroundColor: [\n+                      '#FDB44B',\n+                      '#FDB44B',\n+                      '#FDB44B'\n+                  ],\n+                  borderColor: [\n+                      '#FDB44B',\n+                      '#FDB44B',\n+                      '#FDB44B'\n+                  ],\n+                  borderWidth: 1\n+              }]\n+          },\n+          options: {\n+              legend: {\n+                  labels: {\n+                      fontColor:\"black\"\n+                  }\n+              },\n+              scales: {\n+                xAxes: [{\n+                    ticks: {\n+                        fontColor:\"black\"\n+                    }\n+                }],\n+                yAxes: [{\n+                    ticks: {\n+                        beginAtZero:true,\n+                        fontColor:\"black\"\n+                    }\n+                }]\n+              }\n+          }\n+      });\n+    </script>\n+  </body>\n+</html>"
            },
            {
                "sha": "ab918e5c83d9f39a659db66c4957aa57ccfcd967",
                "filename": "django/bdaSite/bdaProject/templates/iotdp/index.html",
                "status": "added",
                "additions": 170,
                "deletions": 0,
                "changes": 170,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/templates/iotdp/index.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/templates/iotdp/index.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/iotdp/index.html?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,170 @@\n+{% load static %}\n+<!DOCTYPE html>\n+<html lang=\"en\" dir=\"ltr\">\n+  <head>\n+    <meta charset=\"utf-8\">\n+    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n+    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n+    <style>\n+     @font-face\n+     { font-family: R; src: url({% static 'fonts/Roboto/Roboto-Regular.ttf' %}); }\n+\n+    </style>\n+    <title></title>\n+  </head>\n+  <body id='body' style=\"font-family:R\">\n+    <!-- Navbar goes here -->\n+    <nav>\n+      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n+        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform</a>\n+      </div>\n+    </nav>\n+\n+    <!-- Page Layout here -->\n+    <div class=\"row\">\n+\n+      <div id='col1' class=\"col s3\" style=\"background:#336699;\">\n+        <a href=\"/iotdp/{{ user }}\">\n+          <div class=\"col s12 m12 l12\" style=\"color:#17365D;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; background:#F1BA65; padding:0px\">\n+            OVERALL VIEW\n+          </div>\n+        </a>\n+        <a href=\"/iotdp/{{ user }}/graph\">\n+          <div class=\"col s12 m12 l12 white-text\" style=\"display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px\">\n+            GRAPH\n+          </div>\n+        </a>\n+        <a href=\"/iotdp/{{ user }}/logs\">\n+          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n+            LOGS\n+          </div>\n+        </a>\n+      </div>\n+\n+      <div id='col2' class=\"col s9 center\" style=\"background: #EBEBD3;\">\n+        <div style=\"width:100%; overflow-x:scroll\">\n+          <table style=\"\">\n+            <thead id='tableHead'>\n+              <th>Timestamp</th>\n+              <th>Use</th>\n+              <th>General</th>\n+              <th>House Overall</th>\n+              <th>Dishwasher</th>\n+              <th>Furnace1</th>\n+              <th>Furnace2</th>\n+              <th>Home Office</th>\n+              <th>Fridge</th>\n+              <th>Wine Cellar</th>\n+              <th>Garage Door</th>\n+              <th>Kitchen 12</th>\n+              <th>Kitchen 14</th>\n+              <th>Kitchen 38</th>\n+              <th>Barn</th>\n+              <th>Well</th>\n+              <th>Microwave</th>\n+              <th>Living Room</th>\n+              <th>Solar</th>\n+              <th>Temperature</th>\n+              <th>Icon</th>\n+              <th>Humidity</th>\n+              <th>Visibility</th>\n+              <th>Summary</th>\n+              <th>Apparent Temperature</th>\n+              <th>Pressure</th>\n+              <th>Wind Speed</th>\n+              <th>Cloud Cover</th>\n+              <th>Wind Bearing</th>\n+              <th>Precipitation Intensity</th>\n+              <th>Dew Point</th>\n+              <th>Precipitation Porbab</th>\n+            </thead>\n+            <tbody id='tableBody'>\n+\n+            </tbody>\n+          </table>\n+          <a id='prev' class=\"btn-floating\" style=\"background: #17365D\"><i class=\"material-icons\">keyboard_arrow_left</i></a>\n+          <a id='next' class=\"btn-floating\" style=\"background: #17365D\"><i class=\"material-icons\">keyboard_arrow_right</i></a>\n+        </div>\n+      </div>\n+\n+    </div>\n+\n+    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n+    <script src={% static 'js/materialize.min.js' %}></script>\n+    <script src={% static 'js/chart.min.js' %}></script>\n+    <script type=\"text/javascript\">\n+      navHeight = $('#nav').height();\n+      columnHeight = (window.innerHeight - navHeight);\n+      $('#col1').height(columnHeight);\n+      $('#col2').height(columnHeight);\n+    </script>\n+    <script type=\"text/javascript\">\n+        current_index = 0;\n+        size = {{ size }};\n+        function dataUpdate(current_index)\n+        {\n+          $.get('http://35.230.40.217:5000/get_data/{{ user }}/'+current_index, function(data, status) {\n+            for(i=0; i<data['result'].length; i++)\n+            {\n+              row = \"<tr>\";\n+              date = new Date(parseInt(data['result'][i]['time'])*1000).toLocaleString();\n+              row += \"<td>\"+date+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['use1']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['gen']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['house_overall']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['dishwasher']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['furnace1']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['furnace2']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['home_office']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['fridge']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['wine_cellar']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['garage_door']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['kitchen12']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['kitchen14']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['kitchen38']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['barn']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['well']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['microwave']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['living_room']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['solar']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['temperature']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+data['result'][i]['icon']+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['humidity']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['visibility']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+data['result'][i]['summary']+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['apparent_temp']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['pressure']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['wind_speed']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+data['result'][i]['cloud_cover']+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['wind_bearing']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['precip_intensity']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['dew_point']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['precip_probability']* 10000)) / 10000)+\"</td>\";\n+              row += \"</tr>\";\n+              $('#tableBody').append(row);\n+            }\n+          });\n+        }\n+\n+        dataUpdate(0);\n+        // alert(size);\n+        $('#prev').click(function(){\n+            if(current_index > 0)\n+            {\n+              $('#tableBody').empty();\n+              current_index = current_index - 7;\n+              dataUpdate(current_index);\n+            }\n+        });\n+        $('#next').click(function(){\n+            if (current_index <= size-7)\n+            {\n+              $('#tableBody').empty();\n+              current_index = current_index + 7;\n+              dataUpdate(current_index);\n+            }\n+        });\n+\n+    </script>\n+  </body>\n+</html>"
            },
            {
                "sha": "0df695c397fe3d580ee82eb06165abce940e55cf",
                "filename": "django/bdaSite/bdaProject/templates/iotdp/logs.html",
                "status": "added",
                "additions": 74,
                "deletions": 0,
                "changes": 74,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/templates/iotdp/logs.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/templates/iotdp/logs.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/iotdp/logs.html?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,74 @@\n+{% load static %}\n+<!DOCTYPE html>\n+<html lang=\"en\" dir=\"ltr\">\n+  <head>\n+    <meta charset=\"utf-8\">\n+    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n+    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n+    <style>\n+     @font-face\n+     { font-family: R; src: url({% static 'fonts/Roboto/Roboto-Regular.ttf' %}); }\n+\n+    </style>\n+    <script src=\"{% static 'js/terminal.js' %}\"></script>\n+    <script type=\"text/javascript\">\n+\n+    </script>\n+    <title></title>\n+  </head>\n+  <body id='body' style=\"font-family:R\">\n+    <!-- Navbar goes here -->\n+    <nav>\n+      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n+        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform - Logs</a>\n+      </div>\n+    </nav>\n+\n+    <div class=\"row\">\n+      <div id='col1' class=\"col s3\" style=\"background:#336699;\">\n+        <a href=\"/iotdp/{{ user }}\">\n+          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n+            OVERALL VIEW\n+          </div>\n+        </a>\n+        <a href=\"/iotdp/{{ user }}/graph\">\n+          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px;\">\n+            GRAPH\n+          </div>\n+        </a>\n+        <a href=\"/iotdp/{{ user }}/logs\">\n+          <div class=\"col s12 m12 l12\" style=\"color:#17365D;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px;  background:#F1BA65\">\n+            LOGS\n+          </div>\n+        </a>\n+      </div>\n+\n+      <div id='col2' class=\"col s9 black\" style=\"height:750px;background: #EBEBD3; margin:0px; padding:0px; overflow-y:scroll\">\n+        <div id='term' class=\"row\" style=\"height:750px;display:flex; margin:0px; padding:0px; overflow-y:scroll;\">\n+\n+        </div>\n+      </div>\n+\n+    <!-- Page Layout here -->\n+\n+\n+    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n+    <script src={% static 'js/materialize.min.js' %}></script>\n+    <script src={% static 'js/chart.min.js' %}></script>\n+    <script type=\"text/javascript\">\n+      navHeight = $('#nav').height();\n+      columnHeight = (window.innerHeight - navHeight);\n+      $('#col1').height(columnHeight);\n+    </script>\n+    <script type=\"text/javascript\">\n+      var term = new Terminal();\n+      var logs = {% autoescape off %} {{ logs }} {% endautoescape %}\n+      var logLines = logs.split(';;;')\n+      for(var i=0; i<logLines.length; i++)\n+      {\n+          term.print(logLines[i]);\n+      }\n+      $('#term').append(term.html)\n+    </script>\n+  </body>\n+</html>"
            },
            {
                "sha": "7ce503c2dd97ba78597f6ff6e4393132753573f6",
                "filename": "django/bdaSite/bdaProject/tests.py",
                "status": "added",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/tests.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/tests.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/tests.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,3 @@\n+from django.test import TestCase\n+\n+# Create your tests here."
            },
            {
                "sha": "8217837f53c43e1ac10f97a7e9aaf176a076e31e",
                "filename": "django/bdaSite/bdaProject/urls.py",
                "status": "added",
                "additions": 7,
                "deletions": 0,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/urls.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/urls.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/urls.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,7 @@\n+from django.urls import path\n+from . import views\n+\n+urlpatterns = [\n+    path('', views.index, name='index'),\n+    path('trial/', views.trial, name='trial')\n+]\n\\ No newline at end of file"
            },
            {
                "sha": "ccefa56bda69b38a6ae6aedfd7e3413e127754a4",
                "filename": "django/bdaSite/bdaProject/views.py",
                "status": "added",
                "additions": 9,
                "deletions": 0,
                "changes": 9,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/views.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaProject/views.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/views.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,9 @@\n+from django.shortcuts import render\n+from django.http import HttpResponse\n+\n+\n+def index(request):\n+    return HttpResponse(\"Hello, world. You're at the bdaProject index.\")\n+\n+def trial(request):\n+    return HttpResponse(\"Trial\")"
            },
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "django/bdaSite/bdaSite/__init__.py",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__init__.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "ef448b20dac8ff827a0b3cb100f906e75060ff4c",
                "filename": "django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "229b2217b6530366aa04af8c165f77af971e8d27",
                "filename": "django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "abf7528accc4cd0182652f8d597491fd778aca7a",
                "filename": "django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "45f5ff925cdf0aa1dac25e82e4b0a797c54b4c98",
                "filename": "django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "70d2b307cf04da098d5fcca6d7a9c154352265e0",
                "filename": "django/bdaSite/bdaSite/asgi.py",
                "status": "added",
                "additions": 16,
                "deletions": 0,
                "changes": 16,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/asgi.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/asgi.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/asgi.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,16 @@\n+\"\"\"\n+ASGI config for bdaSite project.\n+\n+It exposes the ASGI callable as a module-level variable named ``application``.\n+\n+For more information on this file, see\n+https://docs.djangoproject.com/en/3.0/howto/deployment/asgi/\n+\"\"\"\n+\n+import os\n+\n+from django.core.asgi import get_asgi_application\n+\n+os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'bdaSite.settings')\n+\n+application = get_asgi_application()"
            },
            {
                "sha": "7060e5a04b7390cdaecc582b15c3a6c6d4ebf136",
                "filename": "django/bdaSite/bdaSite/settings.py",
                "status": "added",
                "additions": 120,
                "deletions": 0,
                "changes": 120,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/settings.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/settings.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/settings.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,120 @@\n+\"\"\"\n+Django settings for bdaSite project.\n+\n+Generated by 'django-admin startproject' using Django 3.0.\n+\n+For more information on this file, see\n+https://docs.djangoproject.com/en/3.0/topics/settings/\n+\n+For the full list of settings and their values, see\n+https://docs.djangoproject.com/en/3.0/ref/settings/\n+\"\"\"\n+\n+import os\n+\n+# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\n+BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+\n+\n+# Quick-start development settings - unsuitable for production\n+# See https://docs.djangoproject.com/en/3.0/howto/deployment/checklist/\n+\n+# SECURITY WARNING: keep the secret key used in production secret!\n+SECRET_KEY = '8ub_@r6)4)29i4%c&q12!je_)20l(=m977f6d)oa9sx&1tr8h4'\n+\n+# SECURITY WARNING: don't run with debug turned on in production!\n+DEBUG = True\n+\n+ALLOWED_HOSTS = []\n+\n+\n+# Application definition\n+\n+INSTALLED_APPS = [\n+    'django.contrib.admin',\n+    'django.contrib.auth',\n+    'django.contrib.contenttypes',\n+    'django.contrib.sessions',\n+    'django.contrib.messages',\n+    'django.contrib.staticfiles',\n+]\n+\n+MIDDLEWARE = [\n+    'django.middleware.security.SecurityMiddleware',\n+    'django.contrib.sessions.middleware.SessionMiddleware',\n+    'django.middleware.common.CommonMiddleware',\n+    'django.middleware.csrf.CsrfViewMiddleware',\n+    'django.contrib.auth.middleware.AuthenticationMiddleware',\n+    'django.contrib.messages.middleware.MessageMiddleware',\n+    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n+]\n+\n+ROOT_URLCONF = 'bdaSite.urls'\n+\n+TEMPLATES = [\n+    {\n+        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n+        'DIRS': [],\n+        'APP_DIRS': True,\n+        'OPTIONS': {\n+            'context_processors': [\n+                'django.template.context_processors.debug',\n+                'django.template.context_processors.request',\n+                'django.contrib.auth.context_processors.auth',\n+                'django.contrib.messages.context_processors.messages',\n+            ],\n+        },\n+    },\n+]\n+\n+WSGI_APPLICATION = 'bdaSite.wsgi.application'\n+\n+\n+# Database\n+# https://docs.djangoproject.com/en/3.0/ref/settings/#databases\n+\n+DATABASES = {\n+    'default': {\n+        'ENGINE': 'django.db.backends.sqlite3',\n+        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n+    }\n+}\n+\n+\n+# Password validation\n+# https://docs.djangoproject.com/en/3.0/ref/settings/#auth-password-validators\n+\n+AUTH_PASSWORD_VALIDATORS = [\n+    {\n+        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n+    },\n+    {\n+        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n+    },\n+    {\n+        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n+    },\n+    {\n+        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n+    },\n+]\n+\n+\n+# Internationalization\n+# https://docs.djangoproject.com/en/3.0/topics/i18n/\n+\n+LANGUAGE_CODE = 'en-us'\n+\n+TIME_ZONE = 'UTC'\n+\n+USE_I18N = True\n+\n+USE_L10N = True\n+\n+USE_TZ = True\n+\n+\n+# Static files (CSS, JavaScript, Images)\n+# https://docs.djangoproject.com/en/3.0/howto/static-files/\n+\n+STATIC_URL = '/static/'"
            },
            {
                "sha": "2cebd51ae478237fdddb02335522080361ee1add",
                "filename": "django/bdaSite/bdaSite/urls.py",
                "status": "added",
                "additions": 23,
                "deletions": 0,
                "changes": 23,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/urls.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/urls.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/urls.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,23 @@\n+\"\"\"bdaSite URL Configuration\n+\n+The `urlpatterns` list routes URLs to views. For more information please see:\n+    https://docs.djangoproject.com/en/3.0/topics/http/urls/\n+Examples:\n+Function views\n+    1. Add an import:  from my_app import views\n+    2. Add a URL to urlpatterns:  path('', views.home, name='home')\n+Class-based views\n+    1. Add an import:  from other_app.views import Home\n+    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')\n+Including another URLconf\n+    1. Import the include() function: from django.urls import include, path\n+    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))\n+\"\"\"\n+from django.contrib import admin\n+from django.urls import path\n+from django.urls import include\n+\n+urlpatterns = [\n+    path('admin/', admin.site.urls),\n+    path('bdaProject/', include('bdaProject.urls')),\n+]"
            },
            {
                "sha": "1b6950bce23de2d17f71c0529288332c18fdf303",
                "filename": "django/bdaSite/bdaSite/wsgi.py",
                "status": "added",
                "additions": 16,
                "deletions": 0,
                "changes": 16,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/wsgi.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/bdaSite/wsgi.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/wsgi.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,16 @@\n+\"\"\"\n+WSGI config for bdaSite project.\n+\n+It exposes the WSGI callable as a module-level variable named ``application``.\n+\n+For more information on this file, see\n+https://docs.djangoproject.com/en/3.0/howto/deployment/wsgi/\n+\"\"\"\n+\n+import os\n+\n+from django.core.wsgi import get_wsgi_application\n+\n+os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'bdaSite.settings')\n+\n+application = get_wsgi_application()"
            },
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "django/bdaSite/db.sqlite3",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/db.sqlite3",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/db.sqlite3",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/db.sqlite3?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "f2d2f3482d56576cbaba5805bdbc00a6143f9f0a",
                "filename": "django/bdaSite/manage.py",
                "status": "added",
                "additions": 21,
                "deletions": 0,
                "changes": 21,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/manage.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/django/bdaSite/manage.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/manage.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,21 @@\n+#!/usr/bin/env python\n+\"\"\"Django's command-line utility for administrative tasks.\"\"\"\n+import os\n+import sys\n+\n+\n+def main():\n+    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'bdaSite.settings')\n+    try:\n+        from django.core.management import execute_from_command_line\n+    except ImportError as exc:\n+        raise ImportError(\n+            \"Couldn't import Django. Are you sure it's installed and \"\n+            \"available on your PYTHONPATH environment variable? Did you \"\n+            \"forget to activate a virtual environment?\"\n+        ) from exc\n+    execute_from_command_line(sys.argv)\n+\n+\n+if __name__ == '__main__':\n+    main()"
            },
            {
                "sha": "cc208b1d8e40b0382fafa2737a63ce76cbcc0e80",
                "filename": "github-crawler/Templates_commands/org_template.txt",
                "status": "modified",
                "additions": 2,
                "deletions": 7,
                "changes": 9,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/Templates_commands/org_template.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/Templates_commands/org_template.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/org_template.txt?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -5,18 +5,13 @@ PUT _template/org_template\n     \"number_of_shards\": 1\n   },\n   \"mappings\": {\n-    \"_source\": {\n-      \"enabled\": false\n-    },\n     \"properties\": {\n-      \"host_name\": {\n-        \"type\": \"keyword\"\n-      },\n     \"login\": {\n         \"type\": \"keyword\"\n       },\n     \"id\": {\n-      \"type\":\"long\"},\n+      \"type\":\"long\"\n+      },\n     \"node_id\": {\n         \"type\": \"keyword\"\n       },"
            },
            {
                "sha": "64153d1d9e5a15babcbf8d01a8a6065bc47a1d34",
                "filename": "github-crawler/Templates_commands/put_org_index.txt",
                "status": "modified",
                "additions": 2,
                "deletions": 10,
                "changes": 12,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/Templates_commands/put_org_index.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/Templates_commands/put_org_index.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/put_org_index.txt?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -1,21 +1,13 @@\n PUT /org1\n {\n-    \"settings\" : {\n-        \"number_of_shards\" : 1\n-    },\n     \"mappings\": {\n-    \"_source\": {\n-      \"enabled\": false\n-    },\n     \"properties\": {\n-      \"host_name\": {\n-        \"type\": \"keyword\"\n-      },\n     \"login\": {\n         \"type\": \"keyword\"\n       },\n     \"id\": {\n-      \"type\":\"long\"},\n+      \"type\":\"long\"\n+      },\n     \"node_id\": {\n         \"type\": \"keyword\"\n       },"
            },
            {
                "sha": "048facb90e501192503e0571f4c76dc840368c4e",
                "filename": "github-crawler/Templates_commands/users_command.txt",
                "status": "added",
                "additions": 61,
                "deletions": 0,
                "changes": 61,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/Templates_commands/users_command.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/Templates_commands/users_command.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/users_command.txt?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -0,0 +1,61 @@\n+PUT /users\n+{\n+    \"mappings\": {\n+    \"properties\": {\n+    \"login\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"id\": {\n+        \"type\": \"long\"\n+      },\n+    \"node_id\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"avatar_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"gravatar_id\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"html_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"followers_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"following_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"gists_url\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"starred_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"subscriptions_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"organizations_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"repos_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"events_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"received_events_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"type\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"site_admin\": {\n+        \"type\": \"boolean\"\n+      }\n+  }\n+    }\n+}"
            },
            {
                "sha": "e15721c8f9b03fd0685d71c58785284e64479785",
                "filename": "github-crawler/app.py",
                "status": "modified",
                "additions": 6,
                "deletions": 0,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/app.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -20,6 +20,7 @@ def org_parser(orgname):\n     org_data = helper.get_org_information(OWNER,github_api)\n     print(\"Got Org Info!!!\")\n     print(\"sending Org Info to elastic search!!!\")\n+    #print(org_data)\n     helper.send_to_elasticInstance(org_data,'org1',org_data['id'])\n     print(\"Getting Repos for \"+orgname)\n     repo_list = helper.get_repositories(OWNER,github_api)\n@@ -28,6 +29,11 @@ def org_parser(orgname):\n         repo['license']=\"test\"\n         helper.send_to_elasticInstance(repo,'repos',repo['id'])\n         print(\"repo sent \"+ repo['name'] )\n+    print(\"Getting Org Members for \"+orgname)\n+    member_list = helper.get_org_users(OWNER,github_api)\n+    print(\"sending user info to elasticsearch\")\n+    for member in member_list:\n+        helper.send_to_elasticInstance(member,'users',member['id'])\n     print(\"Done!!!!!!!!!\")\n     return 'We got your org name ' + orgname + ' give us some time to process your request, please check server output for progress'\n "
            },
            {
                "sha": "8f83b6e5582748c864c9edff6955c71fbba1b2d3",
                "filename": "github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/config.cpython-37.pyc?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "7074af8f8c78ed4ad46a924622f2bd31632fe95f",
                "filename": "github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/helper.cpython-37.pyc?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b"
            },
            {
                "sha": "ff0119b94421c5e585e3fef879f3210ab2588e13",
                "filename": "github-crawler/lib/helper.py",
                "status": "modified",
                "additions": 47,
                "deletions": 6,
                "changes": 53,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/lib/helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/lib/helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/helper.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -12,9 +12,9 @@\n from elasticsearch import Elasticsearch\n \n GITEA_APP_URL = 'YOUR_GITEA_API'\n-GITEA_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+GITEA_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n GITHUB_USERNAME = 'vishwakulkarni'\n-GITHUB_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+GITHUB_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n SQL_ALCHEMY_STRING = ''\n \n \n@@ -44,7 +44,48 @@ def send_to_elasticInstance(self,data,index_name,id_val):\n         self.es.index(index=index_name, doc_type='_doc',id=id_val, body=data)\n     \n     def get_repositories(self,owner,api):\n-        url = api + '/orgs/{}/repos'.format(self.orgname)\n-        org_repos_data = self.gh_session.get(url = url)\n-        org_repos_data=json.loads(org_repos_data.content)\n-        return org_repos_data\n+        repos_list = []\n+        next = True\n+        i=1\n+        while next == True:\n+            url = api + '/orgs/{}/repos?page={}&per_page=20'.format(self.orgname,i)\n+            original_data = self.gh_session.get(url=url)\n+            repos = json.loads(original_data.content)\n+            for repo in repos:\n+                repos_list.append(repo)\n+            if 'Link' in original_data.headers:\n+                if 'rel=\"next\"' not in original_data.headers['Link']:\n+                    print(i)\n+                    next = False\n+            i = i + 1\n+        return repos_list\n+    \n+    def get_org_users(self,owner,api):\n+        members_list = []\n+        next = True\n+        i=1\n+        while next == True:\n+            url = api + '/orgs/{}/members?page={}&per_page=100'.format(self.orgname,i)\n+            original_data = self.gh_session.get(url=url)\n+            members = json.loads(original_data.content)\n+            for member in members:\n+                members_list.append(member)\n+            if 'Link' in original_data.headers:\n+                if 'rel=\"next\"' not in original_data.headers['Link']:\n+                    print(i)\n+                    next = False\n+            i = i + 1\n+        \n+        return members_list\n+\n+\n+\n+#testing comment after use\n+h = Helper()\n+github_api = \"https://api.github.com\"\n+h.set_org_name(\"CUBigDataClass\")\n+#print(h.get_org_information(\"vishwakulkarni\",github_api))\n+#k=h.get_repositories('vishwakulkarni',github_api)\n+#print(len(k))\n+#for mem in k:\n+#    print(mem['name'])"
            },
            {
                "sha": "eb528b8858abe979e3d3c23392da2641e6e2614a",
                "filename": "github-crawler/lib/pygit-helper.py",
                "status": "modified",
                "additions": 5,
                "deletions": 1,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/lib/pygit-helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b/github-crawler/lib/pygit-helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/pygit-helper.py?ref=c171e433b7bc1353fab9b1c4b0ae7162a7e4aa9b",
                "patch": "@@ -10,11 +10,15 @@\n print(org.login)\n names = org.get_members()\n repos = org.get_repos()\n+members = org.get_members()\n #\n #for name in names:\n #    print(name)\n \n i=0\n for repo in repos:\n     i=i+1\n-    print(i,repo)\n\\ No newline at end of file\n+    #print(i,repo.name)\n+\n+for mem in members:\n+    print(mem.name)\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "cff04dbe8bc646295decebc6b5b295830267e300",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmNmZjA0ZGJlOGJjNjQ2Mjk1ZGVjZWJjNmI1YjI5NTgzMDI2N2UzMDA=",
        "commit": {
            "author": {
                "name": "Karthik Siddaramanna",
                "email": "karthiks@Karthiks-MBP.lan",
                "date": "2020-04-16T05:14:42Z"
            },
            "committer": {
                "name": "Karthik Siddaramanna",
                "email": "karthiks@Karthiks-MBP.lan",
                "date": "2020-04-16T05:14:42Z"
            },
            "message": "Updated primary keys in models, added get_data api",
            "tree": {
                "sha": "7ee07bff02d7a00859fbcde470a020d2d7afa0e0",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/7ee07bff02d7a00859fbcde470a020d2d7afa0e0"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/cff04dbe8bc646295decebc6b5b295830267e300",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/cff04dbe8bc646295decebc6b5b295830267e300",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/cff04dbe8bc646295decebc6b5b295830267e300",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/cff04dbe8bc646295decebc6b5b295830267e300/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "e50f02c0163493202f4f84fe3b9a534f5f41fbb8",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/e50f02c0163493202f4f84fe3b9a534f5f41fbb8",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/e50f02c0163493202f4f84fe3b9a534f5f41fbb8"
            }
        ],
        "stats": {
            "total": 19,
            "additions": 15,
            "deletions": 4
        },
        "files": [
            {
                "sha": "4e35b7f6eefc4832b34f2bbe4644e78cfb2372c0",
                "filename": "db-apis/app.py",
                "status": "modified",
                "additions": 7,
                "deletions": 1,
                "changes": 8,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/cff04dbe8bc646295decebc6b5b295830267e300/db-apis/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/cff04dbe8bc646295decebc6b5b295830267e300/db-apis/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/app.py?ref=cff04dbe8bc646295decebc6b5b295830267e300",
                "patch": "@@ -41,7 +41,13 @@ def insert():\n     # TODO: populate response P0\n     # TODO: Catch exceptions P1\n     response = core.insert(request)\n-    # return response\n+    return response\n+\n+@app.route('/get', methods=['POST'])\n+def get_data():\n+    # TODO: populate response P0\n+    # TODO: Catch exceptions P1\n+    return core.get_data(request)\n \n \n # If we're running in stand alone mode, run the application"
            },
            {
                "sha": "bb548fc9875bdc24ca474caff4ca63b5f18777aa",
                "filename": "db-apis/dao/core.py",
                "status": "modified",
                "additions": 6,
                "deletions": 0,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/cff04dbe8bc646295decebc6b5b295830267e300/db-apis/dao/core.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/cff04dbe8bc646295decebc6b5b295830267e300/db-apis/dao/core.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/core.py?ref=cff04dbe8bc646295decebc6b5b295830267e300",
                "patch": "@@ -19,3 +19,9 @@ def insert(request):\n     obj = eval(content[TABLE]).create(**(content[BODY]))\n     return obj\n \n+\n+def get_data(request):\n+    content = request.get_json()\n+    connection.setup(CASSANDRA_HOSTS, content[ORG])\n+    obj = eval(content[TABLE]).objects.filter(**(content[BODY]))\n+    return dict(obj.get())"
            },
            {
                "sha": "6344922a391657e963edb0e7d820c84a57b0cedc",
                "filename": "db-apis/dao/models.py",
                "status": "modified",
                "additions": 2,
                "deletions": 3,
                "changes": 5,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/cff04dbe8bc646295decebc6b5b295830267e300/db-apis/dao/models.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/cff04dbe8bc646295decebc6b5b295830267e300/db-apis/dao/models.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/models.py?ref=cff04dbe8bc646295decebc6b5b295830267e300",
                "patch": "@@ -14,8 +14,7 @@ class Users(Model):\n \n \n class Data(Model):\n-    uid = columns.UUID(primary_key=True, default=uuid.uuid4)\n     gitid = columns.Text(required=True)\n-    name = columns.Text(required=True)\n-    repo = columns.Text(required=True)\n+    name = columns.Text(primary_key=True,required=True)\n+    repo = columns.Text(primary_key=True,required=True)\n     commit_num = columns.Integer(required=False, default=0)"
            }
        ]
    },
    {
        "sha": "496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjQ5NjMyM2NiOWVhOWMxN2E4YmZkNWVkYTdmZWYzYjVlNzU0NDlkYjM=",
        "commit": {
            "author": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T04:14:34Z"
            },
            "committer": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T04:14:34Z"
            },
            "message": "Frontend 1",
            "tree": {
                "sha": "af5c4fcc36c868e2eae2c2bb7f9bff91e08e0a53",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/af5c4fcc36c868e2eae2c2bb7f9bff91e08e0a53"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "574e048b6063142a23a94f68604d66b51a49bef1",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/574e048b6063142a23a94f68604d66b51a49bef1",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/574e048b6063142a23a94f68604d66b51a49bef1"
            }
        ],
        "stats": {
            "total": 28291,
            "additions": 28291,
            "deletions": 0
        },
        "files": [
            {
                "sha": "b45edbfdde79c13d94aa4ff3ef21f1daa64bee3b",
                "filename": "django/bdaSite/.DS_Store",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/.DS_Store?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "744381f3bd26406868047a0c54e8853e1af9573d",
                "filename": "django/bdaSite/bdaProject/.DS_Store",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/.DS_Store?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "9fe0613e3a9f7304fda2644019cb7de55a86b800",
                "filename": "django/bdaSite/bdaProject/static/.DS_Store",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/.DS_Store",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/.DS_Store",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/.DS_Store?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "77ff7497d4d132cb736615709b107fb006a1ec6e",
                "filename": "django/bdaSite/bdaProject/static/css/materialize.css",
                "status": "added",
                "additions": 8952,
                "deletions": 0,
                "changes": 8952,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/css/materialize.css",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/css/materialize.css",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/css/materialize.css?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "103b4dffbe9201084b96b18129be2bc6a042a134",
                "filename": "django/bdaSite/bdaProject/static/css/materialize.min.css",
                "status": "added",
                "additions": 16,
                "deletions": 0,
                "changes": 16,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/css/materialize.min.css",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/css/materialize.min.css",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/css/materialize.min.css?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "75b52484ea471f882c29e02693b4f02dba175b5e",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/LICENSE.txt",
                "status": "added",
                "additions": 202,
                "deletions": 0,
                "changes": 202,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/LICENSE.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/LICENSE.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/LICENSE.txt?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
                "patch": "@@ -0,0 +1,202 @@\n+\r\n+                                 Apache License\r\n+                           Version 2.0, January 2004\r\n+                        http://www.apache.org/licenses/\r\n+\r\n+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\r\n+\r\n+   1. Definitions.\r\n+\r\n+      \"License\" shall mean the terms and conditions for use, reproduction,\r\n+      and distribution as defined by Sections 1 through 9 of this document.\r\n+\r\n+      \"Licensor\" shall mean the copyright owner or entity authorized by\r\n+      the copyright owner that is granting the License.\r\n+\r\n+      \"Legal Entity\" shall mean the union of the acting entity and all\r\n+      other entities that control, are controlled by, or are under common\r\n+      control with that entity. For the purposes of this definition,\r\n+      \"control\" means (i) the power, direct or indirect, to cause the\r\n+      direction or management of such entity, whether by contract or\r\n+      otherwise, or (ii) ownership of fifty percent (50%) or more of the\r\n+      outstanding shares, or (iii) beneficial ownership of such entity.\r\n+\r\n+      \"You\" (or \"Your\") shall mean an individual or Legal Entity\r\n+      exercising permissions granted by this License.\r\n+\r\n+      \"Source\" form shall mean the preferred form for making modifications,\r\n+      including but not limited to software source code, documentation\r\n+      source, and configuration files.\r\n+\r\n+      \"Object\" form shall mean any form resulting from mechanical\r\n+      transformation or translation of a Source form, including but\r\n+      not limited to compiled object code, generated documentation,\r\n+      and conversions to other media types.\r\n+\r\n+      \"Work\" shall mean the work of authorship, whether in Source or\r\n+      Object form, made available under the License, as indicated by a\r\n+      copyright notice that is included in or attached to the work\r\n+      (an example is provided in the Appendix below).\r\n+\r\n+      \"Derivative Works\" shall mean any work, whether in Source or Object\r\n+      form, that is based on (or derived from) the Work and for which the\r\n+      editorial revisions, annotations, elaborations, or other modifications\r\n+      represent, as a whole, an original work of authorship. For the purposes\r\n+      of this License, Derivative Works shall not include works that remain\r\n+      separable from, or merely link (or bind by name) to the interfaces of,\r\n+      the Work and Derivative Works thereof.\r\n+\r\n+      \"Contribution\" shall mean any work of authorship, including\r\n+      the original version of the Work and any modifications or additions\r\n+      to that Work or Derivative Works thereof, that is intentionally\r\n+      submitted to Licensor for inclusion in the Work by the copyright owner\r\n+      or by an individual or Legal Entity authorized to submit on behalf of\r\n+      the copyright owner. For the purposes of this definition, \"submitted\"\r\n+      means any form of electronic, verbal, or written communication sent\r\n+      to the Licensor or its representatives, including but not limited to\r\n+      communication on electronic mailing lists, source code control systems,\r\n+      and issue tracking systems that are managed by, or on behalf of, the\r\n+      Licensor for the purpose of discussing and improving the Work, but\r\n+      excluding communication that is conspicuously marked or otherwise\r\n+      designated in writing by the copyright owner as \"Not a Contribution.\"\r\n+\r\n+      \"Contributor\" shall mean Licensor and any individual or Legal Entity\r\n+      on behalf of whom a Contribution has been received by Licensor and\r\n+      subsequently incorporated within the Work.\r\n+\r\n+   2. Grant of Copyright License. Subject to the terms and conditions of\r\n+      this License, each Contributor hereby grants to You a perpetual,\r\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\r\n+      copyright license to reproduce, prepare Derivative Works of,\r\n+      publicly display, publicly perform, sublicense, and distribute the\r\n+      Work and such Derivative Works in Source or Object form.\r\n+\r\n+   3. Grant of Patent License. Subject to the terms and conditions of\r\n+      this License, each Contributor hereby grants to You a perpetual,\r\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\r\n+      (except as stated in this section) patent license to make, have made,\r\n+      use, offer to sell, sell, import, and otherwise transfer the Work,\r\n+      where such license applies only to those patent claims licensable\r\n+      by such Contributor that are necessarily infringed by their\r\n+      Contribution(s) alone or by combination of their Contribution(s)\r\n+      with the Work to which such Contribution(s) was submitted. If You\r\n+      institute patent litigation against any entity (including a\r\n+      cross-claim or counterclaim in a lawsuit) alleging that the Work\r\n+      or a Contribution incorporated within the Work constitutes direct\r\n+      or contributory patent infringement, then any patent licenses\r\n+      granted to You under this License for that Work shall terminate\r\n+      as of the date such litigation is filed.\r\n+\r\n+   4. Redistribution. You may reproduce and distribute copies of the\r\n+      Work or Derivative Works thereof in any medium, with or without\r\n+      modifications, and in Source or Object form, provided that You\r\n+      meet the following conditions:\r\n+\r\n+      (a) You must give any other recipients of the Work or\r\n+          Derivative Works a copy of this License; and\r\n+\r\n+      (b) You must cause any modified files to carry prominent notices\r\n+          stating that You changed the files; and\r\n+\r\n+      (c) You must retain, in the Source form of any Derivative Works\r\n+          that You distribute, all copyright, patent, trademark, and\r\n+          attribution notices from the Source form of the Work,\r\n+          excluding those notices that do not pertain to any part of\r\n+          the Derivative Works; and\r\n+\r\n+      (d) If the Work includes a \"NOTICE\" text file as part of its\r\n+          distribution, then any Derivative Works that You distribute must\r\n+          include a readable copy of the attribution notices contained\r\n+          within such NOTICE file, excluding those notices that do not\r\n+          pertain to any part of the Derivative Works, in at least one\r\n+          of the following places: within a NOTICE text file distributed\r\n+          as part of the Derivative Works; within the Source form or\r\n+          documentation, if provided along with the Derivative Works; or,\r\n+          within a display generated by the Derivative Works, if and\r\n+          wherever such third-party notices normally appear. The contents\r\n+          of the NOTICE file are for informational purposes only and\r\n+          do not modify the License. You may add Your own attribution\r\n+          notices within Derivative Works that You distribute, alongside\r\n+          or as an addendum to the NOTICE text from the Work, provided\r\n+          that such additional attribution notices cannot be construed\r\n+          as modifying the License.\r\n+\r\n+      You may add Your own copyright statement to Your modifications and\r\n+      may provide additional or different license terms and conditions\r\n+      for use, reproduction, or distribution of Your modifications, or\r\n+      for any such Derivative Works as a whole, provided Your use,\r\n+      reproduction, and distribution of the Work otherwise complies with\r\n+      the conditions stated in this License.\r\n+\r\n+   5. Submission of Contributions. Unless You explicitly state otherwise,\r\n+      any Contribution intentionally submitted for inclusion in the Work\r\n+      by You to the Licensor shall be under the terms and conditions of\r\n+      this License, without any additional terms or conditions.\r\n+      Notwithstanding the above, nothing herein shall supersede or modify\r\n+      the terms of any separate license agreement you may have executed\r\n+      with Licensor regarding such Contributions.\r\n+\r\n+   6. Trademarks. This License does not grant permission to use the trade\r\n+      names, trademarks, service marks, or product names of the Licensor,\r\n+      except as required for reasonable and customary use in describing the\r\n+      origin of the Work and reproducing the content of the NOTICE file.\r\n+\r\n+   7. Disclaimer of Warranty. Unless required by applicable law or\r\n+      agreed to in writing, Licensor provides the Work (and each\r\n+      Contributor provides its Contributions) on an \"AS IS\" BASIS,\r\n+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\r\n+      implied, including, without limitation, any warranties or conditions\r\n+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\r\n+      PARTICULAR PURPOSE. You are solely responsible for determining the\r\n+      appropriateness of using or redistributing the Work and assume any\r\n+      risks associated with Your exercise of permissions under this License.\r\n+\r\n+   8. Limitation of Liability. In no event and under no legal theory,\r\n+      whether in tort (including negligence), contract, or otherwise,\r\n+      unless required by applicable law (such as deliberate and grossly\r\n+      negligent acts) or agreed to in writing, shall any Contributor be\r\n+      liable to You for damages, including any direct, indirect, special,\r\n+      incidental, or consequential damages of any character arising as a\r\n+      result of this License or out of the use or inability to use the\r\n+      Work (including but not limited to damages for loss of goodwill,\r\n+      work stoppage, computer failure or malfunction, or any and all\r\n+      other commercial damages or losses), even if such Contributor\r\n+      has been advised of the possibility of such damages.\r\n+\r\n+   9. Accepting Warranty or Additional Liability. While redistributing\r\n+      the Work or Derivative Works thereof, You may choose to offer,\r\n+      and charge a fee for, acceptance of support, warranty, indemnity,\r\n+      or other liability obligations and/or rights consistent with this\r\n+      License. However, in accepting such obligations, You may act only\r\n+      on Your own behalf and on Your sole responsibility, not on behalf\r\n+      of any other Contributor, and only if You agree to indemnify,\r\n+      defend, and hold each Contributor harmless for any liability\r\n+      incurred by, or claims asserted against, such Contributor by reason\r\n+      of your accepting any such warranty or additional liability.\r\n+\r\n+   END OF TERMS AND CONDITIONS\r\n+\r\n+   APPENDIX: How to apply the Apache License to your work.\r\n+\r\n+      To apply the Apache License to your work, attach the following\r\n+      boilerplate notice, with the fields enclosed by brackets \"[]\"\r\n+      replaced with your own identifying information. (Don't include\r\n+      the brackets!)  The text should be enclosed in the appropriate\r\n+      comment syntax for the file format. We also recommend that a\r\n+      file or class name and description of purpose be included on the\r\n+      same \"printed page\" as the copyright notice for easier\r\n+      identification within third-party archives.\r\n+\r\n+   Copyright [yyyy] [name of copyright owner]\r\n+\r\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\r\n+   you may not use this file except in compliance with the License.\r\n+   You may obtain a copy of the License at\r\n+\r\n+       http://www.apache.org/licenses/LICENSE-2.0\r\n+\r\n+   Unless required by applicable law or agreed to in writing, software\r\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+   See the License for the specific language governing permissions and\r\n+   limitations under the License.\r"
            },
            {
                "sha": "689fe5cb3c715f2944fec30e43ccb8a2b10625d3",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Black.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Black.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Black.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Black.ttf?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "0b4e0ee108899ddfef739f48d2aa9475b8b41a03",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BlackItalic.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BlackItalic.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BlackItalic.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BlackItalic.ttf?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "d3f01ad245b628f386ac95786f53167038720eb2",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Bold.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Bold.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Bold.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Bold.ttf?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "41cc1e753153e343a0ab73a341f545fec9eb7816",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BoldItalic.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BoldItalic.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BoldItalic.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-BoldItalic.ttf?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "6a1cee5b2948dbddf8fe6bb050a5cdca1c206dbf",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Italic.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Italic.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Italic.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Italic.ttf?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "219063a578a486b7c00262057efcbc44ebab0eeb",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Light.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Light.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Light.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Light.ttf?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "0e81e876fcab86b7cc99356aa0e19c0916a907bf",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-LightItalic.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-LightItalic.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-LightItalic.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-LightItalic.ttf?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "1a7f3b0bba45b7470a4240c3ec67595eeeb02192",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Medium.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Medium.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Medium.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Medium.ttf?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "003029527cc651faa12b18f02ce81d74f5031756",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-MediumItalic.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-MediumItalic.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-MediumItalic.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-MediumItalic.ttf?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "2c97eeadffe1a34bd67d3ff1c3887fd53e22c2ca",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Regular.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Regular.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Regular.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Regular.ttf?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "b74a4fd1a2ed1960da1d0f8f0e9b8d05a5819000",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Thin.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Thin.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Thin.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-Thin.ttf?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "dd0ddb852645db815ce4f3d45ec4a37b601ccb5f",
                "filename": "django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-ThinItalic.ttf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-ThinItalic.ttf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-ThinItalic.ttf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/fonts/Roboto/Roboto-ThinItalic.ttf?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "7c16b0d1287d0e9e69efd568f85bbc7989d3e631",
                "filename": "django/bdaSite/bdaProject/static/js/chart.min.js",
                "status": "added",
                "additions": 7,
                "deletions": 0,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/js/chart.min.js",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/js/chart.min.js",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/js/chart.min.js?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "072e308110fcf5d9e1c32750aba69557909dc82f",
                "filename": "django/bdaSite/bdaProject/static/js/jquery-3.1.1.js",
                "status": "added",
                "additions": 10220,
                "deletions": 0,
                "changes": 10220,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/js/jquery-3.1.1.js",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/js/jquery-3.1.1.js",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/js/jquery-3.1.1.js?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "f7b76adde388fa12f5163d37ab4f55fa707e0fdd",
                "filename": "django/bdaSite/bdaProject/static/js/main.js",
                "status": "added",
                "additions": 29,
                "deletions": 0,
                "changes": 29,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/js/main.js",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/js/main.js",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/js/main.js?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
                "patch": "@@ -0,0 +1,29 @@\n+function show(val, element, file) {\n+  //element = '#' + element;\n+\n+  if (val == \"\") {\n+    //$(element).html(\"\");\n+    document.getElementById(element).innerHTML = \"\";\n+    return;\n+  }\n+  else {\n+    if (window.XMLHttpRequest) {\n+      // code for IE7+, Firefox, Chrome, Opera, Safari\n+      xmlhttp = new XMLHttpRequest();\n+    }\n+    else {\n+      // code for IE6, IE5\n+      xmlhttp = new ActiveXObject(\"Microsoft.XMLHTTP\");\n+    }\n+    xmlhttp.onreadystatechange = function() {\n+      if (xmlhttp.readyState == 4 && xmlhttp.status == 200) {\n+        //$(element).html(xmlhttp.responseText);\n+        document.getElementById(element).innerHTML = xmlhttp.responseText;\n+      }\n+    }\n+    xmlhttp.open(\"GET\", file+\"?q=\"+val, true);\n+    xmlhttp.send();\n+    return val;\n+    console.log(val+\" \"+element+\" \"+file);\n+  }\n+}"
            },
            {
                "sha": "fc440ee58184fa72e377315a0a466358e0cc3025",
                "filename": "django/bdaSite/bdaProject/static/js/materialize.js",
                "status": "added",
                "additions": 8297,
                "deletions": 0,
                "changes": 8297,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/js/materialize.js",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/js/materialize.js",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/js/materialize.js?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "57dfd28c3e173e1fd8013d5eec938e2f3c927969",
                "filename": "django/bdaSite/bdaProject/static/js/materialize.min.js",
                "status": "added",
                "additions": 10,
                "deletions": 0,
                "changes": 10,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/js/materialize.min.js",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/js/materialize.min.js",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/js/materialize.min.js?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3"
            },
            {
                "sha": "5b695942c0ac1b758ea6591c97778062a0a98201",
                "filename": "django/bdaSite/bdaProject/static/js/terminal.js",
                "status": "added",
                "additions": 185,
                "deletions": 0,
                "changes": 185,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/js/terminal.js",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/static/js/terminal.js",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/static/js/terminal.js?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
                "patch": "@@ -0,0 +1,185 @@\n+/*! terminal.js v2.0 | (c) 2014 Erik \u00d6sterberg | https://github.com/eosterberg/terminaljs */\n+\n+var Terminal = (function () {\n+\t// PROMPT_TYPE\n+\tvar PROMPT_INPUT = 1, PROMPT_PASSWORD = 2, PROMPT_CONFIRM = 3\n+\n+\tvar fireCursorInterval = function (inputField, terminalObj) {\n+\t\tvar cursor = terminalObj._cursor\n+\t\tsetTimeout(function () {\n+\t\t\tif (inputField.parentElement && terminalObj._shouldBlinkCursor) {\n+\t\t\t\tcursor.style.visibility = cursor.style.visibility === 'visible' ? 'hidden' : 'visible'\n+\t\t\t\tfireCursorInterval(inputField, terminalObj)\n+\t\t\t} else {\n+\t\t\t\tcursor.style.visibility = 'visible'\n+\t\t\t}\n+\t\t}, 500)\n+\t}\n+\n+\tvar firstPrompt = true;\n+\tpromptInput = function (terminalObj, message, PROMPT_TYPE, callback) {\n+\t\tvar shouldDisplayInput = (PROMPT_TYPE === PROMPT_INPUT)\n+\t\tvar inputField = document.createElement('input')\n+\n+\t\tinputField.style.position = 'absolute'\n+\t\tinputField.style.zIndex = '-100'\n+\t\tinputField.style.outline = 'none'\n+\t\tinputField.style.border = 'none'\n+\t\tinputField.style.opacity = '0'\n+\t\tinputField.style.fontSize = '0.2em'\n+\n+\t\tterminalObj._inputLine.textContent = ''\n+\t\tterminalObj._input.style.display = 'block'\n+\t\tterminalObj.html.appendChild(inputField)\n+\t\tfireCursorInterval(inputField, terminalObj)\n+\n+\t\tif (message.length) terminalObj.print(PROMPT_TYPE === PROMPT_CONFIRM ? message + ' (y/n)' : message)\n+\n+\t\tinputField.onblur = function () {\n+\t\t\tterminalObj._cursor.style.display = 'none'\n+\t\t}\n+\n+\t\tinputField.onfocus = function () {\n+\t\t\tinputField.value = terminalObj._inputLine.textContent\n+\t\t\tterminalObj._cursor.style.display = 'inline'\n+\t\t}\n+\n+\t\tterminalObj.html.onclick = function () {\n+\t\t\tinputField.focus()\n+\t\t}\n+\n+\t\tinputField.onkeydown = function (e) {\n+\t\t\tif (e.which === 37 || e.which === 39 || e.which === 38 || e.which === 40 || e.which === 9) {\n+\t\t\t\te.preventDefault()\n+\t\t\t} else if (shouldDisplayInput && e.which !== 13) {\n+\t\t\t\tsetTimeout(function () {\n+\t\t\t\t\tterminalObj._inputLine.textContent = inputField.value\n+\t\t\t\t}, 1)\n+\t\t\t}\n+\t\t}\n+\t\tinputField.onkeyup = function (e) {\n+\t\t\tif (PROMPT_TYPE === PROMPT_CONFIRM || e.which === 13) {\n+\t\t\t\tterminalObj._input.style.display = 'none'\n+\t\t\t\tvar inputValue = inputField.value\n+\t\t\t\tif (shouldDisplayInput) terminalObj.print(inputValue)\n+\t\t\t\tterminalObj.html.removeChild(inputField)\n+\t\t\t\tif (typeof(callback) === 'function') {\n+\t\t\t\t\tif (PROMPT_TYPE === PROMPT_CONFIRM) {\n+\t\t\t\t\t\tcallback(inputValue.toUpperCase()[0] === 'Y' ? true : false)\n+\t\t\t\t\t} else callback(inputValue)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif (firstPrompt) {\n+\t\t\tfirstPrompt = false\n+\t\t\tsetTimeout(function () { inputField.focus()\t}, 50)\n+\t\t} else {\n+\t\t\tinputField.focus()\n+\t\t}\n+\t}\n+\n+\tvar terminalBeep\n+\n+\tvar TerminalConstructor = function (id) {\n+\t\tif (!terminalBeep) {\n+\t\t\tterminalBeep = document.createElement('audio')\n+\t\t\tvar source = '<source src=\"http://www.erikosterberg.com/terminaljs/beep.'\n+\t\t\tterminalBeep.innerHTML = source + 'mp3\" type=\"audio/mpeg\">' + source + 'ogg\" type=\"audio/ogg\">'\n+\t\t\tterminalBeep.volume = 0.05\n+\t\t}\n+\n+\t\tthis.html = document.createElement('div')\n+\t\tthis.html.className = 'Terminal'\n+\t\tif (typeof(id) === 'string') { this.html.id = id }\n+\n+\t\tthis._innerWindow = document.createElement('div')\n+\t\tthis._output = document.createElement('p')\n+\t\tthis._inputLine = document.createElement('span') //the span element where the users input is put\n+\t\tthis._cursor = document.createElement('span')\n+\t\tthis._input = document.createElement('p') //the full element administering the user input, including cursor\n+\n+\t\tthis._shouldBlinkCursor = true\n+\n+\t\tthis.beep = function () {\n+\t\t\tterminalBeep.load()\n+\t\t\tterminalBeep.play()\n+\t\t}\n+\n+\t\tthis.print = function (message) {\n+\t\t\tvar newLine = document.createElement('div')\n+\t\t\tnewLine.textContent = message\n+\t\t\tthis._output.appendChild(newLine)\n+\t\t}\n+\n+\t\tthis.input = function (message, callback) {\n+\t\t\tpromptInput(this, message, PROMPT_INPUT, callback)\n+\t\t}\n+\n+\t\tthis.password = function (message, callback) {\n+\t\t\tpromptInput(this, message, PROMPT_PASSWORD, callback)\n+\t\t}\n+\n+\t\tthis.confirm = function (message, callback) {\n+\t\t\tpromptInput(this, message, PROMPT_CONFIRM, callback)\n+\t\t}\n+\n+\t\tthis.clear = function () {\n+\t\t\tthis._output.innerHTML = ''\n+\t\t}\n+\n+\t\tthis.sleep = function (milliseconds, callback) {\n+\t\t\tsetTimeout(callback, milliseconds)\n+\t\t}\n+\n+\t\tthis.setTextSize = function (size) {\n+\t\t\tthis._output.style.fontSize = size\n+\t\t\tthis._input.style.fontSize = size\n+\t\t}\n+\n+\t\tthis.setTextColor = function (col) {\n+\t\t\tthis.html.style.color = col\n+\t\t\tthis._cursor.style.background = col\n+\t\t}\n+\n+\t\tthis.setBackgroundColor = function (col) {\n+\t\t\tthis.html.style.background = col\n+\t\t}\n+\n+\t\tthis.setWidth = function (width) {\n+\t\t\tthis.html.style.width = width\n+\t\t}\n+\n+\t\tthis.setHeight = function (height) {\n+\t\t\tthis.html.style.height = height\n+\t\t}\n+\n+\t\tthis.blinkingCursor = function (bool) {\n+\t\t\tbool = bool.toString().toUpperCase()\n+\t\t\tthis._shouldBlinkCursor = (bool === 'TRUE' || bool === '1' || bool === 'YES')\n+\t\t}\n+\n+\t\tthis._input.appendChild(this._inputLine)\n+\t\tthis._input.appendChild(this._cursor)\n+\t\tthis._innerWindow.appendChild(this._output)\n+\t\tthis._innerWindow.appendChild(this._input)\n+\t\tthis.html.appendChild(this._innerWindow)\n+\n+\t\tthis.setBackgroundColor('black')\n+\t\tthis.setTextColor('white')\n+\t\tthis.setTextSize('1em')\n+\t\tthis.setWidth('100%')\n+\t\tthis.setHeight('100%')\n+\n+\t\tthis.html.style.fontFamily = 'Monaco, Courier'\n+\t\tthis.html.style.margin = '0'\n+\t\tthis._innerWindow.style.padding = '10px'\n+\t\tthis._input.style.margin = '0'\n+\t\tthis._output.style.margin = '0'\n+\t\tthis._cursor.style.background = 'white'\n+\t\tthis._cursor.innerHTML = 'C' //put something in the cursor..\n+\t\tthis._cursor.style.display = 'none' //then hide it\n+\t\tthis._input.style.display = 'none'\n+\t}\n+\n+\treturn TerminalConstructor\n+}())\n\\ No newline at end of file"
            },
            {
                "sha": "b41ad27c60f86b852a9cf345885c611bbef7776c",
                "filename": "django/bdaSite/bdaProject/templates/iotdp/graph.html",
                "status": "added",
                "additions": 129,
                "deletions": 0,
                "changes": 129,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/templates/iotdp/graph.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/templates/iotdp/graph.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/iotdp/graph.html?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
                "patch": "@@ -0,0 +1,129 @@\n+{% load static %}\n+<!DOCTYPE html>\n+<html lang=\"en\" dir=\"ltr\">\n+  <head>\n+    <meta charset=\"utf-8\">\n+    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n+    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n+    <style>\n+     @font-face\n+     { font-family: R; src: url({% static 'fonts/Roboto/Roboto-Regular.ttf' %}); }\n+\n+    </style>\n+    <title></title>\n+  </head>\n+  <body id='body' style=\"font-family:R\">\n+    <!-- Navbar goes here -->\n+    <nav>\n+      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n+        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform</a>\n+      </div>\n+    </nav>\n+\n+    <!-- Page Layout here -->\n+    <div class=\"row\">\n+      <div id='col1' class=\"col s3\" style=\"background:#336699;\">\n+        <a href=\"/iotdp/{{ user }}\">\n+          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n+            OVERALL VIEW\n+          </div>\n+        </a>\n+        <a href=\"/iotdp/{{ user }}/graph\">\n+          <div class=\"col s12 m12 l12\" style=\"color:#17365D;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px;  background:#F1BA65\">\n+            GRAPH\n+          </div>\n+        </a>\n+        <a href=\"/iotdp/{{ user }}/logs\">\n+          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n+            LOGS\n+          </div>\n+        </a>\n+      </div>\n+\n+      <div id='col2' class=\"col s9\" style=\"background: #EBEBD3;\">\n+        <h4 class=\"center\">POWER CONSUMPTION IN THE LAST 30 MINUTES</h4>\n+        <canvas id=\"chart1\" class=\"\" style=\"width:100%;\"></canvas>\n+        <div class=\"row\">\n+          <div class=\"col m8 offset-m2\">\n+            <div class=\"card center\" style=\"background: #EBEBD3;\">\n+              <h5>Generated Power from Solar and other Sources: {{ gen_pow }} kW</h5>\n+            </div>\n+          </div>\n+          <div class=\"col m8 offset-m2\">\n+            <div class=\"card center\" style=\"background: #EBEBD3;\">\n+              <table>\n+                <tr>\n+                  <th>Small Appliances</th>\n+                  <td>Microwave, Dishwasher, Fridge</td>\n+                </tr>\n+                <tr>\n+                  <th>Big Appliances</th>\n+                  <td>Garage Door, Well, Barn, Furnaces</td>\n+                </tr>\n+                <tr>\n+                  <th>Rooms</th>\n+                  <td>Home Office, Wine Cellar, Living Room, Kitchens</td>\n+                </tr>\n+              </table>\n+            </div>\n+          </div>\n+        </div>\n+      </div>\n+\n+    </div>\n+\n+    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n+    <script src={% static 'js/materialize.min.js' %}></script>\n+    <script src={% static 'js/chart.min.js' %}></script>\n+    <script type=\"text/javascript\">\n+      navHeight = $('#nav').height();\n+      columnHeight = (window.innerHeight - navHeight);\n+      $('#col1').height(columnHeight);\n+      $('#col2').height(columnHeight);\n+    </script>\n+    <script type=\"text/javascript\">\n+      var ctx3 = document.getElementById(\"chart1\");\n+      var myChart = new Chart(ctx3, {\n+          type: 'bar',\n+          data: {\n+              labels: {% autoescape off %} {{ graph_labels }} {% endautoescape %},\n+              datasets: [{\n+                  label: 'Number of Units [kW]',\n+                  data: {% autoescape off %} {{ graph_data }} {% endautoescape %},\n+                  backgroundColor: [\n+                      '#FDB44B',\n+                      '#FDB44B',\n+                      '#FDB44B'\n+                  ],\n+                  borderColor: [\n+                      '#FDB44B',\n+                      '#FDB44B',\n+                      '#FDB44B'\n+                  ],\n+                  borderWidth: 1\n+              }]\n+          },\n+          options: {\n+              legend: {\n+                  labels: {\n+                      fontColor:\"black\"\n+                  }\n+              },\n+              scales: {\n+                xAxes: [{\n+                    ticks: {\n+                        fontColor:\"black\"\n+                    }\n+                }],\n+                yAxes: [{\n+                    ticks: {\n+                        beginAtZero:true,\n+                        fontColor:\"black\"\n+                    }\n+                }]\n+              }\n+          }\n+      });\n+    </script>\n+  </body>\n+</html>"
            },
            {
                "sha": "ab918e5c83d9f39a659db66c4957aa57ccfcd967",
                "filename": "django/bdaSite/bdaProject/templates/iotdp/index.html",
                "status": "added",
                "additions": 170,
                "deletions": 0,
                "changes": 170,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/templates/iotdp/index.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/templates/iotdp/index.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/iotdp/index.html?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
                "patch": "@@ -0,0 +1,170 @@\n+{% load static %}\n+<!DOCTYPE html>\n+<html lang=\"en\" dir=\"ltr\">\n+  <head>\n+    <meta charset=\"utf-8\">\n+    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n+    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n+    <style>\n+     @font-face\n+     { font-family: R; src: url({% static 'fonts/Roboto/Roboto-Regular.ttf' %}); }\n+\n+    </style>\n+    <title></title>\n+  </head>\n+  <body id='body' style=\"font-family:R\">\n+    <!-- Navbar goes here -->\n+    <nav>\n+      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n+        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform</a>\n+      </div>\n+    </nav>\n+\n+    <!-- Page Layout here -->\n+    <div class=\"row\">\n+\n+      <div id='col1' class=\"col s3\" style=\"background:#336699;\">\n+        <a href=\"/iotdp/{{ user }}\">\n+          <div class=\"col s12 m12 l12\" style=\"color:#17365D;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; background:#F1BA65; padding:0px\">\n+            OVERALL VIEW\n+          </div>\n+        </a>\n+        <a href=\"/iotdp/{{ user }}/graph\">\n+          <div class=\"col s12 m12 l12 white-text\" style=\"display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px\">\n+            GRAPH\n+          </div>\n+        </a>\n+        <a href=\"/iotdp/{{ user }}/logs\">\n+          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n+            LOGS\n+          </div>\n+        </a>\n+      </div>\n+\n+      <div id='col2' class=\"col s9 center\" style=\"background: #EBEBD3;\">\n+        <div style=\"width:100%; overflow-x:scroll\">\n+          <table style=\"\">\n+            <thead id='tableHead'>\n+              <th>Timestamp</th>\n+              <th>Use</th>\n+              <th>General</th>\n+              <th>House Overall</th>\n+              <th>Dishwasher</th>\n+              <th>Furnace1</th>\n+              <th>Furnace2</th>\n+              <th>Home Office</th>\n+              <th>Fridge</th>\n+              <th>Wine Cellar</th>\n+              <th>Garage Door</th>\n+              <th>Kitchen 12</th>\n+              <th>Kitchen 14</th>\n+              <th>Kitchen 38</th>\n+              <th>Barn</th>\n+              <th>Well</th>\n+              <th>Microwave</th>\n+              <th>Living Room</th>\n+              <th>Solar</th>\n+              <th>Temperature</th>\n+              <th>Icon</th>\n+              <th>Humidity</th>\n+              <th>Visibility</th>\n+              <th>Summary</th>\n+              <th>Apparent Temperature</th>\n+              <th>Pressure</th>\n+              <th>Wind Speed</th>\n+              <th>Cloud Cover</th>\n+              <th>Wind Bearing</th>\n+              <th>Precipitation Intensity</th>\n+              <th>Dew Point</th>\n+              <th>Precipitation Porbab</th>\n+            </thead>\n+            <tbody id='tableBody'>\n+\n+            </tbody>\n+          </table>\n+          <a id='prev' class=\"btn-floating\" style=\"background: #17365D\"><i class=\"material-icons\">keyboard_arrow_left</i></a>\n+          <a id='next' class=\"btn-floating\" style=\"background: #17365D\"><i class=\"material-icons\">keyboard_arrow_right</i></a>\n+        </div>\n+      </div>\n+\n+    </div>\n+\n+    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n+    <script src={% static 'js/materialize.min.js' %}></script>\n+    <script src={% static 'js/chart.min.js' %}></script>\n+    <script type=\"text/javascript\">\n+      navHeight = $('#nav').height();\n+      columnHeight = (window.innerHeight - navHeight);\n+      $('#col1').height(columnHeight);\n+      $('#col2').height(columnHeight);\n+    </script>\n+    <script type=\"text/javascript\">\n+        current_index = 0;\n+        size = {{ size }};\n+        function dataUpdate(current_index)\n+        {\n+          $.get('http://35.230.40.217:5000/get_data/{{ user }}/'+current_index, function(data, status) {\n+            for(i=0; i<data['result'].length; i++)\n+            {\n+              row = \"<tr>\";\n+              date = new Date(parseInt(data['result'][i]['time'])*1000).toLocaleString();\n+              row += \"<td>\"+date+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['use1']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['gen']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['house_overall']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['dishwasher']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['furnace1']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['furnace2']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['home_office']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['fridge']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['wine_cellar']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['garage_door']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['kitchen12']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['kitchen14']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['kitchen38']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['barn']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['well']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['microwave']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['living_room']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['solar']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['temperature']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+data['result'][i]['icon']+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['humidity']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['visibility']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+data['result'][i]['summary']+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['apparent_temp']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['pressure']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['wind_speed']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+data['result'][i]['cloud_cover']+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['wind_bearing']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['precip_intensity']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['dew_point']* 10000)) / 10000)+\"</td>\";\n+              row += \"<td>\"+(Math.round((data['result'][i]['precip_probability']* 10000)) / 10000)+\"</td>\";\n+              row += \"</tr>\";\n+              $('#tableBody').append(row);\n+            }\n+          });\n+        }\n+\n+        dataUpdate(0);\n+        // alert(size);\n+        $('#prev').click(function(){\n+            if(current_index > 0)\n+            {\n+              $('#tableBody').empty();\n+              current_index = current_index - 7;\n+              dataUpdate(current_index);\n+            }\n+        });\n+        $('#next').click(function(){\n+            if (current_index <= size-7)\n+            {\n+              $('#tableBody').empty();\n+              current_index = current_index + 7;\n+              dataUpdate(current_index);\n+            }\n+        });\n+\n+    </script>\n+  </body>\n+</html>"
            },
            {
                "sha": "0df695c397fe3d580ee82eb06165abce940e55cf",
                "filename": "django/bdaSite/bdaProject/templates/iotdp/logs.html",
                "status": "added",
                "additions": 74,
                "deletions": 0,
                "changes": 74,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/templates/iotdp/logs.html",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/496323cb9ea9c17a8bfd5eda7fef3b5e75449db3/django/bdaSite/bdaProject/templates/iotdp/logs.html",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/templates/iotdp/logs.html?ref=496323cb9ea9c17a8bfd5eda7fef3b5e75449db3",
                "patch": "@@ -0,0 +1,74 @@\n+{% load static %}\n+<!DOCTYPE html>\n+<html lang=\"en\" dir=\"ltr\">\n+  <head>\n+    <meta charset=\"utf-8\">\n+    <link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n+    <link href={% static 'css/materialize.css' %} type=\"text/css\" rel=\"stylesheet\" media=\"screen,projection\"/>\n+    <style>\n+     @font-face\n+     { font-family: R; src: url({% static 'fonts/Roboto/Roboto-Regular.ttf' %}); }\n+\n+    </style>\n+    <script src=\"{% static 'js/terminal.js' %}\"></script>\n+    <script type=\"text/javascript\">\n+\n+    </script>\n+    <title></title>\n+  </head>\n+  <body id='body' style=\"font-family:R\">\n+    <!-- Navbar goes here -->\n+    <nav>\n+      <div id='nav' style=\"background:#17365D\" class=\"nav-wrapper\">\n+        <a href=\"#\" class=\"brand-logo center\">IoT Data Platform - Logs</a>\n+      </div>\n+    </nav>\n+\n+    <div class=\"row\">\n+      <div id='col1' class=\"col s3\" style=\"background:#336699;\">\n+        <a href=\"/iotdp/{{ user }}\">\n+          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px;; padding:0px\">\n+            OVERALL VIEW\n+          </div>\n+        </a>\n+        <a href=\"/iotdp/{{ user }}/graph\">\n+          <div class=\"col s12 m12 l12\" style=\"color:white;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px;\">\n+            GRAPH\n+          </div>\n+        </a>\n+        <a href=\"/iotdp/{{ user }}/logs\">\n+          <div class=\"col s12 m12 l12\" style=\"color:#17365D;display:flex; justify-content:center; align-items:center; width:100%; height:10vh; font-size:150%; margin:0px; padding:0px;  background:#F1BA65\">\n+            LOGS\n+          </div>\n+        </a>\n+      </div>\n+\n+      <div id='col2' class=\"col s9 black\" style=\"height:750px;background: #EBEBD3; margin:0px; padding:0px; overflow-y:scroll\">\n+        <div id='term' class=\"row\" style=\"height:750px;display:flex; margin:0px; padding:0px; overflow-y:scroll;\">\n+\n+        </div>\n+      </div>\n+\n+    <!-- Page Layout here -->\n+\n+\n+    <script src={% static 'js/jquery-3.1.1.js' %}></script>\n+    <script src={% static 'js/materialize.min.js' %}></script>\n+    <script src={% static 'js/chart.min.js' %}></script>\n+    <script type=\"text/javascript\">\n+      navHeight = $('#nav').height();\n+      columnHeight = (window.innerHeight - navHeight);\n+      $('#col1').height(columnHeight);\n+    </script>\n+    <script type=\"text/javascript\">\n+      var term = new Terminal();\n+      var logs = {% autoescape off %} {{ logs }} {% endautoescape %}\n+      var logLines = logs.split(';;;')\n+      for(var i=0; i<logLines.length; i++)\n+      {\n+          term.print(logLines[i]);\n+      }\n+      $('#term').append(term.html)\n+    </script>\n+  </body>\n+</html>"
            }
        ]
    },
    {
        "sha": "574e048b6063142a23a94f68604d66b51a49bef1",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjU3NGUwNDhiNjA2MzE0MmEyM2E5NGY2ODYwNGQ2NmI1MWE0OWJlZjE=",
        "commit": {
            "author": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T04:10:04Z"
            },
            "committer": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T04:10:04Z"
            },
            "message": "Test",
            "tree": {
                "sha": "183fcd80e3c22cf4679de93406228523e8bd3592",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/183fcd80e3c22cf4679de93406228523e8bd3592"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/574e048b6063142a23a94f68604d66b51a49bef1",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/574e048b6063142a23a94f68604d66b51a49bef1",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/574e048b6063142a23a94f68604d66b51a49bef1",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/574e048b6063142a23a94f68604d66b51a49bef1/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "3ad956490769914023e6480d787252ba56f69517",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/3ad956490769914023e6480d787252ba56f69517",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/3ad956490769914023e6480d787252ba56f69517"
            }
        ],
        "stats": {
            "total": 6,
            "additions": 5,
            "deletions": 1
        },
        "files": [
            {
                "sha": "22171cf1ea8eadc37bf270bfc0db75e52d83d73e",
                "filename": "django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/574e048b6063142a23a94f68604d66b51a49bef1/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/574e048b6063142a23a94f68604d66b51a49bef1/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc?ref=574e048b6063142a23a94f68604d66b51a49bef1"
            },
            {
                "sha": "94a0e6d75398a265f03825e36ada9ad3b8b54df9",
                "filename": "django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/574e048b6063142a23a94f68604d66b51a49bef1/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/574e048b6063142a23a94f68604d66b51a49bef1/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc?ref=574e048b6063142a23a94f68604d66b51a49bef1"
            },
            {
                "sha": "8217837f53c43e1ac10f97a7e9aaf176a076e31e",
                "filename": "django/bdaSite/bdaProject/urls.py",
                "status": "modified",
                "additions": 2,
                "deletions": 1,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/574e048b6063142a23a94f68604d66b51a49bef1/django/bdaSite/bdaProject/urls.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/574e048b6063142a23a94f68604d66b51a49bef1/django/bdaSite/bdaProject/urls.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/urls.py?ref=574e048b6063142a23a94f68604d66b51a49bef1",
                "patch": "@@ -2,5 +2,6 @@\n from . import views\n \n urlpatterns = [\n-    path('', views.index, name='index')\n+    path('', views.index, name='index'),\n+    path('trial/', views.trial, name='trial')\n ]\n\\ No newline at end of file"
            },
            {
                "sha": "ccefa56bda69b38a6ae6aedfd7e3413e127754a4",
                "filename": "django/bdaSite/bdaProject/views.py",
                "status": "modified",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/574e048b6063142a23a94f68604d66b51a49bef1/django/bdaSite/bdaProject/views.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/574e048b6063142a23a94f68604d66b51a49bef1/django/bdaSite/bdaProject/views.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/views.py?ref=574e048b6063142a23a94f68604d66b51a49bef1",
                "patch": "@@ -4,3 +4,6 @@\n \n def index(request):\n     return HttpResponse(\"Hello, world. You're at the bdaProject index.\")\n+\n+def trial(request):\n+    return HttpResponse(\"Trial\")"
            }
        ]
    },
    {
        "sha": "3ad956490769914023e6480d787252ba56f69517",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjNhZDk1NjQ5MDc2OTkxNDAyM2U2NDgwZDc4NzI1MmJhNTZmNjk1MTc=",
        "commit": {
            "author": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T04:05:32Z"
            },
            "committer": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T04:05:32Z"
            },
            "message": "Merge branch 'master' of https://github.com/CUBigDataClass/Koe-Kallas",
            "tree": {
                "sha": "3e4e26610b537fe69ebadd3cdb629c5f73bc7b58",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/3e4e26610b537fe69ebadd3cdb629c5f73bc7b58"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/3ad956490769914023e6480d787252ba56f69517",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/3ad956490769914023e6480d787252ba56f69517",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/3ad956490769914023e6480d787252ba56f69517",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/3ad956490769914023e6480d787252ba56f69517/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c"
            },
            {
                "sha": "0e249d1e805a5b292759d51f88a5b9e380e0f633",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/0e249d1e805a5b292759d51f88a5b9e380e0f633",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/0e249d1e805a5b292759d51f88a5b9e380e0f633"
            }
        ],
        "stats": {
            "total": 576,
            "additions": 550,
            "deletions": 26
        },
        "files": [
            {
                "sha": "a86165d1469792833005cbe18ed0b2a58468201f",
                "filename": "Architecture-Diagram.pdf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/Architecture-Diagram.pdf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/Architecture-Diagram.pdf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/Architecture-Diagram.pdf?ref=3ad956490769914023e6480d787252ba56f69517"
            },
            {
                "sha": "2b63cace2f5e424d58ceaf5eb70ffae4f5d7f121",
                "filename": "README.md",
                "status": "modified",
                "additions": 79,
                "deletions": 2,
                "changes": 81,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -18,6 +18,72 @@\n  1) Install and keep following software\n  \n \n+ - Docker\n+ - Python-Flask\n+ - Python3\n+ - ElasticSearch\n+ - \n+\n+2) Run Below Comands after starting Docker\n+\n+\t\tdocker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.5.2\n+\t```\n+\tdocker run -d --name kibana --net somenetwork -p 5601:5601 kibana:tag\n+\t```\n+3) Set-up ElasticSearch (Important)\n+   navigate to [http://localhost:5601/app/kibana#/dev_tools/console?_g=()](http://localhost:5601/app/kibana#/dev_tools/console?_g=())\n+   copy and past each and every file from **github-crawler/Templates_commands** into above console (list of files is given below)\n+\n+         * comments_command.txt\n+         * repos_cammand.txt\n+         * put_org_index.txt\n+    \n+\n+3) Starting github-crawler and usage\n+\n+\t\tgit clone https://github.com/CUBigDataClass/Kode-Kallas.git\n+\t\tcd Kode-Kallas/github-crawler\n+\t    python3 app.py\n+navigate to\n+-  http://127.0.0.1:5000  -- you will be able to see proper message\n+- http://127.0.0.1:5000/org/<!orgname_should be added here>  - Just check server logs  \n+\n+4) For stopping and starting your docker images\n+\n+\t     $ docker ps -a    \n+\t     $ docker stop 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\n+\t     $ docker stop d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\n+\t     $ docker start 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\n+\t     $ docker start d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\n+\n+\n+4) Below some Documentation\n+\n+\t\t Work Under progress\n+\n+|**Module**  | **Usage** | **Requirements**|\n+|--|--|--|\n+|Github-Crawler|Used to get Github organization info |Docker and Python3|\n+# Kode - Kallas\n+\n+\n+  \n+\n+## Modules\n+\n+|Name  | Progress |\n+|--|--|\n+|  Github-Crawler| In-Progress|\n+| Github-Analyser|In-Progress|\n+|Dashboard-backend| In-progress|\n+|Dashboard-Frontend||\n+\n+\n+    Github-Crawler Setup\n+\n+ 1) Install and keep following software\n+ \n+\n  - Docker\n  - Python-Flask\n  - Python3\n@@ -65,11 +131,22 @@ navigate to\n \t     $ docker start d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\t\n \n \n+5) Cassandra api setup\n+\n+\t    docker run -e DS_LICENSE=accept --memory 4g -p 7000:7000 -p 7001:7001 -p 7199:7199 -p 9042:9042 -p 9160:9160 -p 9404:9404 --name my-dse -e CASSANDRA_START_RPC=true -d datastax/dse-server:6.8.0\n+\t    docker start my-dse\n+\t \n+    Prereq: Install flask and cassandra-driver using pip.\n+\n+\t    python3 app.py\n+\n \n-5) Below some Documentation\n \n-\t\t Work Under progress\n+\n+6) Below some Documentation\n+\n \n |**Module**  | **Usage** | **Requirements**|\n |--|--|--|\n |Github-Crawler|Used to get Github organization info |Docker and Python3|\n+"
            },
            {
                "sha": "de42b46c5aa78eefa29888e6d368da97e2fffe08",
                "filename": "db-apis/.ipynb_checkpoints/Untitled-checkpoint.ipynb",
                "status": "added",
                "additions": 60,
                "deletions": 0,
                "changes": 60,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/db-apis/.ipynb_checkpoints/Untitled-checkpoint.ipynb",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/db-apis/.ipynb_checkpoints/Untitled-checkpoint.ipynb",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/.ipynb_checkpoints/Untitled-checkpoint.ipynb?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -0,0 +1,60 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from cassandra.cluster import Cluster\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 28,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"cluster = Cluster(['127.0.0.1'], port=9042)\\n\",\n+    \"session = cluster.connect()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 29,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"session.set_keyspace('sample_org')\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": []\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"Python 3\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\",\n+   \"version\": \"3.7.3\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 2\n+}"
            },
            {
                "sha": "757a76032b29b2c1983c26f971ca1d2d3b83a53e",
                "filename": "db-apis/Untitled.ipynb",
                "status": "added",
                "additions": 136,
                "deletions": 0,
                "changes": 136,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/db-apis/Untitled.ipynb",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/db-apis/Untitled.ipynb",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/Untitled.ipynb?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -0,0 +1,136 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from cassandra.cluster import Cluster\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 38,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"cluster = Cluster(['127.0.0.1'], port=9042)\\n\",\n+    \"session = cluster.connect()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 42,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"data\": {\n+      \"text/plain\": [\n+       \"<cassandra.cluster.ResultSet at 0x1208d2e10>\"\n+      ]\n+     },\n+     \"execution_count\": 42,\n+     \"metadata\": {},\n+     \"output_type\": \"execute_result\"\n+    }\n+   ],\n+   \"source\": [\n+    \"create_keyspace_statement = \\\"CREATE KEYSPACE ? WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\\\"\\n\",\n+    \"session.execute(\\\"CREATE KEYSPACE hehe WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 30,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"<cassandra.cluster.Session object at 0x12030eef0>\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"print(session)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 33,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"rows = session.execute('SELECT * FROM users')\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 34,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"data\": {\n+      \"text/plain\": [\n+       \"<cassandra.cluster.ResultSet at 0x120390240>\"\n+      ]\n+     },\n+     \"execution_count\": 34,\n+     \"metadata\": {},\n+     \"output_type\": \"execute_result\"\n+    }\n+   ],\n+   \"source\": [\n+    \"rows\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 36,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Hehehaha\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"for user in rows:\\n\",\n+    \"    print(user.name)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": []\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"Python 3\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\",\n+   \"version\": \"3.7.3\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 2\n+}"
            },
            {
                "sha": "9720cc191b47fa1e2a80ee38dda67c485784a0dd",
                "filename": "db-apis/app.py",
                "status": "added",
                "additions": 49,
                "deletions": 0,
                "changes": 49,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/db-apis/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/db-apis/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/app.py?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -0,0 +1,49 @@\n+from flask import (\n+    Flask,\n+    request\n+)\n+from dao import setup\n+from dao import core\n+from cassandra.cluster import Cluster\n+\n+app = Flask(__name__, template_folder=\"templates\")\n+\n+\n+@app.route('/')\n+def home():\n+    return setup.create_table()\n+\n+\n+@app.route('/setup/<orgname>')\n+def org_setup(orgname):\n+    return setup.create_keyspace(orgname)\n+\n+\n+@app.route('/delete/<orgname>')\n+def org_delete(orgname):\n+    return setup.delete_keyspace(orgname)\n+\n+\n+# TODO: Delete once done\n+@app.route('/tp')\n+def timepass():\n+    return setup.insert_users()\n+\n+\n+# TODO: Delete once done\n+@app.route('/tp2')\n+def timepass2():\n+    return setup.another_insert_users()\n+\n+\n+@app.route('/insert', methods=['POST'])\n+def insert():\n+    # TODO: populate response P0\n+    # TODO: Catch exceptions P1\n+    response = core.insert(request)\n+    # return response\n+\n+\n+# If we're running in stand alone mode, run the application\n+if __name__ == '__main__':\n+    app.run(debug=True)"
            },
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "db-apis/dao/__init__.py",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/__init__.py?ref=3ad956490769914023e6480d787252ba56f69517"
            },
            {
                "sha": "5d2a75f5490357446e5a8e1523e117b73eb241da",
                "filename": "db-apis/dao/__pycache__/__init__.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/__pycache__/__init__.cpython-37.pyc?ref=3ad956490769914023e6480d787252ba56f69517"
            },
            {
                "sha": "854270ad54da3f65df8eeed742d490d5016ce4f7",
                "filename": "db-apis/dao/__pycache__/config.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/__pycache__/config.cpython-37.pyc?ref=3ad956490769914023e6480d787252ba56f69517"
            },
            {
                "sha": "eb53bbf099b0865a15f4b5ca37183bb571a837c5",
                "filename": "db-apis/dao/__pycache__/core.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/__pycache__/core.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/__pycache__/core.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/__pycache__/core.cpython-37.pyc?ref=3ad956490769914023e6480d787252ba56f69517"
            },
            {
                "sha": "5bea301cc1c812ac8cb24a86390ce1efd7e6d4fb",
                "filename": "db-apis/dao/__pycache__/models.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/__pycache__/models.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/__pycache__/models.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/__pycache__/models.cpython-37.pyc?ref=3ad956490769914023e6480d787252ba56f69517"
            },
            {
                "sha": "993fbfc6750f86a32c77c1219be3ab42fd503462",
                "filename": "db-apis/dao/__pycache__/setup.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/__pycache__/setup.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/__pycache__/setup.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/__pycache__/setup.cpython-37.pyc?ref=3ad956490769914023e6480d787252ba56f69517"
            },
            {
                "sha": "48bc7fce612cec226f9c2d06a6949b84fe32d5c1",
                "filename": "db-apis/dao/config.py",
                "status": "added",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/config.py?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -0,0 +1,3 @@\n+# Database options\n+CASSANDRA_HOSTS = ['127.0.0.1']\n+CASSANDRA_KEYSPACE = \"sample_org\"\n\\ No newline at end of file"
            },
            {
                "sha": "97deef1339ad67550025983733ecd09671982887",
                "filename": "db-apis/dao/core.py",
                "status": "added",
                "additions": 21,
                "deletions": 0,
                "changes": 21,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/core.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/core.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/core.py?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -0,0 +1,21 @@\n+from cassandra.cqlengine import connection\n+from dao.config import CASSANDRA_HOSTS\n+from dao.models import Users, Data\n+\n+ORG = \"org\"\n+TABLE = \"table\"\n+BODY = \"body\"\n+USERS = \"users\"\n+\n+\n+# TODO: Enable quorum\n+def insert(request):\n+    # TODO: Data Validation should be done?\n+    content = request.get_json()\n+    connection.setup(CASSANDRA_HOSTS, content[ORG])\n+    # if isinstance(body, Model):\n+    # TODO: Avoid using eval() because\n+    # TODO: this requires me to import all models\n+    obj = eval(content[TABLE]).create(**(content[BODY]))\n+    return obj\n+"
            },
            {
                "sha": "66c3633370328232b06feed9844f1d154a3b67f8",
                "filename": "db-apis/dao/models.py",
                "status": "added",
                "additions": 21,
                "deletions": 0,
                "changes": 21,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/models.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/models.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/models.py?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -0,0 +1,21 @@\n+import uuid\n+from cassandra.cqlengine import columns\n+from cassandra.cqlengine.models import Model\n+\n+class URL():\n+\n+    def __init__(self, url):\n+        self.url = url\n+\n+class Users(Model):\n+    uid = columns.UUID(primary_key=True, default=uuid.uuid4)\n+    name = columns.Text(required=True)\n+    repos = columns.List(value_type=columns.Text, required=False)\n+\n+\n+class Data(Model):\n+    uid = columns.UUID(primary_key=True, default=uuid.uuid4)\n+    gitid = columns.Text(required=True)\n+    name = columns.Text(required=True)\n+    repo = columns.Text(required=True)\n+    commit_num = columns.Integer(required=False, default=0)"
            },
            {
                "sha": "19d7eca412a25184491b56b878fb9c83251dd0be",
                "filename": "db-apis/dao/setup.py",
                "status": "added",
                "additions": 58,
                "deletions": 0,
                "changes": 58,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/setup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/db-apis/dao/setup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/setup.py?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -0,0 +1,58 @@\n+from cassandra.cqlengine import connection\n+from cassandra.cluster import Cluster\n+from cassandra.cqlengine.management import sync_table\n+from cassandra.query import SimpleStatement\n+\n+from dao.config import CASSANDRA_HOSTS\n+from dao.models import Users, Data\n+\n+\n+def get_session(keyspace=None):\n+    # TODO: Should enable ssl: for both inter cluster and app communication P1\n+    cluster = Cluster(CASSANDRA_HOSTS)\n+    session = cluster.connect(keyspace)\n+    return session\n+\n+\n+# TODO: Simple vs Prepared P2\n+def create_keyspace(orgname):\n+    session = get_session()\n+    # TODO: Replication strategy should be updated to 3\n+    query = SimpleStatement(\n+        \"CREATE KEYSPACE %s WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\" % (orgname,))\n+    session.execute(query)\n+    session.shutdown()\n+    create_tables(orgname)\n+\n+\n+def delete_keyspace(orgname):\n+    session = get_session()\n+    query = SimpleStatement(\n+        \"DROP KEYSPACE %s;\" % (orgname,))\n+    session.execute(query)\n+    session.shutdown()\n+\n+\n+# TODO: MAke sure sessions and connections are handled properly: probably read about that\n+def create_tables(orgname):\n+    connection.setup(CASSANDRA_HOSTS, orgname, protocol_version=3)\n+    # session = get_session(orgname)\n+    sync_table(Users)\n+    sync_table(Data)\n+    connection.unregister_connection('default')\n+\n+\n+# TODO: Delete below functions when everything is done\n+def insert_users():\n+    connection.setup(CASSANDRA_HOSTS, \"hehe1\")\n+    # session = get_session(\"hehe1\")\n+    # connection.get_session(session)\n+    manu = Users.create(name=\"manu\")\n+    connection.unregister_connection('default')\n+\n+\n+def another_insert_users():\n+    connection.setup(CASSANDRA_HOSTS, \"hehe1\")\n+    d = {\"name\": \"manya\"}\n+    manu = Users.create(**d)\n+    connection.unregister_connection('default')"
            },
            {
                "sha": "cc208b1d8e40b0382fafa2737a63ce76cbcc0e80",
                "filename": "github-crawler/Templates_commands/org_template.txt",
                "status": "modified",
                "additions": 2,
                "deletions": 7,
                "changes": 9,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/github-crawler/Templates_commands/org_template.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/github-crawler/Templates_commands/org_template.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/org_template.txt?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -5,18 +5,13 @@ PUT _template/org_template\n     \"number_of_shards\": 1\n   },\n   \"mappings\": {\n-    \"_source\": {\n-      \"enabled\": false\n-    },\n     \"properties\": {\n-      \"host_name\": {\n-        \"type\": \"keyword\"\n-      },\n     \"login\": {\n         \"type\": \"keyword\"\n       },\n     \"id\": {\n-      \"type\":\"long\"},\n+      \"type\":\"long\"\n+      },\n     \"node_id\": {\n         \"type\": \"keyword\"\n       },"
            },
            {
                "sha": "64153d1d9e5a15babcbf8d01a8a6065bc47a1d34",
                "filename": "github-crawler/Templates_commands/put_org_index.txt",
                "status": "modified",
                "additions": 2,
                "deletions": 10,
                "changes": 12,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/github-crawler/Templates_commands/put_org_index.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/github-crawler/Templates_commands/put_org_index.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/put_org_index.txt?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -1,21 +1,13 @@\n PUT /org1\n {\n-    \"settings\" : {\n-        \"number_of_shards\" : 1\n-    },\n     \"mappings\": {\n-    \"_source\": {\n-      \"enabled\": false\n-    },\n     \"properties\": {\n-      \"host_name\": {\n-        \"type\": \"keyword\"\n-      },\n     \"login\": {\n         \"type\": \"keyword\"\n       },\n     \"id\": {\n-      \"type\":\"long\"},\n+      \"type\":\"long\"\n+      },\n     \"node_id\": {\n         \"type\": \"keyword\"\n       },"
            },
            {
                "sha": "048facb90e501192503e0571f4c76dc840368c4e",
                "filename": "github-crawler/Templates_commands/users_command.txt",
                "status": "added",
                "additions": 61,
                "deletions": 0,
                "changes": 61,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/github-crawler/Templates_commands/users_command.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/github-crawler/Templates_commands/users_command.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/users_command.txt?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -0,0 +1,61 @@\n+PUT /users\n+{\n+    \"mappings\": {\n+    \"properties\": {\n+    \"login\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"id\": {\n+        \"type\": \"long\"\n+      },\n+    \"node_id\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"avatar_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"gravatar_id\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"html_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"followers_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"following_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"gists_url\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"starred_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"subscriptions_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"organizations_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"repos_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"events_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"received_events_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"type\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"site_admin\": {\n+        \"type\": \"boolean\"\n+      }\n+  }\n+    }\n+}"
            },
            {
                "sha": "e15721c8f9b03fd0685d71c58785284e64479785",
                "filename": "github-crawler/app.py",
                "status": "modified",
                "additions": 6,
                "deletions": 0,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/github-crawler/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/github-crawler/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/app.py?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -20,6 +20,7 @@ def org_parser(orgname):\n     org_data = helper.get_org_information(OWNER,github_api)\n     print(\"Got Org Info!!!\")\n     print(\"sending Org Info to elastic search!!!\")\n+    #print(org_data)\n     helper.send_to_elasticInstance(org_data,'org1',org_data['id'])\n     print(\"Getting Repos for \"+orgname)\n     repo_list = helper.get_repositories(OWNER,github_api)\n@@ -28,6 +29,11 @@ def org_parser(orgname):\n         repo['license']=\"test\"\n         helper.send_to_elasticInstance(repo,'repos',repo['id'])\n         print(\"repo sent \"+ repo['name'] )\n+    print(\"Getting Org Members for \"+orgname)\n+    member_list = helper.get_org_users(OWNER,github_api)\n+    print(\"sending user info to elasticsearch\")\n+    for member in member_list:\n+        helper.send_to_elasticInstance(member,'users',member['id'])\n     print(\"Done!!!!!!!!!\")\n     return 'We got your org name ' + orgname + ' give us some time to process your request, please check server output for progress'\n "
            },
            {
                "sha": "8f83b6e5582748c864c9edff6955c71fbba1b2d3",
                "filename": "github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/config.cpython-37.pyc?ref=3ad956490769914023e6480d787252ba56f69517"
            },
            {
                "sha": "7074af8f8c78ed4ad46a924622f2bd31632fe95f",
                "filename": "github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/helper.cpython-37.pyc?ref=3ad956490769914023e6480d787252ba56f69517"
            },
            {
                "sha": "ff0119b94421c5e585e3fef879f3210ab2588e13",
                "filename": "github-crawler/lib/helper.py",
                "status": "modified",
                "additions": 47,
                "deletions": 6,
                "changes": 53,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/github-crawler/lib/helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/github-crawler/lib/helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/helper.py?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -12,9 +12,9 @@\n from elasticsearch import Elasticsearch\n \n GITEA_APP_URL = 'YOUR_GITEA_API'\n-GITEA_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+GITEA_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n GITHUB_USERNAME = 'vishwakulkarni'\n-GITHUB_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+GITHUB_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n SQL_ALCHEMY_STRING = ''\n \n \n@@ -44,7 +44,48 @@ def send_to_elasticInstance(self,data,index_name,id_val):\n         self.es.index(index=index_name, doc_type='_doc',id=id_val, body=data)\n     \n     def get_repositories(self,owner,api):\n-        url = api + '/orgs/{}/repos'.format(self.orgname)\n-        org_repos_data = self.gh_session.get(url = url)\n-        org_repos_data=json.loads(org_repos_data.content)\n-        return org_repos_data\n+        repos_list = []\n+        next = True\n+        i=1\n+        while next == True:\n+            url = api + '/orgs/{}/repos?page={}&per_page=20'.format(self.orgname,i)\n+            original_data = self.gh_session.get(url=url)\n+            repos = json.loads(original_data.content)\n+            for repo in repos:\n+                repos_list.append(repo)\n+            if 'Link' in original_data.headers:\n+                if 'rel=\"next\"' not in original_data.headers['Link']:\n+                    print(i)\n+                    next = False\n+            i = i + 1\n+        return repos_list\n+    \n+    def get_org_users(self,owner,api):\n+        members_list = []\n+        next = True\n+        i=1\n+        while next == True:\n+            url = api + '/orgs/{}/members?page={}&per_page=100'.format(self.orgname,i)\n+            original_data = self.gh_session.get(url=url)\n+            members = json.loads(original_data.content)\n+            for member in members:\n+                members_list.append(member)\n+            if 'Link' in original_data.headers:\n+                if 'rel=\"next\"' not in original_data.headers['Link']:\n+                    print(i)\n+                    next = False\n+            i = i + 1\n+        \n+        return members_list\n+\n+\n+\n+#testing comment after use\n+h = Helper()\n+github_api = \"https://api.github.com\"\n+h.set_org_name(\"CUBigDataClass\")\n+#print(h.get_org_information(\"vishwakulkarni\",github_api))\n+#k=h.get_repositories('vishwakulkarni',github_api)\n+#print(len(k))\n+#for mem in k:\n+#    print(mem['name'])"
            },
            {
                "sha": "eb528b8858abe979e3d3c23392da2641e6e2614a",
                "filename": "github-crawler/lib/pygit-helper.py",
                "status": "modified",
                "additions": 5,
                "deletions": 1,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/3ad956490769914023e6480d787252ba56f69517/github-crawler/lib/pygit-helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/3ad956490769914023e6480d787252ba56f69517/github-crawler/lib/pygit-helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/pygit-helper.py?ref=3ad956490769914023e6480d787252ba56f69517",
                "patch": "@@ -10,11 +10,15 @@\n print(org.login)\n names = org.get_members()\n repos = org.get_repos()\n+members = org.get_members()\n #\n #for name in names:\n #    print(name)\n \n i=0\n for repo in repos:\n     i=i+1\n-    print(i,repo)\n\\ No newline at end of file\n+    #print(i,repo.name)\n+\n+for mem in members:\n+    print(mem.name)\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjBmYzNmYzMxYzlhMGFkNmUwZTUwM2UzMDY4MGIyNGU1ZGVhNTdkOWM=",
        "commit": {
            "author": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T04:05:03Z"
            },
            "committer": {
                "name": "Siddartha Shankar",
                "email": "siddartha.shankar@Sidds-MacBook-Pro.local",
                "date": "2020-04-16T04:05:03Z"
            },
            "message": "UI First",
            "tree": {
                "sha": "6c63a84363fc411b021222fd716561790abbc4ff",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/6c63a84363fc411b021222fd716561790abbc4ff"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "388e099866586d2be8a74855ce2d0653b616b9ae",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/388e099866586d2be8a74855ce2d0653b616b9ae",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/388e099866586d2be8a74855ce2d0653b616b9ae"
            }
        ],
        "stats": {
            "total": 222,
            "additions": 222,
            "deletions": 0
        },
        "files": [
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "django/bdaSite/bdaProject/__init__.py",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__init__.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c"
            },
            {
                "sha": "dbf6b196b731844d58da03d2e3074adb6fc42c9a",
                "filename": "django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/__init__.cpython-37.pyc?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c"
            },
            {
                "sha": "77737699a98c1496cf4b280f779e85c34dacccd6",
                "filename": "django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/urls.cpython-37.pyc?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c"
            },
            {
                "sha": "c437c6dcc33c17b2bb614cd329bbd30440f32c35",
                "filename": "django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/__pycache__/views.cpython-37.pyc?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c"
            },
            {
                "sha": "8c38f3f3dad51e4585f3984282c2a4bec5349c1e",
                "filename": "django/bdaSite/bdaProject/admin.py",
                "status": "added",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/admin.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/admin.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/admin.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
                "patch": "@@ -0,0 +1,3 @@\n+from django.contrib import admin\n+\n+# Register your models here."
            },
            {
                "sha": "1e3be5584a0ced1e461a728fd279e46f78be69f6",
                "filename": "django/bdaSite/bdaProject/apps.py",
                "status": "added",
                "additions": 5,
                "deletions": 0,
                "changes": 5,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/apps.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/apps.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/apps.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
                "patch": "@@ -0,0 +1,5 @@\n+from django.apps import AppConfig\n+\n+\n+class BdaprojectConfig(AppConfig):\n+    name = 'bdaProject'"
            },
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "django/bdaSite/bdaProject/migrations/__init__.py",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/migrations/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/migrations/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/migrations/__init__.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c"
            },
            {
                "sha": "71a836239075aa6e6e4ecb700e9c42c95c022d91",
                "filename": "django/bdaSite/bdaProject/models.py",
                "status": "added",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/models.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/models.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/models.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
                "patch": "@@ -0,0 +1,3 @@\n+from django.db import models\n+\n+# Create your models here."
            },
            {
                "sha": "7ce503c2dd97ba78597f6ff6e4393132753573f6",
                "filename": "django/bdaSite/bdaProject/tests.py",
                "status": "added",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/tests.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/tests.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/tests.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
                "patch": "@@ -0,0 +1,3 @@\n+from django.test import TestCase\n+\n+# Create your tests here."
            },
            {
                "sha": "306c8bc29b660610aee1cc10dacd4cc698a5f96a",
                "filename": "django/bdaSite/bdaProject/urls.py",
                "status": "added",
                "additions": 6,
                "deletions": 0,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/urls.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/urls.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/urls.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
                "patch": "@@ -0,0 +1,6 @@\n+from django.urls import path\n+from . import views\n+\n+urlpatterns = [\n+    path('', views.index, name='index')\n+]\n\\ No newline at end of file"
            },
            {
                "sha": "2704602697a6535df8716e31855de2287ee77516",
                "filename": "django/bdaSite/bdaProject/views.py",
                "status": "added",
                "additions": 6,
                "deletions": 0,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/views.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaProject/views.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaProject/views.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
                "patch": "@@ -0,0 +1,6 @@\n+from django.shortcuts import render\n+from django.http import HttpResponse\n+\n+\n+def index(request):\n+    return HttpResponse(\"Hello, world. You're at the bdaProject index.\")"
            },
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "django/bdaSite/bdaSite/__init__.py",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__init__.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c"
            },
            {
                "sha": "ef448b20dac8ff827a0b3cb100f906e75060ff4c",
                "filename": "django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/__init__.cpython-37.pyc?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c"
            },
            {
                "sha": "229b2217b6530366aa04af8c165f77af971e8d27",
                "filename": "django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/settings.cpython-37.pyc?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c"
            },
            {
                "sha": "abf7528accc4cd0182652f8d597491fd778aca7a",
                "filename": "django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/urls.cpython-37.pyc?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c"
            },
            {
                "sha": "45f5ff925cdf0aa1dac25e82e4b0a797c54b4c98",
                "filename": "django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/__pycache__/wsgi.cpython-37.pyc?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c"
            },
            {
                "sha": "70d2b307cf04da098d5fcca6d7a9c154352265e0",
                "filename": "django/bdaSite/bdaSite/asgi.py",
                "status": "added",
                "additions": 16,
                "deletions": 0,
                "changes": 16,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/asgi.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/asgi.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/asgi.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
                "patch": "@@ -0,0 +1,16 @@\n+\"\"\"\n+ASGI config for bdaSite project.\n+\n+It exposes the ASGI callable as a module-level variable named ``application``.\n+\n+For more information on this file, see\n+https://docs.djangoproject.com/en/3.0/howto/deployment/asgi/\n+\"\"\"\n+\n+import os\n+\n+from django.core.asgi import get_asgi_application\n+\n+os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'bdaSite.settings')\n+\n+application = get_asgi_application()"
            },
            {
                "sha": "7060e5a04b7390cdaecc582b15c3a6c6d4ebf136",
                "filename": "django/bdaSite/bdaSite/settings.py",
                "status": "added",
                "additions": 120,
                "deletions": 0,
                "changes": 120,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/settings.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/settings.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/settings.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
                "patch": "@@ -0,0 +1,120 @@\n+\"\"\"\n+Django settings for bdaSite project.\n+\n+Generated by 'django-admin startproject' using Django 3.0.\n+\n+For more information on this file, see\n+https://docs.djangoproject.com/en/3.0/topics/settings/\n+\n+For the full list of settings and their values, see\n+https://docs.djangoproject.com/en/3.0/ref/settings/\n+\"\"\"\n+\n+import os\n+\n+# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\n+BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+\n+\n+# Quick-start development settings - unsuitable for production\n+# See https://docs.djangoproject.com/en/3.0/howto/deployment/checklist/\n+\n+# SECURITY WARNING: keep the secret key used in production secret!\n+SECRET_KEY = '8ub_@r6)4)29i4%c&q12!je_)20l(=m977f6d)oa9sx&1tr8h4'\n+\n+# SECURITY WARNING: don't run with debug turned on in production!\n+DEBUG = True\n+\n+ALLOWED_HOSTS = []\n+\n+\n+# Application definition\n+\n+INSTALLED_APPS = [\n+    'django.contrib.admin',\n+    'django.contrib.auth',\n+    'django.contrib.contenttypes',\n+    'django.contrib.sessions',\n+    'django.contrib.messages',\n+    'django.contrib.staticfiles',\n+]\n+\n+MIDDLEWARE = [\n+    'django.middleware.security.SecurityMiddleware',\n+    'django.contrib.sessions.middleware.SessionMiddleware',\n+    'django.middleware.common.CommonMiddleware',\n+    'django.middleware.csrf.CsrfViewMiddleware',\n+    'django.contrib.auth.middleware.AuthenticationMiddleware',\n+    'django.contrib.messages.middleware.MessageMiddleware',\n+    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n+]\n+\n+ROOT_URLCONF = 'bdaSite.urls'\n+\n+TEMPLATES = [\n+    {\n+        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n+        'DIRS': [],\n+        'APP_DIRS': True,\n+        'OPTIONS': {\n+            'context_processors': [\n+                'django.template.context_processors.debug',\n+                'django.template.context_processors.request',\n+                'django.contrib.auth.context_processors.auth',\n+                'django.contrib.messages.context_processors.messages',\n+            ],\n+        },\n+    },\n+]\n+\n+WSGI_APPLICATION = 'bdaSite.wsgi.application'\n+\n+\n+# Database\n+# https://docs.djangoproject.com/en/3.0/ref/settings/#databases\n+\n+DATABASES = {\n+    'default': {\n+        'ENGINE': 'django.db.backends.sqlite3',\n+        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n+    }\n+}\n+\n+\n+# Password validation\n+# https://docs.djangoproject.com/en/3.0/ref/settings/#auth-password-validators\n+\n+AUTH_PASSWORD_VALIDATORS = [\n+    {\n+        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n+    },\n+    {\n+        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n+    },\n+    {\n+        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n+    },\n+    {\n+        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n+    },\n+]\n+\n+\n+# Internationalization\n+# https://docs.djangoproject.com/en/3.0/topics/i18n/\n+\n+LANGUAGE_CODE = 'en-us'\n+\n+TIME_ZONE = 'UTC'\n+\n+USE_I18N = True\n+\n+USE_L10N = True\n+\n+USE_TZ = True\n+\n+\n+# Static files (CSS, JavaScript, Images)\n+# https://docs.djangoproject.com/en/3.0/howto/static-files/\n+\n+STATIC_URL = '/static/'"
            },
            {
                "sha": "2cebd51ae478237fdddb02335522080361ee1add",
                "filename": "django/bdaSite/bdaSite/urls.py",
                "status": "added",
                "additions": 23,
                "deletions": 0,
                "changes": 23,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/urls.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/urls.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/urls.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
                "patch": "@@ -0,0 +1,23 @@\n+\"\"\"bdaSite URL Configuration\n+\n+The `urlpatterns` list routes URLs to views. For more information please see:\n+    https://docs.djangoproject.com/en/3.0/topics/http/urls/\n+Examples:\n+Function views\n+    1. Add an import:  from my_app import views\n+    2. Add a URL to urlpatterns:  path('', views.home, name='home')\n+Class-based views\n+    1. Add an import:  from other_app.views import Home\n+    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')\n+Including another URLconf\n+    1. Import the include() function: from django.urls import include, path\n+    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))\n+\"\"\"\n+from django.contrib import admin\n+from django.urls import path\n+from django.urls import include\n+\n+urlpatterns = [\n+    path('admin/', admin.site.urls),\n+    path('bdaProject/', include('bdaProject.urls')),\n+]"
            },
            {
                "sha": "1b6950bce23de2d17f71c0529288332c18fdf303",
                "filename": "django/bdaSite/bdaSite/wsgi.py",
                "status": "added",
                "additions": 16,
                "deletions": 0,
                "changes": 16,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/wsgi.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/bdaSite/wsgi.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/bdaSite/wsgi.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
                "patch": "@@ -0,0 +1,16 @@\n+\"\"\"\n+WSGI config for bdaSite project.\n+\n+It exposes the WSGI callable as a module-level variable named ``application``.\n+\n+For more information on this file, see\n+https://docs.djangoproject.com/en/3.0/howto/deployment/wsgi/\n+\"\"\"\n+\n+import os\n+\n+from django.core.wsgi import get_wsgi_application\n+\n+os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'bdaSite.settings')\n+\n+application = get_wsgi_application()"
            },
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "django/bdaSite/db.sqlite3",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/db.sqlite3",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/db.sqlite3",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/db.sqlite3?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c"
            },
            {
                "sha": "f2d2f3482d56576cbaba5805bdbc00a6143f9f0a",
                "filename": "django/bdaSite/manage.py",
                "status": "added",
                "additions": 21,
                "deletions": 0,
                "changes": 21,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/manage.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c/django/bdaSite/manage.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/django/bdaSite/manage.py?ref=0fc3fc31c9a0ad6e0e503e30680b24e5dea57d9c",
                "patch": "@@ -0,0 +1,21 @@\n+#!/usr/bin/env python\n+\"\"\"Django's command-line utility for administrative tasks.\"\"\"\n+import os\n+import sys\n+\n+\n+def main():\n+    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'bdaSite.settings')\n+    try:\n+        from django.core.management import execute_from_command_line\n+    except ImportError as exc:\n+        raise ImportError(\n+            \"Couldn't import Django. Are you sure it's installed and \"\n+            \"available on your PYTHONPATH environment variable? Did you \"\n+            \"forget to activate a virtual environment?\"\n+        ) from exc\n+    execute_from_command_line(sys.argv)\n+\n+\n+if __name__ == '__main__':\n+    main()"
            }
        ]
    },
    {
        "sha": "0e249d1e805a5b292759d51f88a5b9e380e0f633",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjBlMjQ5ZDFlODA1YTViMjkyNzU5ZDUxZjg4YTViOWUzODBlMGY2MzM=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-15T16:49:50Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-15T16:49:50Z"
            },
            "message": "Updating key",
            "tree": {
                "sha": "539b403e348c1e14e24d0733e030a9ae9957a9d6",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/539b403e348c1e14e24d0733e030a9ae9957a9d6"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/0e249d1e805a5b292759d51f88a5b9e380e0f633",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/0e249d1e805a5b292759d51f88a5b9e380e0f633",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/0e249d1e805a5b292759d51f88a5b9e380e0f633",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/0e249d1e805a5b292759d51f88a5b9e380e0f633/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "5efb52eb103fdf850edcd953b90657a9638d2a11",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/5efb52eb103fdf850edcd953b90657a9638d2a11",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/5efb52eb103fdf850edcd953b90657a9638d2a11"
            }
        ],
        "stats": {
            "total": 5,
            "additions": 3,
            "deletions": 2
        },
        "files": [
            {
                "sha": "e15721c8f9b03fd0685d71c58785284e64479785",
                "filename": "github-crawler/app.py",
                "status": "modified",
                "additions": 1,
                "deletions": 0,
                "changes": 1,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0e249d1e805a5b292759d51f88a5b9e380e0f633/github-crawler/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0e249d1e805a5b292759d51f88a5b9e380e0f633/github-crawler/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/app.py?ref=0e249d1e805a5b292759d51f88a5b9e380e0f633",
                "patch": "@@ -20,6 +20,7 @@ def org_parser(orgname):\n     org_data = helper.get_org_information(OWNER,github_api)\n     print(\"Got Org Info!!!\")\n     print(\"sending Org Info to elastic search!!!\")\n+    #print(org_data)\n     helper.send_to_elasticInstance(org_data,'org1',org_data['id'])\n     print(\"Getting Repos for \"+orgname)\n     repo_list = helper.get_repositories(OWNER,github_api)"
            },
            {
                "sha": "ff0119b94421c5e585e3fef879f3210ab2588e13",
                "filename": "github-crawler/lib/helper.py",
                "status": "modified",
                "additions": 2,
                "deletions": 2,
                "changes": 4,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0e249d1e805a5b292759d51f88a5b9e380e0f633/github-crawler/lib/helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0e249d1e805a5b292759d51f88a5b9e380e0f633/github-crawler/lib/helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/helper.py?ref=0e249d1e805a5b292759d51f88a5b9e380e0f633",
                "patch": "@@ -12,9 +12,9 @@\n from elasticsearch import Elasticsearch\n \n GITEA_APP_URL = 'YOUR_GITEA_API'\n-GITEA_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+GITEA_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n GITHUB_USERNAME = 'vishwakulkarni'\n-GITHUB_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+GITHUB_TOKEN = 'd625eacdf998ccb9b131cf51a7e769358784a546'\n SQL_ALCHEMY_STRING = ''\n \n "
            }
        ]
    },
    {
        "sha": "5efb52eb103fdf850edcd953b90657a9638d2a11",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjVlZmI1MmViMTAzZmRmODUwZWRjZDk1M2I5MDY1N2E5NjM4ZDJhMTE=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-15T15:28:09Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-15T15:28:09Z"
            },
            "message": "Adding Users info to ealstic search along with user cammand",
            "tree": {
                "sha": "530b1f8d3384064fd61f7a00d9ed31e5da146c82",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/530b1f8d3384064fd61f7a00d9ed31e5da146c82"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/5efb52eb103fdf850edcd953b90657a9638d2a11",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/5efb52eb103fdf850edcd953b90657a9638d2a11",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/5efb52eb103fdf850edcd953b90657a9638d2a11",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/5efb52eb103fdf850edcd953b90657a9638d2a11/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "0771c1578a595eebc7849e2f3cdeca50a6fd1a19",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/0771c1578a595eebc7849e2f3cdeca50a6fd1a19",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/0771c1578a595eebc7849e2f3cdeca50a6fd1a19"
            }
        ],
        "stats": {
            "total": 120,
            "additions": 112,
            "deletions": 8
        },
        "files": [
            {
                "sha": "048facb90e501192503e0571f4c76dc840368c4e",
                "filename": "github-crawler/Templates_commands/users_command.txt",
                "status": "added",
                "additions": 61,
                "deletions": 0,
                "changes": 61,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/5efb52eb103fdf850edcd953b90657a9638d2a11/github-crawler/Templates_commands/users_command.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/5efb52eb103fdf850edcd953b90657a9638d2a11/github-crawler/Templates_commands/users_command.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/users_command.txt?ref=5efb52eb103fdf850edcd953b90657a9638d2a11",
                "patch": "@@ -0,0 +1,61 @@\n+PUT /users\n+{\n+    \"mappings\": {\n+    \"properties\": {\n+    \"login\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"id\": {\n+        \"type\": \"long\"\n+      },\n+    \"node_id\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"avatar_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"gravatar_id\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"html_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"followers_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"following_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"gists_url\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"starred_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"subscriptions_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"organizations_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"repos_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"events_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"received_events_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"type\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"site_admin\": {\n+        \"type\": \"boolean\"\n+      }\n+  }\n+    }\n+}"
            },
            {
                "sha": "4209b36a6b00d33e40a358ce7064d4ff1ba06d66",
                "filename": "github-crawler/app.py",
                "status": "modified",
                "additions": 5,
                "deletions": 0,
                "changes": 5,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/5efb52eb103fdf850edcd953b90657a9638d2a11/github-crawler/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/5efb52eb103fdf850edcd953b90657a9638d2a11/github-crawler/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/app.py?ref=5efb52eb103fdf850edcd953b90657a9638d2a11",
                "patch": "@@ -28,6 +28,11 @@ def org_parser(orgname):\n         repo['license']=\"test\"\n         helper.send_to_elasticInstance(repo,'repos',repo['id'])\n         print(\"repo sent \"+ repo['name'] )\n+    print(\"Getting Org Members for \"+orgname)\n+    member_list = helper.get_org_users(OWNER,github_api)\n+    print(\"sending user info to elasticsearch\")\n+    for member in member_list:\n+        helper.send_to_elasticInstance(member,'users',member['id'])\n     print(\"Done!!!!!!!!!\")\n     return 'We got your org name ' + orgname + ' give us some time to process your request, please check server output for progress'\n "
            },
            {
                "sha": "7074af8f8c78ed4ad46a924622f2bd31632fe95f",
                "filename": "github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/5efb52eb103fdf850edcd953b90657a9638d2a11/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/5efb52eb103fdf850edcd953b90657a9638d2a11/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/helper.cpython-37.pyc?ref=5efb52eb103fdf850edcd953b90657a9638d2a11"
            },
            {
                "sha": "5f81296ff4d08152681859a75b57790b00465c9e",
                "filename": "github-crawler/lib/helper.py",
                "status": "modified",
                "additions": 41,
                "deletions": 7,
                "changes": 48,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/5efb52eb103fdf850edcd953b90657a9638d2a11/github-crawler/lib/helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/5efb52eb103fdf850edcd953b90657a9638d2a11/github-crawler/lib/helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/helper.py?ref=5efb52eb103fdf850edcd953b90657a9638d2a11",
                "patch": "@@ -44,14 +44,48 @@ def send_to_elasticInstance(self,data,index_name,id_val):\n         self.es.index(index=index_name, doc_type='_doc',id=id_val, body=data)\n     \n     def get_repositories(self,owner,api):\n-        url = api + '/orgs/{}/repos'.format(self.orgname)\n-        org_repos_data = self.gh_session.get(url = url)\n-        org_repos_data=json.loads(org_repos_data.content)\n-        return org_repos_data\n+        repos_list = []\n+        next = True\n+        i=1\n+        while next == True:\n+            url = api + '/orgs/{}/repos?page={}&per_page=20'.format(self.orgname,i)\n+            original_data = self.gh_session.get(url=url)\n+            repos = json.loads(original_data.content)\n+            for repo in repos:\n+                repos_list.append(repo)\n+            if 'Link' in original_data.headers:\n+                if 'rel=\"next\"' not in original_data.headers['Link']:\n+                    print(i)\n+                    next = False\n+            i = i + 1\n+        return repos_list\n+    \n+    def get_org_users(self,owner,api):\n+        members_list = []\n+        next = True\n+        i=1\n+        while next == True:\n+            url = api + '/orgs/{}/members?page={}&per_page=100'.format(self.orgname,i)\n+            original_data = self.gh_session.get(url=url)\n+            members = json.loads(original_data.content)\n+            for member in members:\n+                members_list.append(member)\n+            if 'Link' in original_data.headers:\n+                if 'rel=\"next\"' not in original_data.headers['Link']:\n+                    print(i)\n+                    next = False\n+            i = i + 1\n+        \n+        return members_list\n+\n \n \n #testing comment after use\n-'''h = Helper()\n+h = Helper()\n github_api = \"https://api.github.com\"\n-h.set_org_name(\"mozilla\")\n-print(h.get_org_information(\"vishwakulkarni\",github_api))'''\n\\ No newline at end of file\n+h.set_org_name(\"CUBigDataClass\")\n+#print(h.get_org_information(\"vishwakulkarni\",github_api))\n+#k=h.get_repositories('vishwakulkarni',github_api)\n+#print(len(k))\n+#for mem in k:\n+#    print(mem['name'])"
            },
            {
                "sha": "eb528b8858abe979e3d3c23392da2641e6e2614a",
                "filename": "github-crawler/lib/pygit-helper.py",
                "status": "modified",
                "additions": 5,
                "deletions": 1,
                "changes": 6,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/5efb52eb103fdf850edcd953b90657a9638d2a11/github-crawler/lib/pygit-helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/5efb52eb103fdf850edcd953b90657a9638d2a11/github-crawler/lib/pygit-helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/pygit-helper.py?ref=5efb52eb103fdf850edcd953b90657a9638d2a11",
                "patch": "@@ -10,11 +10,15 @@\n print(org.login)\n names = org.get_members()\n repos = org.get_repos()\n+members = org.get_members()\n #\n #for name in names:\n #    print(name)\n \n i=0\n for repo in repos:\n     i=i+1\n-    print(i,repo)\n\\ No newline at end of file\n+    #print(i,repo.name)\n+\n+for mem in members:\n+    print(mem.name)\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "0771c1578a595eebc7849e2f3cdeca50a6fd1a19",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjA3NzFjMTU3OGE1OTVlZWJjNzg0OWUyZjNjZGVjYTUwYTZmZDFhMTk=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-15T13:29:25Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-15T13:29:25Z"
            },
            "message": "Fixing Org issue",
            "tree": {
                "sha": "0e5e70c7a6fda8849f8d8f38b7d6c0c76d0e1be8",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/0e5e70c7a6fda8849f8d8f38b7d6c0c76d0e1be8"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/0771c1578a595eebc7849e2f3cdeca50a6fd1a19",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/0771c1578a595eebc7849e2f3cdeca50a6fd1a19",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/0771c1578a595eebc7849e2f3cdeca50a6fd1a19",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/0771c1578a595eebc7849e2f3cdeca50a6fd1a19/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "5d04c799c5b4de8a8f0d629096967d3295fcfa27",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/5d04c799c5b4de8a8f0d629096967d3295fcfa27",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/5d04c799c5b4de8a8f0d629096967d3295fcfa27"
            }
        ],
        "stats": {
            "total": 28,
            "additions": 11,
            "deletions": 17
        },
        "files": [
            {
                "sha": "cc208b1d8e40b0382fafa2737a63ce76cbcc0e80",
                "filename": "github-crawler/Templates_commands/org_template.txt",
                "status": "modified",
                "additions": 2,
                "deletions": 7,
                "changes": 9,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0771c1578a595eebc7849e2f3cdeca50a6fd1a19/github-crawler/Templates_commands/org_template.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0771c1578a595eebc7849e2f3cdeca50a6fd1a19/github-crawler/Templates_commands/org_template.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/org_template.txt?ref=0771c1578a595eebc7849e2f3cdeca50a6fd1a19",
                "patch": "@@ -5,18 +5,13 @@ PUT _template/org_template\n     \"number_of_shards\": 1\n   },\n   \"mappings\": {\n-    \"_source\": {\n-      \"enabled\": false\n-    },\n     \"properties\": {\n-      \"host_name\": {\n-        \"type\": \"keyword\"\n-      },\n     \"login\": {\n         \"type\": \"keyword\"\n       },\n     \"id\": {\n-      \"type\":\"long\"},\n+      \"type\":\"long\"\n+      },\n     \"node_id\": {\n         \"type\": \"keyword\"\n       },"
            },
            {
                "sha": "64153d1d9e5a15babcbf8d01a8a6065bc47a1d34",
                "filename": "github-crawler/Templates_commands/put_org_index.txt",
                "status": "modified",
                "additions": 2,
                "deletions": 10,
                "changes": 12,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0771c1578a595eebc7849e2f3cdeca50a6fd1a19/github-crawler/Templates_commands/put_org_index.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0771c1578a595eebc7849e2f3cdeca50a6fd1a19/github-crawler/Templates_commands/put_org_index.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/put_org_index.txt?ref=0771c1578a595eebc7849e2f3cdeca50a6fd1a19",
                "patch": "@@ -1,21 +1,13 @@\n PUT /org1\n {\n-    \"settings\" : {\n-        \"number_of_shards\" : 1\n-    },\n     \"mappings\": {\n-    \"_source\": {\n-      \"enabled\": false\n-    },\n     \"properties\": {\n-      \"host_name\": {\n-        \"type\": \"keyword\"\n-      },\n     \"login\": {\n         \"type\": \"keyword\"\n       },\n     \"id\": {\n-      \"type\":\"long\"},\n+      \"type\":\"long\"\n+      },\n     \"node_id\": {\n         \"type\": \"keyword\"\n       },"
            },
            {
                "sha": "8f83b6e5582748c864c9edff6955c71fbba1b2d3",
                "filename": "github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0771c1578a595eebc7849e2f3cdeca50a6fd1a19/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0771c1578a595eebc7849e2f3cdeca50a6fd1a19/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/config.cpython-37.pyc?ref=0771c1578a595eebc7849e2f3cdeca50a6fd1a19"
            },
            {
                "sha": "53eb3fb5bb609e4f843b68bf2efdae7a775593c6",
                "filename": "github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "status": "modified",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0771c1578a595eebc7849e2f3cdeca50a6fd1a19/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0771c1578a595eebc7849e2f3cdeca50a6fd1a19/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/helper.cpython-37.pyc?ref=0771c1578a595eebc7849e2f3cdeca50a6fd1a19"
            },
            {
                "sha": "6774262981e531a784d5ec21f8b89a5ef3425204",
                "filename": "github-crawler/lib/helper.py",
                "status": "modified",
                "additions": 7,
                "deletions": 0,
                "changes": 7,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/0771c1578a595eebc7849e2f3cdeca50a6fd1a19/github-crawler/lib/helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/0771c1578a595eebc7849e2f3cdeca50a6fd1a19/github-crawler/lib/helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/helper.py?ref=0771c1578a595eebc7849e2f3cdeca50a6fd1a19",
                "patch": "@@ -48,3 +48,10 @@ def get_repositories(self,owner,api):\n         org_repos_data = self.gh_session.get(url = url)\n         org_repos_data=json.loads(org_repos_data.content)\n         return org_repos_data\n+\n+\n+#testing comment after use\n+'''h = Helper()\n+github_api = \"https://api.github.com\"\n+h.set_org_name(\"mozilla\")\n+print(h.get_org_information(\"vishwakulkarni\",github_api))'''\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "5d04c799c5b4de8a8f0d629096967d3295fcfa27",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjVkMDRjNzk5YzViNGRlOGE4ZjBkNjI5MDk2OTY3ZDMyOTVmY2ZhMjc=",
        "commit": {
            "author": {
                "name": "Vishwanath Kulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-15T12:34:23Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-15T12:34:23Z"
            },
            "message": "Fixing ReadMe errors",
            "tree": {
                "sha": "eda9db3aa07850beda2c8ad4a7b0f5bea28edbcd",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/eda9db3aa07850beda2c8ad4a7b0f5bea28edbcd"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/5d04c799c5b4de8a8f0d629096967d3295fcfa27",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJelv9PCRBK7hj4Ov3rIwAAdHIIAJpPt6BlSMelcofAVj0RRtlv\naxYxGsHc1Z7Q9UGQOSttyu97kt1DDrDctci6kumg3NjUDBtkwadTFjjtdQFlKYfe\n2zU3gOAUxWh9TOhUZPXEO864s5+0wD86B5CAlRc9hc8N721d3ccm3fk//Hjld1eA\ntjEeIvBY/uu3AT71ziy6kwLlXOZYel5xXABbReQwjQro5C1S4MoLu2h9EJk9oaLA\nYk78gC5BCE3xGAt4ziq/Cqi2NdqGKegIC234CChkA9d9PZzYlEgsusuzIdBi+q5r\nx79TgRt7pG1z2VVSXsB4BzEkNHV/tt4yZV9XeVyBHk/s4zY5c2592PfZfq2hKco=\n=Ts4a\n-----END PGP SIGNATURE-----\n",
                "payload": "tree eda9db3aa07850beda2c8ad4a7b0f5bea28edbcd\nparent 478ee8b174d1080daecd18a129dd007f9654398c\nauthor Vishwanath Kulkarni <vishwa.kulkarni@gmail.com> 1586954063 -0600\ncommitter GitHub <noreply@github.com> 1586954063 -0600\n\nFixing ReadMe errors"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/5d04c799c5b4de8a8f0d629096967d3295fcfa27",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/5d04c799c5b4de8a8f0d629096967d3295fcfa27",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/5d04c799c5b4de8a8f0d629096967d3295fcfa27/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "478ee8b174d1080daecd18a129dd007f9654398c",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/478ee8b174d1080daecd18a129dd007f9654398c",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/478ee8b174d1080daecd18a129dd007f9654398c"
            }
        ],
        "stats": {
            "total": 85,
            "additions": 76,
            "deletions": 9
        },
        "files": [
            {
                "sha": "2b63cace2f5e424d58ceaf5eb70ffae4f5d7f121",
                "filename": "README.md",
                "status": "modified",
                "additions": 76,
                "deletions": 9,
                "changes": 85,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/5d04c799c5b4de8a8f0d629096967d3295fcfa27/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/5d04c799c5b4de8a8f0d629096967d3295fcfa27/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=5d04c799c5b4de8a8f0d629096967d3295fcfa27",
                "patch": "@@ -3,6 +3,72 @@\n \n   \n \n+## Modules\n+\n+|Name  | Progress |\n+|--|--|\n+|  Github-Crawler| In-Progress|\n+| Github-Analyser|In-Progress|\n+|Dashboard-backend||\n+|Dashboard-Frontend||\n+\n+\n+    Github-Crawler Setup\n+\n+ 1) Install and keep following software\n+ \n+\n+ - Docker\n+ - Python-Flask\n+ - Python3\n+ - ElasticSearch\n+ - \n+\n+2) Run Below Comands after starting Docker\n+\n+\t\tdocker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.5.2\n+\t```\n+\tdocker run -d --name kibana --net somenetwork -p 5601:5601 kibana:tag\n+\t```\n+3) Set-up ElasticSearch (Important)\n+   navigate to [http://localhost:5601/app/kibana#/dev_tools/console?_g=()](http://localhost:5601/app/kibana#/dev_tools/console?_g=())\n+   copy and past each and every file from **github-crawler/Templates_commands** into above console (list of files is given below)\n+\n+         * comments_command.txt\n+         * repos_cammand.txt\n+         * put_org_index.txt\n+    \n+\n+3) Starting github-crawler and usage\n+\n+\t\tgit clone https://github.com/CUBigDataClass/Kode-Kallas.git\n+\t\tcd Kode-Kallas/github-crawler\n+\t    python3 app.py\n+navigate to\n+-  http://127.0.0.1:5000  -- you will be able to see proper message\n+- http://127.0.0.1:5000/org/<!orgname_should be added here>  - Just check server logs  \n+\n+4) For stopping and starting your docker images\n+\n+\t     $ docker ps -a    \n+\t     $ docker stop 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\n+\t     $ docker stop d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\n+\t     $ docker start 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\n+\t     $ docker start d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\n+\n+\n+4) Below some Documentation\n+\n+\t\t Work Under progress\n+\n+|**Module**  | **Usage** | **Requirements**|\n+|--|--|--|\n+|Github-Crawler|Used to get Github organization info |Docker and Python3|\n+# Kode - Kallas\n+\n+\n+  \n+\n ## Modules\n \n |Name  | Progress |\n@@ -65,21 +131,22 @@ navigate to\n \t     $ docker start d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\t\n \n \n+5) Cassandra api setup\n \n-5) Below some Documentation\n+\t    docker run -e DS_LICENSE=accept --memory 4g -p 7000:7000 -p 7001:7001 -p 7199:7199 -p 9042:9042 -p 9160:9160 -p 9404:9404 --name my-dse -e CASSANDRA_START_RPC=true -d datastax/dse-server:6.8.0\n+\t    docker start my-dse\n+\t \n+    Prereq: Install flask and cassandra-driver using pip.\n \n-\t\t Cassandra api setup\n+\t    python3 app.py\n \n-1) setup dcoker \n-\t ```docker run -e DS_LICENSE=accept --memory 4g -p 7000:7000 -p 7001:7001 -p 7199:7199 -p 9042:9042 -p 9160:9160 -p 9404:9404 --name my-dse -e CASSANDRA_START_RPC=true -d datastax/dse-server:6.8.0```\n-\t ```docker start my-dse```\n-\t \n-2) Prereq: Install flask and cassandra-driver using pip.\n \n-3) ```python3 app.py```\n \n-4)Postman scripts for api access: TODO include link\n+\n+6) Below some Documentation\n+\n \n |**Module**  | **Usage** | **Requirements**|\n |--|--|--|\n |Github-Crawler|Used to get Github organization info |Docker and Python3|\n+"
            }
        ]
    },
    {
        "sha": "478ee8b174d1080daecd18a129dd007f9654398c",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjQ3OGVlOGIxNzRkMTA4MGRhZWNkMThhMTI5ZGQwMDdmOTY1NDM5OGM=",
        "commit": {
            "author": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-15T05:40:08Z"
            },
            "committer": {
                "name": "shreyas-gopalakrishna",
                "email": "shreyas.g.krishna@gmail.com",
                "date": "2020-04-15T05:40:08Z"
            },
            "message": "Added Architecture Diagram",
            "tree": {
                "sha": "703482a209e1ec6ca54dc3b2e7a0325078adf5f6",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/703482a209e1ec6ca54dc3b2e7a0325078adf5f6"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/478ee8b174d1080daecd18a129dd007f9654398c",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/478ee8b174d1080daecd18a129dd007f9654398c",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/478ee8b174d1080daecd18a129dd007f9654398c",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/478ee8b174d1080daecd18a129dd007f9654398c/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "cae3805e965eb5e00faf49629931d0e369f8cf7c",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/cae3805e965eb5e00faf49629931d0e369f8cf7c",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/cae3805e965eb5e00faf49629931d0e369f8cf7c"
            }
        ],
        "stats": {
            "total": 0,
            "additions": 0,
            "deletions": 0
        },
        "files": [
            {
                "sha": "a86165d1469792833005cbe18ed0b2a58468201f",
                "filename": "Architecture-Diagram.pdf",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/478ee8b174d1080daecd18a129dd007f9654398c/Architecture-Diagram.pdf",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/478ee8b174d1080daecd18a129dd007f9654398c/Architecture-Diagram.pdf",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/Architecture-Diagram.pdf?ref=478ee8b174d1080daecd18a129dd007f9654398c"
            }
        ]
    },
    {
        "sha": "cae3805e965eb5e00faf49629931d0e369f8cf7c",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmNhZTM4MDVlOTY1ZWI1ZTAwZmFmNDk2Mjk5MzFkMGUzNjlmOGNmN2M=",
        "commit": {
            "author": {
                "name": "Karthik S",
                "email": "karthiks1995@gmail.com",
                "date": "2020-04-15T05:12:37Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-15T05:12:37Z"
            },
            "message": "Update readme with cassandra setup",
            "tree": {
                "sha": "ea70a9f105297a313cabee311c161fc9a94e3522",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/ea70a9f105297a313cabee311c161fc9a94e3522"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/cae3805e965eb5e00faf49629931d0e369f8cf7c",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJelpfFCRBK7hj4Ov3rIwAAdHIIAJ0tzsmtUIbUNaU1zbumn6gw\nyo0VKcNlCaDCz7djb/tH8vmBjx8+E6SjJk2QEhwKNwxqg6uK8mLrj8dNrLP0N+/p\nnPhDv29BR0u5sXGsoDkPDMxgU/UlM5lFBj8At6w6PXVDgpOBxpSvEedowlQSWgr2\nzkRtWeE3EzL+woonBWi/hgW6mR2LmDfPQgPYINQxCPfZieQ7m+ev79W1jKAwd+pf\ncqatp5QCwbrfRL3ZpIiN/NIQm+UMVF50/32vd5W8qmnB5954Wc0UU1Cfth6rKczO\nJF0Rxuzb1j0vBPifBa9ANXCJag1EjFEnFoOqnhQzmwaXqGNUkFu/cVwgTpTC7VY=\n=9EJy\n-----END PGP SIGNATURE-----\n",
                "payload": "tree ea70a9f105297a313cabee311c161fc9a94e3522\nparent e50f02c0163493202f4f84fe3b9a534f5f41fbb8\nauthor Karthik S <karthiks1995@gmail.com> 1586927557 -0700\ncommitter GitHub <noreply@github.com> 1586927557 -0700\n\nUpdate readme with cassandra setup"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/cae3805e965eb5e00faf49629931d0e369f8cf7c",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/cae3805e965eb5e00faf49629931d0e369f8cf7c",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/cae3805e965eb5e00faf49629931d0e369f8cf7c/comments",
        "author": {
            "login": "karthiks1995",
            "id": 7661540,
            "node_id": "MDQ6VXNlcjc2NjE1NDA=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/7661540?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/karthiks1995",
            "html_url": "https://github.com/karthiks1995",
            "followers_url": "https://api.github.com/users/karthiks1995/followers",
            "following_url": "https://api.github.com/users/karthiks1995/following{/other_user}",
            "gists_url": "https://api.github.com/users/karthiks1995/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/karthiks1995/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/karthiks1995/subscriptions",
            "organizations_url": "https://api.github.com/users/karthiks1995/orgs",
            "repos_url": "https://api.github.com/users/karthiks1995/repos",
            "events_url": "https://api.github.com/users/karthiks1995/events{/privacy}",
            "received_events_url": "https://api.github.com/users/karthiks1995/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "e50f02c0163493202f4f84fe3b9a534f5f41fbb8",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/e50f02c0163493202f4f84fe3b9a534f5f41fbb8",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/e50f02c0163493202f4f84fe3b9a534f5f41fbb8"
            }
        ],
        "stats": {
            "total": 14,
            "additions": 12,
            "deletions": 2
        },
        "files": [
            {
                "sha": "56837c5ecaecdc6e91b5dde7a5002a5e47a87dc9",
                "filename": "README.md",
                "status": "modified",
                "additions": 12,
                "deletions": 2,
                "changes": 14,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/cae3805e965eb5e00faf49629931d0e369f8cf7c/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/cae3805e965eb5e00faf49629931d0e369f8cf7c/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=cae3805e965eb5e00faf49629931d0e369f8cf7c",
                "patch": "@@ -9,7 +9,7 @@\n |--|--|\n |  Github-Crawler| In-Progress|\n | Github-Analyser|In-Progress|\n-|Dashboard-backend||\n+|Dashboard-backend| In-progress|\n |Dashboard-Frontend||\n \n \n@@ -68,7 +68,17 @@ navigate to\n \n 5) Below some Documentation\n \n-\t\t Work Under progress\n+\t\t Cassandra api setup\n+\n+1) setup dcoker \n+\t ```docker run -e DS_LICENSE=accept --memory 4g -p 7000:7000 -p 7001:7001 -p 7199:7199 -p 9042:9042 -p 9160:9160 -p 9404:9404 --name my-dse -e CASSANDRA_START_RPC=true -d datastax/dse-server:6.8.0```\n+\t ```docker start my-dse```\n+\t \n+2) Prereq: Install flask and cassandra-driver using pip.\n+\n+3) ```python3 app.py```\n+\n+4)Postman scripts for api access: TODO include link\n \n |**Module**  | **Usage** | **Requirements**|\n |--|--|--|"
            }
        ]
    },
    {
        "sha": "e50f02c0163493202f4f84fe3b9a534f5f41fbb8",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmU1MGYwMmMwMTYzNDkzMjAyZjRmODRmZTNiOWE1MzRmNWY0MWZiYjg=",
        "commit": {
            "author": {
                "name": "Karthik Siddaramanna",
                "email": "karthiks@Karthiks-MBP.lan",
                "date": "2020-04-15T05:06:56Z"
            },
            "committer": {
                "name": "Karthik Siddaramanna",
                "email": "karthiks@Karthiks-MBP.lan",
                "date": "2020-04-15T05:06:56Z"
            },
            "message": "Merge branch 'master' of https://github.com/CUBigDataClass/Kode-Kallas",
            "tree": {
                "sha": "41b5968d864f9332c434b52e95c36405fcb1f71d",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/41b5968d864f9332c434b52e95c36405fcb1f71d"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/e50f02c0163493202f4f84fe3b9a534f5f41fbb8",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/e50f02c0163493202f4f84fe3b9a534f5f41fbb8",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/e50f02c0163493202f4f84fe3b9a534f5f41fbb8",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/e50f02c0163493202f4f84fe3b9a534f5f41fbb8/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "c509a7c69a085656801eb10b13e062427e2b9a89",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c509a7c69a085656801eb10b13e062427e2b9a89",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c509a7c69a085656801eb10b13e062427e2b9a89"
            },
            {
                "sha": "388e099866586d2be8a74855ce2d0653b616b9ae",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/388e099866586d2be8a74855ce2d0653b616b9ae",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/388e099866586d2be8a74855ce2d0653b616b9ae"
            }
        ],
        "stats": {
            "total": 229,
            "additions": 223,
            "deletions": 6
        },
        "files": [
            {
                "sha": "6ef10ded7034236b6df7456bac258df218226906",
                "filename": "README.md",
                "status": "modified",
                "additions": 24,
                "deletions": 6,
                "changes": 30,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e50f02c0163493202f4f84fe3b9a534f5f41fbb8/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e50f02c0163493202f4f84fe3b9a534f5f41fbb8/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=e50f02c0163493202f4f84fe3b9a534f5f41fbb8",
                "patch": "@@ -22,20 +22,28 @@\n  - Python-Flask\n  - Python3\n  - ElasticSearch\n- - \n+ - pip install flask \n+ - pip install elasticsearch \n \n 2) Run Below Comands after starting Docker\n-\n-\t\tdocker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.5.2\n \t```\n-\tdocker run -d --name kibana --net somenetwork -p 5601:5601 kibana:tag\n+\tdocker pull docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n+\t```\n+\t``` \n+\tdocker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.6.2 \n+\t```\n+\t```\n+\tdocker pull docker.elastic.co/kibana/kibana:7.6.2\n+\t```\n+\t```\n+\tdocker run --link YOUR_ELASTICSEARCH_CONTAINER_NAME_OR_ID:elasticsearch -p 5601:5601 kibana:7.6.2\n \t```\n 3) Set-up ElasticSearch (Important)\n    navigate to [http://localhost:5601/app/kibana#/dev_tools/console?_g=()](http://localhost:5601/app/kibana#/dev_tools/console?_g=())\n    copy and past each and every file from **github-crawler/Templates_commands** into above console (list of files is given below)\n \n          * comments_command.txt\n-         * repos_cammand.txt\n+         * repos_command.txt\n          * put_org_index.txt\n     \n \n@@ -48,7 +56,17 @@ navigate to\n -  http://127.0.0.1:5000  -- you will be able to see proper message\n - http://127.0.0.1:5000/org/<!orgname_should be added here>  - Just check server logs  \n \n-4) Below some Documentation\n+4) For stopping and starting your docker images\t\n+\n+\t     $ docker ps -a    \t\n+\t     $ docker stop 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\t\n+\t     $ docker stop d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\t\n+\t     $ docker start 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\t\n+\t     $ docker start d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\t\n+\n+\n+\n+5) Below some Documentation\n \n \t\t Work Under progress\n "
            },
            {
                "sha": "fe2ecafd5420dbf76277ae9c9be4141abb2cda3e",
                "filename": "github-crawler/Templates_commands/org_template.txt",
                "status": "modified",
                "additions": 100,
                "deletions": 0,
                "changes": 100,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e50f02c0163493202f4f84fe3b9a534f5f41fbb8/github-crawler/Templates_commands/org_template.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e50f02c0163493202f4f84fe3b9a534f5f41fbb8/github-crawler/Templates_commands/org_template.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/org_template.txt?ref=e50f02c0163493202f4f84fe3b9a534f5f41fbb8",
                "patch": "@@ -0,0 +1,100 @@\n+PUT _template/org_template\n+{\n+  \"index_patterns\": [\"te*\", \"bar*\"],\n+  \"settings\": {\n+    \"number_of_shards\": 1\n+  },\n+  \"mappings\": {\n+    \"_source\": {\n+      \"enabled\": false\n+    },\n+    \"properties\": {\n+      \"host_name\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"login\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"id\": {\n+      \"type\":\"long\"},\n+    \"node_id\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"url\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"repos_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"events_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"hooks_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"issues_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"members_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"public_members_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"avatar_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"description\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"name\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"company\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"blog\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"location\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"email\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"is_verified\": {\n+        \"type\": \"boolean\"\n+      },\n+    \"has_organization_projects\": {\n+        \"type\": \"boolean\"\n+      },\n+    \"has_repository_projects\": {\n+        \"type\": \"boolean\"\n+      },\n+    \"public_repos\": {\n+        \"type\": \"long\"\n+      },\n+    \"public_gists\": {\n+        \"type\": \"long\"\n+      },\n+    \"followers\": {\n+        \"type\": \"long\"\n+      },\n+    \"following\": {\n+        \"type\": \"long\"\n+      },\n+    \"html_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"created_at\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"updated_at\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"type\": {\n+        \"type\": \"keyword\"\n+      }\n+    }\n+  }\n+}"
            },
            {
                "sha": "c9dd9cbdc21f6a11d76ed8a01d0fd29a66b12485",
                "filename": "github-crawler/Templates_commands/put_org_index.txt",
                "status": "modified",
                "additions": 99,
                "deletions": 0,
                "changes": 99,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/e50f02c0163493202f4f84fe3b9a534f5f41fbb8/github-crawler/Templates_commands/put_org_index.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/e50f02c0163493202f4f84fe3b9a534f5f41fbb8/github-crawler/Templates_commands/put_org_index.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/put_org_index.txt?ref=e50f02c0163493202f4f84fe3b9a534f5f41fbb8",
                "patch": "@@ -0,0 +1,99 @@\n+PUT /org1\n+{\n+    \"settings\" : {\n+        \"number_of_shards\" : 1\n+    },\n+    \"mappings\": {\n+    \"_source\": {\n+      \"enabled\": false\n+    },\n+    \"properties\": {\n+      \"host_name\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"login\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"id\": {\n+      \"type\":\"long\"},\n+    \"node_id\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"url\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"repos_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"events_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"hooks_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"issues_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"members_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"public_members_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"avatar_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"description\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"name\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"company\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"blog\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"location\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"email\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"is_verified\": {\n+        \"type\": \"boolean\"\n+      },\n+    \"has_organization_projects\": {\n+        \"type\": \"boolean\"\n+      },\n+    \"has_repository_projects\": {\n+        \"type\": \"boolean\"\n+      },\n+    \"public_repos\": {\n+        \"type\": \"long\"\n+      },\n+    \"public_gists\": {\n+        \"type\": \"long\"\n+      },\n+    \"followers\": {\n+        \"type\": \"long\"\n+      },\n+    \"following\": {\n+        \"type\": \"long\"\n+      },\n+    \"html_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"created_at\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"updated_at\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"type\": {\n+        \"type\": \"keyword\"\n+      }\n+    }\n+  }\n+}"
            }
        ]
    },
    {
        "sha": "c509a7c69a085656801eb10b13e062427e2b9a89",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmM1MDlhN2M2OWEwODU2NTY4MDFlYjEwYjEzZTA2MjQyN2UyYjlhODk=",
        "commit": {
            "author": {
                "name": "Karthik Siddaramanna",
                "email": "karthiks@Karthiks-MBP.lan",
                "date": "2020-04-15T05:05:24Z"
            },
            "committer": {
                "name": "Karthik Siddaramanna",
                "email": "karthiks@Karthiks-MBP.lan",
                "date": "2020-04-15T05:05:24Z"
            },
            "message": "Adding Cassandra db apis",
            "tree": {
                "sha": "806d7a91cb8e2cab46b5eaf17a5beb561efdcc45",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/806d7a91cb8e2cab46b5eaf17a5beb561efdcc45"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/c509a7c69a085656801eb10b13e062427e2b9a89",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c509a7c69a085656801eb10b13e062427e2b9a89",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c509a7c69a085656801eb10b13e062427e2b9a89",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c509a7c69a085656801eb10b13e062427e2b9a89/comments",
        "author": null,
        "committer": null,
        "parents": [
            {
                "sha": "ce97c57d4715385fcc1ce221a4e491c8bfdd172b",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/ce97c57d4715385fcc1ce221a4e491c8bfdd172b",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/ce97c57d4715385fcc1ce221a4e491c8bfdd172b"
            }
        ],
        "stats": {
            "total": 348,
            "additions": 348,
            "deletions": 0
        },
        "files": [
            {
                "sha": "de42b46c5aa78eefa29888e6d368da97e2fffe08",
                "filename": "db-apis/.ipynb_checkpoints/Untitled-checkpoint.ipynb",
                "status": "added",
                "additions": 60,
                "deletions": 0,
                "changes": 60,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/.ipynb_checkpoints/Untitled-checkpoint.ipynb",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/.ipynb_checkpoints/Untitled-checkpoint.ipynb",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/.ipynb_checkpoints/Untitled-checkpoint.ipynb?ref=c509a7c69a085656801eb10b13e062427e2b9a89",
                "patch": "@@ -0,0 +1,60 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from cassandra.cluster import Cluster\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 28,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"cluster = Cluster(['127.0.0.1'], port=9042)\\n\",\n+    \"session = cluster.connect()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 29,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"session.set_keyspace('sample_org')\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": []\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"Python 3\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\",\n+   \"version\": \"3.7.3\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 2\n+}"
            },
            {
                "sha": "757a76032b29b2c1983c26f971ca1d2d3b83a53e",
                "filename": "db-apis/Untitled.ipynb",
                "status": "added",
                "additions": 136,
                "deletions": 0,
                "changes": 136,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/Untitled.ipynb",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/Untitled.ipynb",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/Untitled.ipynb?ref=c509a7c69a085656801eb10b13e062427e2b9a89",
                "patch": "@@ -0,0 +1,136 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from cassandra.cluster import Cluster\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 38,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"cluster = Cluster(['127.0.0.1'], port=9042)\\n\",\n+    \"session = cluster.connect()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 42,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"data\": {\n+      \"text/plain\": [\n+       \"<cassandra.cluster.ResultSet at 0x1208d2e10>\"\n+      ]\n+     },\n+     \"execution_count\": 42,\n+     \"metadata\": {},\n+     \"output_type\": \"execute_result\"\n+    }\n+   ],\n+   \"source\": [\n+    \"create_keyspace_statement = \\\"CREATE KEYSPACE ? WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\\\"\\n\",\n+    \"session.execute(\\\"CREATE KEYSPACE hehe WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 30,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"<cassandra.cluster.Session object at 0x12030eef0>\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"print(session)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 33,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"rows = session.execute('SELECT * FROM users')\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 34,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"data\": {\n+      \"text/plain\": [\n+       \"<cassandra.cluster.ResultSet at 0x120390240>\"\n+      ]\n+     },\n+     \"execution_count\": 34,\n+     \"metadata\": {},\n+     \"output_type\": \"execute_result\"\n+    }\n+   ],\n+   \"source\": [\n+    \"rows\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 36,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Hehehaha\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"for user in rows:\\n\",\n+    \"    print(user.name)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": []\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"Python 3\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\",\n+   \"version\": \"3.7.3\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 2\n+}"
            },
            {
                "sha": "9720cc191b47fa1e2a80ee38dda67c485784a0dd",
                "filename": "db-apis/app.py",
                "status": "added",
                "additions": 49,
                "deletions": 0,
                "changes": 49,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/app.py?ref=c509a7c69a085656801eb10b13e062427e2b9a89",
                "patch": "@@ -0,0 +1,49 @@\n+from flask import (\n+    Flask,\n+    request\n+)\n+from dao import setup\n+from dao import core\n+from cassandra.cluster import Cluster\n+\n+app = Flask(__name__, template_folder=\"templates\")\n+\n+\n+@app.route('/')\n+def home():\n+    return setup.create_table()\n+\n+\n+@app.route('/setup/<orgname>')\n+def org_setup(orgname):\n+    return setup.create_keyspace(orgname)\n+\n+\n+@app.route('/delete/<orgname>')\n+def org_delete(orgname):\n+    return setup.delete_keyspace(orgname)\n+\n+\n+# TODO: Delete once done\n+@app.route('/tp')\n+def timepass():\n+    return setup.insert_users()\n+\n+\n+# TODO: Delete once done\n+@app.route('/tp2')\n+def timepass2():\n+    return setup.another_insert_users()\n+\n+\n+@app.route('/insert', methods=['POST'])\n+def insert():\n+    # TODO: populate response P0\n+    # TODO: Catch exceptions P1\n+    response = core.insert(request)\n+    # return response\n+\n+\n+# If we're running in stand alone mode, run the application\n+if __name__ == '__main__':\n+    app.run(debug=True)"
            },
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "db-apis/dao/__init__.py",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/__init__.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/__init__.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/__init__.py?ref=c509a7c69a085656801eb10b13e062427e2b9a89"
            },
            {
                "sha": "5d2a75f5490357446e5a8e1523e117b73eb241da",
                "filename": "db-apis/dao/__pycache__/__init__.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/__pycache__/__init__.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/__pycache__/__init__.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/__pycache__/__init__.cpython-37.pyc?ref=c509a7c69a085656801eb10b13e062427e2b9a89"
            },
            {
                "sha": "854270ad54da3f65df8eeed742d490d5016ce4f7",
                "filename": "db-apis/dao/__pycache__/config.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/__pycache__/config.cpython-37.pyc?ref=c509a7c69a085656801eb10b13e062427e2b9a89"
            },
            {
                "sha": "eb53bbf099b0865a15f4b5ca37183bb571a837c5",
                "filename": "db-apis/dao/__pycache__/core.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/__pycache__/core.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/__pycache__/core.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/__pycache__/core.cpython-37.pyc?ref=c509a7c69a085656801eb10b13e062427e2b9a89"
            },
            {
                "sha": "5bea301cc1c812ac8cb24a86390ce1efd7e6d4fb",
                "filename": "db-apis/dao/__pycache__/models.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/__pycache__/models.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/__pycache__/models.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/__pycache__/models.cpython-37.pyc?ref=c509a7c69a085656801eb10b13e062427e2b9a89"
            },
            {
                "sha": "993fbfc6750f86a32c77c1219be3ab42fd503462",
                "filename": "db-apis/dao/__pycache__/setup.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/__pycache__/setup.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/__pycache__/setup.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/__pycache__/setup.cpython-37.pyc?ref=c509a7c69a085656801eb10b13e062427e2b9a89"
            },
            {
                "sha": "48bc7fce612cec226f9c2d06a6949b84fe32d5c1",
                "filename": "db-apis/dao/config.py",
                "status": "added",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/config.py?ref=c509a7c69a085656801eb10b13e062427e2b9a89",
                "patch": "@@ -0,0 +1,3 @@\n+# Database options\n+CASSANDRA_HOSTS = ['127.0.0.1']\n+CASSANDRA_KEYSPACE = \"sample_org\"\n\\ No newline at end of file"
            },
            {
                "sha": "97deef1339ad67550025983733ecd09671982887",
                "filename": "db-apis/dao/core.py",
                "status": "added",
                "additions": 21,
                "deletions": 0,
                "changes": 21,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/core.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/core.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/core.py?ref=c509a7c69a085656801eb10b13e062427e2b9a89",
                "patch": "@@ -0,0 +1,21 @@\n+from cassandra.cqlengine import connection\n+from dao.config import CASSANDRA_HOSTS\n+from dao.models import Users, Data\n+\n+ORG = \"org\"\n+TABLE = \"table\"\n+BODY = \"body\"\n+USERS = \"users\"\n+\n+\n+# TODO: Enable quorum\n+def insert(request):\n+    # TODO: Data Validation should be done?\n+    content = request.get_json()\n+    connection.setup(CASSANDRA_HOSTS, content[ORG])\n+    # if isinstance(body, Model):\n+    # TODO: Avoid using eval() because\n+    # TODO: this requires me to import all models\n+    obj = eval(content[TABLE]).create(**(content[BODY]))\n+    return obj\n+"
            },
            {
                "sha": "66c3633370328232b06feed9844f1d154a3b67f8",
                "filename": "db-apis/dao/models.py",
                "status": "added",
                "additions": 21,
                "deletions": 0,
                "changes": 21,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/models.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/models.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/models.py?ref=c509a7c69a085656801eb10b13e062427e2b9a89",
                "patch": "@@ -0,0 +1,21 @@\n+import uuid\n+from cassandra.cqlengine import columns\n+from cassandra.cqlengine.models import Model\n+\n+class URL():\n+\n+    def __init__(self, url):\n+        self.url = url\n+\n+class Users(Model):\n+    uid = columns.UUID(primary_key=True, default=uuid.uuid4)\n+    name = columns.Text(required=True)\n+    repos = columns.List(value_type=columns.Text, required=False)\n+\n+\n+class Data(Model):\n+    uid = columns.UUID(primary_key=True, default=uuid.uuid4)\n+    gitid = columns.Text(required=True)\n+    name = columns.Text(required=True)\n+    repo = columns.Text(required=True)\n+    commit_num = columns.Integer(required=False, default=0)"
            },
            {
                "sha": "19d7eca412a25184491b56b878fb9c83251dd0be",
                "filename": "db-apis/dao/setup.py",
                "status": "added",
                "additions": 58,
                "deletions": 0,
                "changes": 58,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/setup.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c509a7c69a085656801eb10b13e062427e2b9a89/db-apis/dao/setup.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/db-apis/dao/setup.py?ref=c509a7c69a085656801eb10b13e062427e2b9a89",
                "patch": "@@ -0,0 +1,58 @@\n+from cassandra.cqlengine import connection\n+from cassandra.cluster import Cluster\n+from cassandra.cqlengine.management import sync_table\n+from cassandra.query import SimpleStatement\n+\n+from dao.config import CASSANDRA_HOSTS\n+from dao.models import Users, Data\n+\n+\n+def get_session(keyspace=None):\n+    # TODO: Should enable ssl: for both inter cluster and app communication P1\n+    cluster = Cluster(CASSANDRA_HOSTS)\n+    session = cluster.connect(keyspace)\n+    return session\n+\n+\n+# TODO: Simple vs Prepared P2\n+def create_keyspace(orgname):\n+    session = get_session()\n+    # TODO: Replication strategy should be updated to 3\n+    query = SimpleStatement(\n+        \"CREATE KEYSPACE %s WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\" % (orgname,))\n+    session.execute(query)\n+    session.shutdown()\n+    create_tables(orgname)\n+\n+\n+def delete_keyspace(orgname):\n+    session = get_session()\n+    query = SimpleStatement(\n+        \"DROP KEYSPACE %s;\" % (orgname,))\n+    session.execute(query)\n+    session.shutdown()\n+\n+\n+# TODO: MAke sure sessions and connections are handled properly: probably read about that\n+def create_tables(orgname):\n+    connection.setup(CASSANDRA_HOSTS, orgname, protocol_version=3)\n+    # session = get_session(orgname)\n+    sync_table(Users)\n+    sync_table(Data)\n+    connection.unregister_connection('default')\n+\n+\n+# TODO: Delete below functions when everything is done\n+def insert_users():\n+    connection.setup(CASSANDRA_HOSTS, \"hehe1\")\n+    # session = get_session(\"hehe1\")\n+    # connection.get_session(session)\n+    manu = Users.create(name=\"manu\")\n+    connection.unregister_connection('default')\n+\n+\n+def another_insert_users():\n+    connection.setup(CASSANDRA_HOSTS, \"hehe1\")\n+    d = {\"name\": \"manya\"}\n+    manu = Users.create(**d)\n+    connection.unregister_connection('default')"
            }
        ]
    },
    {
        "sha": "388e099866586d2be8a74855ce2d0653b616b9ae",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjM4OGUwOTk4NjY1ODZkMmJlOGE3NDg1NWNlMmQwNjUzYjYxNmI5YWU=",
        "commit": {
            "author": {
                "name": "Vishwanath Kulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-14T02:14:45Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-14T02:14:45Z"
            },
            "message": "Fixing missed commands",
            "tree": {
                "sha": "b6d7d22eb0d24086f4518a24adf1e759eba61f21",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/b6d7d22eb0d24086f4518a24adf1e759eba61f21"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/388e099866586d2be8a74855ce2d0653b616b9ae",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJelRyVCRBK7hj4Ov3rIwAAdHIIAEGwH6F2fRAhDSPjbrb0lrJW\nNq6CdClrrUh2P/hLhqAaj8HDaWLNG4kRKGiAmsTD2qd4TzT2yFIXYzv3JHivMmyr\nnYvl2OJeaAwf8TEBgQeV+tcuXbqXEg3r8eokXTjZvQwZUoHxK9M7yWRG5GJU7jTH\npfwt15yoIHXjm1wAKu7I+CP1eoCkycM4ApQ8pmdWDBn5ncwc2lRSgXWrD5aOiaXq\nD5srVhPgj5EzO56aMtnbT/pAh85M2CTA4dwAgB4s4/GdPhdR1QEwIw+Y5bwdg44v\nTDinA5hBpo6k8pyiCJnPDdMwC9QaNpLTREEC5Sf+9eq8p2XDr3BYDtlSN7Ctxp0=\n=6gua\n-----END PGP SIGNATURE-----\n",
                "payload": "tree b6d7d22eb0d24086f4518a24adf1e759eba61f21\nparent c2ad3aabb42a2ea3b9634895671eeecdd52ade07\nauthor Vishwanath Kulkarni <vishwa.kulkarni@gmail.com> 1586830485 -0600\ncommitter GitHub <noreply@github.com> 1586830485 -0600\n\nFixing missed commands"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/388e099866586d2be8a74855ce2d0653b616b9ae",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/388e099866586d2be8a74855ce2d0653b616b9ae",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/388e099866586d2be8a74855ce2d0653b616b9ae/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "c2ad3aabb42a2ea3b9634895671eeecdd52ade07",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c2ad3aabb42a2ea3b9634895671eeecdd52ade07",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c2ad3aabb42a2ea3b9634895671eeecdd52ade07"
            }
        ],
        "stats": {
            "total": 12,
            "additions": 11,
            "deletions": 1
        },
        "files": [
            {
                "sha": "6ef10ded7034236b6df7456bac258df218226906",
                "filename": "README.md",
                "status": "modified",
                "additions": 11,
                "deletions": 1,
                "changes": 12,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/388e099866586d2be8a74855ce2d0653b616b9ae/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/388e099866586d2be8a74855ce2d0653b616b9ae/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=388e099866586d2be8a74855ce2d0653b616b9ae",
                "patch": "@@ -56,7 +56,17 @@ navigate to\n -  http://127.0.0.1:5000  -- you will be able to see proper message\n - http://127.0.0.1:5000/org/<!orgname_should be added here>  - Just check server logs  \n \n-4) Below some Documentation\n+4) For stopping and starting your docker images\t\n+\n+\t     $ docker ps -a    \t\n+\t     $ docker stop 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\t\n+\t     $ docker stop d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\t\n+\t     $ docker start 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\t\n+\t     $ docker start d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\t\n+\n+\n+\n+5) Below some Documentation\n \n \t\t Work Under progress\n "
            }
        ]
    },
    {
        "sha": "c2ad3aabb42a2ea3b9634895671eeecdd52ade07",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmMyYWQzYWFiYjQyYTJlYTNiOTYzNDg5NTY3MWVlZWNkZDUyYWRlMDc=",
        "commit": {
            "author": {
                "name": "Shreyas Gopalakrishna",
                "email": "11889130+shreyas-gopalakrishna@users.noreply.github.com",
                "date": "2020-04-14T01:55:12Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-14T01:55:12Z"
            },
            "message": "Updated Documentation\n\nUpdated Documentation to include missing commands for elastic search and kibana setup in Docker.",
            "tree": {
                "sha": "21463e7dfd5158a18d4b738398327ae52b1a519c",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/21463e7dfd5158a18d4b738398327ae52b1a519c"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/c2ad3aabb42a2ea3b9634895671eeecdd52ade07",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJelRgACRBK7hj4Ov3rIwAAdHIIADDB0YpVvVXnKH9mO7PYAAoD\n5CwMRZ4yYNRGcaiGVUURU3ApCDnOUblPqxJIohPd8eE5+3N2EowuOcYJNvN+z99W\nJiTtLeIOSQRK+a5YcXg11vZTvH6d0pDtFZh9++FqwLC/tvBInZi441FKGEn7FSjC\nmT7TIBIrsrwki3oyrpFjkNah48o5xnUzLVH4XioQ82cZUBmm743FCkoq0iD50iaZ\nnQgQ+8SXhFDLzOaVpTI7xmAhJ6Z6LMoZbAnmTz/ZmuFIx3epcQqbaooKYA9thx/F\nIOPIDp07sTahC6dXnfp0hsUvQth+NC8cSuINN64vu5mW+N92xCH93G28JTLEItE=\n=eKL/\n-----END PGP SIGNATURE-----\n",
                "payload": "tree 21463e7dfd5158a18d4b738398327ae52b1a519c\nparent 22a8099706695cc5df11c70174284207fe818b7b\nauthor Shreyas Gopalakrishna <11889130+shreyas-gopalakrishna@users.noreply.github.com> 1586829312 -0600\ncommitter GitHub <noreply@github.com> 1586829312 -0600\n\nUpdated Documentation\n\nUpdated Documentation to include missing commands for elastic search and kibana setup in Docker."
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c2ad3aabb42a2ea3b9634895671eeecdd52ade07",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c2ad3aabb42a2ea3b9634895671eeecdd52ade07",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c2ad3aabb42a2ea3b9634895671eeecdd52ade07/comments",
        "author": {
            "login": "shreyas-gopalakrishna",
            "id": 11889130,
            "node_id": "MDQ6VXNlcjExODg5MTMw",
            "avatar_url": "https://avatars2.githubusercontent.com/u/11889130?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shreyas-gopalakrishna",
            "html_url": "https://github.com/shreyas-gopalakrishna",
            "followers_url": "https://api.github.com/users/shreyas-gopalakrishna/followers",
            "following_url": "https://api.github.com/users/shreyas-gopalakrishna/following{/other_user}",
            "gists_url": "https://api.github.com/users/shreyas-gopalakrishna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shreyas-gopalakrishna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shreyas-gopalakrishna/subscriptions",
            "organizations_url": "https://api.github.com/users/shreyas-gopalakrishna/orgs",
            "repos_url": "https://api.github.com/users/shreyas-gopalakrishna/repos",
            "events_url": "https://api.github.com/users/shreyas-gopalakrishna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shreyas-gopalakrishna/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "22a8099706695cc5df11c70174284207fe818b7b",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/22a8099706695cc5df11c70174284207fe818b7b",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/22a8099706695cc5df11c70174284207fe818b7b"
            }
        ],
        "stats": {
            "total": 28,
            "additions": 13,
            "deletions": 15
        },
        "files": [
            {
                "sha": "4af5dbe8fb6852a925a53d18670723ac9dc3528d",
                "filename": "README.md",
                "status": "modified",
                "additions": 13,
                "deletions": 15,
                "changes": 28,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c2ad3aabb42a2ea3b9634895671eeecdd52ade07/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c2ad3aabb42a2ea3b9634895671eeecdd52ade07/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=c2ad3aabb42a2ea3b9634895671eeecdd52ade07",
                "patch": "@@ -22,20 +22,28 @@\n  - Python-Flask\n  - Python3\n  - ElasticSearch\n- - \n+ - pip install flask \n+ - pip install elasticsearch \n \n 2) Run Below Comands after starting Docker\n-\n-\t\tdocker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.5.2\n \t```\n-\tdocker run -d --name kibana --net somenetwork -p 5601:5601 kibana:tag\n+\tdocker pull docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n+\t```\n+\t``` \n+\tdocker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.6.2 \n+\t```\n+\t```\n+\tdocker pull docker.elastic.co/kibana/kibana:7.6.2\n+\t```\n+\t```\n+\tdocker run --link YOUR_ELASTICSEARCH_CONTAINER_NAME_OR_ID:elasticsearch -p 5601:5601 kibana:7.6.2\n \t```\n 3) Set-up ElasticSearch (Important)\n    navigate to [http://localhost:5601/app/kibana#/dev_tools/console?_g=()](http://localhost:5601/app/kibana#/dev_tools/console?_g=())\n    copy and past each and every file from **github-crawler/Templates_commands** into above console (list of files is given below)\n \n          * comments_command.txt\n-         * repos_cammand.txt\n+         * repos_command.txt\n          * put_org_index.txt\n     \n \n@@ -48,20 +56,10 @@ navigate to\n -  http://127.0.0.1:5000  -- you will be able to see proper message\n - http://127.0.0.1:5000/org/<!orgname_should be added here>  - Just check server logs  \n \n-4) For stopping and starting your docker images\n-\n-\t     $ docker ps -a    \n-\t     $ docker stop 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\n-\t     $ docker stop d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\n-\t     $ docker start 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\n-\t     $ docker start d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\n-\n-\n 4) Below some Documentation\n \n \t\t Work Under progress\n \n |**Module**  | **Usage** | **Requirements**|\n |--|--|--|\n |Github-Crawler|Used to get Github organization info |Docker and Python3|\n-"
            }
        ]
    },
    {
        "sha": "22a8099706695cc5df11c70174284207fe818b7b",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjIyYTgwOTk3MDY2OTVjYzVkZjExYzcwMTc0Mjg0MjA3ZmU4MThiN2I=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-14T01:44:46Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-14T01:44:46Z"
            },
            "message": "Merge branch 'master' of https://github.com/CUBigDataClass/Kode-Kallas",
            "tree": {
                "sha": "1abc62ac72ef6d81d4a0688cf8b57806030bc031",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/1abc62ac72ef6d81d4a0688cf8b57806030bc031"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/22a8099706695cc5df11c70174284207fe818b7b",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/22a8099706695cc5df11c70174284207fe818b7b",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/22a8099706695cc5df11c70174284207fe818b7b",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/22a8099706695cc5df11c70174284207fe818b7b/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "c6b7dcdb415e769474b876f633aaeb75872590c0",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c6b7dcdb415e769474b876f633aaeb75872590c0",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c6b7dcdb415e769474b876f633aaeb75872590c0"
            },
            {
                "sha": "5031fcb6889acbbcb9c158459aa9d29b66dbca43",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/5031fcb6889acbbcb9c158459aa9d29b66dbca43",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/5031fcb6889acbbcb9c158459aa9d29b66dbca43"
            }
        ],
        "stats": {
            "total": 19,
            "additions": 19,
            "deletions": 0
        },
        "files": [
            {
                "sha": "b87b1d6ac543a86438f546c783b8facaa4f2c448",
                "filename": "README.md",
                "status": "modified",
                "additions": 19,
                "deletions": 0,
                "changes": 19,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/22a8099706695cc5df11c70174284207fe818b7b/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/22a8099706695cc5df11c70174284207fe818b7b/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=22a8099706695cc5df11c70174284207fe818b7b",
                "patch": "@@ -30,6 +30,15 @@\n \t```\n \tdocker run -d --name kibana --net somenetwork -p 5601:5601 kibana:tag\n \t```\n+3) Set-up ElasticSearch (Important)\n+   navigate to [http://localhost:5601/app/kibana#/dev_tools/console?_g=()](http://localhost:5601/app/kibana#/dev_tools/console?_g=())\n+   copy and past each and every file from **github-crawler/Templates_commands** into above console (list of files is given below)\n+\n+         * comments_command.txt\n+         * repos_cammand.txt\n+         * put_org_index.txt\n+    \n+\n 3) Starting github-crawler and usage\n \n \t\tgit clone https://github.com/CUBigDataClass/Kode-Kallas.git\n@@ -39,10 +48,20 @@ navigate to\n -  http://127.0.0.1:5000  -- you will be able to see proper message\n - http://127.0.0.1:5000/org/<!orgname_should be added here>  - Just check server logs  \n \n+4) For stopping and starting your docker images\n+\n+\t     $ docker ps -a    \n+\t     $ docker stop 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\n+\t     $ docker stop d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\n+\t     $ docker start 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\n+\t     $ docker start d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\n+\n+\n 4) Below some Documentation\n \n \t\t Work Under progress\n \n |**Module**  | **Usage** | **Requirements**|\n |--|--|--|\n |Github-Crawler|Used to get Github organization info |Docker and Python3|\n+"
            }
        ]
    },
    {
        "sha": "c6b7dcdb415e769474b876f633aaeb75872590c0",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmM2YjdkY2RiNDE1ZTc2OTQ3NGI4NzZmNjMzYWFlYjc1ODcyNTkwYzA=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-14T01:44:42Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-14T01:44:42Z"
            },
            "message": "adding put command",
            "tree": {
                "sha": "9bb1105b253cc5fd0700ede351b1a69034756864",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/9bb1105b253cc5fd0700ede351b1a69034756864"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/c6b7dcdb415e769474b876f633aaeb75872590c0",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c6b7dcdb415e769474b876f633aaeb75872590c0",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c6b7dcdb415e769474b876f633aaeb75872590c0",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c6b7dcdb415e769474b876f633aaeb75872590c0/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "c5b3c5970d059e8081a0d84de3ca408be2e8f55d",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c5b3c5970d059e8081a0d84de3ca408be2e8f55d",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c5b3c5970d059e8081a0d84de3ca408be2e8f55d"
            }
        ],
        "stats": {
            "total": 199,
            "additions": 199,
            "deletions": 0
        },
        "files": [
            {
                "sha": "fe2ecafd5420dbf76277ae9c9be4141abb2cda3e",
                "filename": "github-crawler/Templates_commands/org_template.txt",
                "status": "modified",
                "additions": 100,
                "deletions": 0,
                "changes": 100,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c6b7dcdb415e769474b876f633aaeb75872590c0/github-crawler/Templates_commands/org_template.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c6b7dcdb415e769474b876f633aaeb75872590c0/github-crawler/Templates_commands/org_template.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/org_template.txt?ref=c6b7dcdb415e769474b876f633aaeb75872590c0",
                "patch": "@@ -0,0 +1,100 @@\n+PUT _template/org_template\n+{\n+  \"index_patterns\": [\"te*\", \"bar*\"],\n+  \"settings\": {\n+    \"number_of_shards\": 1\n+  },\n+  \"mappings\": {\n+    \"_source\": {\n+      \"enabled\": false\n+    },\n+    \"properties\": {\n+      \"host_name\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"login\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"id\": {\n+      \"type\":\"long\"},\n+    \"node_id\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"url\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"repos_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"events_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"hooks_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"issues_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"members_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"public_members_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"avatar_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"description\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"name\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"company\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"blog\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"location\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"email\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"is_verified\": {\n+        \"type\": \"boolean\"\n+      },\n+    \"has_organization_projects\": {\n+        \"type\": \"boolean\"\n+      },\n+    \"has_repository_projects\": {\n+        \"type\": \"boolean\"\n+      },\n+    \"public_repos\": {\n+        \"type\": \"long\"\n+      },\n+    \"public_gists\": {\n+        \"type\": \"long\"\n+      },\n+    \"followers\": {\n+        \"type\": \"long\"\n+      },\n+    \"following\": {\n+        \"type\": \"long\"\n+      },\n+    \"html_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"created_at\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"updated_at\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"type\": {\n+        \"type\": \"keyword\"\n+      }\n+    }\n+  }\n+}"
            },
            {
                "sha": "c9dd9cbdc21f6a11d76ed8a01d0fd29a66b12485",
                "filename": "github-crawler/Templates_commands/put_org_index.txt",
                "status": "modified",
                "additions": 99,
                "deletions": 0,
                "changes": 99,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c6b7dcdb415e769474b876f633aaeb75872590c0/github-crawler/Templates_commands/put_org_index.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c6b7dcdb415e769474b876f633aaeb75872590c0/github-crawler/Templates_commands/put_org_index.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/put_org_index.txt?ref=c6b7dcdb415e769474b876f633aaeb75872590c0",
                "patch": "@@ -0,0 +1,99 @@\n+PUT /org1\n+{\n+    \"settings\" : {\n+        \"number_of_shards\" : 1\n+    },\n+    \"mappings\": {\n+    \"_source\": {\n+      \"enabled\": false\n+    },\n+    \"properties\": {\n+      \"host_name\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"login\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"id\": {\n+      \"type\":\"long\"},\n+    \"node_id\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"url\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"repos_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"events_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"hooks_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"issues_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"members_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"public_members_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"avatar_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"description\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"name\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"company\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"blog\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"location\":{\n+        \"type\": \"keyword\"\n+      },\n+    \"email\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"is_verified\": {\n+        \"type\": \"boolean\"\n+      },\n+    \"has_organization_projects\": {\n+        \"type\": \"boolean\"\n+      },\n+    \"has_repository_projects\": {\n+        \"type\": \"boolean\"\n+      },\n+    \"public_repos\": {\n+        \"type\": \"long\"\n+      },\n+    \"public_gists\": {\n+        \"type\": \"long\"\n+      },\n+    \"followers\": {\n+        \"type\": \"long\"\n+      },\n+    \"following\": {\n+        \"type\": \"long\"\n+      },\n+    \"html_url\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"created_at\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"updated_at\": {\n+        \"type\": \"keyword\"\n+      },\n+    \"type\": {\n+        \"type\": \"keyword\"\n+      }\n+    }\n+  }\n+}"
            }
        ]
    },
    {
        "sha": "5031fcb6889acbbcb9c158459aa9d29b66dbca43",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjUwMzFmY2I2ODg5YWNiYmNiOWMxNTg0NTlhYTlkMjliNjZkYmNhNDM=",
        "commit": {
            "author": {
                "name": "Vishwanath Kulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-14T01:39:49Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-14T01:39:49Z"
            },
            "message": "Commands for starting and stopping docker images",
            "tree": {
                "sha": "c8c2f3f9eb116bd27430a20edcbfd14425a539aa",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/c8c2f3f9eb116bd27430a20edcbfd14425a539aa"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/5031fcb6889acbbcb9c158459aa9d29b66dbca43",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJelRRlCRBK7hj4Ov3rIwAAdHIIAC9rz6aclmho2VXmSXshzyU3\nm/9LgtKp1FmicAHwkX1oU35QdgfiZZw0kVOBW3ucwfx1/JFjSs/2pNFdi55ak35y\nNFXSctAXS3wNi0yK3aRT4TnEsIb/exfRRBE2Mw32Flfv480uvlhDZqIaEbd2am+/\n4VeQh7xQUtzlJon8+ztu1BWQZgilhSiwTJEhCJhqBFAGaveHO0nkmiqZj90Qv6hN\nr+Sb4gD78itwI+ZreG1P9AgMypeWO49VuxmpWqaUiTnRB4QBXFzTMTbznmB/SuDi\nJO6MwmNk6KW+zQhzKtIkU3heXyKrXZJ4JfpSuTVoTJUsU0nn+5leDHPLPqUS+z8=\n=uJ5n\n-----END PGP SIGNATURE-----\n",
                "payload": "tree c8c2f3f9eb116bd27430a20edcbfd14425a539aa\nparent ce97c57d4715385fcc1ce221a4e491c8bfdd172b\nauthor Vishwanath Kulkarni <vishwa.kulkarni@gmail.com> 1586828389 -0600\ncommitter GitHub <noreply@github.com> 1586828389 -0600\n\nCommands for starting and stopping docker images"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/5031fcb6889acbbcb9c158459aa9d29b66dbca43",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/5031fcb6889acbbcb9c158459aa9d29b66dbca43",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/5031fcb6889acbbcb9c158459aa9d29b66dbca43/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "ce97c57d4715385fcc1ce221a4e491c8bfdd172b",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/ce97c57d4715385fcc1ce221a4e491c8bfdd172b",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/ce97c57d4715385fcc1ce221a4e491c8bfdd172b"
            }
        ],
        "stats": {
            "total": 10,
            "additions": 10,
            "deletions": 0
        },
        "files": [
            {
                "sha": "b87b1d6ac543a86438f546c783b8facaa4f2c448",
                "filename": "README.md",
                "status": "modified",
                "additions": 10,
                "deletions": 0,
                "changes": 10,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/5031fcb6889acbbcb9c158459aa9d29b66dbca43/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/5031fcb6889acbbcb9c158459aa9d29b66dbca43/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=5031fcb6889acbbcb9c158459aa9d29b66dbca43",
                "patch": "@@ -48,10 +48,20 @@ navigate to\n -  http://127.0.0.1:5000  -- you will be able to see proper message\n - http://127.0.0.1:5000/org/<!orgname_should be added here>  - Just check server logs  \n \n+4) For stopping and starting your docker images\n+\n+\t     $ docker ps -a    \n+\t     $ docker stop 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\n+\t     $ docker stop d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\n+\t     $ docker start 0d93ff4520e6 ( copy here your Kibana container ID instead of my Container ID)\n+\t     $ docker start d7edc0290546 ( copy here your Elasticsearch container ID instead of my Container ID)\n+\n+\n 4) Below some Documentation\n \n \t\t Work Under progress\n \n |**Module**  | **Usage** | **Requirements**|\n |--|--|--|\n |Github-Crawler|Used to get Github organization info |Docker and Python3|\n+"
            }
        ]
    },
    {
        "sha": "ce97c57d4715385fcc1ce221a4e491c8bfdd172b",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmNlOTdjNTdkNDcxNTM4NWZjYzFjZTIyMWE0ZTQ5MWM4YmZkZDE3MmI=",
        "commit": {
            "author": {
                "name": "Vishwanath Kulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-12T05:08:36Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-12T05:08:36Z"
            },
            "message": "Adding Documentation for ElasticSearch instance setup",
            "tree": {
                "sha": "773deca7cdf0da38abc22d8fae0b7ef3681ef668",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/773deca7cdf0da38abc22d8fae0b7ef3681ef668"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/ce97c57d4715385fcc1ce221a4e491c8bfdd172b",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJekqJUCRBK7hj4Ov3rIwAAdHIIABZv5kHFD8OZPElq5hDDwU37\nyOUY3Knela1VKoYN+m8rrtUWV1ayQ2BRjWBmB1yiOhlRPKLSmciWYLUBUWvi/OG9\nkZ2NgZqumRufeRim28XDvGS5fnRZI3F0pXnbvUSzwFswm5opCE/VYtLJT3EDLCOa\n4cRFWjFyUym0aM/LIXcimhEUppjUwVVwvakj77PvWDkFVPTUg0uJHHmvlxVWQfl2\ng1Ide1VU09MpmhmADHQxGO448MITXjoOopOmDRFRDKwvwgW4y3Yn5cRsl6m/5cLQ\ndKWR9pTYLjch8zQVUHlvUf8x73rOONhp1eVja51qZemYcnFrY2pSclGpp4h57Yw=\n=oAe4\n-----END PGP SIGNATURE-----\n",
                "payload": "tree 773deca7cdf0da38abc22d8fae0b7ef3681ef668\nparent c5b3c5970d059e8081a0d84de3ca408be2e8f55d\nauthor Vishwanath Kulkarni <vishwa.kulkarni@gmail.com> 1586668116 -0600\ncommitter GitHub <noreply@github.com> 1586668116 -0600\n\nAdding Documentation for ElasticSearch instance setup"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/ce97c57d4715385fcc1ce221a4e491c8bfdd172b",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/ce97c57d4715385fcc1ce221a4e491c8bfdd172b",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/ce97c57d4715385fcc1ce221a4e491c8bfdd172b/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "c5b3c5970d059e8081a0d84de3ca408be2e8f55d",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c5b3c5970d059e8081a0d84de3ca408be2e8f55d",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c5b3c5970d059e8081a0d84de3ca408be2e8f55d"
            }
        ],
        "stats": {
            "total": 9,
            "additions": 9,
            "deletions": 0
        },
        "files": [
            {
                "sha": "731358ac1864ed2815a5eea55d4b06b6c5fd8dfe",
                "filename": "README.md",
                "status": "modified",
                "additions": 9,
                "deletions": 0,
                "changes": 9,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/ce97c57d4715385fcc1ce221a4e491c8bfdd172b/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/ce97c57d4715385fcc1ce221a4e491c8bfdd172b/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=ce97c57d4715385fcc1ce221a4e491c8bfdd172b",
                "patch": "@@ -30,6 +30,15 @@\n \t```\n \tdocker run -d --name kibana --net somenetwork -p 5601:5601 kibana:tag\n \t```\n+3) Set-up ElasticSearch (Important)\n+   navigate to [http://localhost:5601/app/kibana#/dev_tools/console?_g=()](http://localhost:5601/app/kibana#/dev_tools/console?_g=())\n+   copy and past each and every file from **github-crawler/Templates_commands** into above console (list of files is given below)\n+\n+         * comments_command.txt\n+         * repos_cammand.txt\n+         * put_org_index.txt\n+    \n+\n 3) Starting github-crawler and usage\n \n \t\tgit clone https://github.com/CUBigDataClass/Kode-Kallas.git"
            }
        ]
    },
    {
        "sha": "c5b3c5970d059e8081a0d84de3ca408be2e8f55d",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmM1YjNjNTk3MGQwNTllODA4MWEwZDg0ZGUzY2E0MDhiZTJlOGY1NWQ=",
        "commit": {
            "author": {
                "name": "Vishwanath Kulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-12T04:40:06Z"
            },
            "committer": {
                "name": "GitHub",
                "email": "noreply@github.com",
                "date": "2020-04-12T04:40:06Z"
            },
            "message": "Added documentation for local setup",
            "tree": {
                "sha": "8dba895facc3cf1f9d4d46da7ff8c034ec6f6dcb",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/8dba895facc3cf1f9d4d46da7ff8c034ec6f6dcb"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/c5b3c5970d059e8081a0d84de3ca408be2e8f55d",
            "comment_count": 0,
            "verification": {
                "verified": true,
                "reason": "valid",
                "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJekpumCRBK7hj4Ov3rIwAAdHIIABJ1J0g34Dc9/oKMmoRYZ+NU\nYVILbfT0SHm+nFtlgBMfIK9zvxTwszfT1T6nkK6bj/JalLjD5zJiqGdw/oxMtB22\nBXyonYSZzTxk+oNlyc8TWWkWNcEc5oZY64nD3xjxCfjk7MlrM5Ip53tvsH8El4yK\n07PBqwN6KQCRBMuY7AUoG1hirbhDZRCVi/LTxm5MOZPcADijJD54kAyG3hnPlMNO\n1jDxSjEuptlFJon9pDQj40Igck3P9sLYjDKnHky5tF4iUXLTLVxNvtmf7EGm7O7p\nmMTCUFbJrPBjkL2URU8F/oSgJdh2f6i5FzfCH4z3ooyi8TQOH20wyPPrTnK43sQ=\n=Vldj\n-----END PGP SIGNATURE-----\n",
                "payload": "tree 8dba895facc3cf1f9d4d46da7ff8c034ec6f6dcb\nparent 42db70323b8283b4707225ade99dab1a5d2a9d43\nauthor Vishwanath Kulkarni <vishwa.kulkarni@gmail.com> 1586666406 -0600\ncommitter GitHub <noreply@github.com> 1586666406 -0600\n\nAdded documentation for local setup"
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c5b3c5970d059e8081a0d84de3ca408be2e8f55d",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c5b3c5970d059e8081a0d84de3ca408be2e8f55d",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c5b3c5970d059e8081a0d84de3ca408be2e8f55d/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "web-flow",
            "id": 19864447,
            "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/19864447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/web-flow",
            "html_url": "https://github.com/web-flow",
            "followers_url": "https://api.github.com/users/web-flow/followers",
            "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
            "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
            "organizations_url": "https://api.github.com/users/web-flow/orgs",
            "repos_url": "https://api.github.com/users/web-flow/repos",
            "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
            "received_events_url": "https://api.github.com/users/web-flow/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "42db70323b8283b4707225ade99dab1a5d2a9d43",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/42db70323b8283b4707225ade99dab1a5d2a9d43",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/42db70323b8283b4707225ade99dab1a5d2a9d43"
            }
        ],
        "stats": {
            "total": 49,
            "additions": 48,
            "deletions": 1
        },
        "files": [
            {
                "sha": "849c92148d2a46cda33c4ca264381ad1dc601779",
                "filename": "README.md",
                "status": "modified",
                "additions": 48,
                "deletions": 1,
                "changes": 49,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c5b3c5970d059e8081a0d84de3ca408be2e8f55d/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c5b3c5970d059e8081a0d84de3ca408be2e8f55d/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=c5b3c5970d059e8081a0d84de3ca408be2e8f55d",
                "patch": "@@ -1 +1,48 @@\n-#Kode-Kallas\n\\ No newline at end of file\n+# Kode - Kallas\n+\n+\n+  \n+\n+## Modules\n+\n+|Name  | Progress |\n+|--|--|\n+|  Github-Crawler| In-Progress|\n+| Github-Analyser|In-Progress|\n+|Dashboard-backend||\n+|Dashboard-Frontend||\n+\n+\n+    Github-Crawler Setup\n+\n+ 1) Install and keep following software\n+ \n+\n+ - Docker\n+ - Python-Flask\n+ - Python3\n+ - ElasticSearch\n+ - \n+\n+2) Run Below Comands after starting Docker\n+\n+\t\tdocker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.5.2\n+\t```\n+\tdocker run -d --name kibana --net somenetwork -p 5601:5601 kibana:tag\n+\t```\n+3) Starting github-crawler and usage\n+\n+\t\tgit clone https://github.com/CUBigDataClass/Kode-Kallas.git\n+\t\tcd Kode-Kallas/github-crawler\n+\t    python3 app.py\n+navigate to\n+-  http://127.0.0.1:5000  -- you will be able to see proper message\n+- http://127.0.0.1:5000/org/<!orgname_should be added here>  - Just check server logs  \n+\n+4) Below some Documentation\n+\n+\t\t Work Under progress\n+\n+|**Module**  | **Usage** | **Requirements**|\n+|--|--|--|\n+|Github-Crawler|Used to get Github organization info |Docker and Python3|"
            }
        ]
    },
    {
        "sha": "42db70323b8283b4707225ade99dab1a5d2a9d43",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjQyZGI3MDMyM2I4MjgzYjQ3MDcyMjVhZGU5OWRhYjFhNWQyYTlkNDM=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-12T04:12:45Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-12T04:12:45Z"
            },
            "message": "Creating Flask server to automate everythig along with helper file",
            "tree": {
                "sha": "ae43686ddda07b8535a1ad8c962f429d33957601",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/ae43686ddda07b8535a1ad8c962f429d33957601"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/42db70323b8283b4707225ade99dab1a5d2a9d43",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/42db70323b8283b4707225ade99dab1a5d2a9d43",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/42db70323b8283b4707225ade99dab1a5d2a9d43",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/42db70323b8283b4707225ade99dab1a5d2a9d43/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "727342076a16bdb01975faa088e9a99d65f688ea",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/727342076a16bdb01975faa088e9a99d65f688ea",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/727342076a16bdb01975faa088e9a99d65f688ea"
            }
        ],
        "stats": {
            "total": 107,
            "additions": 103,
            "deletions": 4
        },
        "files": [
            {
                "sha": "2657245937b5d963c0d83cb090fe4a196957568b",
                "filename": "github-crawler/app.py",
                "status": "modified",
                "additions": 36,
                "deletions": 0,
                "changes": 36,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/42db70323b8283b4707225ade99dab1a5d2a9d43/github-crawler/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/42db70323b8283b4707225ade99dab1a5d2a9d43/github-crawler/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/app.py?ref=42db70323b8283b4707225ade99dab1a5d2a9d43",
                "patch": "@@ -0,0 +1,36 @@\n+from flask import Flask\n+app = Flask(__name__)\n+import lib.helper as helpp\n+\n+#constants below\n+OWNER = 'vishwakulkarni'\n+github_api = \"https://api.github.com\"\n+\n+helper = helpp.Helper()\n+\n+@app.route('/')\n+def home_page():\n+    return 'Hello guys nothing here! just submit your github organization name at \"/org/<orgname>\"!'\n+\n+\n+@app.route('/org/<orgname>')\n+def org_parser(orgname):\n+    print(\"setting UP!!!\")\n+    helper.set_org_name(orgname)\n+    org_data = helper.get_org_information(OWNER,github_api)\n+    print(\"Got Org Info!!!\")\n+    print(\"sending Org Info to elastic search!!!\")\n+    helper.send_to_elasticInstance(org_data,'org1',org_data['id'])\n+    print(\"Getting Repos for \"+orgname)\n+    repo_list = helper.get_repositories(OWNER,github_api)\n+    print(\"sending repo info to elasticsearch\")\n+    for repo in repo_list:\n+        repo['license']=\"test\"\n+        helper.send_to_elasticInstance(repo,'repos',repo['id'])\n+        print(\"repo sent \"+ repo['name'] )\n+    print(\"Done!!!!!!!!!\")\n+    return 'We got your org name ' + orgname + ' give us some time to process your request, please check server output for progress'\n+\n+    \n+if __name__ == \"__main__\":\n+    app.run(debug=True)\n\\ No newline at end of file"
            },
            {
                "sha": "d1584744ae1c442a154f9ac50d6e84505db839c3",
                "filename": "github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/42db70323b8283b4707225ade99dab1a5d2a9d43/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/42db70323b8283b4707225ade99dab1a5d2a9d43/github-crawler/lib/__pycache__/helper.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/helper.cpython-37.pyc?ref=42db70323b8283b4707225ade99dab1a5d2a9d43"
            },
            {
                "sha": "8b10532a32533b336ad50468b3a98fb4bf429662",
                "filename": "github-crawler/lib/helper.py",
                "status": "added",
                "additions": 50,
                "deletions": 0,
                "changes": 50,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/42db70323b8283b4707225ade99dab1a5d2a9d43/github-crawler/lib/helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/42db70323b8283b4707225ade99dab1a5d2a9d43/github-crawler/lib/helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/helper.py?ref=42db70323b8283b4707225ade99dab1a5d2a9d43",
                "patch": "@@ -0,0 +1,50 @@\n+import json\n+import requests\n+from pandas.io.json import json_normalize\n+from sqlalchemy import create_engine, engine, text, types, MetaData, Table, String\n+from datetime import datetime\n+\n+\n+import config\n+import os\n+\n+#elasticsearch\n+from elasticsearch import Elasticsearch\n+\n+GITEA_APP_URL = 'YOUR_GITEA_API'\n+GITEA_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+GITHUB_USERNAME = 'vishwakulkarni'\n+GITHUB_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+SQL_ALCHEMY_STRING = ''\n+\n+\n+\n+class Helper():\n+    def __init__(self):\n+        self.orgname = \"\"\n+        #set config working\n+        self.res = requests.get('http://localhost:9200')\n+        self.es = Elasticsearch([{'host': 'localhost', 'port': '9200'}])\n+        #below setup for github\n+        self.github_api = \"https://api.github.com\"\n+        self.gh_session = requests.Session()\n+        self.gh_session.auth = (GITHUB_USERNAME, GITHUB_TOKEN)\n+    \n+    def set_org_name(self,orgname):\n+        self.orgname = orgname\n+        return\n+\n+    def get_org_information(self,owner,api):\n+        url = api + '/orgs/{}'.format(self.orgname)\n+        org_data = self.gh_session.get(url = url)\n+        org_data=json.loads(org_data.content)\n+        return org_data\n+\n+    def send_to_elasticInstance(self,data,index_name,id_val):\n+        self.es.index(index=index_name, doc_type='_doc',id=id_val, body=data)\n+    \n+    def get_repositories(self,owner,api):\n+        url = api + '/orgs/{}/repos'.format(self.orgname)\n+        org_repos_data = self.gh_session.get(url = url)\n+        org_repos_data=json.loads(org_repos_data.content)\n+        return org_repos_data"
            },
            {
                "sha": "e4a8c006924d86289e5b415c96e5663b853c3448",
                "filename": "github-crawler/lib/test.py",
                "status": "modified",
                "additions": 17,
                "deletions": 4,
                "changes": 21,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/42db70323b8283b4707225ade99dab1a5d2a9d43/github-crawler/lib/test.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/42db70323b8283b4707225ade99dab1a5d2a9d43/github-crawler/lib/test.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/test.py?ref=42db70323b8283b4707225ade99dab1a5d2a9d43",
                "patch": "@@ -46,6 +46,10 @@ def objects_to_strings(table):\n gh_session = requests.Session()\n gh_session.auth = (config.GITHUB_USERNAME, config.GITHUB_TOKEN)\n \n+def setUp():\n+    github_api = \"https://api.github.com\"\n+    gh_session = requests.Session()\n+    gh_session.auth = (config.GITHUB_USERNAME, config.GITHUB_TOKEN)\n \n def branches_of_repo(repo, owner, api):\n     branches = []\n@@ -159,13 +163,22 @@ def commits_of_repo_github(repo, owner, api):\n         i = i + 1\n     return commits\n \n-commits = commits_of_repo_github('hard-decisions','CUBigDataClass',github_api)\n+#commits = commits_of_repo_github('hard-decisions','CUBigDataClass',github_api)\n \n def add_commits_to_elasticsearch(commits):\n     for commit in commits:\n         es.index(index='commits', doc_type='_doc',id=commit['sha'], body=commit)\n \n-add_commits_to_elasticsearch(commits)\n+#add_commits_to_elasticsearch(commits)\n \n-with open(\"data/commits.json\", \"w\") as outfile: \n-    outfile.write(json.dumps(commits,indent=4)) \n+#with open(\"data/commits.json\", \"w\") as outfile: \n+#   outfile.write(json.dumps(commits,indent=4)) \n+#\n+\n+org_repos = get_repositories('CUBigDataClass', 'vishwakulkarni',github_api)\n+\n+for repo in org_repos:\n+    repo['license']=\"test\"\n+    #send_to_elasticInstance(repo,'repos',repo['id'])\n+    commits = commits_of_repo_github(repo['name'],'CUBigDataClass',github_api)\n+    add_commits_to_elasticsearch(commits)\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "727342076a16bdb01975faa088e9a99d65f688ea",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjcyNzM0MjA3NmExNmJkYjAxOTc1ZmFhMDg4ZTlhOTlkNjVmNjg4ZWE=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-11T23:31:47Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-11T23:31:47Z"
            },
            "message": " adding comments to elastic search",
            "tree": {
                "sha": "ea78af64aceb587cfcb1e9cce44fcac827b257c8",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/ea78af64aceb587cfcb1e9cce44fcac827b257c8"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/727342076a16bdb01975faa088e9a99d65f688ea",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/727342076a16bdb01975faa088e9a99d65f688ea",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/727342076a16bdb01975faa088e9a99d65f688ea",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/727342076a16bdb01975faa088e9a99d65f688ea/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "37bded434253420e2973bdb8ef43ea4d3078946e",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/37bded434253420e2973bdb8ef43ea4d3078946e",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/37bded434253420e2973bdb8ef43ea4d3078946e"
            }
        ],
        "stats": {
            "total": 8157,
            "additions": 8152,
            "deletions": 5
        },
        "files": [
            {
                "sha": "5c0f5b89ca57185596d8f77cd4b7932a6fbd9e2c",
                "filename": "github-crawler/Templates_commands/comments_command.txt",
                "status": "added",
                "additions": 307,
                "deletions": 0,
                "changes": 307,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/727342076a16bdb01975faa088e9a99d65f688ea/github-crawler/Templates_commands/comments_command.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/727342076a16bdb01975faa088e9a99d65f688ea/github-crawler/Templates_commands/comments_command.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/comments_command.txt?ref=727342076a16bdb01975faa088e9a99d65f688ea",
                "patch": "@@ -0,0 +1,307 @@\n+\n+PUT comments\n+{\n+\t\"mappings\": {\n+\t\t\"properties\": {\n+\t\t\t\"sha\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"node_id\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"commit\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"author\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"name\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"email\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"date\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\t\"committer\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"name\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"email\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"date\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\t\"message\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"tree\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"sha\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"comment_count\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"verification\": {\n+\t\t\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\t\t\"properties\": {\n+\t\t\t\t\t\t\t\"verified\": {\n+\t\t\t\t\t\t\t\t\"type\": \"boolean\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"reason\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"signature\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\"payload\": {\n+\t\t\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\t\"url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"html_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"comments_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"author\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"login\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"id\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"node_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"avatar_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gravatar_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"html_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"followers_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"following_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gists_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"starred_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"subscriptions_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"organizations_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"repos_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"received_events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"type\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"site_admin\": {\n+\t\t\t\t\t\t\"type\": \"boolean\"\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\t\"committer\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"login\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"id\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"node_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"avatar_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gravatar_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"html_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"followers_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"following_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gists_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"starred_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"subscriptions_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"organizations_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"repos_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"received_events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"type\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"site_admin\": {\n+\t\t\t\t\t\t\"type\": \"boolean\"\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\t\"parents\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"sha\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"html_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+\n+POST comments/_doc/1\n+{\n+        \"sha\": \"eaf17d8c38b413aa788b4851f965236fc7280506\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6ZWFmMTdkOGMzOGI0MTNhYTc4OGI0ODUxZjk2NTIzNmZjNzI4MDUwNg==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-05-04T17:19:11Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-05-04T17:19:11Z\"\n+            },\n+            \"message\": \"update for abtest variable instead of color\",\n+            \"tree\": {\n+                \"sha\": \"3a7dcdc23fc936bc98e26a57e812b7ae57793d3c\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/3a7dcdc23fc936bc98e26a57e812b7ae57793d3c\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/eaf17d8c38b413aa788b4851f965236fc7280506\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/eaf17d8c38b413aa788b4851f965236fc7280506\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/eaf17d8c38b413aa788b4851f965236fc7280506\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/eaf17d8c38b413aa788b4851f965236fc7280506/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"2c7b86c6112b7f832e42ec4768fe175dd90af62c\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/2c7b86c6112b7f832e42ec4768fe175dd90af62c\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/2c7b86c6112b7f832e42ec4768fe175dd90af62c\"\n+            }\n+        ]\n+    }\n\\ No newline at end of file"
            },
            {
                "sha": "4b67ce40c24d75bb5247f34dde20e0adc764ecb7",
                "filename": "github-crawler/Templates_commands/repos_command.txt",
                "status": "added",
                "additions": 398,
                "deletions": 0,
                "changes": 398,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/727342076a16bdb01975faa088e9a99d65f688ea/github-crawler/Templates_commands/repos_command.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/727342076a16bdb01975faa088e9a99d65f688ea/github-crawler/Templates_commands/repos_command.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/repos_command.txt?ref=727342076a16bdb01975faa088e9a99d65f688ea",
                "patch": "@@ -0,0 +1,398 @@\n+PUT repos\n+{\n+  \"mappings\": {\n+    \"properties\":{\n+\t\t\t\"id\": {\n+\t\t\t\t\"type\": \"long\"\n+\t\t\t},\n+\t\t\t\"node_id\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"name\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"full_name\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"private\": {\n+\t\t\t\t\"type\": \"boolean\"\n+\t\t\t},\n+\t\t\t\"owner\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"login\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"id\": {\n+\t\t\t\t\t\t\"type\": \"long\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"node_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"avatar_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gravatar_id\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"html_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"followers_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"following_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"gists_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"starred_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"subscriptions_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"organizations_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"repos_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"received_events_url\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"type\": {\n+\t\t\t\t\t\t\"type\": \"keyword\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"site_admin\": {\n+\t\t\t\t\t\t\"type\": \"boolean\"\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\t\"html_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"description\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"fork\": {\n+\t\t\t\t\"type\": \"boolean\"\n+\t\t\t},\n+\t\t\t\"url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"forks_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"keys_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"collaborators_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"teams_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"hooks_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"issue_events_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"events_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"assignees_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"branches_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"tags_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"blobs_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"git_tags_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"git_refs_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"trees_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"statuses_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"languages_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"stargazers_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"contributors_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"subscribers_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"subscription_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"commits_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"git_commits_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"comments_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"issue_comment_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"contents_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"compare_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"merges_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"archive_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"downloads_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"issues_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"pulls_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"milestones_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"notifications_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"labels_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"releases_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"deployments_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"created_at\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"updated_at\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"pushed_at\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"git_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"ssh_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"clone_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"svn_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"homepage\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"size\": {\n+\t\t\t\t\"type\": \"long\"\n+\t\t\t},\n+\t\t\t\"stargazers_count\": {\n+\t\t\t\t\"type\": \"long\"\n+\t\t\t},\n+\t\t\t\"watchers_count\": {\n+\t\t\t\t\"type\": \"long\"\n+\t\t\t},\n+\t\t\t\"language\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"has_issues\": {\n+\t\t\t\t\"type\": \"boolean\"\n+\t\t\t},\n+\t\t\t\"has_projects\": {\n+\t\t\t\t\"type\": \"boolean\"\n+\t\t\t},\n+\t\t\t\"has_downloads\": {\n+\t\t\t\t\"type\": \"boolean\"\n+\t\t\t},\n+\t\t\t\"has_wiki\": {\n+\t\t\t\t\"type\": \"boolean\"\n+\t\t\t},\n+\t\t\t\"has_pages\": {\n+\t\t\t\t\"type\": \"boolean\"\n+\t\t\t},\n+\t\t\t\"forks_count\": {\n+\t\t\t\t\"type\": \"long\"\n+\t\t\t},\n+\t\t\t\"mirror_url\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"archived\": {\n+\t\t\t\t\"type\": \"boolean\"\n+\t\t\t},\n+\t\t\t\"disabled\": {\n+\t\t\t\t\"type\": \"boolean\"\n+\t\t\t},\n+\t\t\t\"open_issues_count\": {\n+\t\t\t\t\"type\": \"long\"\n+\t\t\t},\n+\t\t\t\"license\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"forks\": {\n+\t\t\t\t\"type\": \"long\"\n+\t\t\t},\n+\t\t\t\"open_issues\": {\n+\t\t\t\t\"type\": \"long\"\n+\t\t\t},\n+\t\t\t\"watchers\": {\n+\t\t\t\t\"type\": \"long\"\n+\t\t\t},\n+\t\t\t\"default_branch\": {\n+\t\t\t\t\"type\": \"keyword\"\n+\t\t\t},\n+\t\t\t\"permissions\": {\n+\t\t\t\t\"type\": \"object\",\n+\t\t\t\t\"properties\": {\n+\t\t\t\t\t\"admin\": {\n+\t\t\t\t\t\t\"type\": \"boolean\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"push\": {\n+\t\t\t\t\t\t\"type\": \"boolean\"\n+\t\t\t\t\t},\n+\t\t\t\t\t\"pull\": {\n+\t\t\t\t\t\t\"type\": \"boolean\"\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+  }\n+}\n+\n+\n+POST repos/_doc\n+ {\n+      \"id\": 16750294,\n+      \"node_id\": \"MDEwOlJlcG9zaXRvcnkxNjc1MDI5NA==\",\n+      \"name\": \"Twitter-EmotiMap\",\n+      \"full_name\": \"CUBigDataClass/Twitter-EmotiMap\",\n+      \"private\": false,\n+      \"owner\": {\n+        \"login\": \"CUBigDataClass\",\n+        \"id\": 6345918,\n+        \"node_id\": \"MDEyOk9yZ2FuaXphdGlvbjYzNDU5MTg=\",\n+        \"avatar_url\": \"https://avatars0.githubusercontent.com/u/6345918?v=4\",\n+        \"gravatar_id\": \"\",\n+        \"url\": \"https://api.github.com/users/CUBigDataClass\",\n+        \"html_url\": \"https://github.com/CUBigDataClass\",\n+        \"followers_url\": \"https://api.github.com/users/CUBigDataClass/followers\",\n+        \"following_url\": \"https://api.github.com/users/CUBigDataClass/following{/other_user}\",\n+        \"gists_url\": \"https://api.github.com/users/CUBigDataClass/gists{/gist_id}\",\n+        \"starred_url\": \"https://api.github.com/users/CUBigDataClass/starred{/owner}{/repo}\",\n+        \"subscriptions_url\": \"https://api.github.com/users/CUBigDataClass/subscriptions\",\n+        \"organizations_url\": \"https://api.github.com/users/CUBigDataClass/orgs\",\n+        \"repos_url\": \"https://api.github.com/users/CUBigDataClass/repos\",\n+        \"events_url\": \"https://api.github.com/users/CUBigDataClass/events{/privacy}\",\n+        \"received_events_url\": \"https://api.github.com/users/CUBigDataClass/received_events\",\n+        \"type\": \"Organization\",\n+        \"site_admin\": false\n+      },\n+      \"html_url\": \"https://github.com/CUBigDataClass/Twitter-EmotiMap\",\n+      \"description\": \"Twitter feed mining with geographical emotional trend analysis.\",\n+      \"fork\": false,\n+      \"url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap\",\n+      \"forks_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/forks\",\n+      \"keys_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/keys{/key_id}\",\n+      \"collaborators_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/collaborators{/collaborator}\",\n+      \"teams_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/teams\",\n+      \"hooks_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/hooks\",\n+      \"issue_events_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/issues/events{/number}\",\n+      \"events_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/events\",\n+      \"assignees_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/assignees{/user}\",\n+      \"branches_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/branches{/branch}\",\n+      \"tags_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/tags\",\n+      \"blobs_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/git/blobs{/sha}\",\n+      \"git_tags_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/git/tags{/sha}\",\n+      \"git_refs_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/git/refs{/sha}\",\n+      \"trees_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/git/trees{/sha}\",\n+      \"statuses_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/statuses/{sha}\",\n+      \"languages_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/languages\",\n+      \"stargazers_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/stargazers\",\n+      \"contributors_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/contributors\",\n+      \"subscribers_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/subscribers\",\n+      \"subscription_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/subscription\",\n+      \"commits_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/commits{/sha}\",\n+      \"git_commits_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/git/commits{/sha}\",\n+      \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/comments{/number}\",\n+      \"issue_comment_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/issues/comments{/number}\",\n+      \"contents_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/contents/{+path}\",\n+      \"compare_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/compare/{base}...{head}\",\n+      \"merges_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/merges\",\n+      \"archive_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/{archive_format}{/ref}\",\n+      \"downloads_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/downloads\",\n+      \"issues_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/issues{/number}\",\n+      \"pulls_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/pulls{/number}\",\n+      \"milestones_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/milestones{/number}\",\n+      \"notifications_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/notifications{?since,all,participating}\",\n+      \"labels_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/labels{/name}\",\n+      \"releases_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/releases{/id}\",\n+      \"deployments_url\": \"https://api.github.com/repos/CUBigDataClass/Twitter-EmotiMap/deployments\",\n+      \"created_at\": \"2014-02-12T00:05:56Z\",\n+      \"updated_at\": \"2016-04-04T16:36:01Z\",\n+      \"pushed_at\": \"2014-05-16T14:32:48Z\",\n+      \"git_url\": \"git://github.com/CUBigDataClass/Twitter-EmotiMap.git\",\n+      \"ssh_url\": \"git@github.com:CUBigDataClass/Twitter-EmotiMap.git\",\n+      \"clone_url\": \"https://github.com/CUBigDataClass/Twitter-EmotiMap.git\",\n+      \"svn_url\": \"https://github.com/CUBigDataClass/Twitter-EmotiMap\",\n+      \"homepage\": \"http://www.mattkgross.com/TwitterMap\",\n+      \"size\": 99797,\n+      \"stargazers_count\": 0,\n+      \"watchers_count\": 0,\n+      \"language\": \"Python\",\n+      \"has_issues\": true,\n+      \"has_projects\": true,\n+      \"has_downloads\": true,\n+      \"has_wiki\": true,\n+      \"has_pages\": false,\n+      \"forks_count\": 0,\n+      \"mirror_url\": null,\n+      \"archived\": false,\n+      \"disabled\": false,\n+      \"open_issues_count\": 0,\n+      \"license\": null,\n+      \"forks\": 0,\n+      \"open_issues\": 0,\n+      \"watchers\": 0,\n+      \"default_branch\": \"master\",\n+      \"permissions\": {\n+        \"admin\": false,\n+        \"push\": false,\n+        \"pull\": true\n+      }\n+    }\n\\ No newline at end of file"
            },
            {
                "sha": "1e0b31e22838240b163180a887b79aa45cff14ab",
                "filename": "github-crawler/lib/data/commits.json",
                "status": "added",
                "additions": 1385,
                "deletions": 0,
                "changes": 1385,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/727342076a16bdb01975faa088e9a99d65f688ea/github-crawler/lib/data/commits.json",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/727342076a16bdb01975faa088e9a99d65f688ea/github-crawler/lib/data/commits.json",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/data/commits.json?ref=727342076a16bdb01975faa088e9a99d65f688ea",
                "patch": "@@ -0,0 +1,1385 @@\n+[\n+    {\n+        \"sha\": \"eaf17d8c38b413aa788b4851f965236fc7280506\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6ZWFmMTdkOGMzOGI0MTNhYTc4OGI0ODUxZjk2NTIzNmZjNzI4MDUwNg==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-05-04T17:19:11Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-05-04T17:19:11Z\"\n+            },\n+            \"message\": \"update for abtest variable instead of color\",\n+            \"tree\": {\n+                \"sha\": \"3a7dcdc23fc936bc98e26a57e812b7ae57793d3c\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/3a7dcdc23fc936bc98e26a57e812b7ae57793d3c\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/eaf17d8c38b413aa788b4851f965236fc7280506\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/eaf17d8c38b413aa788b4851f965236fc7280506\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/eaf17d8c38b413aa788b4851f965236fc7280506\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/eaf17d8c38b413aa788b4851f965236fc7280506/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"2c7b86c6112b7f832e42ec4768fe175dd90af62c\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/2c7b86c6112b7f832e42ec4768fe175dd90af62c\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/2c7b86c6112b7f832e42ec4768fe175dd90af62c\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"2c7b86c6112b7f832e42ec4768fe175dd90af62c\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6MmM3Yjg2YzYxMTJiN2Y4MzJlNDJlYzQ3NjhmZTE3NWRkOTBhZjYyYw==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-05-04T16:58:28Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-05-04T16:58:28Z\"\n+            },\n+            \"message\": \"add d3 nod\",\n+            \"tree\": {\n+                \"sha\": \"17a6ee88d2d3a280bd303edf0151593535d826a4\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/17a6ee88d2d3a280bd303edf0151593535d826a4\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/2c7b86c6112b7f832e42ec4768fe175dd90af62c\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/2c7b86c6112b7f832e42ec4768fe175dd90af62c\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/2c7b86c6112b7f832e42ec4768fe175dd90af62c\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/2c7b86c6112b7f832e42ec4768fe175dd90af62c/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"64bc181e4026561a2a12651fce3afc58959545d6\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/64bc181e4026561a2a12651fce3afc58959545d6\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/64bc181e4026561a2a12651fce3afc58959545d6\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"64bc181e4026561a2a12651fce3afc58959545d6\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6NjRiYzE4MWU0MDI2NTYxYTJhMTI2NTFmY2UzYWZjNTg5NTk1NDVkNg==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-05-04T16:56:25Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-05-04T16:56:25Z\"\n+            },\n+            \"message\": \"add in color data to bar chart\",\n+            \"tree\": {\n+                \"sha\": \"6b4639d9fdbf34eba6c38625d00b2f79ed473ec5\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/6b4639d9fdbf34eba6c38625d00b2f79ed473ec5\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/64bc181e4026561a2a12651fce3afc58959545d6\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/64bc181e4026561a2a12651fce3afc58959545d6\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/64bc181e4026561a2a12651fce3afc58959545d6\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/64bc181e4026561a2a12651fce3afc58959545d6/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"a1ed89f132e3dcc35cc4b9aa55470fcef8bb708f\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/a1ed89f132e3dcc35cc4b9aa55470fcef8bb708f\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/a1ed89f132e3dcc35cc4b9aa55470fcef8bb708f\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"a1ed89f132e3dcc35cc4b9aa55470fcef8bb708f\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6YTFlZDg5ZjEzMmUzZGNjMzVjYzRiOWFhNTU0NzBmY2VmOGJiNzA4Zg==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-05-04T16:51:12Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-05-04T16:51:12Z\"\n+            },\n+            \"message\": \"clean up and comment\",\n+            \"tree\": {\n+                \"sha\": \"f94f89d7445cb123496e1ffb9ad0621cf46d7847\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/f94f89d7445cb123496e1ffb9ad0621cf46d7847\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/a1ed89f132e3dcc35cc4b9aa55470fcef8bb708f\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/a1ed89f132e3dcc35cc4b9aa55470fcef8bb708f\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/a1ed89f132e3dcc35cc4b9aa55470fcef8bb708f\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/a1ed89f132e3dcc35cc4b9aa55470fcef8bb708f/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"3158ab9667867914cfb84618dd8060464fde0b15\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/3158ab9667867914cfb84618dd8060464fde0b15\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/3158ab9667867914cfb84618dd8060464fde0b15\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"3158ab9667867914cfb84618dd8060464fde0b15\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6MzE1OGFiOTY2Nzg2NzkxNGNmYjg0NjE4ZGQ4MDYwNDY0ZmRlMGIxNQ==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-05-04T16:44:27Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-05-04T16:44:27Z\"\n+            },\n+            \"message\": \"responsive with hover to display percent\",\n+            \"tree\": {\n+                \"sha\": \"48321b342d44ca5a029ae7e991e6d31704ef5b08\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/48321b342d44ca5a029ae7e991e6d31704ef5b08\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/3158ab9667867914cfb84618dd8060464fde0b15\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/3158ab9667867914cfb84618dd8060464fde0b15\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/3158ab9667867914cfb84618dd8060464fde0b15\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/3158ab9667867914cfb84618dd8060464fde0b15/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"0d6b90a5fe61bec036c742ab0d413285247733a5\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/0d6b90a5fe61bec036c742ab0d413285247733a5\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/0d6b90a5fe61bec036c742ab0d413285247733a5\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"0d6b90a5fe61bec036c742ab0d413285247733a5\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6MGQ2YjkwYTVmZTYxYmVjMDM2Yzc0MmFiMGQ0MTMyODUyNDc3MzNhNQ==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-04-24T03:00:53Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-04-24T03:00:53Z\"\n+            },\n+            \"message\": \"Merge branch 'master' of https://github.com/CUBigDataClass/hard-decisions\\n\\nmerge to update file structure\",\n+            \"tree\": {\n+                \"sha\": \"d178869d332674ef51b802bc2adc82e029968309\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/d178869d332674ef51b802bc2adc82e029968309\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/0d6b90a5fe61bec036c742ab0d413285247733a5\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/0d6b90a5fe61bec036c742ab0d413285247733a5\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/0d6b90a5fe61bec036c742ab0d413285247733a5\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/0d6b90a5fe61bec036c742ab0d413285247733a5/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"fd07042f2a153c300752f3aa510bf2a56818678a\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/fd07042f2a153c300752f3aa510bf2a56818678a\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/fd07042f2a153c300752f3aa510bf2a56818678a\"\n+            },\n+            {\n+                \"sha\": \"3b054d0a716dffa2eec43ebcc35da4b9eaf7e794\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/3b054d0a716dffa2eec43ebcc35da4b9eaf7e794\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/3b054d0a716dffa2eec43ebcc35da4b9eaf7e794\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"fd07042f2a153c300752f3aa510bf2a56818678a\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6ZmQwNzA0MmYyYTE1M2MzMDA3NTJmM2FhNTEwYmYyYTU2ODE4Njc4YQ==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-04-24T02:59:59Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-04-24T02:59:59Z\"\n+            },\n+            \"message\": \"remove old files\",\n+            \"tree\": {\n+                \"sha\": \"d178869d332674ef51b802bc2adc82e029968309\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/d178869d332674ef51b802bc2adc82e029968309\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/fd07042f2a153c300752f3aa510bf2a56818678a\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/fd07042f2a153c300752f3aa510bf2a56818678a\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/fd07042f2a153c300752f3aa510bf2a56818678a\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/fd07042f2a153c300752f3aa510bf2a56818678a/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"0d68b3a4ca45f5e11e7f4f6c7d263c4fa60409ec\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/0d68b3a4ca45f5e11e7f4f6c7d263c4fa60409ec\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/0d68b3a4ca45f5e11e7f4f6c7d263c4fa60409ec\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"0d68b3a4ca45f5e11e7f4f6c7d263c4fa60409ec\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6MGQ2OGIzYTRjYTQ1ZjVlMTFlN2Y0ZjZjN2QyNjNjNGZhNjA0MDllYw==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-04-24T02:58:19Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-04-24T02:58:19Z\"\n+            },\n+            \"message\": \"update with hover percent instead of tooltip\",\n+            \"tree\": {\n+                \"sha\": \"0d528e6d477e1560cacf528293769582910b5024\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/0d528e6d477e1560cacf528293769582910b5024\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/0d68b3a4ca45f5e11e7f4f6c7d263c4fa60409ec\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/0d68b3a4ca45f5e11e7f4f6c7d263c4fa60409ec\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/0d68b3a4ca45f5e11e7f4f6c7d263c4fa60409ec\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/0d68b3a4ca45f5e11e7f4f6c7d263c4fa60409ec/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"689373d6beb12a591d79bd17ebbf280d0de002c7\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/689373d6beb12a591d79bd17ebbf280d0de002c7\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/689373d6beb12a591d79bd17ebbf280d0de002c7\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"689373d6beb12a591d79bd17ebbf280d0de002c7\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6Njg5MzczZDZiZWIxMmE1OTFkNzliZDE3ZWJiZjI4MGQwZGUwMDJjNw==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-04-22T21:21:58Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-04-22T21:21:58Z\"\n+            },\n+            \"message\": \"updated d3 with tooltips and background color\",\n+            \"tree\": {\n+                \"sha\": \"c398885510dc9a85caf898e31c759721904bbec2\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/c398885510dc9a85caf898e31c759721904bbec2\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/689373d6beb12a591d79bd17ebbf280d0de002c7\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/689373d6beb12a591d79bd17ebbf280d0de002c7\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/689373d6beb12a591d79bd17ebbf280d0de002c7\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/689373d6beb12a591d79bd17ebbf280d0de002c7/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"937c07ea52242ab2f22e4d78c5a76f5a27b3c9e4\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/937c07ea52242ab2f22e4d78c5a76f5a27b3c9e4\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/937c07ea52242ab2f22e4d78c5a76f5a27b3c9e4\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"3b054d0a716dffa2eec43ebcc35da4b9eaf7e794\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6M2IwNTRkMGE3MTZkZmZhMmVlYzQzZWJjYzM1ZGE0YjllYWY3ZTc5NA==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"robertdenton\",\n+                \"email\": \"robertdenton@users.noreply.github.com\",\n+                \"date\": \"2014-04-08T23:39:08Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"robertdenton\",\n+                \"email\": \"robertdenton@users.noreply.github.com\",\n+                \"date\": \"2014-04-08T23:39:08Z\"\n+            },\n+            \"message\": \"Delete index.html\",\n+            \"tree\": {\n+                \"sha\": \"241d2e58d03b3bb081f5c4fc2bcafcf3798057e0\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/241d2e58d03b3bb081f5c4fc2bcafcf3798057e0\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/3b054d0a716dffa2eec43ebcc35da4b9eaf7e794\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/3b054d0a716dffa2eec43ebcc35da4b9eaf7e794\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/3b054d0a716dffa2eec43ebcc35da4b9eaf7e794\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/3b054d0a716dffa2eec43ebcc35da4b9eaf7e794/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"dce5fcf3e1665e797344104213cc4b415d177c39\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/dce5fcf3e1665e797344104213cc4b415d177c39\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/dce5fcf3e1665e797344104213cc4b415d177c39\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"dce5fcf3e1665e797344104213cc4b415d177c39\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6ZGNlNWZjZjNlMTY2NWU3OTczNDQxMDQyMTNjYzRiNDE1ZDE3N2MzOQ==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"robertdenton\",\n+                \"email\": \"robertdenton@users.noreply.github.com\",\n+                \"date\": \"2014-04-08T23:39:01Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"robertdenton\",\n+                \"email\": \"robertdenton@users.noreply.github.com\",\n+                \"date\": \"2014-04-08T23:39:01Z\"\n+            },\n+            \"message\": \"Delete data.json\",\n+            \"tree\": {\n+                \"sha\": \"5746f725a581c17cb837e1be44c5bce5b9fd7def\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/5746f725a581c17cb837e1be44c5bce5b9fd7def\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/dce5fcf3e1665e797344104213cc4b415d177c39\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/dce5fcf3e1665e797344104213cc4b415d177c39\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/dce5fcf3e1665e797344104213cc4b415d177c39\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/dce5fcf3e1665e797344104213cc4b415d177c39/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"f15c77b0095dd0ffec45d22a56ebf04c7fb175a5\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/f15c77b0095dd0ffec45d22a56ebf04c7fb175a5\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/f15c77b0095dd0ffec45d22a56ebf04c7fb175a5\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"f15c77b0095dd0ffec45d22a56ebf04c7fb175a5\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6ZjE1Yzc3YjAwOTVkZDBmZmVjNDVkMjJhNTZlYmYwNGM3ZmIxNzVhNQ==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"robertdenton\",\n+                \"email\": \"robertdenton@users.noreply.github.com\",\n+                \"date\": \"2014-04-08T23:38:55Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"robertdenton\",\n+                \"email\": \"robertdenton@users.noreply.github.com\",\n+                \"date\": \"2014-04-08T23:38:55Z\"\n+            },\n+            \"message\": \"Delete index.html\",\n+            \"tree\": {\n+                \"sha\": \"42e5a2377af0d4316f6f34608159109fd74094f7\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/42e5a2377af0d4316f6f34608159109fd74094f7\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/f15c77b0095dd0ffec45d22a56ebf04c7fb175a5\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/f15c77b0095dd0ffec45d22a56ebf04c7fb175a5\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/f15c77b0095dd0ffec45d22a56ebf04c7fb175a5\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/f15c77b0095dd0ffec45d22a56ebf04c7fb175a5/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"b0b4190d03618492764ce43d0a0a2a288ebc9c85\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/b0b4190d03618492764ce43d0a0a2a288ebc9c85\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/b0b4190d03618492764ce43d0a0a2a288ebc9c85\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"b0b4190d03618492764ce43d0a0a2a288ebc9c85\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6YjBiNDE5MGQwMzYxODQ5Mjc2NGNlNDNkMGEwYTJhMjg4ZWJjOWM4NQ==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"robertdenton\",\n+                \"email\": \"robertdenton@users.noreply.github.com\",\n+                \"date\": \"2014-04-08T23:38:48Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"robertdenton\",\n+                \"email\": \"robertdenton@users.noreply.github.com\",\n+                \"date\": \"2014-04-08T23:38:48Z\"\n+            },\n+            \"message\": \"Delete data.json\",\n+            \"tree\": {\n+                \"sha\": \"b27de7af274db206115a4091a189c078e2fcc8fa\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/b27de7af274db206115a4091a189c078e2fcc8fa\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/b0b4190d03618492764ce43d0a0a2a288ebc9c85\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/b0b4190d03618492764ce43d0a0a2a288ebc9c85\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/b0b4190d03618492764ce43d0a0a2a288ebc9c85\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/b0b4190d03618492764ce43d0a0a2a288ebc9c85/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"937c07ea52242ab2f22e4d78c5a76f5a27b3c9e4\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/937c07ea52242ab2f22e4d78c5a76f5a27b3c9e4\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/937c07ea52242ab2f22e4d78c5a76f5a27b3c9e4\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"937c07ea52242ab2f22e4d78c5a76f5a27b3c9e4\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6OTM3YzA3ZWE1MjI0MmFiMmYyMmU0ZDc4YzVhNzZmNWEyN2IzYzllNA==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-04-08T23:36:09Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-04-08T23:36:09Z\"\n+            },\n+            \"message\": \"fix file structure\",\n+            \"tree\": {\n+                \"sha\": \"6752d16b023a61e04a35468abfce15671d5ce221\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/6752d16b023a61e04a35468abfce15671d5ce221\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/937c07ea52242ab2f22e4d78c5a76f5a27b3c9e4\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/937c07ea52242ab2f22e4d78c5a76f5a27b3c9e4\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/937c07ea52242ab2f22e4d78c5a76f5a27b3c9e4\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/937c07ea52242ab2f22e4d78c5a76f5a27b3c9e4/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"d378617271326e0348d7bab76f86594fd7d23f9b\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/d378617271326e0348d7bab76f86594fd7d23f9b\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/d378617271326e0348d7bab76f86594fd7d23f9b\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"d378617271326e0348d7bab76f86594fd7d23f9b\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6ZDM3ODYxNzI3MTMyNmUwMzQ4ZDdiYWI3NmY4NjU5NGZkN2QyM2Y5Yg==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-04-08T22:59:47Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-04-08T22:59:47Z\"\n+            },\n+            \"message\": \"d3 with styling for fission\",\n+            \"tree\": {\n+                \"sha\": \"b1e1d8ef01f8522dcc39f57f54454bf5790f6ebd\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/b1e1d8ef01f8522dcc39f57f54454bf5790f6ebd\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/d378617271326e0348d7bab76f86594fd7d23f9b\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/d378617271326e0348d7bab76f86594fd7d23f9b\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/d378617271326e0348d7bab76f86594fd7d23f9b\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/d378617271326e0348d7bab76f86594fd7d23f9b/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"5a352f14f66741dd9078100d166b5bab8755ee69\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/5a352f14f66741dd9078100d166b5bab8755ee69\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/5a352f14f66741dd9078100d166b5bab8755ee69\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"5a352f14f66741dd9078100d166b5bab8755ee69\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6NWEzNTJmMTRmNjY3NDFkZDkwNzgxMDBkMTY2YjViYWI4NzU1ZWU2OQ==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-03-05T18:45:08Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-03-05T18:45:08Z\"\n+            },\n+            \"message\": \"updates with hover\",\n+            \"tree\": {\n+                \"sha\": \"1f88f76abcd2495c6597b94d0eaa6d941dc45f24\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/1f88f76abcd2495c6597b94d0eaa6d941dc45f24\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/5a352f14f66741dd9078100d166b5bab8755ee69\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/5a352f14f66741dd9078100d166b5bab8755ee69\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/5a352f14f66741dd9078100d166b5bab8755ee69\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/5a352f14f66741dd9078100d166b5bab8755ee69/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"dd22dd275cc20b455fe7ef445d7f0e14f050a87e\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/dd22dd275cc20b455fe7ef445d7f0e14f050a87e\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/dd22dd275cc20b455fe7ef445d7f0e14f050a87e\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"dd22dd275cc20b455fe7ef445d7f0e14f050a87e\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6ZGQyMmRkMjc1Y2MyMGI0NTVmZTdlZjQ0NWQ3ZjBlMTRmMDUwYTg3ZQ==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-03-05T16:20:22Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Robert Denton\",\n+                \"email\": \"robert.denton9@gmail.com\",\n+                \"date\": \"2014-03-05T16:20:22Z\"\n+            },\n+            \"message\": \"rough cut of the initial data viz and stats. - Rob\",\n+            \"tree\": {\n+                \"sha\": \"419c5925f653e72bd5247878aed814a0004df826\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/419c5925f653e72bd5247878aed814a0004df826\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/dd22dd275cc20b455fe7ef445d7f0e14f050a87e\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/dd22dd275cc20b455fe7ef445d7f0e14f050a87e\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/dd22dd275cc20b455fe7ef445d7f0e14f050a87e\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/dd22dd275cc20b455fe7ef445d7f0e14f050a87e/comments\",\n+        \"author\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"committer\": {\n+            \"login\": \"robertdenton\",\n+            \"id\": 4853944,\n+            \"node_id\": \"MDQ6VXNlcjQ4NTM5NDQ=\",\n+            \"avatar_url\": \"https://avatars0.githubusercontent.com/u/4853944?v=4\",\n+            \"gravatar_id\": \"\",\n+            \"url\": \"https://api.github.com/users/robertdenton\",\n+            \"html_url\": \"https://github.com/robertdenton\",\n+            \"followers_url\": \"https://api.github.com/users/robertdenton/followers\",\n+            \"following_url\": \"https://api.github.com/users/robertdenton/following{/other_user}\",\n+            \"gists_url\": \"https://api.github.com/users/robertdenton/gists{/gist_id}\",\n+            \"starred_url\": \"https://api.github.com/users/robertdenton/starred{/owner}{/repo}\",\n+            \"subscriptions_url\": \"https://api.github.com/users/robertdenton/subscriptions\",\n+            \"organizations_url\": \"https://api.github.com/users/robertdenton/orgs\",\n+            \"repos_url\": \"https://api.github.com/users/robertdenton/repos\",\n+            \"events_url\": \"https://api.github.com/users/robertdenton/events{/privacy}\",\n+            \"received_events_url\": \"https://api.github.com/users/robertdenton/received_events\",\n+            \"type\": \"User\",\n+            \"site_admin\": false\n+        },\n+        \"parents\": [\n+            {\n+                \"sha\": \"db83f764b8e788e1d42b9d2405db06c36080f64b\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/db83f764b8e788e1d42b9d2405db06c36080f64b\",\n+                \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/db83f764b8e788e1d42b9d2405db06c36080f64b\"\n+            }\n+        ]\n+    },\n+    {\n+        \"sha\": \"db83f764b8e788e1d42b9d2405db06c36080f64b\",\n+        \"node_id\": \"MDY6Q29tbWl0MTY3NTE0NDI6ZGI4M2Y3NjRiOGU3ODhlMWQ0MmI5ZDI0MDVkYjA2YzM2MDgwZjY0Yg==\",\n+        \"commit\": {\n+            \"author\": {\n+                \"name\": \"Devon Tivona\",\n+                \"email\": \"devon@tivona.me\",\n+                \"date\": \"2014-02-12T01:06:36Z\"\n+            },\n+            \"committer\": {\n+                \"name\": \"Devon Tivona\",\n+                \"email\": \"devon@tivona.me\",\n+                \"date\": \"2014-02-12T01:06:36Z\"\n+            },\n+            \"message\": \"Initial commit\",\n+            \"tree\": {\n+                \"sha\": \"f93e3a1a1525fb5b91020da86e44810c87a2d7bc\",\n+                \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/trees/f93e3a1a1525fb5b91020da86e44810c87a2d7bc\"\n+            },\n+            \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/git/commits/db83f764b8e788e1d42b9d2405db06c36080f64b\",\n+            \"comment_count\": 0,\n+            \"verification\": {\n+                \"verified\": false,\n+                \"reason\": \"unsigned\",\n+                \"signature\": null,\n+                \"payload\": null\n+            }\n+        },\n+        \"url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/db83f764b8e788e1d42b9d2405db06c36080f64b\",\n+        \"html_url\": \"https://github.com/CUBigDataClass/hard-decisions/commit/db83f764b8e788e1d42b9d2405db06c36080f64b\",\n+        \"comments_url\": \"https://api.github.com/repos/CUBigDataClass/hard-decisions/commits/db83f764b8e788e1d42b9d2405db06c36080f64b/comments\",\n+        \"author\": null,\n+        \"committer\": null,\n+        \"parents\": []\n+    }\n+]\n\\ No newline at end of file"
            },
            {
                "sha": "7c8898880649b6e46275a837773c354ad575677d",
                "filename": "github-crawler/lib/data/repos.json",
                "status": "added",
                "additions": 3002,
                "deletions": 0,
                "changes": 3002,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/727342076a16bdb01975faa088e9a99d65f688ea/github-crawler/lib/data/repos.json",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/727342076a16bdb01975faa088e9a99d65f688ea/github-crawler/lib/data/repos.json",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/data/repos.json?ref=727342076a16bdb01975faa088e9a99d65f688ea"
            },
            {
                "sha": "d623d1da738a12b3975b2909820ae0aadbdc4c59",
                "filename": "github-crawler/lib/data/repos_list.json",
                "status": "added",
                "additions": 3002,
                "deletions": 0,
                "changes": 3002,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/727342076a16bdb01975faa088e9a99d65f688ea/github-crawler/lib/data/repos_list.json",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/727342076a16bdb01975faa088e9a99d65f688ea/github-crawler/lib/data/repos_list.json",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/data/repos_list.json?ref=727342076a16bdb01975faa088e9a99d65f688ea"
            },
            {
                "sha": "12c1535c7956fc2853da564e88a889a83e88c39b",
                "filename": "github-crawler/lib/test.py",
                "status": "modified",
                "additions": 58,
                "deletions": 5,
                "changes": 63,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/727342076a16bdb01975faa088e9a99d65f688ea/github-crawler/lib/test.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/727342076a16bdb01975faa088e9a99d65f688ea/github-crawler/lib/test.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/test.py?ref=727342076a16bdb01975faa088e9a99d65f688ea",
                "patch": "@@ -70,7 +70,7 @@ def branches_of_repo(repo, owner, api):\n #branches.to_csv('data/branches.csv')\n \n \n-def get_repositories(org_name,owner,api):\n+def get_repositories_itterative(org_name,owner,api):\n     repos = []\n     next = True\n     i = 1\n@@ -89,7 +89,7 @@ def get_repositories(org_name,owner,api):\n         break\n     return repos\n #org repo fetcher\n-#orgs = json_normalize(get_repositories('CUBigDataClass', 'vishwakulkarni', github_api))\n+#orgs = json_normalize(get_repositories_itterative('CUBigDataClass', 'vishwakulkarni', github_api))\n #orgs.to_csv('data/org.csv')\n \n def get_org_information(org_name,owner,api):\n@@ -98,13 +98,13 @@ def get_org_information(org_name,owner,api):\n     org_data=json.loads(org_data.content)\n     return org_data\n \n-def send_to_elasticInstance(data):\n-    es.index(index='org1', doc_type='_doc', body=data)\n+def send_to_elasticInstance(data,index_name,id_val):\n+    es.index(index=index_name, doc_type='_doc',id=id_val, body=data)\n \n \n #getting org info\n #org_data = get_org_information('duckduckgo', 'vishwakulkarni', github_api)\n-#send_to_elasticInstance(org_data)\n+#send_to_elasticInstance(org_data,'org1',org_data['id'])\n \n #how to get documents\n '''\n@@ -116,3 +116,56 @@ def send_to_elasticInstance(data):\n    }\n res = es.search(index='org1', doc_type='_doc', body=doc,scroll='1m')\n print(res)'''\n+\n+\n+#get Repositorries and send to es\n+\n+def get_repositories(org_name,owner,api):\n+    url = api + '/orgs/{}/repos'.format(org_name)\n+    org_repos_data = gh_session.get(url = url)\n+    org_repos_data=json.loads(org_repos_data.content)\n+    return org_repos_data\n+\n+#uncomment below after testing\n+'''\n+#get repos and push it to elasticsearch\n+org_repos = get_repositories('CUBigDataClass', 'vishwakulkarni',github_api)\n+\n+for repo in org_repos:\n+    repo['license']=\"test\"\n+    send_to_elasticInstance(repo,'repos',repo['id'])\n+    #commits_of_repo_github()\n+\n+\n+#write repo list to repos_list\n+#with open(\"data/repos_list.json\", \"w\") as outfile: \n+#    outfile.write(json.dumps(org_repos,indent=4)) \n+'''\n+\n+#save commits of repos as json\n+def commits_of_repo_github(repo, owner, api):\n+    commits = []\n+    next = True\n+    i = 1\n+    while next == True:\n+        url = api + '/repos/{}/{}/commits?page={}&per_page=100'.format(owner, repo, i)\n+        commit_pg = gh_session.get(url = url)\n+        commit_tp = json.loads(commit_pg.content)\n+        for commit in commit_tp:\n+            commits.append(commit) \n+        if 'Link' in commit_pg.headers:\n+            if 'rel=\"next\"' not in commit_pg.headers['Link']:\n+                next = False\n+        i = i + 1\n+    return commits\n+\n+commits = commits_of_repo_github('hard-decisions','CUBigDataClass',github_api)\n+\n+def add_commits_to_elasticsearch(commits):\n+    for commit in commits:\n+        es.index(index='commits', doc_type='_doc',id=commit['sha'], body=commit)\n+\n+add_commits_to_elasticsearch(commits)\n+\n+with open(\"data/commits.json\", \"w\") as outfile: \n+    outfile.write(json.dumps(commits,indent=4)) "
            }
        ]
    },
    {
        "sha": "37bded434253420e2973bdb8ef43ea4d3078946e",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjM3YmRlZDQzNDI1MzQyMGUyOTczYmRiOGVmNDNlYTRkMzA3ODk0NmU=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-11T18:52:44Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-11T18:52:44Z"
            },
            "message": "Adding org information to elastic search",
            "tree": {
                "sha": "2f26b17a90aeb3f558d098d71db5af4c7903f668",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/2f26b17a90aeb3f558d098d71db5af4c7903f668"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/37bded434253420e2973bdb8ef43ea4d3078946e",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/37bded434253420e2973bdb8ef43ea4d3078946e",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/37bded434253420e2973bdb8ef43ea4d3078946e",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/37bded434253420e2973bdb8ef43ea4d3078946e/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "d24222597d60130cd825c049c58fbbd76f6ac2a1",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/d24222597d60130cd825c049c58fbbd76f6ac2a1",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/d24222597d60130cd825c049c58fbbd76f6ac2a1"
            }
        ],
        "stats": {
            "total": 71,
            "additions": 66,
            "deletions": 5
        },
        "files": [
            {
                "sha": "21765c348f7627dc92cb821fe318cc2cd912e49d",
                "filename": "git-statistics/test_2.py",
                "status": "renamed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/37bded434253420e2973bdb8ef43ea4d3078946e/git-statistics/test_2.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/37bded434253420e2973bdb8ef43ea4d3078946e/git-statistics/test_2.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/git-statistics/test_2.py?ref=37bded434253420e2973bdb8ef43ea4d3078946e",
                "previous_filename": "git-statistics/test.py"
            },
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "github-crawler/Templates_commands/org_template.txt",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/37bded434253420e2973bdb8ef43ea4d3078946e/github-crawler/Templates_commands/org_template.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/37bded434253420e2973bdb8ef43ea4d3078946e/github-crawler/Templates_commands/org_template.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/org_template.txt?ref=37bded434253420e2973bdb8ef43ea4d3078946e"
            },
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "github-crawler/Templates_commands/put_org_index.txt",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/37bded434253420e2973bdb8ef43ea4d3078946e/github-crawler/Templates_commands/put_org_index.txt",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/37bded434253420e2973bdb8ef43ea4d3078946e/github-crawler/Templates_commands/put_org_index.txt",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/Templates_commands/put_org_index.txt?ref=37bded434253420e2973bdb8ef43ea4d3078946e"
            },
            {
                "sha": "0b273b049a3ed5b512bddab2922eb655bc8258f9",
                "filename": "github-crawler/lib/data/org.json",
                "status": "added",
                "additions": 30,
                "deletions": 0,
                "changes": 30,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/37bded434253420e2973bdb8ef43ea4d3078946e/github-crawler/lib/data/org.json",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/37bded434253420e2973bdb8ef43ea4d3078946e/github-crawler/lib/data/org.json",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/data/org.json?ref=37bded434253420e2973bdb8ef43ea4d3078946e",
                "patch": "@@ -0,0 +1,30 @@\n+{\n+    \"login\": \"CUBigDataClass\",\n+    \"id\": 6345918,\n+    \"node_id\": \"MDEyOk9yZ2FuaXphdGlvbjYzNDU5MTg=\",\n+    \"url\": \"https://api.github.com/orgs/CUBigDataClass\",\n+    \"repos_url\": \"https://api.github.com/orgs/CUBigDataClass/repos\",\n+    \"events_url\": \"https://api.github.com/orgs/CUBigDataClass/events\",\n+    \"hooks_url\": \"https://api.github.com/orgs/CUBigDataClass/hooks\",\n+    \"issues_url\": \"https://api.github.com/orgs/CUBigDataClass/issues\",\n+    \"members_url\": \"https://api.github.com/orgs/CUBigDataClass/members{/member}\",\n+    \"public_members_url\": \"https://api.github.com/orgs/CUBigDataClass/public_members{/member}\",\n+    \"avatar_url\": \"https://avatars0.githubusercontent.com/u/6345918?v=4\",\n+    \"description\": null,\n+    \"name\": null,\n+    \"company\": null,\n+    \"blog\": null,\n+    \"location\": null,\n+    \"email\": null,\n+    \"is_verified\": false,\n+    \"has_organization_projects\": true,\n+    \"has_repository_projects\": true,\n+    \"public_repos\": 93,\n+    \"public_gists\": 0,\n+    \"followers\": 0,\n+    \"following\": 0,\n+    \"html_url\": \"https://github.com/CUBigDataClass\",\n+    \"created_at\": \"2014-01-08T04:29:23Z\",\n+    \"updated_at\": \"2019-02-25T23:24:32Z\",\n+    \"type\": \"Organization\"\n+  }\n\\ No newline at end of file"
            },
            {
                "sha": "7422dffdf382fb4d272493bb3ae5ca673f69ceac",
                "filename": "github-crawler/lib/test.py",
                "status": "modified",
                "additions": 36,
                "deletions": 5,
                "changes": 41,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/37bded434253420e2973bdb8ef43ea4d3078946e/github-crawler/lib/test.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/37bded434253420e2973bdb8ef43ea4d3078946e/github-crawler/lib/test.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/test.py?ref=37bded434253420e2973bdb8ef43ea4d3078946e",
                "patch": "@@ -21,6 +21,14 @@\n import config\n import os\n \n+#elasticsearch\n+from elasticsearch import Elasticsearch\n+\n+\n+#set config working\n+res = requests.get('http://localhost:9200')\n+#print (res.content)\n+es = Elasticsearch([{'host': 'localhost', 'port': '9200'}])\n \n \n # function that converts all object columns to strings, in order to store them efficiently into the database\n@@ -80,8 +88,31 @@ def get_repositories(org_name,owner,api):\n         i = i + 1\n         break\n     return repos\n-\n-orgs = json_normalize(get_repositories('CUBigDataClass', 'vishwakulkarni', github_api))\n-\n-\n-orgs.to_csv('data/org.csv')\n\\ No newline at end of file\n+#org repo fetcher\n+#orgs = json_normalize(get_repositories('CUBigDataClass', 'vishwakulkarni', github_api))\n+#orgs.to_csv('data/org.csv')\n+\n+def get_org_information(org_name,owner,api):\n+    url = api + '/orgs/{}'.format(org_name)\n+    org_data = gh_session.get(url = url)\n+    org_data=json.loads(org_data.content)\n+    return org_data\n+\n+def send_to_elasticInstance(data):\n+    es.index(index='org1', doc_type='_doc', body=data)\n+\n+\n+#getting org info\n+#org_data = get_org_information('duckduckgo', 'vishwakulkarni', github_api)\n+#send_to_elasticInstance(org_data)\n+\n+#how to get documents\n+'''\n+doc = {\n+        'size' : 10000,\n+        'query': {\n+            'match_all' : {}\n+       }\n+   }\n+res = es.search(index='org1', doc_type='_doc', body=doc,scroll='1m')\n+print(res)'''"
            }
        ]
    },
    {
        "sha": "d24222597d60130cd825c049c58fbbd76f6ac2a1",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmQyNDIyMjU5N2Q2MDEzMGNkODI1YzA0OWM1OGZiYmQ3NmY2YWMyYTE=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-11T13:53:20Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-11T13:53:20Z"
            },
            "message": "Adding ReadMe to main Folder",
            "tree": {
                "sha": "ecfa6f13edd7de3a27ce7b6fe4e06a02268c7462",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/ecfa6f13edd7de3a27ce7b6fe4e06a02268c7462"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/d24222597d60130cd825c049c58fbbd76f6ac2a1",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/d24222597d60130cd825c049c58fbbd76f6ac2a1",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/d24222597d60130cd825c049c58fbbd76f6ac2a1",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/d24222597d60130cd825c049c58fbbd76f6ac2a1/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "25676edf24fed7e72158bc890e0a0679e9c82b3e",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/25676edf24fed7e72158bc890e0a0679e9c82b3e",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/25676edf24fed7e72158bc890e0a0679e9c82b3e"
            }
        ],
        "stats": {
            "total": 0,
            "additions": 0,
            "deletions": 0
        },
        "files": [
            {
                "sha": "699c997bb03722d7e34ab59fa892b3ef82d0fb38",
                "filename": "README.md",
                "status": "renamed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/d24222597d60130cd825c049c58fbbd76f6ac2a1/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/d24222597d60130cd825c049c58fbbd76f6ac2a1/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/README.md?ref=d24222597d60130cd825c049c58fbbd76f6ac2a1",
                "previous_filename": "github-crawler/README.md"
            }
        ]
    },
    {
        "sha": "25676edf24fed7e72158bc890e0a0679e9c82b3e",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOjI1Njc2ZWRmMjRmZWQ3ZTcyMTU4YmM4OTBlMGEwNjc5ZTljODJiM2U=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-11T13:52:44Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-11T13:52:44Z"
            },
            "message": "Adding ReadMe",
            "tree": {
                "sha": "8f50b592915980603ce682717b5d0595e4a523dc",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/8f50b592915980603ce682717b5d0595e4a523dc"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/25676edf24fed7e72158bc890e0a0679e9c82b3e",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/25676edf24fed7e72158bc890e0a0679e9c82b3e",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/25676edf24fed7e72158bc890e0a0679e9c82b3e",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/25676edf24fed7e72158bc890e0a0679e9c82b3e/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [
            {
                "sha": "c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c9003cb82dffc7ca8353e416bbb2c0d8c7614694"
            }
        ],
        "stats": {
            "total": 1,
            "additions": 1,
            "deletions": 0
        },
        "files": [
            {
                "sha": "699c997bb03722d7e34ab59fa892b3ef82d0fb38",
                "filename": "github-crawler/README.md",
                "status": "added",
                "additions": 1,
                "deletions": 0,
                "changes": 1,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/25676edf24fed7e72158bc890e0a0679e9c82b3e/github-crawler/README.md",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/25676edf24fed7e72158bc890e0a0679e9c82b3e/github-crawler/README.md",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/README.md?ref=25676edf24fed7e72158bc890e0a0679e9c82b3e",
                "patch": "@@ -0,0 +1 @@\n+#Kode-Kallas\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
        "node_id": "MDY6Q29tbWl0MjM5MzczNzkxOmM5MDAzY2I4MmRmZmM3Y2E4MzUzZTQxNmJiYjJjMGQ4Yzc2MTQ2OTQ=",
        "commit": {
            "author": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-11T13:50:45Z"
            },
            "committer": {
                "name": "vishwakulkarni",
                "email": "vishwa.kulkarni@gmail.com",
                "date": "2020-04-11T13:50:45Z"
            },
            "message": "Gitup data crawler",
            "tree": {
                "sha": "0c92bb0b5d44564c9c27155840a853f3ec3f86d3",
                "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/trees/0c92bb0b5d44564c9c27155840a853f3ec3f86d3"
            },
            "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/git/commits/c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
            "comment_count": 0,
            "verification": {
                "verified": false,
                "reason": "unsigned",
                "signature": null,
                "payload": null
            }
        },
        "url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
        "html_url": "https://github.com/CUBigDataClass/Kode-Kallas/commit/c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
        "comments_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/commits/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/comments",
        "author": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "committer": {
            "login": "vishwakulkarni",
            "id": 5782419,
            "node_id": "MDQ6VXNlcjU3ODI0MTk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/5782419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishwakulkarni",
            "html_url": "https://github.com/vishwakulkarni",
            "followers_url": "https://api.github.com/users/vishwakulkarni/followers",
            "following_url": "https://api.github.com/users/vishwakulkarni/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishwakulkarni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishwakulkarni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishwakulkarni/subscriptions",
            "organizations_url": "https://api.github.com/users/vishwakulkarni/orgs",
            "repos_url": "https://api.github.com/users/vishwakulkarni/repos",
            "events_url": "https://api.github.com/users/vishwakulkarni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishwakulkarni/received_events",
            "type": "User",
            "site_admin": false
        },
        "parents": [],
        "stats": {
            "total": 195329,
            "additions": 195329,
            "deletions": 0
        },
        "files": [
            {
                "sha": "9f73e97ce364f0fd0e1d45f231f1409076df35ad",
                "filename": "git-statistics/.gitignore",
                "status": "added",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/.gitignore",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/.gitignore",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/git-statistics/.gitignore?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "patch": "@@ -0,0 +1,3 @@\n+.ipynb_checkpoints/\n+__pycache__/\n+config.py\n\\ No newline at end of file"
            },
            {
                "sha": "67753dc941b73f81e0e4f408ff4431736dcd584e",
                "filename": "git-statistics/Git_Data_Analysis.ipynb",
                "status": "added",
                "additions": 16096,
                "deletions": 0,
                "changes": 16096,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/Git_Data_Analysis.ipynb",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/Git_Data_Analysis.ipynb",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/git-statistics/Git_Data_Analysis.ipynb?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694"
            },
            {
                "sha": "38e4ae39824a6cd5285c715d3967ed04057f6e45",
                "filename": "git-statistics/Git_Data_Analysis.py",
                "status": "added",
                "additions": 263,
                "deletions": 0,
                "changes": 263,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/Git_Data_Analysis.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/Git_Data_Analysis.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/git-statistics/Git_Data_Analysis.py?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "patch": "@@ -0,0 +1,263 @@\n+#!/usr/bin/env python\n+# coding: utf-8\n+\n+# In[1]:\n+\n+\n+import re\n+from datetime import datetime\n+\n+import numpy as np\n+import pandas as pd\n+pd.set_option('display.max_columns', 20)\n+pd.set_option('display.width', 1000)\n+pd.set_option('display.max_colwidth', -1)\n+from pandas.io.json import json_normalize\n+import json\n+import ast\n+\n+#get_ipython().run_line_magic('load_ext', 'autotime')\n+\n+import plotly\n+import chart_studio.plotly as py\n+import plotly.graph_objs as go\n+from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n+init_notebook_mode(connected=True)\n+\n+\n+# In[33]:\n+\n+\n+commits = pd.read_csv('data/commits.csv', parse_dates=True)\n+\n+\n+# In[34]:\n+\n+\n+commits.info()\n+\n+\n+# ## Data Preprocessing\n+\n+# In[56]:\n+\n+\n+commits['date'] =  pd.to_datetime(commits['commit.committer.date'])\n+\n+\n+# In[57]:\n+\n+\n+commits['date'] =  pd.to_datetime(commits['date'], utc=True)\n+\n+\n+# In[58]:\n+\n+\n+commits['commit_date'] = commits['date'].dt.date\n+\n+\n+# In[59]:\n+\n+\n+commits['commit_week'] = commits['date'].dt.week\n+\n+\n+# In[60]:\n+\n+\n+commits['commit_hour'] = commits['date'].dt.hour\n+\n+\n+# In[61]:\n+\n+\n+commits['commit_month'] = commits['date'].dt.month\n+\n+\n+# In[62]:\n+\n+\n+commits['commit_year'] = commits['date'].dt.year\n+\n+\n+# In[63]:\n+\n+\n+# drop unnecessary columns\n+commits = commits[['sha', 'author.login', 'commit_date', 'commit_hour', 'commit_month', 'commit_year']]\n+\n+\n+# In[79]:\n+\n+\n+commits.head()\n+\n+\n+# ## Data Analysis\n+\n+# In[91]:\n+\n+\n+## number of contributors\n+commits['author.login'].unique().size\n+\n+\n+# In[92]:\n+\n+\n+commits_by_hour = commits.groupby('commit_hour')[['sha']].count()\n+commits_by_hour = commits_by_hour.rename(columns = {'sha': 'commit_count'})\n+\n+\n+# In[93]:\n+\n+\n+fig = go.Figure([go.Bar(\n+    x=commits_by_hour.index, \n+    y=commits_by_hour.commit_count, \n+    text=commits_by_hour.commit_count, \n+    textposition='auto')])\n+fig.update_layout(\n+    title = 'Commits by Hour', \n+    xaxis_title = 'Hour', \n+    yaxis_title = 'Commits Count', \n+    xaxis_tickmode = 'linear')\n+fig.show()\n+\n+\n+# In[94]:\n+\n+\n+commits_by_day = commits.groupby('commit_date')[['sha']].count()\n+commits_by_day = commits_by_day.rename(columns = {'sha': 'commit_count'})\n+\n+\n+# In[95]:\n+\n+\n+fig = go.Figure([go.Scatter(\n+    x=commits_by_day.index, \n+    y=commits_by_day.commit_count, \n+    text=commits_by_day.commit_count, \n+    fill='tozeroy')])\n+fig.update_layout(\n+    title = 'Commits by Date', \n+    xaxis_title = 'Date', \n+    yaxis_title = 'Commits Count')\n+fig.show()\n+\n+\n+# In[96]:\n+\n+\n+commits_by_author = commits.groupby('author.login')[['sha']].count()\n+commits_by_author = commits_by_author.rename(columns = {'sha': 'commit_count'})\n+commits_by_author = commits_by_author.sort_values(by='commit_count', ascending=False)\n+top_authors = commits_by_author.head(30)\n+\n+\n+# In[97]:\n+\n+\n+fig = go.Figure([go.Bar(\n+    x=top_authors.index, \n+    y=top_authors.commit_count)])\n+fig.update_layout(\n+    title = 'Top Committers', \n+    xaxis_title = 'Author', \n+    yaxis_title = 'Commits Count', \n+    xaxis_tickmode = 'linear',\n+    xaxis_tickangle=-40)\n+fig.show()\n+\n+\n+# ## Open Pull Requests\n+\n+# In[98]:\n+\n+\n+pulls = pd.read_csv('data/pulls.csv', parse_dates=True)\n+\n+\n+# In[99]:\n+\n+\n+pulls.info()\n+\n+\n+# In[100]:\n+\n+\n+pulls['date'] = pd.to_datetime(pulls['created_at'])\n+pulls['date'] = pd.to_datetime(pulls['date'], utc=True)\n+pulls['pull_date'] = pulls['date'].dt.date\n+\n+\n+# In[101]:\n+\n+\n+pulls_by_date = pulls.groupby('pull_date')[['id']].count()\n+pulls_by_date = pulls_by_date.rename(columns = {'id': 'commit_count'})\n+\n+\n+# In[104]:\n+\n+\n+fig = go.Figure([go.Scatter(\n+    x=pulls_by_date.index, \n+    y=pulls_by_date.commit_count, \n+    text=pulls_by_date.commit_count)])\n+fig.update_layout(\n+    title = 'Open Pull Requests by Date', \n+    xaxis_title = 'Date', \n+    yaxis_title = 'Pulls Count')\n+fig.show()\n+\n+\n+# ### Pull Request Labels\n+\n+# **NOTES**:  \n+# - ast.literal_eval converts string to list.\n+# - The following two lines convert a Pandas column that contains list of dictionaries into a Pandas Dataframe where each dictionary corresponds to a dataframe row.\n+\n+# In[105]:\n+\n+\n+labels_list = [ast.literal_eval(i) for i in pulls['labels'].tolist() if i != '[]']\n+\n+\n+# In[106]:\n+\n+\n+labels = [j for i in labels_list for j in i]\n+\n+\n+# In[108]:\n+\n+\n+labels_df = pd.DataFrame(labels)\n+\n+\n+# In[114]:\n+\n+\n+labels_by_name = labels_df.groupby('name')[['id']].count()\n+labels_by_name = labels_by_name.rename(columns = {'id': 'label_count'})\n+labels_by_name = labels_by_name.sort_values(by=['label_count'], ascending=False)\n+\n+\n+# In[115]:\n+\n+\n+fig = go.Figure([go.Bar(\n+    x=labels_by_name.index, \n+    y=labels_by_name.label_count)])\n+fig.update_layout(\n+    title = 'Pull Requests by Label', \n+    xaxis_title = 'Labels', \n+    yaxis_title = 'PR Count', \n+    xaxis_tickmode = 'linear',\n+    xaxis_tickangle=-40)\n+fig.show()\n+"
            },
            {
                "sha": "03aa8fbb9e7a0ca8bbce776dcdd4ae780a082286",
                "filename": "git-statistics/Git_Data_Extraction_and_Processing.ipynb",
                "status": "added",
                "additions": 556,
                "deletions": 0,
                "changes": 556,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/Git_Data_Extraction_and_Processing.ipynb",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/Git_Data_Extraction_and_Processing.ipynb",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/git-statistics/Git_Data_Extraction_and_Processing.ipynb?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "patch": "@@ -0,0 +1,556 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"In order to extract data from Github, we are going to leverage the Github REST API v3, that can be found in this link https://developer.github.com/v3/.\\n\",\n+    \"In `config.py` file we need to define the following configuration variables, that are going to be accessed by the current notebook:\\n\",\n+    \"- `GITHUB_USERNAME`\\n\",\n+    \"- `GITHUB_TOKEN`\\n\",\n+    \"- `SQL_ALCHEMY_STRING` (only if we want to save our Github results in a relational database)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 1,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import json\\n\",\n+    \"import requests\\n\",\n+    \"from pandas.io.json import json_normalize\\n\",\n+    \"from sqlalchemy import create_engine, engine, text, types, MetaData, Table, String\\n\",\n+    \"import pandas as pd\\n\",\n+    \"pd.set_option('display.max_columns', 500)\\n\",\n+    \"pd.set_option('display.width', 1000)\\n\",\n+    \"import numpy as np\\n\",\n+    \"from datetime import datetime\\n\",\n+    \"\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 2,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import config\\n\",\n+    \"import os\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 3,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# function that converts all object columns to strings, in order to store them efficiently into the database\\n\",\n+    \"def objects_to_strings(table):\\n\",\n+    \"    measurer = np.vectorize(len)\\n\",\n+    \"    df_object = table.select_dtypes(include=[object])\\n\",\n+    \"    string_columns = dict(zip(df_object, measurer(\\n\",\n+    \"        df_object.values.astype(str)).max(axis=0)))\\n\",\n+    \"    string_columns = {key: String(length=value) if value > 0 else String(length=1)\\n\",\n+    \"                      for key, value in string_columns.items() }\\n\",\n+    \"    return string_columns\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"#engine = create_engine(config.SQL_ALCHEMY_STRING)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 5,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"github_api = \\\"https://api.github.com\\\"\\n\",\n+    \"gh_session = requests.Session()\\n\",\n+    \"gh_session.auth = (config.GITHUB_USERNAME, config.GITHUB_TOKEN)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## 1. Branches\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 6,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def branches_of_repo(repo, owner, api):\\n\",\n+    \"    branches = []\\n\",\n+    \"    next = True\\n\",\n+    \"    i = 1\\n\",\n+    \"    while next == True:\\n\",\n+    \"        url = api + '/repos/{}/{}/branches?page={}&per_page=100'.format(owner, repo, i)\\n\",\n+    \"        branch_pg = gh_session.get(url = url)\\n\",\n+    \"        branch_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in branch_pg.json()]    \\n\",\n+    \"        branch_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in branch_pg_list]\\n\",\n+    \"        branches = branches + branch_pg_list\\n\",\n+    \"        if 'Link' in branch_pg.headers:\\n\",\n+    \"            if 'rel=\\\"next\\\"' not in branch_pg.headers['Link']:\\n\",\n+    \"                next = False\\n\",\n+    \"        i = i + 1\\n\",\n+    \"    return branches\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 7,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"branches = json_normalize(branches_of_repo('spark', 'apache', github_api))\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 8,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"## store permanently into an SQL database\\n\",\n+    \"#branches.to_sql(con=engine, name='branches',\\n\",\n+    \"#                 if_exists='replace', dtype=objects_to_strings(branches))\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 9,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"branches.to_csv('data/branches.csv')\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## 2. Commits\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 8,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def commits_of_repo_github(repo, owner, api):\\n\",\n+    \"    commits = []\\n\",\n+    \"    next = True\\n\",\n+    \"    i = 1\\n\",\n+    \"    while next == True:\\n\",\n+    \"        url = api + '/repos/{}/{}/commits?page={}&per_page=100'.format(owner, repo, i)\\n\",\n+    \"        commit_pg = gh_session.get(url = url)\\n\",\n+    \"        commit_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in commit_pg.json()]    \\n\",\n+    \"        commit_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in commit_pg_list]\\n\",\n+    \"        commits = commits + commit_pg_list\\n\",\n+    \"        if 'Link' in commit_pg.headers:\\n\",\n+    \"            if 'rel=\\\"next\\\"' not in commit_pg.headers['Link']:\\n\",\n+    \"                next = False\\n\",\n+    \"        i = i + 1\\n\",\n+    \"    return commits\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 9,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def create_commits_df(repo, owner, api):\\n\",\n+    \"    commits_list = commits_of_repo_github(repo, owner, api)\\n\",\n+    \"    return json_normalize(commits_list)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 10,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"ename\": \"KeyboardInterrupt\",\n+     \"evalue\": \"\",\n+     \"output_type\": \"error\",\n+     \"traceback\": [\n+      \"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\n+      \"\\u001b[0;31mTypeError\\u001b[0m                                 Traceback (most recent call last)\",\n+      \"\\u001b[0;32m/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py\\u001b[0m in \\u001b[0;36m_make_request\\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\\u001b[0m\\n\\u001b[1;32m    379\\u001b[0m             \\u001b[0;32mtry\\u001b[0m\\u001b[0;34m:\\u001b[0m  \\u001b[0;31m# Python 2.7, use buffering of HTTP responses\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 380\\u001b[0;31m                 \\u001b[0mhttplib_response\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mconn\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mgetresponse\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mbuffering\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0;32mTrue\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    381\\u001b[0m             \\u001b[0;32mexcept\\u001b[0m \\u001b[0mTypeError\\u001b[0m\\u001b[0;34m:\\u001b[0m  \\u001b[0;31m# Python 2.6 and older, Python 3\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;31mTypeError\\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'\",\n+      \"\\nDuring handling of the above exception, another exception occurred:\\n\",\n+      \"\\u001b[0;31mKeyboardInterrupt\\u001b[0m                         Traceback (most recent call last)\",\n+      \"\\u001b[0;32m<ipython-input-10-9ac091e6aee0>\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m\\u001b[0m\\n\\u001b[0;32m----> 1\\u001b[0;31m \\u001b[0mcommits\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mcreate_commits_df\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m'spark'\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m'apache'\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mgithub_api\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\",\n+      \"\\u001b[0;32m<ipython-input-9-bf07c373d395>\\u001b[0m in \\u001b[0;36mcreate_commits_df\\u001b[0;34m(repo, owner, api)\\u001b[0m\\n\\u001b[1;32m      1\\u001b[0m \\u001b[0;32mdef\\u001b[0m \\u001b[0mcreate_commits_df\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mrepo\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mowner\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mapi\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 2\\u001b[0;31m     \\u001b[0mcommits_list\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mcommits_of_repo_github\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mrepo\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mowner\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mapi\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      3\\u001b[0m     \\u001b[0;32mreturn\\u001b[0m \\u001b[0mjson_normalize\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mcommits_list\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;32m<ipython-input-8-af71d73a56f9>\\u001b[0m in \\u001b[0;36mcommits_of_repo_github\\u001b[0;34m(repo, owner, api)\\u001b[0m\\n\\u001b[1;32m      5\\u001b[0m     \\u001b[0;32mwhile\\u001b[0m \\u001b[0mnext\\u001b[0m \\u001b[0;34m==\\u001b[0m \\u001b[0;32mTrue\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      6\\u001b[0m         \\u001b[0murl\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mapi\\u001b[0m \\u001b[0;34m+\\u001b[0m \\u001b[0;34m'/repos/{}/{}/commits?page={}&per_page=100'\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mformat\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mowner\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mrepo\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mi\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 7\\u001b[0;31m         \\u001b[0mcommit_pg\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mgh_session\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mget\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0murl\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0murl\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      8\\u001b[0m         \\u001b[0mcommit_pg_list\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;34m[\\u001b[0m\\u001b[0mdict\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mitem\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m**\\u001b[0m\\u001b[0;34m{\\u001b[0m\\u001b[0;34m'repo_name'\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m'{}'\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mformat\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mrepo\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m}\\u001b[0m\\u001b[0;34m)\\u001b[0m \\u001b[0;32mfor\\u001b[0m \\u001b[0mitem\\u001b[0m \\u001b[0;32min\\u001b[0m \\u001b[0mcommit_pg\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mjson\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m]\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      9\\u001b[0m         \\u001b[0mcommit_pg_list\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;34m[\\u001b[0m\\u001b[0mdict\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mitem\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m**\\u001b[0m\\u001b[0;34m{\\u001b[0m\\u001b[0;34m'owner'\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m'{}'\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mformat\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mowner\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m}\\u001b[0m\\u001b[0;34m)\\u001b[0m \\u001b[0;32mfor\\u001b[0m \\u001b[0mitem\\u001b[0m \\u001b[0;32min\\u001b[0m \\u001b[0mcommit_pg_list\\u001b[0m\\u001b[0;34m]\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;32m/usr/local/lib/python3.7/site-packages/requests/sessions.py\\u001b[0m in \\u001b[0;36mget\\u001b[0;34m(self, url, **kwargs)\\u001b[0m\\n\\u001b[1;32m    544\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    545\\u001b[0m         \\u001b[0mkwargs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msetdefault\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m'allow_redirects'\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;32mTrue\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 546\\u001b[0;31m         \\u001b[0;32mreturn\\u001b[0m \\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mrequest\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m'GET'\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0murl\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m**\\u001b[0m\\u001b[0mkwargs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    547\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    548\\u001b[0m     \\u001b[0;32mdef\\u001b[0m \\u001b[0moptions\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mself\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0murl\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m**\\u001b[0m\\u001b[0mkwargs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;32m/usr/local/lib/python3.7/site-packages/requests/sessions.py\\u001b[0m in \\u001b[0;36mrequest\\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\\u001b[0m\\n\\u001b[1;32m    531\\u001b[0m         }\\n\\u001b[1;32m    532\\u001b[0m         \\u001b[0msend_kwargs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mupdate\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0msettings\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 533\\u001b[0;31m         \\u001b[0mresp\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msend\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mprep\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m**\\u001b[0m\\u001b[0msend_kwargs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    534\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    535\\u001b[0m         \\u001b[0;32mreturn\\u001b[0m \\u001b[0mresp\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;32m/usr/local/lib/python3.7/site-packages/requests/sessions.py\\u001b[0m in \\u001b[0;36msend\\u001b[0;34m(self, request, **kwargs)\\u001b[0m\\n\\u001b[1;32m    644\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    645\\u001b[0m         \\u001b[0;31m# Send the request\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 646\\u001b[0;31m         \\u001b[0mr\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0madapter\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msend\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mrequest\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m**\\u001b[0m\\u001b[0mkwargs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    647\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    648\\u001b[0m         \\u001b[0;31m# Total elapsed time of the request (approximately)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;32m/usr/local/lib/python3.7/site-packages/requests/adapters.py\\u001b[0m in \\u001b[0;36msend\\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\\u001b[0m\\n\\u001b[1;32m    447\\u001b[0m                     \\u001b[0mdecode_content\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0;32mFalse\\u001b[0m\\u001b[0;34m,\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    448\\u001b[0m                     \\u001b[0mretries\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mmax_retries\\u001b[0m\\u001b[0;34m,\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 449\\u001b[0;31m                     \\u001b[0mtimeout\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mtimeout\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    450\\u001b[0m                 )\\n\\u001b[1;32m    451\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;32m/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py\\u001b[0m in \\u001b[0;36murlopen\\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\\u001b[0m\\n\\u001b[1;32m    599\\u001b[0m                                                   \\u001b[0mtimeout\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mtimeout_obj\\u001b[0m\\u001b[0;34m,\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    600\\u001b[0m                                                   \\u001b[0mbody\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mbody\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mheaders\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mheaders\\u001b[0m\\u001b[0;34m,\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 601\\u001b[0;31m                                                   chunked=chunked)\\n\\u001b[0m\\u001b[1;32m    602\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    603\\u001b[0m             \\u001b[0;31m# If we're going to release the connection in ``finally:``, then\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;32m/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py\\u001b[0m in \\u001b[0;36m_make_request\\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\\u001b[0m\\n\\u001b[1;32m    381\\u001b[0m             \\u001b[0;32mexcept\\u001b[0m \\u001b[0mTypeError\\u001b[0m\\u001b[0;34m:\\u001b[0m  \\u001b[0;31m# Python 2.6 and older, Python 3\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    382\\u001b[0m                 \\u001b[0;32mtry\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 383\\u001b[0;31m                     \\u001b[0mhttplib_response\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mconn\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mgetresponse\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    384\\u001b[0m                 \\u001b[0;32mexcept\\u001b[0m \\u001b[0mException\\u001b[0m \\u001b[0;32mas\\u001b[0m \\u001b[0me\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    385\\u001b[0m                     \\u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;32m/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\\u001b[0m in \\u001b[0;36mgetresponse\\u001b[0;34m(self)\\u001b[0m\\n\\u001b[1;32m   1334\\u001b[0m         \\u001b[0;32mtry\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m   1335\\u001b[0m             \\u001b[0;32mtry\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m-> 1336\\u001b[0;31m                 \\u001b[0mresponse\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mbegin\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m   1337\\u001b[0m             \\u001b[0;32mexcept\\u001b[0m \\u001b[0mConnectionError\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m   1338\\u001b[0m                 \\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mclose\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;32m/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\\u001b[0m in \\u001b[0;36mbegin\\u001b[0;34m(self)\\u001b[0m\\n\\u001b[1;32m    304\\u001b[0m         \\u001b[0;31m# read until we get a non-100 response\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    305\\u001b[0m         \\u001b[0;32mwhile\\u001b[0m \\u001b[0;32mTrue\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 306\\u001b[0;31m             \\u001b[0mversion\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mstatus\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mreason\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0m_read_status\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    307\\u001b[0m             \\u001b[0;32mif\\u001b[0m \\u001b[0mstatus\\u001b[0m \\u001b[0;34m!=\\u001b[0m \\u001b[0mCONTINUE\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    308\\u001b[0m                 \\u001b[0;32mbreak\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;32m/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\\u001b[0m in \\u001b[0;36m_read_status\\u001b[0;34m(self)\\u001b[0m\\n\\u001b[1;32m    265\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    266\\u001b[0m     \\u001b[0;32mdef\\u001b[0m \\u001b[0m_read_status\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mself\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 267\\u001b[0;31m         \\u001b[0mline\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mstr\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mfp\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mreadline\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0m_MAXLINE\\u001b[0m \\u001b[0;34m+\\u001b[0m \\u001b[0;36m1\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m\\\"iso-8859-1\\\"\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    268\\u001b[0m         \\u001b[0;32mif\\u001b[0m \\u001b[0mlen\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mline\\u001b[0m\\u001b[0;34m)\\u001b[0m \\u001b[0;34m>\\u001b[0m \\u001b[0m_MAXLINE\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    269\\u001b[0m             \\u001b[0;32mraise\\u001b[0m \\u001b[0mLineTooLong\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m\\\"status line\\\"\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;32m/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socket.py\\u001b[0m in \\u001b[0;36mreadinto\\u001b[0;34m(self, b)\\u001b[0m\\n\\u001b[1;32m    587\\u001b[0m         \\u001b[0;32mwhile\\u001b[0m \\u001b[0;32mTrue\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    588\\u001b[0m             \\u001b[0;32mtry\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 589\\u001b[0;31m                 \\u001b[0;32mreturn\\u001b[0m \\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0m_sock\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mrecv_into\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mb\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    590\\u001b[0m             \\u001b[0;32mexcept\\u001b[0m \\u001b[0mtimeout\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    591\\u001b[0m                 \\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0m_timeout_occurred\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;32mTrue\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;32m/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\\u001b[0m in \\u001b[0;36mrecv_into\\u001b[0;34m(self, buffer, nbytes, flags)\\u001b[0m\\n\\u001b[1;32m   1069\\u001b[0m                   \\u001b[0;34m\\\"non-zero flags not allowed in calls to recv_into() on %s\\\"\\u001b[0m \\u001b[0;34m%\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m   1070\\u001b[0m                   self.__class__)\\n\\u001b[0;32m-> 1071\\u001b[0;31m             \\u001b[0;32mreturn\\u001b[0m \\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mread\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mnbytes\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mbuffer\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m   1072\\u001b[0m         \\u001b[0;32melse\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m   1073\\u001b[0m             \\u001b[0;32mreturn\\u001b[0m \\u001b[0msuper\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mrecv_into\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mbuffer\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mnbytes\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mflags\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;32m/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\\u001b[0m in \\u001b[0;36mread\\u001b[0;34m(self, len, buffer)\\u001b[0m\\n\\u001b[1;32m    927\\u001b[0m         \\u001b[0;32mtry\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    928\\u001b[0m             \\u001b[0;32mif\\u001b[0m \\u001b[0mbuffer\\u001b[0m \\u001b[0;32mis\\u001b[0m \\u001b[0;32mnot\\u001b[0m \\u001b[0;32mNone\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 929\\u001b[0;31m                 \\u001b[0;32mreturn\\u001b[0m \\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0m_sslobj\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mread\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mlen\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mbuffer\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    930\\u001b[0m             \\u001b[0;32melse\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    931\\u001b[0m                 \\u001b[0;32mreturn\\u001b[0m \\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0m_sslobj\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mread\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mlen\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;31mKeyboardInterrupt\\u001b[0m: \"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"commits = create_commits_df('spark', 'apache', github_api)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"## store permanently into an SQL database\\n\",\n+    \"commits.to_sql(con=engine, name='commits',\\n\",\n+    \"                 if_exists='replace', dtype=objects_to_strings(commits))\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 27,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"commits.to_csv('data/commits.csv')\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## 3. Pull Requests\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 11,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def pulls_of_repo(repo, owner, api):\\n\",\n+    \"    pulls = []\\n\",\n+    \"    next = True\\n\",\n+    \"    i = 1\\n\",\n+    \"    while next == True:\\n\",\n+    \"        url = api + '/repos/{}/{}/pulls?page={}&per_page=100'.format(owner, repo, i)\\n\",\n+    \"        pull_pg = gh_session.get(url = url)\\n\",\n+    \"        pull_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in pull_pg.json()]    \\n\",\n+    \"        pull_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in pull_pg_list]\\n\",\n+    \"        pulls = pulls + pull_pg_list\\n\",\n+    \"        if 'Link' in pull_pg.headers:\\n\",\n+    \"            if 'rel=\\\"next\\\"' not in pull_pg.headers['Link']:\\n\",\n+    \"                next = False\\n\",\n+    \"        i = i + 1\\n\",\n+    \"    return pulls\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 12,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"pulls = json_normalize(pulls_of_repo('spark', 'apache', github_api))\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"## store permanently into an SQL database\\n\",\n+    \"pulls.to_sql(con=engine, name='pulls',\\n\",\n+    \"                 if_exists='replace', dtype=objects_to_strings(pulls))\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 33,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"pulls.to_csv('data/pulls.csv')\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## 4. Issues\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 13,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def issues_of_repo(repo, owner, api):\\n\",\n+    \"    issues = []\\n\",\n+    \"    next = True\\n\",\n+    \"    i = 1\\n\",\n+    \"    while next == True:\\n\",\n+    \"        url = api + '/repos/{}/{}/issues?page={}&per_page=100'.format(owner, repo, i)\\n\",\n+    \"        issue_pg = gh_session.get(url = url)\\n\",\n+    \"        issue_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in issue_pg.json()]    \\n\",\n+    \"        issue_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in issue_pg_list]\\n\",\n+    \"        issues = issues + issue_pg_list\\n\",\n+    \"        if 'Link' in issue_pg.headers:\\n\",\n+    \"            if 'rel=\\\"next\\\"' not in issue_pg.headers['Link']:\\n\",\n+    \"                next = False\\n\",\n+    \"        i = i + 1\\n\",\n+    \"    return issues\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 14,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"issues = json_normalize(issues_of_repo('spark', 'apache', github_api))\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"## store permanently into an SQL database\\n\",\n+    \"issues.to_sql(con=engine, name='issues',\\n\",\n+    \"                 if_exists='replace', dtype=objects_to_strings(issues))\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 98,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"time: 32 ms\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"issues.to_csv('data/issues.csv')\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def get_org_info(repo,owner,api):\\n\",\n+    \"    \"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Generating All Repo Data\\n\",\n+    \"The following function is used for generating all the previously disscussed data in a single operation.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def generate_repo_data(repo, owner, api):\\n\",\n+    \"    branches = json_normalize(branches_of_repo(repo, owner, api))\\n\",\n+    \"    commits = create_commits_df(repo, owner, api)\\n\",\n+    \"    pulls = json_normalize(pulls_of_repo(repo, owner, api))\\n\",\n+    \"    issues = json_normalize(issues_of_repo(repo, owner, api))\\n\",\n+    \"    branches.to_csv('data/branches.csv')\\n\",\n+    \"    commits.to_csv('data/commits.csv')\\n\",\n+    \"    pulls.to_csv('data/pulls.csv')\\n\",\n+    \"    issues.to_csv('data/issues.csv')\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"#generate_repo_data('spark', 'apache', github_api)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Contribution Statistics\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 19,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def statistics_of_repo(repo, owner, api):\\n\",\n+    \"    contributors = []\\n\",\n+    \"    next = True\\n\",\n+    \"    i = 1\\n\",\n+    \"    while next == True:\\n\",\n+    \"        print(i)\\n\",\n+    \"        url = api + '/repos/{}/{}/stats/contributors?page={}&per_page=100'.format(owner, repo, i)\\n\",\n+    \"        contrib_pg = gh_session.get(url = url)\\n\",\n+    \"        contrib_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in contrib_pg.json()]    \\n\",\n+    \"        contrib_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in contrib_pg_list]\\n\",\n+    \"        contributors = contributors + contrib_pg_list\\n\",\n+    \"        if 'Link' in contrib_pg.headers:\\n\",\n+    \"            if 'rel=\\\"next\\\"' not in contrib_pg.headers['Link']:\\n\",\n+    \"                next = False\\n\",\n+    \"        i = i + 1\\n\",\n+    \"        #break\\n\",\n+    \"        if i == 10:\\n\",\n+    \"            break\\n\",\n+    \"    return contributors\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 20,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"1\\n\",\n+      \"2\\n\",\n+      \"3\\n\",\n+      \"4\\n\",\n+      \"5\\n\",\n+      \"6\\n\",\n+      \"7\\n\",\n+      \"8\\n\",\n+      \"9\\n\"\n+     ]\n+    },\n+    {\n+     \"name\": \"stderr\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"IOPub data rate exceeded.\\n\",\n+      \"The notebook server will temporarily stop sending output\\n\",\n+      \"to the client in order to avoid crashing it.\\n\",\n+      \"To change this limit, set the config variable\\n\",\n+      \"`--NotebookApp.iopub_data_rate_limit`.\\n\",\n+      \"\\n\",\n+      \"Current values:\\n\",\n+      \"NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\\n\",\n+      \"NotebookApp.rate_limit_window=3.0 (secs)\\n\",\n+      \"\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"contribs = statistics_of_repo('spark', 'apache', github_api)\\n\",\n+    \"print(contribs)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 21,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"ename\": \"NameError\",\n+     \"evalue\": \"name 'contrib_list' is not defined\",\n+     \"output_type\": \"error\",\n+     \"traceback\": [\n+      \"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\n+      \"\\u001b[0;31mNameError\\u001b[0m                                 Traceback (most recent call last)\",\n+      \"\\u001b[0;32m<ipython-input-21-a5beaa20eeb1>\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m\\u001b[0m\\n\\u001b[1;32m      1\\u001b[0m \\u001b[0mweeks_list\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;34m[\\u001b[0m\\u001b[0;34m]\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 2\\u001b[0;31m \\u001b[0;32mfor\\u001b[0m \\u001b[0mi\\u001b[0m \\u001b[0;32min\\u001b[0m \\u001b[0;34m(\\u001b[0m\\u001b[0mcontrib_list\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      3\\u001b[0m     \\u001b[0;32mfor\\u001b[0m \\u001b[0mj\\u001b[0m \\u001b[0;32min\\u001b[0m \\u001b[0mi\\u001b[0m\\u001b[0;34m[\\u001b[0m\\u001b[0;34m'weeks'\\u001b[0m\\u001b[0;34m]\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      4\\u001b[0m         \\u001b[0mj\\u001b[0m\\u001b[0;34m[\\u001b[0m\\u001b[0;34m'author'\\u001b[0m\\u001b[0;34m]\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mi\\u001b[0m\\u001b[0;34m[\\u001b[0m\\u001b[0;34m'author'\\u001b[0m\\u001b[0;34m]\\u001b[0m\\u001b[0;34m[\\u001b[0m\\u001b[0;34m'login'\\u001b[0m\\u001b[0;34m]\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      5\\u001b[0m \\u001b[0mweeks_list\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mappend\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mj\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n+      \"\\u001b[0;31mNameError\\u001b[0m: name 'contrib_list' is not defined\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"weeks_list = []\\n\",\n+    \"for i in (contrib_list):\\n\",\n+    \"    for j in i['weeks']:\\n\",\n+    \"        j['author'] = i['author']['login']\\n\",\n+    \"weeks_list.append(j)\\n\",\n+    \"weeks_df = json_normalize(weeks_list)\\n\",\n+    \"weeks_df['date'] = pd.to_datetime(weeks_df['w'],unit='s')\\n\",\n+    \"weeks_df['week'] = weeks_df['date'].dt.week\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 89,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"time: 6 ms\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"weeks_df.to_sql(con=engine, name='contributions',\\n\",\n+    \"                 if_exists='replace', dtype=objects_to_strings(weeks_df))\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"colab\": {\n+   \"name\": \"pygit.ipynb\",\n+   \"provenance\": []\n+  },\n+  \"kernelspec\": {\n+   \"display_name\": \"Python 3\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\",\n+   \"version\": \"3.7.4\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 4\n+}"
            },
            {
                "sha": "21765c348f7627dc92cb821fe318cc2cd912e49d",
                "filename": "git-statistics/Git_Data_Extraction_and_Processing.py",
                "status": "added",
                "additions": 307,
                "deletions": 0,
                "changes": 307,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/Git_Data_Extraction_and_Processing.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/Git_Data_Extraction_and_Processing.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/git-statistics/Git_Data_Extraction_and_Processing.py?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "patch": "@@ -0,0 +1,307 @@\n+#!/usr/bin/env python\n+# coding: utf-8\n+\n+# In order to extract data from Github, we are going to leverage the Github REST API v3, that can be found in this link https://developer.github.com/v3/.\n+# In `config.py` file we need to define the following configuration variables, that are going to be accessed by the current notebook:\n+# - `GITHUB_USERNAME`\n+# - `GITHUB_TOKEN`\n+# - `SQL_ALCHEMY_STRING` (only if we want to save our Github results in a relational database)\n+\n+# In[ ]:\n+\n+\n+import json\n+import requests\n+from pandas.io.json import json_normalize\n+from sqlalchemy import create_engine, engine, text, types, MetaData, Table, String\n+import pandas as pd\n+pd.set_option('display.max_columns', 500)\n+pd.set_option('display.width', 1000)\n+import numpy as np\n+from datetime import datetime\n+\n+#get_ipython().run_line_magic('load_ext', 'autotime')\n+\n+\n+# In[2]:\n+\n+\n+import config\n+import os\n+\n+\n+# In[3]:\n+\n+\n+# function that converts all object columns to strings, in order to store them efficiently into the database\n+def objects_to_strings(table):\n+    measurer = np.vectorize(len)\n+    df_object = table.select_dtypes(include=[object])\n+    string_columns = dict(zip(df_object, measurer(\n+        df_object.values.astype(str)).max(axis=0)))\n+    string_columns = {key: String(length=value) if value > 0 else String(length=1)\n+                      for key, value in string_columns.items() }\n+    return string_columns\n+\n+\n+# In[4]:\n+\n+\n+#engine = create_engine(config.SQL_ALCHEMY_STRING)\n+\n+\n+# In[5]:\n+\n+github_api = \"https://api.github.com\"\n+gh_session = requests.Session()\n+gh_session.auth = (config.GITHUB_USERNAME, config.GITHUB_TOKEN)\n+\n+\n+# ## 1. Branches\n+\n+# In[14]:\n+\n+\n+def branches_of_repo(repo, owner, api):\n+    branches = []\n+    next = True\n+    i = 1\n+    while next == True:\n+        url = api + '/repos/{}/{}/branches?page={}&per_page=100'.format(owner, repo, i)\n+        branch_pg = gh_session.get(url = url)\n+        branch_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in branch_pg.json()]    \n+        branch_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in branch_pg_list]\n+        branches = branches + branch_pg_list\n+        if 'Link' in branch_pg.headers:\n+            if 'rel=\"next\"' not in branch_pg.headers['Link']:\n+                next = False\n+        i = i + 1\n+    return branches\n+\n+\n+# In[7]:\n+\n+\n+branches = json_normalize(branches_of_repo('spark', 'apache', github_api))\n+\n+\n+# In[ ]:\n+\n+\n+## store permanently into an SQL database\n+#branches.to_sql(con=engine, name='branches',\n+#                 if_exists='replace', dtype=objects_to_strings(branches))\n+\n+\n+# In[9]:\n+\n+\n+branches.to_csv('data/branches.csv')\n+\n+\n+# ## 2. Commits\n+\n+# In[15]:\n+\n+\n+def commits_of_repo_github(repo, owner, api):\n+    commits = []\n+    next = True\n+    i = 1\n+    while next == True:\n+        url = api + '/repos/{}/{}/commits?page={}&per_page=100'.format(owner, repo, i)\n+        commit_pg = gh_session.get(url = url)\n+        commit_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in commit_pg.json()]    \n+        commit_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in commit_pg_list]\n+        commits = commits + commit_pg_list\n+        if 'Link' in commit_pg.headers:\n+            if 'rel=\"next\"' not in commit_pg.headers['Link']:\n+                next = False\n+        i = i + 1\n+    return commits\n+\n+\n+# In[16]:\n+\n+\n+def create_commits_df(repo, owner, api):\n+    commits_list = commits_of_repo_github(repo, owner, api)\n+    return json_normalize(commits_list)\n+\n+\n+# In[8]:\n+\n+\n+#commits = create_commits_df('spark', 'apache', github_api)\n+\n+\n+# In[ ]:\n+\n+\n+## store permanently into an SQL database\n+#commits.to_sql(con=engine, name='commits',\n+#                 if_exists='replace', dtype=objects_to_strings(commits))\n+\n+\n+# In[9]:\n+\n+\n+#ommits.to_csv('data/commits.csv')\n+\n+\n+# ## 3. Pull Requests\n+\n+# In[17]:\n+\n+\n+def pulls_of_repo(repo, owner, api):\n+    pulls = []\n+    next = True\n+    i = 1\n+    while next == True:\n+        url = api + '/repos/{}/{}/pulls?page={}&per_page=100'.format(owner, repo, i)\n+        pull_pg = gh_session.get(url = url)\n+        pull_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in pull_pg.json()]    \n+        pull_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in pull_pg_list]\n+        pulls = pulls + pull_pg_list\n+        if 'Link' in pull_pg.headers:\n+            if 'rel=\"next\"' not in pull_pg.headers['Link']:\n+                next = False\n+        i = i + 1\n+    return pulls\n+\n+\n+# In[91]:\n+\n+\n+#pulls = json_normalize(pulls_of_repo('spark', 'apache', github_api))\n+\n+\n+# In[ ]:\n+\n+\n+## store permanently into an SQL database\n+#pulls.to_sql(con=engine, name='pulls',\n+#                 if_exists='replace', dtype=objects_to_strings(pulls))\n+\n+\n+# In[93]:\n+\n+\n+#pulls.to_csv('data/pulls.csv')\n+\n+\n+# ## 4. Issues\n+\n+# In[18]:\n+\n+\n+def issues_of_repo(repo, owner, api):\n+    issues = []\n+    next = True\n+    i = 1\n+    while next == True:\n+        url = api + '/repos/{}/{}/issues?page={}&per_page=100'.format(owner, repo, i)\n+        issue_pg = gh_session.get(url = url)\n+        issue_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in issue_pg.json()]    \n+        issue_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in issue_pg_list]\n+        issues = issues + issue_pg_list\n+        if 'Link' in issue_pg.headers:\n+            if 'rel=\"next\"' not in issue_pg.headers['Link']:\n+                next = False\n+        i = i + 1\n+    return issues\n+\n+\n+# In[95]:\n+\n+\n+#issues = json_normalize(issues_of_repo('spark', 'apache', github_api))\n+\n+\n+# In[ ]:\n+\n+\n+## store permanently into an SQL database\n+#issues.to_sql(con=engine, name='issues',\n+#                 if_exists='replace', dtype=objects_to_strings(issues))\n+\n+\n+# In[98]:\n+\n+\n+#issues.to_csv('data/issues.csv')\n+\n+\n+# ## Generating All Repo Data\n+# The following function is used for generating all the previously disscussed data in a single operation.\n+\n+# In[19]:\n+\n+\n+def generate_repo_data(repo, owner, api):\n+    branches = json_normalize(branches_of_repo(repo, owner, api))\n+    commits = create_commits_df(repo, owner, api)\n+    pulls = json_normalize(pulls_of_repo(repo, owner, api))\n+    issues = json_normalize(issues_of_repo(repo, owner, api))\n+    branches.to_csv('data/branches.csv')\n+    commits.to_csv('data/commits.csv')\n+    pulls.to_csv('data/pulls.csv')\n+    issues.to_csv('data/issues.csv')\n+\n+\n+# In[20]:\n+\n+\n+#generate_repo_data('spark', 'apache', github_api)\n+\n+\n+# ## Contribution Statistics\n+\n+# In[22]:\n+\n+\n+def statistics_of_repo(repo, owner, api):\n+    contributors = []\n+    next = True\n+    i = 1\n+    while next == True:\n+        url = api + '/repos/{}/{}/stats/contributors?page={}&per_page=100'.format(owner, repo, i)\n+        contrib_pg = gh_session.get(url = url)\n+        contrib_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in contrib_pg.json()]    \n+        contrib_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in contrib_pg_list]\n+        contributors = contributors + contrib_pg_list\n+        if 'Link' in contrib_pg.headers:\n+            if 'rel=\"next\"' not in contrib_pg.headers['Link']:\n+                next = False\n+        i = i + 1\n+    return contributors\n+\n+\n+# In[ ]:\n+\n+\n+#contribs = statistics_of_repo('spark', 'apache', github_api)\n+\n+\n+# In[ ]:\n+\n+'''\n+\n+weeks_list = []\n+for i in (contrib_list):\n+    for j in i['weeks']:\n+        j['author'] = i['author']['login']\n+weeks_list.append(j)\n+weeks_df = json_normalize(weeks_list)\n+weeks_df['date'] = pd.to_datetime(weeks_df['w'],unit='s')\n+weeks_df['week'] = weeks_df['date'].dt.week\n+'''\n+\n+\n+# In[89]:\n+\n+\n+#weeks_df.to_sql(con=engine, name='contributions',\n+#                 if_exists='replace', dtype=objects_to_strings(weeks_df))\n+"
            },
            {
                "sha": "a58021b193e55f622fe5ddaf667d5e8b3a51d07a",
                "filename": "git-statistics/config_example.py",
                "status": "added",
                "additions": 5,
                "deletions": 0,
                "changes": 5,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/config_example.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/config_example.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/git-statistics/config_example.py?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "patch": "@@ -0,0 +1,5 @@\n+GITEA_APP_URL = 'YOUR_GITEA_API'\n+GITEA_TOKEN = 'YOUR_GITEA_TOKEN'\n+GITHUB_USERNAME = 'YOUR_GITHUB_USERNAME'\n+GITHUB_TOKEN = 'YOUR_GITHUB_TOKEN'\n+SQL_ALCHEMY_STRING = 'YOUR_SQL_ALCHEMY_CONNECTIN_STRING'"
            },
            {
                "sha": "04c1baab79ed263935291007df318904822ceaee",
                "filename": "git-statistics/data/branches.csv",
                "status": "added",
                "additions": 21,
                "deletions": 0,
                "changes": 21,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/data/branches.csv",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/data/branches.csv",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/git-statistics/data/branches.csv?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "patch": "@@ -0,0 +1,21 @@\n+,name,protected,repo_name,owner,commit.sha,commit.url\n+0,branch-0.5,False,spark,apache,5b021ce0990ec675afc6939cc2c06f041c973d17,https://api.github.com/repos/apache/spark/commits/5b021ce0990ec675afc6939cc2c06f041c973d17\n+1,branch-0.6,False,spark,apache,d46c54c5be5dc62237a0cdf584787d7fe16eab31,https://api.github.com/repos/apache/spark/commits/d46c54c5be5dc62237a0cdf584787d7fe16eab31\n+2,branch-0.7,False,spark,apache,379567fcdb0d85ca0c5598db70c5a8b9efca552a,https://api.github.com/repos/apache/spark/commits/379567fcdb0d85ca0c5598db70c5a8b9efca552a\n+3,branch-0.8,False,spark,apache,62b3158b80c14668b16476818d72f1296b06da66,https://api.github.com/repos/apache/spark/commits/62b3158b80c14668b16476818d72f1296b06da66\n+4,branch-0.9,False,spark,apache,e63783a23f49fafc5d9f464ecfd107d19bd87787,https://api.github.com/repos/apache/spark/commits/e63783a23f49fafc5d9f464ecfd107d19bd87787\n+5,branch-1.0-jdbc,False,spark,apache,9caf3a9659821a3b4fd4394c9f4134adff9caf88,https://api.github.com/repos/apache/spark/commits/9caf3a9659821a3b4fd4394c9f4134adff9caf88\n+6,branch-1.0,False,spark,apache,117843f85e8e69a43ffa531716a37c8c05dbabb0,https://api.github.com/repos/apache/spark/commits/117843f85e8e69a43ffa531716a37c8c05dbabb0\n+7,branch-1.1,False,spark,apache,11ee9d191e26a41a44ff0ca8730a129934942ee7,https://api.github.com/repos/apache/spark/commits/11ee9d191e26a41a44ff0ca8730a129934942ee7\n+8,branch-1.2,False,spark,apache,307f27e24e17afd92030194a3e6fec312fc19f4f,https://api.github.com/repos/apache/spark/commits/307f27e24e17afd92030194a3e6fec312fc19f4f\n+9,branch-1.3,False,spark,apache,65cc451c89fd03daed8c315a91d93067ccdc3a5c,https://api.github.com/repos/apache/spark/commits/65cc451c89fd03daed8c315a91d93067ccdc3a5c\n+10,branch-1.4,False,spark,apache,b2680ae70f18a39b549ece2e6ca9fc8331148c2f,https://api.github.com/repos/apache/spark/commits/b2680ae70f18a39b549ece2e6ca9fc8331148c2f\n+11,branch-1.5,False,spark,apache,0a04721973c34a3324c41ac68b4f9c203ecedf40,https://api.github.com/repos/apache/spark/commits/0a04721973c34a3324c41ac68b4f9c203ecedf40\n+12,branch-1.6,False,spark,apache,a233fac0b8bf8229d938a24f2ede2d9d8861c284,https://api.github.com/repos/apache/spark/commits/a233fac0b8bf8229d938a24f2ede2d9d8861c284\n+13,branch-2.0,False,spark,apache,5ed89ceaf367590f79401abbf9ff7fc66507fe4e,https://api.github.com/repos/apache/spark/commits/5ed89ceaf367590f79401abbf9ff7fc66507fe4e\n+14,branch-2.1,False,spark,apache,4d2d3d47e00e78893b1ecd5a9a9070adc5243ac9,https://api.github.com/repos/apache/spark/commits/4d2d3d47e00e78893b1ecd5a9a9070adc5243ac9\n+15,branch-2.2,False,spark,apache,7c7d7f6a878b02ece881266ee538f3e1443aa8c1,https://api.github.com/repos/apache/spark/commits/7c7d7f6a878b02ece881266ee538f3e1443aa8c1\n+16,branch-2.3,False,spark,apache,75cc3b2da9ee0b51ecf0f13169f2b634e36a60c4,https://api.github.com/repos/apache/spark/commits/75cc3b2da9ee0b51ecf0f13169f2b634e36a60c4\n+17,branch-2.4,False,spark,apache,965757572bd4bfd04d6f547a6094d9b2891b34d6,https://api.github.com/repos/apache/spark/commits/965757572bd4bfd04d6f547a6094d9b2891b34d6\n+18,branch-3.0,False,spark,apache,15d45c0dfe60a4353791fbfaca953bad801f267c,https://api.github.com/repos/apache/spark/commits/15d45c0dfe60a4353791fbfaca953bad801f267c\n+19,master,True,spark,apache,f0e2fc37d1dc2a85fd08c87add5106bb51305182,https://api.github.com/repos/apache/spark/commits/f0e2fc37d1dc2a85fd08c87add5106bb51305182"
            },
            {
                "sha": "5d15cd94aa901623507effc8136ea01e94196ca9",
                "filename": "git-statistics/data/commits.csv",
                "status": "added",
                "additions": 152127,
                "deletions": 0,
                "changes": 152127,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/data/commits.csv",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/data/commits.csv",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/git-statistics/data/commits.csv?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694"
            },
            {
                "sha": "4ae3c8d73d1231b7a13d2866663599b2f93a3d4d",
                "filename": "git-statistics/data/issues.csv",
                "status": "added",
                "additions": 15353,
                "deletions": 0,
                "changes": 15353,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/data/issues.csv",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/data/issues.csv",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/git-statistics/data/issues.csv?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694"
            },
            {
                "sha": "9bcd222d70febb1c132f10656021c216d9cb8385",
                "filename": "git-statistics/data/pulls.csv",
                "status": "added",
                "additions": 10073,
                "deletions": 0,
                "changes": 10073,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/data/pulls.csv",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/data/pulls.csv",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/git-statistics/data/pulls.csv?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694"
            },
            {
                "sha": "21765c348f7627dc92cb821fe318cc2cd912e49d",
                "filename": "git-statistics/test.py",
                "status": "added",
                "additions": 307,
                "deletions": 0,
                "changes": 307,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/test.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/git-statistics/test.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/git-statistics/test.py?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "patch": "@@ -0,0 +1,307 @@\n+#!/usr/bin/env python\n+# coding: utf-8\n+\n+# In order to extract data from Github, we are going to leverage the Github REST API v3, that can be found in this link https://developer.github.com/v3/.\n+# In `config.py` file we need to define the following configuration variables, that are going to be accessed by the current notebook:\n+# - `GITHUB_USERNAME`\n+# - `GITHUB_TOKEN`\n+# - `SQL_ALCHEMY_STRING` (only if we want to save our Github results in a relational database)\n+\n+# In[ ]:\n+\n+\n+import json\n+import requests\n+from pandas.io.json import json_normalize\n+from sqlalchemy import create_engine, engine, text, types, MetaData, Table, String\n+import pandas as pd\n+pd.set_option('display.max_columns', 500)\n+pd.set_option('display.width', 1000)\n+import numpy as np\n+from datetime import datetime\n+\n+#get_ipython().run_line_magic('load_ext', 'autotime')\n+\n+\n+# In[2]:\n+\n+\n+import config\n+import os\n+\n+\n+# In[3]:\n+\n+\n+# function that converts all object columns to strings, in order to store them efficiently into the database\n+def objects_to_strings(table):\n+    measurer = np.vectorize(len)\n+    df_object = table.select_dtypes(include=[object])\n+    string_columns = dict(zip(df_object, measurer(\n+        df_object.values.astype(str)).max(axis=0)))\n+    string_columns = {key: String(length=value) if value > 0 else String(length=1)\n+                      for key, value in string_columns.items() }\n+    return string_columns\n+\n+\n+# In[4]:\n+\n+\n+#engine = create_engine(config.SQL_ALCHEMY_STRING)\n+\n+\n+# In[5]:\n+\n+github_api = \"https://api.github.com\"\n+gh_session = requests.Session()\n+gh_session.auth = (config.GITHUB_USERNAME, config.GITHUB_TOKEN)\n+\n+\n+# ## 1. Branches\n+\n+# In[14]:\n+\n+\n+def branches_of_repo(repo, owner, api):\n+    branches = []\n+    next = True\n+    i = 1\n+    while next == True:\n+        url = api + '/repos/{}/{}/branches?page={}&per_page=100'.format(owner, repo, i)\n+        branch_pg = gh_session.get(url = url)\n+        branch_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in branch_pg.json()]    \n+        branch_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in branch_pg_list]\n+        branches = branches + branch_pg_list\n+        if 'Link' in branch_pg.headers:\n+            if 'rel=\"next\"' not in branch_pg.headers['Link']:\n+                next = False\n+        i = i + 1\n+    return branches\n+\n+\n+# In[7]:\n+\n+\n+branches = json_normalize(branches_of_repo('spark', 'apache', github_api))\n+\n+\n+# In[ ]:\n+\n+\n+## store permanently into an SQL database\n+#branches.to_sql(con=engine, name='branches',\n+#                 if_exists='replace', dtype=objects_to_strings(branches))\n+\n+\n+# In[9]:\n+\n+\n+branches.to_csv('data/branches.csv')\n+\n+\n+# ## 2. Commits\n+\n+# In[15]:\n+\n+\n+def commits_of_repo_github(repo, owner, api):\n+    commits = []\n+    next = True\n+    i = 1\n+    while next == True:\n+        url = api + '/repos/{}/{}/commits?page={}&per_page=100'.format(owner, repo, i)\n+        commit_pg = gh_session.get(url = url)\n+        commit_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in commit_pg.json()]    \n+        commit_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in commit_pg_list]\n+        commits = commits + commit_pg_list\n+        if 'Link' in commit_pg.headers:\n+            if 'rel=\"next\"' not in commit_pg.headers['Link']:\n+                next = False\n+        i = i + 1\n+    return commits\n+\n+\n+# In[16]:\n+\n+\n+def create_commits_df(repo, owner, api):\n+    commits_list = commits_of_repo_github(repo, owner, api)\n+    return json_normalize(commits_list)\n+\n+\n+# In[8]:\n+\n+\n+#commits = create_commits_df('spark', 'apache', github_api)\n+\n+\n+# In[ ]:\n+\n+\n+## store permanently into an SQL database\n+#commits.to_sql(con=engine, name='commits',\n+#                 if_exists='replace', dtype=objects_to_strings(commits))\n+\n+\n+# In[9]:\n+\n+\n+#ommits.to_csv('data/commits.csv')\n+\n+\n+# ## 3. Pull Requests\n+\n+# In[17]:\n+\n+\n+def pulls_of_repo(repo, owner, api):\n+    pulls = []\n+    next = True\n+    i = 1\n+    while next == True:\n+        url = api + '/repos/{}/{}/pulls?page={}&per_page=100'.format(owner, repo, i)\n+        pull_pg = gh_session.get(url = url)\n+        pull_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in pull_pg.json()]    \n+        pull_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in pull_pg_list]\n+        pulls = pulls + pull_pg_list\n+        if 'Link' in pull_pg.headers:\n+            if 'rel=\"next\"' not in pull_pg.headers['Link']:\n+                next = False\n+        i = i + 1\n+    return pulls\n+\n+\n+# In[91]:\n+\n+\n+#pulls = json_normalize(pulls_of_repo('spark', 'apache', github_api))\n+\n+\n+# In[ ]:\n+\n+\n+## store permanently into an SQL database\n+#pulls.to_sql(con=engine, name='pulls',\n+#                 if_exists='replace', dtype=objects_to_strings(pulls))\n+\n+\n+# In[93]:\n+\n+\n+#pulls.to_csv('data/pulls.csv')\n+\n+\n+# ## 4. Issues\n+\n+# In[18]:\n+\n+\n+def issues_of_repo(repo, owner, api):\n+    issues = []\n+    next = True\n+    i = 1\n+    while next == True:\n+        url = api + '/repos/{}/{}/issues?page={}&per_page=100'.format(owner, repo, i)\n+        issue_pg = gh_session.get(url = url)\n+        issue_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in issue_pg.json()]    \n+        issue_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in issue_pg_list]\n+        issues = issues + issue_pg_list\n+        if 'Link' in issue_pg.headers:\n+            if 'rel=\"next\"' not in issue_pg.headers['Link']:\n+                next = False\n+        i = i + 1\n+    return issues\n+\n+\n+# In[95]:\n+\n+\n+#issues = json_normalize(issues_of_repo('spark', 'apache', github_api))\n+\n+\n+# In[ ]:\n+\n+\n+## store permanently into an SQL database\n+#issues.to_sql(con=engine, name='issues',\n+#                 if_exists='replace', dtype=objects_to_strings(issues))\n+\n+\n+# In[98]:\n+\n+\n+#issues.to_csv('data/issues.csv')\n+\n+\n+# ## Generating All Repo Data\n+# The following function is used for generating all the previously disscussed data in a single operation.\n+\n+# In[19]:\n+\n+\n+def generate_repo_data(repo, owner, api):\n+    branches = json_normalize(branches_of_repo(repo, owner, api))\n+    commits = create_commits_df(repo, owner, api)\n+    pulls = json_normalize(pulls_of_repo(repo, owner, api))\n+    issues = json_normalize(issues_of_repo(repo, owner, api))\n+    branches.to_csv('data/branches.csv')\n+    commits.to_csv('data/commits.csv')\n+    pulls.to_csv('data/pulls.csv')\n+    issues.to_csv('data/issues.csv')\n+\n+\n+# In[20]:\n+\n+\n+#generate_repo_data('spark', 'apache', github_api)\n+\n+\n+# ## Contribution Statistics\n+\n+# In[22]:\n+\n+\n+def statistics_of_repo(repo, owner, api):\n+    contributors = []\n+    next = True\n+    i = 1\n+    while next == True:\n+        url = api + '/repos/{}/{}/stats/contributors?page={}&per_page=100'.format(owner, repo, i)\n+        contrib_pg = gh_session.get(url = url)\n+        contrib_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in contrib_pg.json()]    \n+        contrib_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in contrib_pg_list]\n+        contributors = contributors + contrib_pg_list\n+        if 'Link' in contrib_pg.headers:\n+            if 'rel=\"next\"' not in contrib_pg.headers['Link']:\n+                next = False\n+        i = i + 1\n+    return contributors\n+\n+\n+# In[ ]:\n+\n+\n+#contribs = statistics_of_repo('spark', 'apache', github_api)\n+\n+\n+# In[ ]:\n+\n+'''\n+\n+weeks_list = []\n+for i in (contrib_list):\n+    for j in i['weeks']:\n+        j['author'] = i['author']['login']\n+weeks_list.append(j)\n+weeks_df = json_normalize(weeks_list)\n+weeks_df['date'] = pd.to_datetime(weeks_df['w'],unit='s')\n+weeks_df['week'] = weeks_df['date'].dt.week\n+'''\n+\n+\n+# In[89]:\n+\n+\n+#weeks_df.to_sql(con=engine, name='contributions',\n+#                 if_exists='replace', dtype=objects_to_strings(weeks_df))\n+"
            },
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "github-crawler/app.py",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/app.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/app.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/app.py?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694"
            },
            {
                "sha": "8f9e8868153b0ee3486489a328bea5776c2a9733",
                "filename": "github-crawler/config/config.py",
                "status": "added",
                "additions": 5,
                "deletions": 0,
                "changes": 5,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/config/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/config/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/config/config.py?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "patch": "@@ -0,0 +1,5 @@\n+GITEA_APP_URL = 'YOUR_GITEA_API'\n+GITEA_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+GITHUB_USERNAME = 'vishwakulkarni'\n+GITHUB_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+SQL_ALCHEMY_STRING = ''"
            },
            {
                "sha": "ee67fc89efff7799ec2d99e5920520f76da14343",
                "filename": "github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/__pycache__/config.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/config.cpython-37.pyc?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694"
            },
            {
                "sha": "51458b63002ac5ae67acb9c53e0cb49ad2fef6a5",
                "filename": "github-crawler/lib/__pycache__/org.cpython-37.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/__pycache__/org.cpython-37.pyc",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/__pycache__/org.cpython-37.pyc",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/__pycache__/org.cpython-37.pyc?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694"
            },
            {
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filename": "github-crawler/lib/commit.py",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/commit.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/commit.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/commit.py?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694"
            },
            {
                "sha": "8f9e8868153b0ee3486489a328bea5776c2a9733",
                "filename": "github-crawler/lib/config.py",
                "status": "added",
                "additions": 5,
                "deletions": 0,
                "changes": 5,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/config.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/config.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/config.py?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "patch": "@@ -0,0 +1,5 @@\n+GITEA_APP_URL = 'YOUR_GITEA_API'\n+GITEA_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+GITHUB_USERNAME = 'vishwakulkarni'\n+GITHUB_TOKEN = 'f75b46df7511241ab8481caf80994d4aab7afb68'\n+SQL_ALCHEMY_STRING = ''"
            },
            {
                "sha": "013196378d0255ff9f8794672ae44c2793f2fa67",
                "filename": "github-crawler/lib/data/branches.csv",
                "status": "added",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/data/branches.csv",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/data/branches.csv",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/data/branches.csv?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "patch": "@@ -0,0 +1,3 @@\n+,name,protected,protection_url,repo_name,owner,commit.sha,commit.url,protection.enabled,protection.required_status_checks.enforcement_level,protection.required_status_checks.contexts\n+0,akilesh223-patch-1,False,https://api.github.com/repos/vishwakulkarni/Social-media-impact-cryptocurrency/branches/akilesh223-patch-1/protection,Social-media-impact-cryptocurrency,vishwakulkarni,5c84a33ba894708e1835de5b9985d710bfa6fd8e,https://api.github.com/repos/vishwakulkarni/Social-media-impact-cryptocurrency/commits/5c84a33ba894708e1835de5b9985d710bfa6fd8e,False,off,[]\n+1,master,False,https://api.github.com/repos/vishwakulkarni/Social-media-impact-cryptocurrency/branches/master/protection,Social-media-impact-cryptocurrency,vishwakulkarni,d4f0f911521df6f96f7f48026c56493a4e0379d7,https://api.github.com/repos/vishwakulkarni/Social-media-impact-cryptocurrency/commits/d4f0f911521df6f96f7f48026c56493a4e0379d7,False,off,[]"
            },
            {
                "sha": "d11245b5cfbf3a1cdaac34eda19bd860c211673f",
                "filename": "github-crawler/lib/data/org.csv",
                "status": "added",
                "additions": 95,
                "deletions": 0,
                "changes": 95,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/data/org.csv",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/data/org.csv",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/data/org.csv?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694"
            },
            {
                "sha": "013196378d0255ff9f8794672ae44c2793f2fa67",
                "filename": "github-crawler/lib/data/orgs.csv",
                "status": "added",
                "additions": 3,
                "deletions": 0,
                "changes": 3,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/data/orgs.csv",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/data/orgs.csv",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/data/orgs.csv?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "patch": "@@ -0,0 +1,3 @@\n+,name,protected,protection_url,repo_name,owner,commit.sha,commit.url,protection.enabled,protection.required_status_checks.enforcement_level,protection.required_status_checks.contexts\n+0,akilesh223-patch-1,False,https://api.github.com/repos/vishwakulkarni/Social-media-impact-cryptocurrency/branches/akilesh223-patch-1/protection,Social-media-impact-cryptocurrency,vishwakulkarni,5c84a33ba894708e1835de5b9985d710bfa6fd8e,https://api.github.com/repos/vishwakulkarni/Social-media-impact-cryptocurrency/commits/5c84a33ba894708e1835de5b9985d710bfa6fd8e,False,off,[]\n+1,master,False,https://api.github.com/repos/vishwakulkarni/Social-media-impact-cryptocurrency/branches/master/protection,Social-media-impact-cryptocurrency,vishwakulkarni,d4f0f911521df6f96f7f48026c56493a4e0379d7,https://api.github.com/repos/vishwakulkarni/Social-media-impact-cryptocurrency/commits/d4f0f911521df6f96f7f48026c56493a4e0379d7,False,off,[]"
            },
            {
                "sha": "b213bfe69160c075cb9f8e3f1fc475beb6cd6978",
                "filename": "github-crawler/lib/pygit-helper.py",
                "status": "added",
                "additions": 20,
                "deletions": 0,
                "changes": 20,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/pygit-helper.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/pygit-helper.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/pygit-helper.py?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "patch": "@@ -0,0 +1,20 @@\n+from github import Github\n+\n+# using username and password\n+#g = Github(\"user\", \"password\")\n+\n+# or using an access token\n+g = Github(\"f75b46df7511241ab8481caf80994d4aab7afb68\")\n+\n+org = g.get_organization(\"CUBigDataClass\")\n+print(org.login)\n+names = org.get_members()\n+repos = org.get_repos()\n+#\n+#for name in names:\n+#    print(name)\n+\n+i=0\n+for repo in repos:\n+    i=i+1\n+    print(i,repo)\n\\ No newline at end of file"
            },
            {
                "sha": "1aa41ad1ed6130c8b1faa84995895ff0020ccb16",
                "filename": "github-crawler/lib/test.py",
                "status": "added",
                "additions": 87,
                "deletions": 0,
                "changes": 87,
                "blob_url": "https://github.com/CUBigDataClass/Kode-Kallas/blob/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/test.py",
                "raw_url": "https://github.com/CUBigDataClass/Kode-Kallas/raw/c9003cb82dffc7ca8353e416bbb2c0d8c7614694/github-crawler/lib/test.py",
                "contents_url": "https://api.github.com/repos/CUBigDataClass/Kode-Kallas/contents/github-crawler/lib/test.py?ref=c9003cb82dffc7ca8353e416bbb2c0d8c7614694",
                "patch": "@@ -0,0 +1,87 @@\n+#!/usr/bin/env python\n+# coding: utf-8\n+\n+# In order to extract data from Github, we are going to leverage the Github REST API v3, that can be found in this link https://developer.github.com/v3/.\n+# In `config.py` file we need to define the following configuration variables, that are going to be accessed by the current notebook:\n+# - `GITHUB_USERNAME`\n+# - `GITHUB_TOKEN`\n+# - `SQL_ALCHEMY_STRING` (only if we want to save our Github results in a relational database)\n+\n+import json\n+import requests\n+from pandas.io.json import json_normalize\n+from sqlalchemy import create_engine, engine, text, types, MetaData, Table, String\n+import pandas as pd\n+pd.set_option('display.max_columns', 500)\n+pd.set_option('display.width', 1000)\n+import numpy as np\n+from datetime import datetime\n+\n+\n+import config\n+import os\n+\n+\n+\n+# function that converts all object columns to strings, in order to store them efficiently into the database\n+def objects_to_strings(table):\n+    measurer = np.vectorize(len)\n+    df_object = table.select_dtypes(include=[object])\n+    string_columns = dict(zip(df_object, measurer(\n+        df_object.values.astype(str)).max(axis=0)))\n+    string_columns = {key: String(length=value) if value > 0 else String(length=1)\n+                      for key, value in string_columns.items() }\n+    return string_columns\n+\n+\n+github_api = \"https://api.github.com\"\n+gh_session = requests.Session()\n+gh_session.auth = (config.GITHUB_USERNAME, config.GITHUB_TOKEN)\n+\n+\n+def branches_of_repo(repo, owner, api):\n+    branches = []\n+    next = True\n+    i = 1\n+    while next == True:\n+        url = api + '/repos/{}/{}/branches?page={}&per_page=100'.format(owner, repo, i)\n+        branch_pg = gh_session.get(url = url)\n+        branch_pg_list = [dict(item, **{'repo_name':'{}'.format(repo)}) for item in branch_pg.json()]    \n+        branch_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in branch_pg_list]\n+        branches = branches + branch_pg_list\n+        if 'Link' in branch_pg.headers:\n+            if 'rel=\"next\"' not in branch_pg.headers['Link']:\n+                next = False\n+        i = i + 1\n+    return branches\n+\n+\n+#branches = json_normalize(branches_of_repo('Social-media-impact-cryptocurrency', 'vishwakulkarni', github_api))\n+\n+\n+#branches.to_csv('data/branches.csv')\n+\n+\n+def get_repositories(org_name,owner,api):\n+    repos = []\n+    next = True\n+    i = 1\n+    while next == True:\n+        url = api + '/orgs/{}/repos??page={}&per_page=100'.format(org_name,i)\n+        repos_pg = gh_session.get(url = url)\n+        #print(repos_pg.json())\n+        repos_pg_list = [dict(item, **{'repo_name':'{}'.format(org_name)}) for item in repos_pg.json()]    \n+        repos_pg_list = [dict(item, **{'owner':'{}'.format(owner)}) for item in repos_pg_list]\n+        print(repos_pg_list)\n+        repos = repos + repos_pg_list\n+        if 'Link' in repos_pg.headers:\n+            if 'rel=\"next\"' not in repos_pg.headers['Link']:\n+                next = False\n+        i = i + 1\n+        break\n+    return repos\n+\n+orgs = json_normalize(get_repositories('CUBigDataClass', 'vishwakulkarni', github_api))\n+\n+\n+orgs.to_csv('data/org.csv')\n\\ No newline at end of file"
            }
        ]
    }
]